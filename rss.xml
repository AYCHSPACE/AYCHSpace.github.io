<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Problems with ImageNet and its Solutions</title>
<description><![CDATA[

<p>The <a href="http://www.image-net.org/">ImageNet</a> challenges play an important role in the development of computer vision. The great <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">success</a> of neural nets on ImageNet has contributed to general fervor around artificial intelligence. While the applied breakthroughs are real, issues with ImageNet and modern networks indicate gaps between current practice and intelligent perception.</p>
<p>For this investigation I used a <a href="/20170430-sampling_imagenet/">subset</a> of ImageNet consisting of 5 images from each of 200 classes and a <a href="https://arxiv.org/abs/1512.03385">ResNet</a>50 network pre-trained on the 1,000-class ImageNet recognition task, as included with <a href="https://keras.io/">Keras</a>. The overlap is 150 classes, and on those the network has 92.3% top-5 accuracy, while it can't ever be correct for the other 50 subset classes. I also averaged the last layer's activations for each set of five examples to make class centroids, inducing a nearest-neighbor model.</p>
<p><img alt="not a croquet ball" src="img/n03134739_1193_croquet_ball.jpg"></p>
<p>The least important problem with ImageNet is that sometimes the ground truth labels are bad. My favorite example is an image labeled "<a href="https://en.wikipedia.org/wiki/Croquet">croquet</a> ball," which the centroid model more correctly labels "tennis ball." Perhaps the original labeler saw what looks like a <a href="https://en.wikipedia.org/wiki/Polo">Polo</a> <a href="https://en.wikipedia.org/wiki/Ralph_Lauren_Corporation">brand</a> wristband and thought it was close enough to call croquet.</p>
<p>Neural nets are pretty robust to dirty training labels, so this is not the end of the world for model performance, but the occasional obviously incorrect label does highlight that "ground truth" is not absolute.</p>
<p>Apart from correctness, using a single label for an object is also a choice of representation that may not be optimal. ImageNet is labeled with <a href="https://wordnet.princeton.edu/">WordNet</a>, which is hierarchical. It might be beneficial for a system to know that croquet balls and tennis balls are both balls, for example, but this kind of information is not usually used.</p>
<p><img alt="occluded unicycle(s)?" src="img/n04509417_4503_unicycle.jpg"></p>
<p>Another interesting example is an image which the centroid model labels "person" but which is supposed to be "unicycle." The unicycles aren't clearly visible in the image, so the only way to get "unicycle" is by context.</p>
<p><img alt="fish, cat, goblet" src="img/n01443537_4691_goldfish.jpg"></p>
<p>Images frequently include multiple distinct entities, as in an image that is labeled "goldfish" but which also includes a cat in a glass bowl. The ResNet model's best guess is "goblet," which is not entirely baseless. Should "cat" be incorrect?</p>
<p>The problem of multiple subjects can be addressed by labeling bounding boxes within images, or even labeling every pixel. Recent ImageNet challenges seem to be focusing more on this kind of object detection. But even when there's clearly one main object in view, there are problems not well addressed by ImageNet models.</p>
<p><img alt="jellyfish... statue? light?" src="img/n01910747_13396_jellyfish.jpg"></p>
<p>A garden statue of a jellyfish is labeled "jellyfish" by ImageNet. The centroid model guesses it's "mushroom." This is likely due at least in part to context clues. It's also indicative of the ResNet model's broader failure to have a flexible conception of jellyfish.</p>
<p><img alt="man, not dumbbell" src="img/n00007846_98724_person.jpg"></p>
<p>Overreliance on context is a known problem with image classifiers. For example, Google has <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">shown</a> that some of its high-performing recognizers didn't distinguish dumbbells from the arms that held them. I saw something like this when the ResNet's top three guesses for a picture of a man were "dumbbell," "water bottle," and "hammer."</p>
<p>Focusing on the parts of images that contain only a specific object can treat symptoms like the dumbbell/arm problem, but they don't address the deeper issue that current neural nets don't really have a mechanism for understanding objects in visual scenes abstractly. Caption-generators have similar failure modes. I only know of <a href="https://arxiv.org/abs/1609.05518">one explicit approach</a> to the problem, and it's not particularly elegant.</p>
<p><a href="https://arxiv.org/abs/1604.00289"><img alt="bad captions" src="img/bad_captions.png"></a></p>
<p>Neural net research seems to mostly hope that systems will learn an appropriate internal representation for a given task. For image classification, the representation is effective, but it seems different from human perception. If we knew better how thoughts are represented in the brain, we could emulate those representations directly. For the moment, we can only evaluate artificial representations indirectly.</p>
<p>I started this investigation hoping to find visual analogies with neural net activations, similar to <a href="/20170705-word_vectors_and_sat_analogies/">word analogies</a> with word vectors. I didn't expect it to work great, but I was disappointed by just how poorly it worked.</p>
<p>Word vectors are trained based on how words appear in text. One common method puts words near each other in space if they could be substituted in a sentence. Syntax and semantics dictate word placement, so word vectors pick up these characteristics. For example, phrases like "She pet the ___" will tend to put "cat" and "dog" close in word vector representation. Image activations in a neural net are based on how things look, so it's less clear how much "meaning" they should pick up.</p>
<p>I was also using a very small collection of just 200 classes, so the space is super sparse. But I still have both "bicycle" and "motorcycle" (among others) so I was hoping to find some interesting analogies.</p>
<p>Unfortunately, for my 200 categories, there are no analogies by closest item with four distinct items. That is, when you find the closest thing to x + y - z, it's always one of x, y, or z. This is <a href="http://anthology.aclweb.org/W16-2503">a known problem</a> with word vector analogies as well. The standard cheat is to just find the nearest new thing, which tends to mean you're just finding nearby items.</p>
<p>Even cheating to get uniqueness, the analogies I can come up with are not super compelling. Here are the best ones I found:</p>
<ul>
<li>sheep : camel :: seal : hippopotamus</li>
<li>flute : oboe :: domestic cat : lion</li>
<li>brassiere : maillot :: guacamole : burrito</li>
</ul>
<p>You could try to convince yourself that there's some meaning there, but it's really just that some things are close to each other in space; the relations are not super meaningful. Here are some examples with similar goodness in terms of distance:</p>
<ul>
<li>flute : oboe :: snake : centipede</li>
<li>lemon : orange :: lizard : frog</li>
<li>lemon : orange :: pitcher : bowl</li>
</ul>
<p>What about the bicycle and motorcycle?</p>
<ul>
<li>bicycle : motorcycle :: flute : oboe</li>
<li>bicycle : motorcycle :: guacamole : burrito</li>
</ul>
<p>What we see is that some things are close together; bicycle is close to motorcycle and flute is close to oboe. So when we ask for something close to bicycle + flute - oboe, for example, flute - oboe is basically zero and we find the closest thing to bicycle. The representations are successfully putting things that look similar close together.</p>
<p><img alt="burrito? guacamole?" src="img/n07880968_4922_burrito.jpg"></p>
<p>In the case of "guacamole" and "burrito" images can frequently be used for either class. At best, the representation learns about co-occurrence. So the best case is the dumbbell/arm problem again.</p>
<p>It isn't really fair to ask activations trained for visual classification to also learn interesting semantics and give fun analogies, just as it isn't reasonable to anthropomorphize neural nets and imagine that they're close to emulating human perception or approaching general artificial intelligence.</p>
<p>One direction that aims to address the limitations of simple classification tasks, for example, is sometimes described as grounded perception or embodied cognition. The idea is that an agent needs to interact in an environment to learn things like "this is used for that" and so on. This might be part of an advance.</p>
<p>In my estimation, however, the core problem of neural-symbolic integration seems unlikely to be solved by some kind of accidental emergent property. We struggle to represent input to an intelligent system, and we have very little idea how to represent the internals. Perhaps further inspiration will come from neuroscience.</p>    
    ]]></description>
<link>http://planspace.org/20170911-problems_with_imagenet_and_its_solutions/</link>
<guid>http://planspace.org/20170911-problems_with_imagenet_and_its_solutions/</guid>
<pubDate>Mon, 11 Sep 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Berkeley Deep RL Bootcamp</title>
<description><![CDATA[

<p>At its conclusion, <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> said a major goal of his 2017 <a href="https://www.deeprlbootcamp.berkeley.edu/">Deep Reinforcement Learning Bootcamp</a> was to broaden the application of RL techniques. Around 250 representatives from research and industry had just emerged from 22 scheduled hours over a Saturday and Sunday in Berkeley. Abbeel asked the attendees to report back with tales of applying algorithms they may not have known existed previously.</p>
<p>Instruction came from leaders of modern reinforcement learning research, all delivering their expertise within a framework that highlighted the large-scale structure of the field. I found their organizing diagram to be particularly helpful.</p>
<p><img alt="landscape" src="img/landscape.png"></p>
<p>The reinforcement learning problem statement is simple: at every time step, an agent gets some observation of state \( s \), some reward \( r \), and chooses an action \( a \). So an agent is \( s,r \rightarrow a \), and reinforcement learning is largely about rearranging these three letters.</p>
<p>For example, the Q function is \( s,a \rightarrow \sum{r} \). You <a href="https://en.wikipedia.org/wiki/Bellman_equation">can</a> learn that function from experience, and it nicely induces a policy \( s \rightarrow a \). If you use a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">ConvNet</a> to learn the Q function, you can then <a href="https://deepmind.com/research/dqn/">publish</a> in <a href="https://www.nature.com/">Nature</a>.</p>
<p>Of the many good things about the bootcamp, I most valued getting a better conceptual feel for RL. It was also great to hear from experts about practical details, intuitions, and future directions. For all of these I particularly appreciated <a href="https://www.cs.toronto.edu/~vmnih/">Vlad Mnih</a>'s sessions.</p>
<p>The concerns of RL illuminate deep architectures from unique angles. The big networks that win classification challenges don't win in RL, for example, possibly getting at something about the nature of neural net training and generalization. Vlad described how fixing their target Q-network was more important with the smaller nets they used in development than on their final networks. Wins with <a href="https://arxiv.org/abs/1707.06887">distributional RL</a> may connect to a fundamental affinity of neural nets for categorical problems over regression problems.</p>
<p>Approaches like <a href="https://arxiv.org/abs/1611.05397">unsupervised auxiliary tasks</a> prompt comparisons with how the brain might work. But even cutting-edge techniques with models (\( s,a \rightarrow s \)) and planning, like <a href="https://arxiv.org/abs/1707.06203">imagination-augmented agents</a> and the <a href="https://arxiv.org/abs/1612.08810">predictron</a>, do as much to highlight differences as similarities with how we think. RL is at least as close to <a href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a> as it is to <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">AGI</a>.</p>
<p>Reinforcement learning is home to a profusion of acronyms and initialisms that can be intimidating. As best I can, I've extended the Deep RL Bootcamp's diagram to include everything that was covered, together with a few terms common elsewhere and a set of expansions and links. For didactic resources, start from <a href="https://twitter.com/karpathy">Karpathy</a>'s <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a>.</p>
<p><img alt="annotated" src="img/annotated.jpg"></p>
<ul>
<li>(<a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">PO</a>)<a href="https://en.wikipedia.org/wiki/Markov_decision_process">MDP</a>: (Partially Observable) Markov Decision Process</li>
<li><a href="https://en.wikipedia.org/wiki/Derivative-free_optimization">DFO</a>: Derivative-Free Optimization</li>
<li><a href="https://en.wikipedia.org/wiki/Cross-entropy_method">cross-entropy method</a><ul>
<li><a href="http://iew3.technion.ac.il/CE/files/papers/Learning%20Tetris%20Using%20the%20Noisy%20Cross-Entropy%20Method.pdf">Learning Tetris using the noisy cross-entropy method</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/CMA-ES">CMA</a>: Covariance Matrix Adaptation</li>
<li><a href="https://en.wikipedia.org/wiki/Natural_evolution_strategy">NES</a>: Natural Evolution Strategy<ul>
<li><a href="https://blog.openai.com/evolution-strategies/">Evolution strategies as a scalable alternative to reinforcement learning</a></li>
</ul>
</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.2545&amp;rep=rep1&amp;type=pdf">REINFORCE</a>: REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility</li>
<li><a href="https://arxiv.org/abs/1502.05477">TRPO</a>: Trust Region Policy Optimization</li>
<li><a href="https://blog.openai.com/openai-baselines-ppo/">PPO</a>: Proximal Policy Optimization</li>
<li><a href="https://arxiv.org/abs/1510.09142">SVG</a>: Stochastic Value Gradients</li>
<li><a href="https://arxiv.org/abs/1602.01783">A3C</a>: Asynchronous Advantage Actor Critic</li>
<li><a href="https://blog.openai.com/baselines-acktr-a2c/">A2C</a>: (Synchronous) Advantage Actor Critic</li>
<li><a href="https://arxiv.org/abs/1708.05144">ACKTR</a>: Actor Critic using Kronecker-Factored Trust Region</li>
<li><a href="https://arxiv.org/abs/1611.05397">UNREAL</a>: UNsupervised REinforcement and Auxiliary Learning</li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">TD</a>: Temporal Difference (also \( TD(\lambda) \))</li>
<li>FQI: Fitted Q Iteration</li>
<li><a href="https://deepmind.com/research/dqn/">DQN</a>: Deep Q-Network</li>
<li><a href="https://arxiv.org/abs/1509.02971">DDPG</a>: Deep Deterministic Policy Gradient</li>
<li><a href="https://arxiv.org/abs/1603.00748">NAF</a>: Normalized Advantage Functions</li>
<li><a href="http://realai.org/imitation-learning/">Imitation</a> Learning</li>
<li><a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf">IRL</a>: Inverse Reinforcement Learning</li>
<li><a href="https://arxiv.org/abs/1603.00448">GCL</a>: Guided Cost Learning</li>
<li><a href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_363">HRL</a>: Hierarchical Reinforcement Learning</li>
<li>FuN: FeUdal Networks<ul>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/dh93.pdf">Feudal reinforcement learning</a></li>
<li><a href="https://arxiv.org/abs/1703.01161">FeUdal Networks for hierarchical reinforcement learning</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Model_predictive_control">MPC</a>: Model Predictive Control</li>
<li><a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">PILCO</a>: Probabilistic Inference
for Learning COntrol</li>
<li>local models specifically as in <a href="https://arxiv.org/abs/1501.05611">Learning contact-rich manipulation skills with guided policy search</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">LQR</a>: Linear-Quadratic Regulator</li>
<li><a href="https://arxiv.org/abs/1612.08810">The Predictron: End-to-end learning and planning</a></li>
<li><a href="https://arxiv.org/abs/1707.06203">I2A</a>: Imagination-Augmented Agent</li>
<li>(<a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo</a>) (<a href="https://en.wikipedia.org/wiki/Tree_traversal">tree</a>) <a href="https://en.wikipedia.org/wiki/Artificial_intelligence#Search_and_optimization">search</a>, <a href="https://en.wikipedia.org/wiki/Minimax">minimax</a>, etc.</li>
<li><a href="https://en.wikipedia.org/wiki/Gaussian_process">GP</a>: Gaussian Process</li>
<li><a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">GMM</a>: Gaussian Mixture Model</li>
<li>(<a href="https://en.wikipedia.org/wiki/Deep_learning">D</a>/<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">C</a>/<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">R</a>)<a href="https://en.wikipedia.org/wiki/Artificial_neural_network">NN</a>: (Deep/Convolutional/Recurrent) Neural Network</li>
<li><a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a>: Long Short-Term Memory</li>
</ul><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20170830-berkeley_deep_rl_bootcamp/</link>
<guid>http://planspace.org/20170830-berkeley_deep_rl_bootcamp/</guid>
<pubDate>Wed, 30 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Characteristics of good theories</title>
<description><![CDATA[

<p>In <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a> gives a partial list of characteristics that good theories share:</p>
<ol>
<li><strong>Accurate</strong>: within its domain, consequences deducible from a theory should be in demonstrated agreement with the results of existing experiments and observations</li>
<li><strong>Consistent</strong>: internally (with itself) and with other currently accepted theories applicable to related aspects of nature</li>
<li><strong>Broad scope</strong>: consequences should extend far beyond the particular observations, laws, or subtheories it was originally designed to explain</li>
<li><strong>Simple</strong>: bringing order to phenomena that in its absence would be individually isolated and, as a set, confused</li>
<li><strong>Fruitful</strong>: leading to new research findings; disclosing new phenomena or previously unnoted relationships among those already known</li>
</ol>
<p>In <a href="https://en.wikipedia.org/wiki/The_Beginning_of_Infinity">The Beginning of Infinity</a>, <a href="https://en.wikipedia.org/wiki/David_Deutsch">David Deutsch</a> objects to positivism or anti-realism, but has not so different criteria to Kuhn. Deutsch focuses on these two:</p>
<ol>
<li><strong>Hard to vary</strong>: having no extraneous or non-explanatory characteristics; possibly related to Kuhn's "consistent" and "simple"</li>
<li><strong>Reach</strong>: similar to Kuhn's "broad scope," "fruitful," and possibly even "simple"</li>
</ol>
<p>Deutsch mostly takes "accurate" as a given or obvious requirement. Even this seemingly simple goal becomes complicated when people <a href="/20170823-transfer_of_allegiance_from_theory_to_theory/">disagree</a> about "the results of existing experiments and observations."</p>
<p>The focus of both authors is on scientific theories, as in (for example) physics. When explaining human affairs, the same criteria may not always be perfect guides. For example, many non-simple conspiracy theories are not true, but there can really be conspiracies, and things may not be obvious.</p>    
    ]]></description>
<link>http://planspace.org/20170825-characteristics_of_good_theories/</link>
<guid>http://planspace.org/20170825-characteristics_of_good_theories/</guid>
<pubDate>Fri, 25 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Unique IDs and Nielsen abuse</title>
<description><![CDATA[

<p>The Wall Street Journal ran an article online with the headline <a href="https://www.wsj.com/articles/in-tv-ratings-game-networks-try-to-dissguys-bad-newz-from-nielsen-1499350955">In TV Ratings Game, Networks Try to Dissguys Bad Newz from Nielsen</a>. It got picked up by <a href="https://www.theverge.com/2017/7/6/15923722/tv-network-ratings-nielsen-viewership">other outlets</a>, and ran in the print WSJ the next day, on Friday July 7, 2017, on the front page, below the fold.</p>
<p>The implication is that networks identify their programs to Nielsen via a free text field with no validation; programs don't have unique IDs. Not having unique IDs is a huge pain even when everyone is trying to be consistent. Inevitably you have to do some kind of deduplication and merging. In this case, it leads to abuse (possibly sanctioned unofficially).</p>
<p><img alt="page 1" src="img/page1.png"></p>
<p><img alt="page 9" src="img/page9.png"></p>    
    ]]></description>
<link>http://planspace.org/20170724-unique_ids_and_nielsen_abuse/</link>
<guid>http://planspace.org/20170724-unique_ids_and_nielsen_abuse/</guid>
<pubDate>Mon, 24 Jul 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer of allegiance from theory to theory</title>
<description><![CDATA[

<p>Toward the end of <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a>'s <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, he says something that feels relevant to modern popular debate:</p>
<blockquote>
<p>...communication between proponents of different theories is
inevitably partial, that what each takes to be facts depends in part
on the theory he espouses, and that an individual's transfer of
allegiance from theory to theory is often better described as
conversion than as choice.</p>
</blockquote>
<p>Kuhn describes the investigation and development of non-mainstream
ideas as a benefit resulting from diversity of opinion. At the level
of conspiracy theory, and when choice of what to believe is based on
problematic desiderata like greed and racism, it seems to become a
pathology.</p>    
    ]]></description>
<link>http://planspace.org/20170823-transfer_of_allegiance_from_theory_to_theory/</link>
<guid>http://planspace.org/20170823-transfer_of_allegiance_from_theory_to_theory/</guid>
<pubDate>Wed, 23 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Scientific and historical levels of explanation</title>
<description><![CDATA[

<p>From page 1 of <a href="https://en.wikipedia.org/wiki/James_Flynn_(academic)">Flynn</a>'s <a href="https://en.wikipedia.org/wiki/What_Is_Intelligence%3F">What is intelligence?</a>:</p>
<blockquote>
<p>A warning for everyone: there are problems that can simply be
settled by evidence, for example, whether some swans are black. But
there are deeper problems that pose paradoxes. Sometimes the
evidence that would solve them lies in an inaccessible past. That
means we have to retreat from the scientific level of explanation to
the historical level where we demand only plausibility that conforms
to the known facts. I believe that my efforts to resolve the
historical paradoxes we will discuss should be judged by whether
someone has a more satisfactory resolution to offer. The reader
should be wary throughout to distinguish the contentions I evidence
from the contentions to which I lend only plausibility.</p>
</blockquote>
<p>I'm not sure whether an existence claim (about black swans) is the
best example, or whether it matters much if other problems are deeper
or pose paradoxes, but I do think this is an interesting way to phrase
the epistemological difference between a claim supported
experimentally, as by a randomized controlled trial, and a claim based
on observational evidence that is not direct. It's the kind of thing
that I think is worth considering, especially as there seems to be so
much disagreement about how to settle problems.</p>    
    ]]></description>
<link>http://planspace.org/20170822-scientific_and_historical_levels_of_explanation/</link>
<guid>http://planspace.org/20170822-scientific_and_historical_levels_of_explanation/</guid>
<pubDate>Tue, 22 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>A downward spiral destructive of civic virtue</title>
<description><![CDATA[

<p>From page 166 of <a href="https://en.wikipedia.org/wiki/James_Flynn_(academic)">Flynn</a>'s <a href="https://en.wikipedia.org/wiki/What_Is_Intelligence%3F">What is intelligence?</a>:</p>
<blockquote>
<p>Competition for possessions without a rationally imposed limit
engenders pessimism about acceptance of the restraints necessary to
avoid ecological disaster.</p>
<p>Competition for possessions also creates a downward spiral
destructive of civic virtue. Those who wish to maximize their
economic status are reluctant to pay taxes and this diminishes state
provision of health, education, and security against misfortune. As
the quality of state provision declines, it becomes imperative to
maximize private wealth for reasons of security even if status
seeking is set aside. Even principled socialists will pay fees to
jump the queue for medical care and to get education for their
children in schools that are not a test of physical survival. The
more that is true, the more you resent any dollar leaving your
pocket in tax, so public provision drops further, so willingness to
be taxed drops further, and so forth. Indeed, since only a few can
amass the fortune needed to provide self-security, no amount of
money you can realistically hope to acquire is enough.</p>
</blockquote>
<p>I think this passage touches a lot of modern debates.</p>    
    ]]></description>
<link>http://planspace.org/20170821-downward_spiral_destructive_of_civic_virtue/</link>
<guid>http://planspace.org/20170821-downward_spiral_destructive_of_civic_virtue/</guid>
<pubDate>Mon, 21 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Word Vectors and SAT Analogies</title>
<description><![CDATA[

<p>In 2013, <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> popularized <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">word vectors</a> and <em>king - man + woman = queen</em>.</p>
<p><a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"><img alt="king queen etc." src="img/king_queen.png"></a></p>
<p>This reminded me of <a href="https://en.wikipedia.org/wiki/SAT">SAT</a> analogy questions, which <a href="http://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them">disappeared from the SAT in 2005</a>, but looked like this:</p>
<pre><code>PALTRY : SIGNIFICANCE ::

A. redundant : discussion
B. austere : landscape
C. opulent : wealth
D. oblique : familiarity
E. banal : originality</code></pre>

<p>The king/queen example is not difficult, and I don't know whether it was tested or discovered. A better evaluation would use a set of challenging pre-determined questions.</p>
<p>There is a Google set of analogy questions, but all the relationships are grammatical, geographical, or by gender. Typical: "fast : fastest :: old : oldest." (<a href="http://download.tensorflow.org/data/questions-words.txt">dataset</a>, <a href="https://arxiv.org/abs/1301.3781">paper</a>, <a href="https://aclweb.org/aclwiki/Google_analogy_test_set_(State_of_the_art)">context</a>)</p>
<p>SAT questions are more interesting. Selecting from fixed answer choices provides a nice guessing baseline (1/5 is 20%) and using a human test means it's easier to get human performance levels (average US college applicant is 57%; human voting is 81.5%).</p>
<p>Michael Littman and Peter Turney have made available <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">a set of 374 SAT analogy questions</a> since 2003. You have to email Turney to get them, and I appreciate that he helped me out.</p>
<p>Littman and Turney <a href="http://cogprints.org/4518/1/NRC-48273.pdf">used</a> a vector-based approach on their dataset back in 2005. They achieved 47% accuracy (state of the art at the time) which is a nice benchmark.</p>
<p>They made vectors for each word pair using web data. To get one value for "banal : originality" they would search <a href="https://en.wikipedia.org/wiki/AltaVista">AltaVista</a> for "banal and not originality" and take the log of the number of hits. With a list of 64 connectives they made vectors with 128 components.</p>
<p>I'm using <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> and word2vec word vectors that are per-word and based on various text corpora directly. Since the vectors are not specific to particular pairings, they may at a relative disadvantage for the SAT task. To get a vector for a word pair, I just subtract.</p>
<p>Stanford provides a variety of GloVe vectors pre-trained on three different corpora:</p>
<ul>
<li>Twitter (<code>glove.twitter</code>)<ul>
<li>2B tweets, 27B tokens, 1.2M vocab, uncased</li>
<li>as each of 25d, 50d, 100d, &amp; 200d vectors</li>
</ul>
</li>
<li>Wikipedia 2014 + Gigaword 5 (<code>glove.6B</code>)<ul>
<li>6B tokens, 400K vocab, uncased</li>
<li>as each of 50d, 100d, 200d, &amp; 300d vectors</li>
</ul>
</li>
<li>Common Crawl<ul>
<li>uncased (<code>glove.42B</code>) 42B tokens, 1.9M vocab, 300d vectors</li>
<li>cased (<code>glove.840B</code>) 840B tokens, 2.2M vocab, 300d vectors</li>
</ul>
</li>
</ul>
<p>I have one word2vec model: <code>GoogleNews-vectors</code> trained on a Google News dataset, providing 300d vectors.</p>
<p><img src="img/accuracy.png" width="480"></p>
<ul>
<li>The best word vectors for the SAT analogies task (<code>glove.840B</code>) achieve 49% accuracy. This outperforms the 2005 47% result, but is still within the confidence bounds of 42.2% to 52.5% that they report.</li>
<li>Holding the training set constant and varying the size of the vectors affects performance on the SAT analogies task: bigger vectors work better, though performance may be plateauing around 300d.</li>
<li>Twitter data does markedly worse on the SAT analogies task. This is consistent with Twitter's limitations and reputation for being less than erudite.</li>
</ul>
<p>Trained word vectors provide a fixed vocabulary. When a word was missing, I used a vector of zeros. Most embeddings only missed one to eight words from the 374 questions, but <code>glove.twitter</code> missed 105. I also checked accuracies when excluding questions that had unsupported words for a set of embeddings, and the results were remarkably close, even with the Twitter embeddings. So the accuracies are due to the embeddings and not just gaps in the vocabularies.</p>
<p>It is pretty impressive that word vectors work as well as they do on this task, but nobody should consider word vectors a solution to natural language understanding. Problems with word vectors have been pointed out (<a href="http://www.aclweb.org/anthology/N15-1098">source</a>, <a href="http://www.aclweb.org/anthology/C/C16/C16-1262.pdf">source</a>, <a href="http://anthology.aclweb.org/W16-2503">source</a>, <a href="https://arxiv.org/abs/1705.11168">source</a>).</p>
<p>Still, the idea of word vectors (translating sparse data to a learned dense representation) is super useful, and not just for words. See implementations <a href="https://keras.io/layers/embeddings/">in Keras</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embedding_column">in TensorFlow</a>.</p>
<p>These methods are not the best-performing non-human technique for these SAT analogy questions. Littman and Turney <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">report</a> several. Latent Relational Analysis comes in at 56% accuracy, against the average US college applicant at 57%.</p>
<p>This is a case in which "the average human" is not a good bar for measuring AI success. The average human has a remarkably small vocabulary relative to what a computer should easily handle, not to mention that the average human would not be admitted to most colleges you could name.</p>
<hr>
<p>I'm grateful to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for helpful discussions, and particularly Shabnam Tafreshi and Dmitrijs Milajevs for sharing references at <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/events/237114317/">the January 26, 2017 meeting</a>. Thanks also to <a href="https://twitter.com/metasemantic">Travis Hoppe</a> of <a href="http://dc.hackandtell.org/">DC Hack and Tell</a> for always doing cool things with NLP. Thanks to Peter Turney for providing the dataset and commenting on distances.</p>
<p>Notes:</p>
<ul>
<li>An <a href="https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/">example</a> of word2vec popularization.</li>
<li>My code is available at <a href="https://github.com/ajschumacher/sat_analogies">ajschumacher/sat_analogies</a>.</li>
<li>All but 20 of the 374 questions have five designed answer choices; those 20 have four and "no : choice" as the last option. To maintain comparability I kept those questions in, though it means a human guessing baseline should be slightly over 20%.</li>
<li>For efficiency, I used GloVe values as 2-byte floats, which is lossy. I also tested with 4-byte floats, and the results varied by at most one question, while being much slower to generate.</li>
<li>Cosine distance is more effective than Euclidean distance for this task, and the advantage increases with dimensionality.</li>
</ul>
<p><img src="img/cosine_advantage.png" width="480"></p>    
    ]]></description>
<link>http://planspace.org/20170705-word_vectors_and_sat_analogies/</link>
<guid>http://planspace.org/20170705-word_vectors_and_sat_analogies/</guid>
<pubDate>Wed, 05 Jul 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Questions from TensorFlow APIs Webinar</title>
<description><![CDATA[

<p>I did <a href="/20170524-more_and_better_new_tensorflow_apis/">More and Better: The New TensorFlow APIs</a> as a <a href="https://www.altoros.com/blog/event/more-and-better-the-new-tensorflow-apis/">webinar</a>. <a href="https://www.altoros.com/">Altoros</a> ran it using <a href="https://www.gotomeeting.com/webinar">GoToWebinar</a>, which reported all questions from attendees. Questions and answers:</p>
<hr>
<h3>What should I learn?</h3>
<blockquote>
<p>"New to DL and TF... several people have said to not waste time w/Keras, etc, but go deep into TF to really learn it.  From what you said, it seems like TF changes a lot, though.  Recommendations in terms of sequence/path for learning DL concepts and the tools?"</p>
</blockquote>
<p>First, regarding the rate of change of TensorFlow: The low-level API should really be quite stable now. It's the high-level APIs that are still settling down a little bit. And even there, it looks like things are getting pretty set. I wouldn't expect Keras to change a lot, in particular.</p>
<p>The question of whether to use a high-level API or a low-level API depends on your goals. I don't think it's a waste of time to use Keras. In fact, I think it would be easier to argue that it's "a waste of time" to dig into low-level TensorFlow implementation details, because there's a lot you can get done much more quickly by using Keras.</p>
<p>It depends on your goals. Do you want to understand every step in the internals of your system? I've heard people like Matt Zeiler suggest that you should write your own deep learning framework from scratch in order to understand all the details of backprop, etc. How far do you want to go?</p>
<p>For most people, I think higher-level interfaces are a good choice. Sometimes they can actually make it easier to learn the concepts that are really important, because you don't have to deal with all the details that can complicate things.</p>
<p>That's not to say that it isn't valuable to get into the low-level details as well. Sometimes you really do need to make changes or understand phenomena that are down at that level. But you don't necessarily need to start at the low level; you can also start at higher levels of abstraction and then learn lower levels later.</p>
<p>As for learning DL concepts and tools, I'm currently a big advocate of <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a> by Fran&#231;ois Chollet, the author of Keras. It's still in pre-release, but the chapters that are already available are quite good. I think it's a great book especially for people who are newer to deep learning because it explains a lot of fundamental things in approachable ways.</p>
<hr>
<h3>Keras outside or inside TensorFlow?</h3>
<blockquote>
<p>"What is the link between keras and tf that is being shown? I have used Keras and I simply indicate that it should use tensorflow in the backend and my calls do not include tf.xxx.keras.xxx... One of the goals of Keras is to be framework independent. To call from Tensorflow doesn&#8217;t really maintain that spirit."</p>
</blockquote>
<p>You can still use Keras directly with TensorFlow as the backend, maintaining framework independence. As far as I know, Keras will continue to be developed separately from TensorFlow as well as within the TensorFlow codebase. And for many things, using Keras as it appears inside TensorFlow will work just the same way as using independent Keras with the TensorFlow backend.</p>
<p>Having Keras inside TensorFlow is a convenience, for one, but it should also enable some tighter integrations. I'm expecting the layers API will become even more unified and allow for mixing and matching even more design possibilities. More importantly, I think, Keras models in TensorFlow will be easily convertible to TensorFlow Estimators, which should be pretty neat.</p>
<hr>
<h3>C? C++?</h3>
<blockquote>
<p>"how come [TensorFlow is] written in C++ but recommended interfaces should talk to the C part?"</p>
</blockquote>
<p>I suppose there are at least two reasons. One is that it standardizes the interface, making it very clear what is stable and safe to build an API to. Another is that as a practical matter, lots of languages have established standard ways to connect to a C API. To read more about TensorFlow's take, check out the <a href="https://www.tensorflow.org/extend/language_bindings">language support documentation</a>.</p>
<hr>
<h3>On-graph? Off-graph?</h3>
<blockquote>
<p>"Can you remind folks about what 'on-graph' is?"</p>
</blockquote>
<p>TensorFlow's core way of working is by defining a graph of operations and then running them; anything that runs that way is "on-graph."</p>
<p>Normal Python code doesn't work that way; you don't define a graph and then run it, you just run your Python code right away.</p>
<p>Even within the TensorFlow Python API, not everything works by putting things on the graph. For example, <code>tf.python_io.tf_record_iterator</code> doesn't use the TensorFlow graph.</p>
<p>I wrote some more about this distinction in <a href="/20170428-everything_in_the_graph_even_glob/">Everything in the Graph? Even Glob?</a>. An example of TensorFlow functionality that exists in both off-graph and on-graph versions is TFRecords processing. I have a post that uses the <a href="/20170323-tfrecords_for_humans/">off-graph API</a> and two posts that use the on-graph API: <a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">reading from disk</a> and <a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">parsing TFRecords</a>.</p>
<hr>
<h3>Python 2? Python 3?</h3>
<blockquote>
<p>"Are you using Python 2.x?  If so why?"</p>
</blockquote>
<p>I use Python 2.7 a lot because projects I work on use it. I usually recommend that new projects start up with Python 3.</p>
<p>The inertia of Python 2.7 is really incredible, but maybe the tide is turning at last. Under a year ago, this quote <a href="https://blog.openai.com/infrastructure-for-deep-learning/">appeared</a> on OpenAI's blog:</p>
<blockquote>
<p>"Like much of the deep learning community, we use Python 2.7."</p>
</blockquote>
<p>More recently though, another <a href="https://blog.openai.com/openai-baselines-dqn/">post</a> from OpenAI indicated at least some work is now on Python 3:</p>
<blockquote>
<p>"We use Python 3 and TensorFlow."</p>
</blockquote>
<hr>
<h3>Hyperparameter tuning?</h3>
<blockquote>
<p>"I would like to know if there is a recommended way to make hyperparameter tuning (number of layers, number of nodes in each layer, learning rate, number of epochs)"</p>
</blockquote>
<p>For number of epochs, you can evaluate your model multiple times during training to get train and eval performance curves which should help in making that choice.</p>
<p>For architectural choices like number of layers and number of nodes in each layer, and optimization parameters like learning rate, you can parameterize your code so that choices become command-line arguments, which makes it easier to run experiments and determine what works best.</p>
<p>Making choices based on your previous experience and what's worked for others can save you a lot of time; usually you don't really want to search over <em>all</em> the things you <em>could</em> vary.</p>
<p>When you're really not sure, you can search over all your possible parameters. You may be able to do better than grid search: even <a href="http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization">random search</a> can be better, and <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a> could be better yet. Google has <a href="https://cloud.google.com/ml-engine/docs/how-tos/using-hyperparameter-tuning">productized hyperparameter tuning</a>, which may be worth checking out.</p>
<hr>
<h3>Visualization for very large models?</h3>
<blockquote>
<p>"I have a question, can we also visualize distributed tensorflow on very large models with billion parameters with tensorboard?"</p>
</blockquote>
<p>The answer is probably "yes" but there are multiple aspects to this question.</p>
<p>First, when running  distributed TensorFlow training following the standard pattern, probably you have your chief worker writing out TensorBoard summaries. So that centralizes the "where" of TensorBoard summary writing.</p>
<p>Second, even with small models, it's possible to generate unwieldy amounts of TensorBoard data. You probably want to think carefully about what you want to save during training. For example, you probably don't need to log things every iteration, and you probably don't need to log every parameter value if a histogram will do the job.</p>
<p>Third, some of the things you likely want to visualize for a large model are the same as things you want to visualize for a small model, like loss progression through training. There might not be any changes necessary when model size is different.</p>
<p>If you just want to visualize the model structure (the model graph) then you can take advantage of TensorBoard's grouping functionality so that you can see and reason about components across multiple scales.</p>
<p>In the end, it will depend on exactly what you want to visualize. TensorBoard has a lot of neat features, but it won't be the right choice for every possible problem.</p>
<hr>
<h3>What about serving?</h3>
<blockquote>
<p>How does TensorFlow Serving fit in currently?</p>
</blockquote>
<p><a href="https://tensorflow.github.io/serving/">TensorFlow Serving</a> is largely distinct from the APIs that I showed, except that Estimators can easily <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel">export</a> the SavedModel format that you want to use for serving.</p>    
    ]]></description>
<link>http://planspace.org/20170526-questions_from_tensorflow_apis_webinar/</link>
<guid>http://planspace.org/20170526-questions_from_tensorflow_apis_webinar/</guid>
<pubDate>Fri, 26 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>More and Better: The New TensorFlow APIs</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://www.altoros.com/blog/event/more-and-better-the-new-tensorflow-apis/">webinar</a> given Wednesday May 24, 2017. (<a href="big.html">slides</a>) (<a href="https://altoros.wistia.com/medias/e5su4b1vtz">video</a>) The Jupyter notebook and supporting files for code demos are available in a <a href="https://github.com/ajschumacher/tf_api_20170524">repository</a> on GitHub.</em></p>
<hr>
<p>Thank you!</p>
<hr>
<p>Before starting, I want to take a moment to notice how great it is to have a webinar. Thank you for checking it out! We are all really lucky. If this isn't nice, I don't know what is.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work for a company called Deep Learning Analytics. You can find out more about DLA at <a href="https://www.deeplearninganalytics.com/">deeplearninganalytics.com</a>. We work with a number of frameworks, and I've been keeping an eye on TensorFlow.</p>
<p>I do <em>not</em> work for Google, and I'm not a core TensorFlow developer. I'm just talking about my view of things, which is not authoritative or official.</p>
<hr>
<p>motivation</p>
<hr>
<p>Why this talk? What is it about, and who should care?</p>
<hr>
<p>"a low-level library, meaning you&#8217;ll be multiplying matrices and vectors." (2015)</p>
<hr>
<p>This is a description of TensorFlow from an <a href="http://fastml.com/what-you-wanted-to-know-about-tensorflow/">article</a> that came out the same month TensorFlow was released, in November of 2015. It was pretty true then.</p>
<p>Some people thought this was fine. It was like <a href="http://deeplearning.net/software/theano/">Theano</a>.</p>
<p>Some people thought it was not so fine, and so they may have decided TensorFlow was not for them.</p>
<p>But while Theano hasn't radically expanded its API to include tons of higher-level ways of using it, TensorFlow has. TensorFlow is no longer only a low-level library.</p>
<hr>
<pre><code class="language-python">...TensorFlow 1.2.0rc0</code></pre>

<hr>
<p>TensorFlow wasn't "finished" when it was released as open source. It only hit version 1.0 in February of this year. And that means the low-level API is largely stabilized. But high-level APIs are being added at a rapid pace.</p>
<p>I gave a <a href="/20170509-building_tensorflow_systems_from_components/">workshop</a> on TensorFlow at <a href="https://conferences.oreilly.com/oscon/">OSCON</a> two weeks ago, and one of the APIs I'll talk about today is new since then. I'll also mention things that aren't going to be in TensorFlow for at least a few weeks yet.</p>
<p>(I'm referring to the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data"><code>Dataset</code> API</a>, which entered <code>contrib</code> in 1.2, and the Keras model <code>get_estimator</code> method, respectively.)</p>
<hr>
<p>"I don't know which API to use." (2017)</p>
<hr>
<p>This is a sentiment I encountered in talking to people at the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a>.</p>
<p>There are now so many ways of using TensorFlow, it can be overwhelming, and you might not know where to start. If you do jump in, you might only see one part of the API and not know what else you're missing.</p>
<hr>
<p>a tour of TensorFlow APIs</p>
<hr>
<p>This talk is not for people who are expert TensorFlow developers or people who have been following TensorFlow development super closely.</p>
<p>This talk is for people who are new to TensorFlow, or people who may have tried TensorFlow in the past but haven't been following more recent developments.</p>
<p>The hope is that you'll understand better the options you have for using TensorFlow and be able to choose what to explore.</p>
<hr>
<ul>
<li>language survey</li>
<li>Python survey</li>
<li>Python focus</li>
</ul>
<hr>
<p>We'll start very broad and then narrow in on a few high-level APIs for TensorFlow.</p>
<p>First, considering the way TensorFlow works with multiple programming languages can illuminate the core of TensorFlow's low-level design. This introduces the low-level Python API, and the key distinction between implementations on and off the TensorFlow graph.</p>
<p>We're not going to cover them all equally, but we'll survey a lot of Python API options and how they relate.</p>
<p>Finally, we'll focus on aspects of the core TensorFlow Python APIs, especially Keras and the Estimator API, with a little bit more detail and code examples.</p>
<p>(The following section is based on <a href="/20170320-tensorflow_apis_for_various_languages/">TensorFlow APIs for Various Languages</a>.)</p>
<hr>
<p>What even is TensorFlow?</p>
<hr>
<p>Here's one way to think about TensorFlow.</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>The beating heart of TensorFlow is the Distributed Execution Engine, or runtime.</p>
<p>One way to think of it is as a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">virtual machine</a> whose language is the TensorFlow graph.</p>
<p>That core is written in C++, but lots of TensorFlow functionality lives in the frontends. In particular, there's a lot in the Python frontend.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<p>Python is the richest frontend for TensorFlow. It's what we'll use today.</p>
<p>You should remember that not everything in the Python TensorFlow API touches the graph. Some parts are just Python.</p>
<hr>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>R has an unofficial <a href="https://rstudio.github.io/tensorflow/">TensorFlow API</a> which is kind of interesting in that it just wraps the Python API. It's really more like an R API to the Python API to TensorFlow. So when you use it, you write R, but Python also runs. This is not how TensorFlow encourages languages to implement TensorFlow APIs, but it's out there.</p>
<hr>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<p>The way TensorFlow encourages API development is via TensorFlow's C bindings.</p>
<hr>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<p>You could use C++, of course.</p>
<hr>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<p>Also there's Java.</p>
<hr>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<p>And there's Go.</p>
<p>Python, C++, Java, and Go are the only languages with official Google-blessed TensorFlow APIs. But there are projects offering APIs that interface with TensorFlow via the C bindings.</p>
<hr>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<p>There's Rust.</p>
<hr>
<p><img alt="C#" src="img/c_sharp.png"></p>
<hr>
<p>And there's C#.</p>
<hr>
<p><img alt="Julia" src="img/julia.png"></p>
<hr>
<p>And there's Julia.</p>
<hr>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<p>And there's even Haskell!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Currently, a rough summary is that languages other than Python have TensorFlow support that is close to the runtime. Basically make your ops and run them.</p>
<p>This is likely to be enough support to deploy a TensorFlow system doing inference in whatever language you like, but if you're developing and training systems you probably still want to use Python.</p>
<hr>
<p>graph or not graph?</p>
<hr>
<p>So this is a distinction to think about: Am I using the TensorFlow graph, or not?</p>
<p>The distinction will be pretty clear when using the low-level API. It becomes less obvious when using higher-level APIs, but the situation there is that you can safely assume they're using the graph.</p>
<hr>
<p>Python APIs</p>
<hr>
<p>So, what's this about multiple Python APIs?</p>
<p>TensorFlow started with a low-level API, and it's always been possible and even a good idea to build higher-level APIs on top of it. Plenty of people have done so.</p>
<p>Let's start by checking out some APIs that are entirely outside of TensorFlow.</p>
<p>(The following section is based on <a href="/20170321-various_tensorflow_apis_for_python/">Various TensorFlow APIs for Python</a>.)</p>
<hr>
<p><img alt="Bucksstar Coffee" src="img/bucksstar.png" height="100%"></p>
<hr>
<p>It seems like lots of projects got names involving one or the other of "tensor" or "flow". It doesn't mean they're part of TensorFlow, and it doesn't mean they're good.</p>
<p>Of course, maybe they are; some of these could be good for you, for all I know.</p>
<hr>
<p><img alt="TFLearn logo" src="img/tflearn.png" height="100%"></p>
<hr>
<p>The confusingly named <a href="https://github.com/tflearn/tflearn">TFLearn</a> (no space; perhaps more clearly identified as <a href="(http://tflearn.org/)">TFLearn.org</a>) is not at all the same thing as the TF Learn (with a space) that appears in TensorFlow at <code>tf.contrib.learn</code>. TFLearn is a <a href="http://stackoverflow.com/questions/38859354/what-is-the-difference-between-tf-learn-aka-scikit-flow-and-tflearn-aka-tflea">separate</a> Python package that uses TensorFlow. It seems like it aspires to be like Keras. Even the TFLearn logo looks like a knock-off.</p>
<hr>
<p><img alt="TensorLayer logo" src="img/tensorlayer.png"></p>
<hr>
<p>Another one with a confusing name is <a href="https://github.com/zsdonghao/tensorlayer/">TensorLayer</a>. This is a separate package from TensorFlow and it's different from TensorFlow's layers API.</p>
<hr>
<p>ImageFlow</p>
<hr>
<p><a href="https://github.com/HamedMP/ImageFlow">ImageFlow</a> was supposed to make it easier to do image work with TensorFlow. It looks like it's abandoned.</p>
<hr>
<p>Pretty Tensor</p>
<hr>
<p><a href="https://github.com/google/prettytensor">Pretty Tensor</a> is still a Google project, so it might be worth checking out if you really like <a href="https://en.wikipedia.org/wiki/Fluent_interface">fluent interfaces</a> with lots of chaining, like <a href="https://d3js.org/">d3</a>.</p>
<hr>
<p><img alt="Sonnet" src="img/sonnet.png"></p>
<hr>
<p>Google's <a href="https://deepmind.com/">DeepMind</a> group <a href="https://deepmind.com/blog/open-sourcing-sonnet/">released</a> their <a href="https://github.com/deepmind/sonnet">Sonnet</a> library, which is used in the code for their <a href="https://github.com/deepmind/learning-to-learn">learning to learn</a> and <a href="https://github.com/deepmind/dnc">Differentiable Neural Computer</a> projects. Sonnet isn't part of TensorFlow (it builds on top) but it is a Google project. Sonnet has a modular approach compared to that of <a href="https://github.com/torch/nn">Torch/NN</a>. Also they have a cool logo.</p>
<hr>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<hr>
<p>The previous APIs exist outside of TensorFlow. Let's now talk about APIs <em>inside</em> TensorFlow.</p>
<p>We'll trace the development of the stack of available APIs before looking at some code examples.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p>Python Frontend (op level)</p>
<hr>
<p>You can continue to use TensorFlow's low-level APIs.</p>
<p>There are ops, like <code>tf.matmul</code> and <code>tf.nn.relu</code>, which you might use to build a neural network architecture in full detail. To do really novel things, you may want this level of control. But you may also prefer to work with larger building blocks. The other other APIs below will mostly specialize in this kind of application.</p>
<p>There are also ops like <code>tf.image.decode_jpeg</code> (and many others) which may be necessary but don't necessarily relate to what is usually considered the architecture of neural networks. Some higher-level APIs wrap some of this functionality, but they usually stay close to the building of network architectures and the training of such networks once defined.</p>
<hr>
<p>Layers</p>
<hr>
<p>The "layer" abstraction is a natural way to think about deep neural network design.</p>
<p>This <code>layers</code> API provides a first higher level of abstraction over writing things out by individual ops. For example, <code>tf.layers.conv2d</code> implements a convolution layer that involves multiple individual ops.</p>
<hr>
<p><img alt="slim... shady?" src="img/slim_shady.png"></p>
<hr>
<p>You may have heard of <a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html">TF-Slim</a>. TF-Slim has a number of <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">components</a> but it looks like essentially we're seeing the following: <code>tf.contrib.slim.layers</code> became <code>tf.contrib.layers</code> becomes <code>tf.layers</code>.</p>
<p>Sergio Guadarrama, one of the TF-Slim authors, generously confirmed that TensorFlow is converging on a layers API and implementation along these lines, but warns that there can still be some differences. He points out that <code>tf.contrib.layers</code> have <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/arg_scope"><code>arg_scope</code></a>, while <code>tf.layers</code> don't.</p>
<p>Other parts of TF-Slim are likely also worth using, and there is a collection of <a href="https://github.com/tensorflow/models/tree/master/slim">models that use TF-Slim</a> in the <a href="https://github.com/tensorflow/models">TensorFlow models repository</a>.</p>
<p>Historical note: It looks like before calling them layers, TF-Slim overloaded the word "op" for their layer concept (see <a href="https://github.com/tensorflow/models/tree/master/inception/inception/slim">earlier documentation</a>).</p>
<p>TF-Slim is in the TensorFlow codebase as <code>tf.contrib.slim</code>.</p>
<hr>
<p><img alt="Keras" src="img/keras.jpg" height="100%"></p>
<hr>
<p><a href="https://keras.io/">Keras</a> was around before TensorFlow. It was always a high-level API for neural nets, originally running with <a href="http://www.deeplearning.net/software/theano/">Theano</a> as its backend.</p>
<p>After the release of TensorFlow, Keras moved to also work with TensorFlow as a backend. And now TensorFlow is absorbing at least some aspects of the Keras project into the TensorFlow codebase, though <a href="https://twitter.com/fchollet">Fran&#231;ois Chollet</a> seems likely to continue championing Keras as a very fine project in its own right. (See also his response to a <a href="https://www.quora.com/What-will-Keras-do-with-TensorFlow-Slim">Quora question</a> about any possible relationship between TF-Slim and Keras.)</p>
<p>Keras is appearing in the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/keras">tf.contrib.keras</a> and should move to <code>tf.keras</code> at TensorFlow 1.3, at which point it will also be possible to turn Keras models directly into Estimators, which we'll see next.</p>
<hr>
<p><img alt="scikit-learn logo" src="img/sklearn.png" height="100%"></p>
<hr>
<p>Distinct from TensorFlow, the <a href="http://scikit-learn.org/">scikit-learn</a> project makes a lot of machine learning models conveniently available in Python.</p>
<p>A key design element of scikit is that many different models offer the same simple API: they are all implemented as <em>estimators</em>. So regardless of whether the model is linear regression or a GBM (Gradient Boosting Machine), you get the same simple interface.</p>
<p>The estimators API started as <a href="https://github.com/tensorflow/skflow">skflow</a> ("scikit-flow") before moving into the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">tf.contrib.learn</a> ("TF Learn") and now the base estimator code is getting situated in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator">tf.estimator</a> and <a href="/20170521-navigating_tensorflow_estimator_documentation/">documentation</a> is accumulating.</p>
<hr>
<p>Estimators</p>
<hr>
<p>The Estimators API really just defines an interface. You can design models that fit into the Estimator system, or you can use Estimator-based models that are already "set up" to varying degrees.</p>
<hr>
<p>Canned Estimators</p>
<hr>
<p>Canned estimators are concrete pre-defined models that follow the estimator conventions. Currently there are a bunch right in <code>tf.contrib.learn</code>, such as <code>LinearRegressor</code> and <code>DNNClassifier</code>. There are some elsewhere. For example, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/g3doc/kmeans.md">kmeans</a> is in <code>tf.contrib.factorization</code>. It isn't clear to me exactly where all the canned estimators will eventually settle down in the API; some may eventually move out of <code>contrib</code>.</p>
<p>"Canned Estimators" are a lot like scikit-learn in the level at which they make functionality available to programmers.</p>
<hr>
<ul>
<li>automatic checkpoints</li>
<li>automatic logging</li>
<li>separate train/eval/pred</li>
<li>easily train distributed</li>
</ul>
<hr>
<p>What do you get with Estimators? A bunch of things!</p>
<hr>
<p><img alt="data pipeline" src="img/tf_data_pipeline.gif"></p>
<hr>
<p>One more thing: loading data. A lot of people have gotten hung up on the complicated multi-thread, multi-queue, queue-runner design that TensorFlow has espoused for loading data.</p>
<hr>
<p>Datasets</p>
<hr>
<p>TensorFlow developers <a href="https://github.com/tensorflow/tensorflow/issues/7951">listened</a>, and they came up with <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data">Datasets</a>, which is a new API that wraps all the complexity in a pretty nice interface.</p>
<p>This is new as of about a week ago.</p>
<hr>
<p><img alt="latest diagram" src="img/io17_tiers.png"></p>
<hr>
<p>So this is the latest diagram from Google I/O. It adds XLA, which I won't talk about.</p>
<p>(Image from <a href="https://www.youtube.com/watch?v=OzAdKMPgUt4">TensorFlow Frontiers</a> at Google I/O '17.)</p>
<hr>
<p>code!</p>
<hr>
<p>Let's look at some code, shall we?</p>
<p>Go to a notebook (<a href="https://github.com/ajschumacher/tf_api_20170524/blob/master/APIs_start.ipynb">start</a>/<a href="https://github.com/ajschumacher/tf_api_20170524/blob/master/APIs_finish.ipynb">finish</a>) in the <a href="https://github.com/ajschumacher/tf_api_20170524">tf_api_20170524</a> repo.</p>
<hr>
<p>more</p>
<hr>
<p>For lots more, there are plenty of places you can go.</p>
<p>My <a href="/">blog</a> is one place.</p>
<p><a href="https://www.tensorflow.org/">TensorFlow</a>'s documentation is another.</p>
<p>The latest <a href="https://www.youtube.com/watch?v=OzAdKMPgUt4">video</a>s from Google is another. The I/O 2017 videos include a few TensorFlow ones, and there's also still the February TensorFlow DevFest video collection.</p>
<hr>
<p><img alt="Deep Learning with Python book cover" src="img/chollet_dlp.png"></p>
<hr>
<p>I want to also mention this book by the author of Keras: <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a>. It's pretty good. If you're looking to understand deep learning, this is now the book I recommend. It's currently in pre-release, and the available chapters are pretty great already. It focuses on Keras rather than TensorFlow per se, but especially since Keras is now <em>in</em> TensorFlow, any Keras thing is also a TensorFlow thing.</p>
<p>The first chapter is free.</p>
<hr>
<p><img alt="necessary qualities" src="img/qualities.png"></p>
<hr>
<p>I want to close with a thought that is not so much about programming. This is a question I got on LinkedIn, and it made me think.</p>
<p>It is eventually necessary to have some proficiency with particular tools, but the qualities I find most important are not really particular tool skills.</p>
<p>What problem do you need to solve? Can you identify it, articulate it, frame it in a way that makes it tractable? Can you work with what's available to come up with a solution?</p>
<p>I hope knowing more about TensorFlow is helpful to you as you go forward with your work, but I also think it's important to remember that TensorFlow is just a tool. Your success comes from what you do.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170524-more_and_better_new_tensorflow_apis/</link>
<guid>http://planspace.org/20170524-more_and_better_new_tensorflow_apis/</guid>
<pubDate>Wed, 24 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating TensorFlow Estimator Documentation</title>
<description><![CDATA[

<p>The TensorFlow documentation keeps improving, but it can still be hard to find what you're looking for. Here's a way of organizing <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">Estimator</a>s or <a href="https://www.tensorflow.org/api_guides/python/contrib.learn"><code>tf.contrib.learn</code></a> documentation, from high-level to low-level:</p>
<ul>
<li>Using Estimators (already made or "canned")<ul>
<li><a href="https://www.tensorflow.org/get_started/tflearn"><code>DNNClassifier</code> on iris dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide"><code>LinearClassifier</code> (logistic) on census income dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide_and_deep"><code>DNNLinearCombinedClassifier</code> (extends census income example)</a></li>
</ul>
</li>
<li>Aspects of using Estimators<ul>
<li><a href="https://www.tensorflow.org/tutorials/linear">Feature columns (in context of linear models)</a></li>
<li><a href="https://www.tensorflow.org/get_started/input_fn">Using <code>input_fn</code>; <code>DNNRegressor</code> on Boston housing dataset</a></li>
<li><a href="https://www.tensorflow.org/get_started/monitors">Logging etc. with <code>ValidationMonitor</code> (extends iris example)</a></li>
</ul>
</li>
<li>Making your own Estimator<ul>
<li><a href="https://www.tensorflow.org/extend/estimators">With <code>layers</code> in a <code>model_fn</code> on the abalones dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/layers">With <code>layers</code> in a <code>model_fn</code> on MNIST</a></li>
</ul>
</li>
<li>Lower-level API than Estimators<ul>
<li><a href="https://www.tensorflow.org/get_started/mnist/pros#build_a_multilayer_convolutional_network">Use the lower-level API to make a system for MNIST</a></li>
</ul>
</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</link>
<guid>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</guid>
<pubDate>Sun, 21 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Code Reading Questions at OSCON</title>
<description><![CDATA[

<p><a href="https://www.oreilly.com/">O'Reilly</a>'s 2017 <a href="https://conferences.oreilly.com/oscon/">OSCON</a> had a "<a href="https://conferences.oreilly.com/oscon/oscon-tx/public/content/game">code game</a>" with ten questions covering nine different languages. It was supposed to get people engaged in the expo hall, but the quiz-like gamification content reminded me of my old <a href="/20150616-code_reading_question/">code reading question</a> idea. The questions are available in a <a href="https://cdn.oreillystatic.com/en/assets/1/event/214/oscon2017_code_game.pdf">PDF</a> (<a href="oscon2017_code_game.pdf">mirror</a>). I'll put the questions here as well.</p>
<hr>
<h3>1. Rust</h3>
<pre><code class="language-rust">use std::collections::HashSet;
use std::io::{BufRead, Result};

fn f&lt;I: BufRead&gt;(input: &amp;mut I) -&gt; Result&lt;usize&gt; {
    Ok(input.lines()
       .map(|r| r.expect(&#8220;ara ara&#8221;))
       .flat_map(|l| l.split_whitespace()
                     .map(str::to_owned)
                     .collect::&lt;Vec&lt;_&gt;&gt;())
       .collect::&lt;HashSet&lt;_&gt;&gt;()
       .len())
}</code></pre>

<p>This function reads input. What else does it do?</p>
<ul>
<li>A. Counts the number of white-space-separated &#8220;words&#8221;</li>
<li>B. Counts the number of distinct &#8220;words&#8221;</li>
<li>C. Finds the &#8220;word&#8221; that appears most frequently</li>
<li>D. Finds the longest line</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>2. JavaScript</h3>
<pre><code class="language-javascript">function search(values, target) {
  for(var i = 0; i &lt; values.length; ++i){
    if (values[i] == target) { return i; }
  }
  return -1;
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Depth-first search</li>
<li>B. Binary search</li>
<li>C. Merge search</li>
<li>D. Linear search</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>3. Go</h3>
<pre><code class="language-go">func function(s []float64) float64 {
        var sum float64 = 0.0
        for _, n := range s {
                sum += n
        }
        return sum / float64(len(s))
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Sums the contents of a slice</li>
<li>B. Finds the maximum value in a slice</li>
<li>C. Averages the contents of a slice</li>
<li>D. Appends values to a slice</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>4. Perl 5</h3>
<pre><code class="language-perl">sub mystery {
    return @_ if @_ &lt; 2;
    my $p = pop;
    mystery(grep $_ &lt; $p, @_), $p,
    mystery(grep $_ &gt;= $p, @_);
}</code></pre>

<p>What does the mystery subroutine do?</p>
<ul>
<li>A. Binary search</li>
<li>B. Merge sort</li>
<li>C. Removes items that are too large or too small</li>
<li>D. Quick sort</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>5. Java 8</h3>
<pre><code class="language-java">static void function(int[] ar)
 {
   Random rnd = ThreadLocalRandom.current();
   for (int i = ar.length - 1; i &gt; 0; i--)
   {
     int index = rnd.nextInt(i + 1);
     int a = ar[index];
     ar[index] = ar[i];
     ar[i] = a;
   }
 }</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Merge sort</li>
<li>B. Shuffle</li>
<li>C. Increases size of array</li>
<li>D. Decreases size of array</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>6. Ruby</h3>
<pre><code class="language-ruby">def f(hash)
  prs = hash.inject({}) do |hsh, pr|
    k, v = yield pr
    hsh.merge(k =&gt; v)
  end
  Hash[prs]
end</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Reverses an array</li>
<li>B. Administers a booster shot</li>
<li>C. Enters a freeway safely</li>
<li>D. Transforms a hash</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>7. Python</h3>
<pre><code class="language-python">def function(list):
     return [x for x in list if x == x[::-1]]</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Finds and returns all palindromes within the given list</li>
<li>B. Reverses all strings in the given list</li>
<li>C. Swaps the first and last letter in each word in the given list</li>
<li>D. Returns a list of anagrams for each word in the given list</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>8. Scala</h3>
<pre><code class="language-scala">object Op {
  val r1: Regex = &#8220;&#8221;&#8221;([^aeiouAEIOU\d\s]+)([^\d\s]*)$&#8221;&#8221;&#8221;.r
  val r2: Regex = &#8220;&#8221;&#8221;[aeiouAEIOU][^\d\s]*$&#8221;&#8221;&#8221;.r
  val s1:String = &#8220;\u0061\u0079&#8221;
  val s2:String = &#8220;\u0077&#8221; + s1
  def apply(s: String): String = {
    s.toList match {
      case Nil =&gt; &#8220;&#8221;
      case _ =&gt; s match {
        case r1(c, r) =&gt; r ++ c ++ s1 case r2(_*) =&gt; s ++ s2
        case _ =&gt; throw new
RuntimeException(&#8220;Sorry&#8221;)
      }
    }
  }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Converts a given String to a JavaScript-based String</li>
<li>B. Converts a Unix/MacOSX String format into a Windows String format</li>
<li>C. Converts a word into the children&#8217;s language equivalent called &#8220;Pig Latin&#8221;</li>
<li>D. Converts a given String to IPV6 format since IP numbers are running out</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>9. Swift</h3>
<pre><code class="language-swift">import Foundation

let i = &#8220;Hell&#248;, Swift&#8221;

let t = i.precomposedStringWithCanonicalMapping

let c = t.utf8.map({UnicodeScalar($0+2)})
let j = i.utf8.map({UnicodeScalar($0+1)}).count / 2

var d = String(repeating: String(describing: c[j]), count: j)

d.append(Character(&#8220;&#127462;&#127482;&#127482;&#127480;&#8221;))

let result = &#8220;\(d): \(d.characters.count)&#8221;</code></pre>

<p>What is the value of &#8220;result&#8221;?</p>
<ul>
<li>A. ......&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>B. ......&#127462;&#127482;&#127482;&#127480;: 8</li>
<li>C. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>D. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 8</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>10. JavaScript</h3>
<pre><code class="language-javascript">function thing (n) {
    for (var i = 0; i &lt; n; i++) {
        setTimeout(function () {console.log(i);}, 0);
    }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Prints numbers 0 through n</li>
<li>B. Prints n n time</li>
<li>C. Prints 0 n times</li>
<li>D. Prints nothing</li>
</ul>
<!-- Correct: B -->

<hr>
<p>View HTML source to see the answers.</p>
<p>I tried to keep the above close to what appeared on the physical
cards. I haven't tried to correct the little mistakes and bizarre
indentation throughout.</p>
<p>It's fun!</p>    
    ]]></description>
<link>http://planspace.org/20170517-code_reading_questions_at_oscon/</link>
<guid>http://planspace.org/20170517-code_reading_questions_at_oscon/</guid>
<pubDate>Wed, 17 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Caffe and TensorFlow at Deep Learning Analytics</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">presentation</a> given Wednesday May 10, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a> as part of <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/full/tensorflow-day">TensorFlow Day</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<p>Thank you!</p>
<hr>
<p>Thanks to all the OSCON organizers and participants, and especially to the Google folks organizing TensorFlow Day, and everyone coming out for TensorFlow events! What fun!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png" width="100%"></p>
<hr>
<p>I work at a company called <a href="https://www.deeplearninganalytics.com/">Deep Learning Analytics</a>, or DLA.</p>
<p>The advantage of a name like Deep Learning Analytics is that, assuming you've heard of deep learning, you immediately know something about what we do.</p>
<hr>
<p>deeplearninganalytics.com</p>
<hr>
<p>The disadvantage of a name like Deep Learning Analytics is that our URL is really long.</p>
<hr>
<p><img alt="some of the Deep Learning Analytics team" src="img/dla_team.jpg" width="100%"></p>
<hr>
<p>This is most of us at our Arlington location, just outside DC. We do a mix of government contracting and commercial work.</p>
<hr>
<p><img alt="DarLA" src="img/darla.png" height="100%"></p>
<hr>
<p>We have a cute robot mascot called DarLA.</p>
<p>DarLA the robot could be a metaphor for any machine learning system. You have to build the arms and the legs, and that's a good deal of work. There's also generally a lot of work in getting data to train the system with. But you also need a brain, and that's inevitably sort of the interesting part.</p>
<p>So what do you use for the brain?</p>
<hr>
<p>warning: opinions</p>
<hr>
<p>I'm going to comment on a few aspects of a few systems, based on what I've seen. Everybody involved in this space is incredibly smart, and probably correct in some ways even when I disagree.</p>
<hr>
<ul>
<li>Caffe?</li>
<li>TensorFlow?</li>
</ul>
<hr>
<p>The two systems I'll talk about are Caffe and TensorFlow.</p>
<p>I'm aware of Caffe 2 but I don't have anything to say about it today, aside from that it seems a lot like Caffe.</p>
<hr>
<p><img src="img/nvidia_comparison.jpg" alt="DL framework comparison" height="100%"></p>
<hr>
<p>This is a picture of a slide from a talk some NVIDIA folks gave a couple weeks ago. I will now agree and disagree with it.</p>
<p>Caffe easy to start? I disagree, and I'll talk about why.</p>
<p>Caffe easy to develop? I disagree, and I'll talk about why.</p>
<p>Caffe limited capability? I agree, and I'll talk about why.</p>
<p>TensorFlow portable and nice documentation? I mostly agree.</p>
<p>TensorFlow slow? I think to the extent this is true, it doesn't matter, and I'll talk about some related considerations.</p>
<p>My colleagues have done some work in Torch as well, but I'm not going to talk about these others.</p>
<hr>
<p>Easy to start?</p>
<hr>
<p>So which framework makes it easier to get started?</p>
<hr>
<p><code>pip install:</code><br>
&#10008; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>This is a convenience, but it's also really nice. There are advantages to compiling things yourself too. But for quickly getting started, this is great.</p>
<p>Even if you're happy to compile an optimized build for your system, having easy installation options available can be nice for setting up test environments, for example.</p>
<hr>
<p>"Easy" models:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>By "easy" models, I mean models that are either completely pre-specified and pre-trained, or very easily specified by a very high-level API.</p>
<p>The Caffe model zoo might have been the first recognized collection of pre-trained models and model architecture specifications. One or two years ago, it might have been the best way to start working with an interesting deep net without going through the whole training process yourself.</p>
<p>Now, though, I think TensorFlow is really passing Caffe on this. You can get a number of pre-trained models already with just one line of Python, using <code>tf.contrib.keras</code>. Hopefully others will be able to match that ease of redistribution.</p>
<p>Further, TensorFlow's "canned models" in <code>tf.contrib.learn</code> provide a scikit-learn-like interface that Caffe never attempts.</p>
<hr>
<p>Easy to develop?</p>
<hr>
<p>What about ease of development?</p>
<hr>
<p>API level:</p>
<ul>
<li>high? middle? low?</li>
</ul>
<hr>
<p>For development, it matters what level you want to work at. You can do a lot quickly if you can stay at a high level. But sometimes you need to get low-level to control details or do something slightly non-standard.</p>
<p>I think TensorFlow now is doing a better job than Caffe of providing API surfaces across a range of levels.</p>
<p>Caffe's API, at the protocol buffer text format level you have to eventually get to, is sort of a middle-low level. The vocabulary is more limited than what you get with TensorFlow ops.</p>
<p>You can build higher-level APIs with Caffe, and DLA has an in-house library that we use to make Caffe easier to work with.</p>
<p>TensorFlow has Keras.</p>
<hr>
<p>Python integration:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>Both Caffe and TensorFlow are written with C++, but interfacing with Caffe can feel like interfacing with the separate free-standing program, whereas the TensorFlow interface is seamless.</p>
<p>If there's a way to use Caffe without at some point writing protobuf text files to disk and then having Caffe read them, I don't know it.</p>
<hr>
<p>modular design</p>
<hr>
<p>I'll show an example of a Caffe layer in order to talk about some of the design issues.</p>
<hr>
<pre><code>layer {name: "data"
       type: "Data"
       top: "data"
       top: "label"
       transform_param {crop_size: 227}
       data_param {source: "train_lmdb_path"
                   batch_size: 256
                   backend: LMDB}}</code></pre>

<hr>
<p>I condensed a Caffe data layer spec a bit.</p>
<p>Caffe likes reading data from LMDB databases. So this layer includes a path to a location on disk, where an LMDB database of a particular form needs to be. So this is a layer that reads from disk.</p>
<p>But wait - this layer spec also specifies the training batch size.</p>
<p>Does it do anything else? What's that "<code>crop_size</code>"?</p>
<p>This layer also implicitly takes random crops from the images it reads, 227 pixels by 227 pixels. So the images in the database should be at least that big.</p>
<p>That's a lot happening in one layer!</p>
<p>On the one hand, this is pretty neat. Caffe does a lot for you. And once you've made a lot of choices, you can optimize the implementation, which is part of how Caffe gets pretty fast.</p>
<p>On the other hand, Caffe is making a lot of decisions for us, and it isn't particularly happy if we want to change those decisions. Caffe can feel more like final application code than framework code.</p>
<p>What if we want non-square crops? What if we want to introduce random distortions? Or if we want to read data from JPG files instead of LMDB? None of these are supported by this LMDB layer, and these changes aren't easily composable. We can switch to a memory data layer to get training batches in, but then we have to independently implement cropping if we want it, and so on.</p>
<p>In contrast, TensorFlow doesn't have such tight integrations. The closest analog in TensorFlow might be reading from TFRecords files, but that functionality is isolated, and doesn't bundle in choices about batch size or cropping.</p>
<p>You get a more modular system with TensorFlow, so you shouldn't find yourself having to pick apart tightly-coupled functionality later on.</p>
<p>At the same time, TensorFlow does include the higher-level APIs mentioned earlier, so it isn't completely a choice between low-level and high-level either.</p>
<hr>
<p>multi-GPU</p>
<hr>
<p>How do we get multi-GPU systems?</p>
<hr>
<p><img src="img/alexnet.png" alt="AlexNet diagram" width="100%"></p>
<hr>
<p>This is the diagram from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>, the 2012 paper that popularized deep learning. It has over 11,000 citations now.</p>
<p>This was a multi-GPU implementation!</p>
<p>I want to point out how crazy it is. Alex Krizhevsky split individual convolutional layers between two GPUs. Whoa.</p>
<hr>
<p>Caffe</p>
<hr>
<p>So how do we do multi-GPU with Caffe?</p>
<hr>
<p><img alt="multi-GPU Caffe docs" src="img/caffe_multi.png"></p>
<hr>
<p>For Caffe, there's this page of documentation. As far as I know there is no link to this page of documentation. I found it <a href="https://github.com/BVLC/caffe/blob/master/docs/multigpu.md">via</a> the GitHub repo that backs the website.</p>
<p>There are some limitations in Caffe's multi-GPU support.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>Well, how do we do multi-GPU with TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same way we do single-GPU! The API stays the same, and you just say where you want things to run. This is also more flexible in terms of the ways you can parallelize things. You could have two copies of a model on two GPUs, or one really big model across two GPUs, for example.</p>
<hr>
<p><img alt="Jenn" src="img/jenn.jpg"></p>
<hr>
<p>My colleague Jenn parallelized some GAN (Generative Adversarial Network) code to use multiple GPUs. I was super curious to see what she had to do. She was really puzzled about why I was so curious, because TensorFlow made it so easy.</p>
<p>Also, if you're curious about GANs, you should really check out Jenn's <a href="https://github.com/jennsleeman/introtogans_dcdatascience_2017">talk on the subject</a>.</p>
<hr>
<p>multi-machine</p>
<hr>
<p>What if we want to train across multiple machines now?</p>
<hr>
<p>Caffe</p>
<hr>
<p>How about Caffe?</p>
<hr>
<p><img src="img/trivial_mpi.png" alt="trivial MPI" width="100%"></p>
<hr>
<p>Caffe doesn't do it, but Yangqing helpfully <a href="https://github.com/BVLC/caffe/issues/876">points out</a> that you could make your own distributed Caffe system by adding MPI. Some people have actually <a href="https://software.intel.com/en-us/articles/caffe-training-on-multi-node-distributed-memory-systems-based-on-intel-xeon-processor-e5">done</a> this. I'm not sure it's trivial.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>And TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same API! You just put computation on various machines!</p>
<p>There is a little bit more to it than that, but I think it's easier than MPI.</p>
<hr>
<p><img alt="infrastructure" src="img/infr.png"></p>
<hr>
<p>Another thing we're excited about is possibilities for scaling out our work with TensorFlow. OpenAI <a href="https://blog.openai.com/infrastructure-for-deep-learning/">published</a> information and code on their system, and it seems like a pretty great model.</p>
<hr>
<p>responsiveness</p>
<hr>
<p>I'm going to talk about one anecdotal measures of developer responsiveness. It isn't totally fair, because Caffe development is basically over as Caffe 2 emerges, but it may still be interesting.</p>
<p>I occasionally submit very minor pull requests, for things like ensuring a variable is the correct type, or fixing typos in documentation. These are simple PRs that don't generally require discussion or alteration; they just get merged in.</p>
<p>So how quickly does that happen with Caffe and with TensorFlow?</p>
<hr>
<p>merge time:</p>
<ul>
<li>TensorFlow: 12 hours</li>
<li>Caffe: 329 days</li>
</ul>
<hr>
<p>TensorFlow developers seem more responsive.</p>
<p>(<a href="https://github.com/tensorflow/tensorflow/pull/9451">TensorFlow PR</a>)</p>
<p>(<a href="https://github.com/BVLC/caffe/pull/4182">Caffe PR</a>)</p>
<hr>
<p><img alt="TensorFlow logo" src="img/tf.jpg" height="100%"></p>
<hr>
<p>And I think that, along with many other things, bodes well for the future of TensorFlow.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>    
    ]]></description>
<link>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</link>
<guid>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</guid>
<pubDate>Wed, 10 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Building TensorFlow Systems from Components</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">workshop</a> given Tuesday May 9, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<h3>Workshop participant materials</h3>
<p>These are things you can use during the workshop. Use them when you need them.</p>
<ul>
<li>Focus one: <em>getting data in</em><ul>
<li>Download notebook: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
<li><a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a></li>
</ul>
</li>
<li>Focus two: <em>distributed programs</em><ul>
<li><a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/"><code>mapreduce_with_tensorflow</code> code and data</a></li>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
</li>
<li>Focus three: <em>high-level ML APIs</em><ul>
<li>Download notebook: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
</li>
</ul>
<p>The whole slide deck and everything follows, and it's long.</p>
<hr>
<p>Thank you!</p>
<hr>
<p>Before starting, I want to take a moment to notice how great it is to have a conference. Thank you for being here! We are all really lucky. If this isn't nice, I don't know what is.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<p>If you want to participate in the workshop, <em>really go</em> to <code>planspace.org</code> and pull up these materials!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work for a company called Deep Learning Analytics.</p>
<p>I'll talk more about DLA tomorrow <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">as part of TensorFlow Day</a>. Hope to see you there as well!</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>You'll want to have TensorFlow version 1.1 installed. TensorFlow 1.1 was only officially released on April 20, and the API really has changed.</p>
<p>The <em>core documented</em> API, mostly the low-level API, is frozen as of TensorFlow 1.0, but a lot of higher-level stuff is still changing. TensorFlow 1.1 brings in some really neat stuff, like Keras, which we'll use later.</p>
<hr>
<p><img src="img/hello_tensorflow.png" alt="Hello, TensorFlow! on O'Reilly" height="100%"></p>
<hr>
<p>As an example: I wrote <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> about a year ago. It used TensorFlow 0.8.</p>
<p>After TensorFlow 1.0 came out, I went and looked at the summary code block at the end of "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>". It had 15 lines of TensorFlow code. Of that, 5 lines no longer worked. I had to update 33% of the lines of my simple TensorFlow example code because of API changes.</p>
<hr>
<p><img alt="one third" src="img/one_third.jpg"></p>
<hr>
<p>I was surprised that it was one third.</p>
<p>I have an <a href="/20160620-hello_tensorflow_just_the_code/">updated code snippet</a>, but we're not doing "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>" today.</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>So please have TensorFlow 1.1 installed, is the point of that whole story.</p>
<p>The stable bits of TensorFlow really are stable, but there's a lot of exciting new stuff, and I don't want you to miss out today!</p>
<hr>
<p>THE BIG IDEA</p>
<hr>
<p>Okay! What is this workshop about?</p>
<hr>
<p>use what you need</p>
<hr>
<p>TensorFlow works for you! Use it where it does things that help you.</p>
<p>TensorFlow is a tool. It's a very general tool, on the one hand. It's also a tool with lots of pieces. You can use <a href="/20170312-use_only_what_you_need_from_tensorflow/">some</a> of the pieces. Or you can decide not to use TensorFlow altogether!</p>
<p>We'll look at several specific aspects of TensorFlow. Maybe you'll want to use them. Maybe you won't. The hope is that you'll be more comfortable with what's available and able to decide what to apply when.</p>
<p>If you like <a href="https://www.infoq.com/presentations/Simple-Made-Easy">Rich Hickey words</a>, maybe I'm trying to <em>decomplect</em> the strands within TensorFlow so they can be understood individually.</p>
<hr>
<p>THE PLAN</p>
<hr>
<p>Here's the plan for this workshop.</p>
<hr>
<ul>
<li>one short work</li>
<li>three longer works</li>
</ul>
<hr>
<p>I'll talk about some things, but the hope of the workshop is that you do some good work.</p>
<p>I hope that you don't finish all the things you think of, but want to keep working after the workshop is over.</p>
<p>Also, there's a break from 10:30 to 11:00 on the official schedule, but I don't really like that. If we happen to be working during the break, fine. Feel free to take a break whenever you need a break.</p>
<hr>
<p>Do what you want!</p>
<hr>
<p>TensorFlow is really big, and not every part will be interesting or important to you.</p>
<p>I'll have very specific things for you to work on for each of the works, but if you think of something better to do, you better do it!</p>
<hr>
<p>Intro by Logo</p>
<hr>
<p>To get creative juices flowing a little, let's explore some logo history.</p>
<hr>
<p><img alt="LOGO turle" src="img/logo_turtle.jpg" height="100%"></p>
<hr>
<p>Not that sort of <a href="https://en.wikipedia.org/wiki/Logo_(programming_language)">Logo</a>!</p>
<p>Fine to think about, though.</p>
<hr>
<p><img alt="G&#246;del, Escher, Bach" src="img/geb.jpg" height="100%"></p>
<hr>
<p>In the beginning, there was <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">G&#246;del, Escher, Bach: An Eternal Golden Braid</a> by Douglas Hofstadter.</p>
<p>This "metaphorical fugue on minds and machines in the spirit of Lewis Carroll" includes the <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> idea and related discussion of consciousness and formal systems. Hofstadter's influence can be seen, for example, <a href="https://cs.illinois.edu/news/strange-loop-conference">in the name</a> of the <a href="http://www.thestrangeloop.com/">Strange Loop</a> tech conference.</p>
<p>It seems likely that many people working in artificial intelligence and machine learning have encountered G&#246;del, Escher, Bach.</p>
<hr>
<p><img src="img/chernin.jpg" alt="Chernin Entertainment" height="100%"></p>
<hr>
<p>Of course, there's also <a href="https://en.wikipedia.org/wiki/Chernin_Entertainment">Chernin Entertainment</a>, the production company.</p>
<p>So we shouldn't rule out the possibility that Google engineers are fans of <a href="http://www.imdb.com/company/co0286257/">Chernin's work</a>. <a href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden Figures</a> is quite good. And I guess a lot of people like <a href="https://en.wikipedia.org/wiki/New_Girl">New Girl</a>?</p>
<hr>
<p><img alt="TensorFlow logo - old?" src="img/tf-old-big.png" height="100%"></p>
<hr>
<p>In any event, somehow we get to this TensorFlow logo:</p>
<p>If you look carefully, does it seem like the right side of the "T" view is too short?</p>
<hr>
<p><img alt="Issue 1922" src="img/issue_1922.png"></p>
<hr>
<p>This very serious concern appears as <a href="https://github.com/tensorflow/tensorflow/issues/1922">issue #1922</a> on the TensorFlow github, and <a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XhO1sqp4l4g">on the TensorFlow mailing list</a> complete with ASCII art illustration.</p>
<p>The consensus response seemed to be some variant of "won't fix" (it wouldn't look as cool, anyway) until...</p>
<hr>
<p><img alt="TensorFlow logo - new?" src="img/tf-new.jpg" height="100%"></p>
<hr>
<p>As of around the 1.0 release of TensorFlow, which was around the first <a href="https://events.withgoogle.com/tensorflow-dev-summit/">TensorFlow Dev Summit</a>, this logo variant seems to be in vogue. It removes the (possibly contentious) shadows, and adds additional imagery in the background.</p>
<p>I want to suggest that the image in the background can be a kind of Rorschach test for at least three ways you might be thinking about TensorFlow.</p>
<ul>
<li>Is it a diagram of connected neurons? Maybe you're interested in TensorFlow because you want to make neural networks.</li>
<li>Is it a diagram of a computation graph? Maybe you're interested in TensorFlow for general calculation, possibly using GPUs.</li>
<li>Is it a diagram of multiple computers in a distributed system? Maybe you're interested in TensorFlow for its distributed capabilities.</li>
</ul>
<p>There can be overlap among those interpretations as well, but I hope the point is not lost that TensorFlow can be different things to different people.</p>
<hr>
<p>short work</p>
<hr>
<p>I want to get you thinking about systems without further restriction. The idea is to imagine and start fleshing out a system that might involve TensorFlow.</p>
<p>Later on we'll follow the usual workshop pattern of showing you something you can do and having you do it. But it's much more realistic and interesting to start from something you want to do and then try to figure out how to do it. So let's start from the big picture.</p>
<hr>
<p>draw a system!</p>
<ul>
<li>block diagram</li>
<li>add detail</li>
<li>pseudocode?</li>
</ul>
<hr>
<p>You can draw a system you've already made, or something you're making, or something you'd like to make. It could be something you've heard about, or something totally unique.</p>
<p>You don't have to know how to build everything in the system. You don't need to know how TensorFlow fits in. Feel free to draw what you <em>wish</em> TensorFlow might let you do.</p>
<p>Keep adding more detail. If you get everything laid out, start to think about what functions or objects you might need to have, and start putting pseudocode together.</p>
<hr>
<p>short work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p>Let's do hard AI!</p>
<hr>
<p>I'm going to walk through an example and talk about how TensorFlow can fit in.</p>
<p>Doing hard AI, or Artificial General Intelligence (AGI), is intended as a joke, but it's increasingly less so, with Google DeepMind and OpenAI both explicitly working on getting to AGI.</p>
<hr>
<p>(build)</p>
<hr>
<p>I've got 35 slides of sketches, so they're in <a href="build.pdf">a separate PDF</a> to go through.</p>
<hr>
<p><img alt="system" src="img/system.jpg"></p>
<hr>
<p>Here's the final system that we built to.</p>
<p>We're not going to do everything in there today.</p>
<hr>
<ul>
<li>getting data in</li>
<li>distributed programs</li>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>These are the three focuses for today.</p>
<p>The <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">original description</a> for this workshop also mentioned serving, but I'm not covering it today. Sorry. Not enough time.</p>
<hr>
<p>What even is TensorFlow?</p>
<hr>
<p>Here's one way to think about TensorFlow.</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>The beating heart of TensorFlow is the Distributed Execution Engine, or runtime.</p>
<p>One way to think of it is as a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">virtual machine</a> whose language is the TensorFlow graph.</p>
<p>That core is written in C++, but lots of TensorFlow functionality lives in the frontends. In particular, there's a lot in the Python frontend.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<p>Python is the richest frontend for TensorFlow. It's what we'll use today.</p>
<p>You should remember that not everything in the Python TensorFlow API touches the graph. Some parts are just Python.</p>
<hr>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>R has an unofficial <a href="https://rstudio.github.io/tensorflow/">TensorFlow API</a> which is kind of interesting in that it just wraps the Python API. It's really more like an R API to the Python API to TensorFlow. So when you use it, you write R, but Python runs. This is not how TensorFlow encourages languages to implement TensorFlow APIs, but it's out there.</p>
<hr>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<p>The way TensorFlow encourages API development is via TensorFlow's C bindings.</p>
<hr>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<p>You could use C++, of course.</p>
<hr>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<p>Also there's Java.</p>
<hr>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<p>And there's Go.</p>
<hr>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<p>And there's Rust.</p>
<hr>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<p>And there's even Haskell!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Currently, languages other than Python have TensorFlow support that is very close to the runtime. Basically make your ops and run them.</p>
<p>This is likely to be enough support to deploy a TensorFlow system in whatever language you like, but if you're developing and training systems you probably still want to use Python.</p>
<hr>
<p>graph or not graph?</p>
<hr>
<p>So this is a distinction to think about: Am I using the TensorFlow graph, or not?</p>
<hr>
<ul>
<li>getting data in</li>
</ul>
<hr>
<p>So here we are at the first focus area.</p>
<p>We'll do two sub-parts.</p>
<hr>
<ul>
<li>getting data in<ul>
<li>to the graph</li>
<li>TFRecords</li>
</ul>
</li>
</ul>
<hr>
<p>First, a quick review of the the graph and putting data into it.</p>
<p>Second, a bit about TensorFlow's TFRecords format.</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
</ul>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>work with data!</p>
<ul>
<li>your own system?</li>
<li>planspace.org: <code>mystery.tfrecords</code></li>
<li>move to graph</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Almost certainly there's some data input that needs to happen. How are you going to read that data, and possibly get it into TensorFlow?</p>
<p>If you want to stay really close to the TensorFlow stuff just demonstrated, here's a fun little puzzle for you: What's in <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a>?</p>
<p>That could be hard, or it could be easy. If you want to extend it, migrate the reading and parsing of the TFRecords file into the TensorFlow graph. The demonstration in the notebook worked with TFRecords/Examples without using the TensorFlow graph. The links on the <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a> page can help with this.</p>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="data" src="img/data_data.jpg" width="100%"></p>
<hr>
<p>As a wrap-up: Working directly with data, trying to get it into the right shape, cleaning it, etc., may not be the most fun, but it's got to be done. Here's a horrible pie chart <a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/">from some guy on Forbes</a>, the point of which is that people spend a good deal of time fighting with data.</p>
<hr>
<ul>
<li>distributed programs</li>
</ul>
<hr>
<p>We arrive at the second focus area. TensorFlow has some pretty wicked distributed computing capabilities.</p>
<hr>
<ul>
<li>distributed programs<ul>
<li>command-line arguments</li>
<li>MapReduce example</li>
</ul>
</li>
</ul>
<hr>
<p>I'm putting a bit about command-line arguments in here because I think it's interesting. It doesn't necessarily fit in with distributed computing, although you might well have a distributed program that takes command-line arguments. Google Cloud ML can use command-line arguments for hyperparameters, for example.</p>
<p>Then we'll get to a real distributed example, in which we implement a distributed MapReduce word count in 50 lines of Python TensorFlow code.</p>
<hr>
<p>command-line arguments</p>
<hr>
<p>So let's take a look at some ways to do command-line arguments.</p>
<p>This section comes from the post <a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a>.</p>
<hr>
<pre><code class="language-bash">$ python script.py --color red
a red flower</code></pre>

<hr>
<p>I'll show eight variants that all do the same thing. You provide a <code>--color</code> argument, and it outputs (in text) a flower of that color.</p>
<hr>
<pre><code class="language-python">import sys

def main():
    assert sys.argv[1] == '--color'
    print('a {} flower'.format(sys.argv[2]))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This is a bare-bones <code>sys.argv</code> method. Arguments become elements of the <code>sys.argv</code> list, and can be accessed as such. This has limitations when arguments get more complicated.</p>
<hr>
<pre><code class="language-python">import sys
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main():
    flags.FLAGS._parse_flags(args=sys.argv[1:])
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This <code>FLAGS</code> API is <a href="/20170313-tensorflow_use_of_google_technologies/">familiar to Googlers</a>, I think. It's interesting to me where some Google-internal things peak out from the corners of TensorFlow.</p>
<hr>
<pre><code class="language-python">import sys
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main():
    gflags.FLAGS(sys.argv)
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>You could also install the <code>gflags</code> module, which works much the same way.</p>
<hr>
<pre><code class="language-python">import sys
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args(sys.argv[1:])
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>Here's Python's own standard <a href="https://docs.python.org/3/library/argparse.html"><code>argparse</code></a>, set up to mimic the <code>gflags</code> example, still using <code>sys.argv</code> explicitly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

def main(args):
    assert args[1] == '--color'
    print('a {} flower'.format(args[2]))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>Using <code>tf.app.run()</code>, another Google-ism, frees us from accessing <code>sys.argv</code> directly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main(args):
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>We can combine <code>tf.app.run()</code> with <code>tf.app.flags</code>.</p>
<hr>
<pre><code class="language-python">import google.apputils.app
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main(args):
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    google.apputils.app.run()</code></pre>

<hr>
<p>To see the equivalent outside the TensorFlow package, we can combine <code>gflags</code> and <code>google.apputils.app</code>.</p>
<hr>
<pre><code class="language-python">import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args()
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This has all been fun, but here's what you should really do. Just use <code>argparse</code>.</p>
<hr>
<p>Whew!</p>
<hr>
<p>That was a lot of arguing.</p>
<p>It may be worth showing all these because you'll encounter various combinations as you read code out there in the world. More recent examples are tending to move to <code>argparse</code>, but there are some of the other variants out there as well.</p>
<hr>
<p>MapReduce example</p>
<hr>
<p>Now to the MapReduce example!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Recall that the core of TensorFlow is a <em>distributed</em> runtime. What does that mean?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>Behold, the power of <code>tf.device()</code>!</p>
<hr>
<p></p><pre><code>with tf.device('/cpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local CPU.</p>
<hr>
<p></p><pre><code>with tf.device('/gpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local GPU.</p>
<hr>
<p></p><pre><code>with tf.device('/job:ps/task:0'):
    # Do something.</code></pre>
<hr>
<p>In exactly the same way, you can specify that work happen on <em>a different computer</em>!</p>
<p>This is pretty amazing. I think of it as sort of <a href="/20170423-tensorflow_as_automatic_mpi/">declarative MPI</a>.</p>
<hr>
<p><img alt="distributing a TensorFlow graph" src="img/distributed_graph.png"></p>
<hr>
<p>TensorFlow automatically figures out when it needs to send information between devices, whether they're on the same machine or on different machines. So cool!</p>
<hr>
<p><img alt="oh map reduce..." src="img/oh_map_reduce.png"></p>
<hr>
<p>MapReduce is often associated with Hadoop. It's just divide and conquer.</p>
<p>So let's do it with TensorFlow!</p>
<hr>
<p>(demo)</p>
<hr>
<p>It's time to see how this looks in practice! (Well, or at least to see how it looks in a cute little demo.)</p>
<p>The demo uses the contents of the <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">mapreduce_with_tensorflow</a> repo on GitHub. For more explanation, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a> and <a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a>.</p>
<hr>
<p>(code)</p>
<hr>
<p>Walking through some details inside <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/blob/master/count.py"><code>count.py</code></a>.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>different distributed functionality?</li>
<li>add command-line?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Maybe command-line args are relevant to you. Maybe running a system across multiple machines is relevant for you. Or maybe not.</p>
<p>If you want to exercise the things just demonstrated, you could add a command-line argument to the distributed word-count program. For example, you could make it count only a particular word, or optionally count characters, or something else. Or you could change the distributed functionality without any command-line fiddling. (You could make it a distributed neural net training program, for example.)</p>
<p>Here are some links that might be helpful:</p>
<ul>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="oh kubernetes..." src="img/oh_kubernetes.jpg"></p>
<hr>
<p>I should probably say that you don't really want to start all your distributed TensorFlow programs by hand. <a href="https://www.docker.com/">Containers</a> and <a href="https://kubernetes.io/">Kubernetes</a> and all that.</p>
<hr>
<ul>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>Let's to some machine learning!</p>
<hr>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<hr>
<p>New and exciting things are being added in TensorFlow Python land, building up the ladder of abstraction.</p>
<p>This material comes from <a href="/20170321-various_tensorflow_apis_for_python/">Various TensorFlow APIs for Python</a>.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="slim... shady?" src="img/slim_shady.png" height="100%"></p>
<hr>
<p>The "layer" abstractions largely from TF-Slim are now appearing at <code>tf.layers</code>.</p>
<hr>
<p><img alt="scikit-learn logo" src="img/sklearn.png" height="100%"></p>
<hr>
<p>The Estimators API now at <code>tf.estimator</code> is drawn from <code>tf.contrib.learn</code> work, which is itself heavily inspired by scikit-learn.</p>
<hr>
<p><img alt="Keras" src="img/keras.jpg" height="100%"></p>
<hr>
<p>And Keras is entering TensorFlow first as <code>tf.contrib.keras</code> and soon just <code>tf.keras</code> with version 1.2.</p>
<hr>
<ul>
<li>high-level ML APIs<ul>
<li>training an Estimator</li>
<li>pre-trained Keras</li>
</ul>
</li>
</ul>
<hr>
<p>So let's try this out!</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
</ul>
<p>There's also a TensorBoard demo baked in there. A couple backup slides follow.</p>
<hr>
<p><img alt="graph" src="img/tensorboard_graph.png"></p>
<hr>
<p>This is what the graph should look like in TensorBoard.</p>
<hr>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<hr>
<p>Steps per second should look something like this.</p>
<hr>
<p><img alt="loss" src="img/loss.png"></p>
<hr>
<p>Loss should look like this.</p>
<hr>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg" height="100%"></p>
<hr>
<p>I just enjoy this image too much not to share it.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>flip regression to classification?</li>
<li>classify some images?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Do you need a model of some kind in your system? Maybe you can use TensorFlow's high-level machine learning APIs.</p>
<p>If you want to work with the stuff just shown some more, that's also totally cool! Instead of simple regression, maybe you want to flip the presidential GDP problem around to be logistic regression. TensorFlow has <code>tf.contrib.learn.LinearClassifier</code> for that. And many more variants!</p>
<p>Or maybe you want to classify your own images, or start to poke around the model some more. Also good! If you want more example images, there's <a href="https://github.com/ajschumacher/imagen">this set</a>.</p>
<p>Here are some links that could be helpful:</p>
<ul>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<hr>
<p>Oh my! Are we out of time already?</p>
<hr>
<p>What else?</p>
<hr>
<p>There are a lot of things we haven't covered.</p>
<hr>
<p>debugging, optimizing (XLA, low-precision, etc.), serving, building custom network architectures, embeddings, recurrent, generative, bazel, protobuf, gRPC, queues, threading...</p>
<hr>
<p>Here's a list of the first things that came to mind.</p>
<p>I hope you continue to explore!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170509-building_tensorflow_systems_from_components/</link>
<guid>http://planspace.org/20170509-building_tensorflow_systems_from_components/</guid>
<pubDate>Tue, 09 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Regression with a TensorFlow Estimator</title>
<description><![CDATA[

<p>With TensorFlow 1.1, the <a href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators">Estimator</a> API is now at <code>tf.estimator</code>. A number of "canned estimators" are <a href="https://www.tensorflow.org/extend/estimators">at</a> <code>tf.contrib.learn</code>. This higher-level API bakes in some best practices and makes it much easier to do a lot quickly with TensorFlow, similar to using APIs available in other languages.</p>
<hr>
<h2>Data</h2>
<p>This example will use the very simple <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/">US Presidential Party and GDP Growth dataset</a>: <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/president_gdp.csv">president_gdp.csv</a>.</p>
<p>The regression problem will be to predict annualized percentage GDP growth from presidential party.</p>
<hr>
<h2>R</h2>
<p><a href="https://www.r-project.org/">R</a> is made for problems such as this, with an API that makes it quite easy:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; model = lm(growth ~ party, data)
&gt; predict(model, data.frame(party=c('R', 'D')))
##        1        2
## 2.544444 4.332857</code></pre>

<p>The dataset is very small, and we won't introduce a train/test split. Linear regression is just a way of calculating means: we expect our model to predict the mean GDP growth conditional on party. Annual GDP growth during Republican presidents has been about 2.5%, and during Democratic presidents about 4.3%.</p>
<hr>
<h2>sklearn</h2>
<p>Moving into Python, let's first read in the data and get it ready, using <a href="http://www.numpy.org/">NumPy</a> and <a href="http://pandas.pydata.org/">Pandas</a>.</p>
<pre><code class="language-python">import numpy as np
import pandas as pd

data = pd.read_csv('president_gdp.csv')
party = data.party == 'D'
party = np.expand_dims(party, axis=1)
growth = data.growth</code></pre>

<p>With R, we relied on automatic handling of categorical variables. Here we explicitly change the strings 'R' and 'D' to be usable in a model: Boolean values will become zeros and ones. We also adjust the <code>party</code> data shape to be one row per observation.</p>
<p>Tracking <a href="/20170321-various_tensorflow_apis_for_python/">TensorFlow Python APIs</a>, the Estimator API comes from TF Learn, which is inspired by <a href="http://scikit-learn.org/">scikit-learn</a>. Here's the regression with scikit:</p>
<pre><code class="language-python">import sklearn.linear_model

model = sklearn.linear_model.LinearRegression()
model.fit(X=party, y=growth)
model.predict([[0], [1]])
## array([ 2.54444444,  4.33285714])</code></pre>

<hr>
<h2>TensorFlow</h2>
<p>This will abuse the API a little to maximize comparability to the examples above; you'll see warnings when you run the code, which will be addressed in the next section.</p>
<pre><code class="language-python">import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col])</code></pre>

<p>Unlike with scikit, we need to specify the structure of our regressors when we instantiate the model object. This is done with FeatureColumns. There are several <a href="https://www.tensorflow.org/tutorials/wide#selecting_and_engineering_features_for_the_model">options</a>; <code>real_valued_column</code> is probably the simplest but others are useful for general categorical data, etc.</p>
<p>We're providing that data as a simple matrix, so it's important that we use the empty string <code>''</code> for <code>column_name</code>. If there is a substantial <code>column_name</code>, we'll have to provide data in dictionaries with column names as keys.</p>
<pre><code class="language-python">model.fit(x=party, y=growth, steps=1000)
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>TensorFlow needs to be told how many steps of gradient descent to run, or it will keep going indefinitely, without additional configuration. A thousand iterations gets very close to the results achieved with R and with scikit.</p>
<p>There are a lot of things that <code>LinearRegressor</code> takes care of. In this code, we did not have to explicitly:</p>
<ul>
<li>Create any TensorFlow variables.</li>
<li>Create any Tensorflow ops.</li>
<li>Choose an optimizer or learning rate.</li>
<li>Create a TensorFlow session.</li>
<li>Run ops in a session.</li>
</ul>
<p>This API also does a lot more than the R or scikit examples above, and allows for even more extensions.</p>
<hr>
<h2>TensorFlow Extensions</h2>
<p>The Estimator API does a lot by default, and allows for a lot more optionally.</p>
<p>First, there is a <code>model_dir</code>. Above, TensorFlow automatically used a temporary directory. It's nicer to explicitly choose a <code>model_dir</code>.</p>
<pre><code class="language-python">model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')</code></pre>

<p>The <code>model_dir</code> is used for two main purposes:</p>
<ul>
<li>Saving TensorBoard summaries (log info)</li>
<li>Saving model checkpoints</li>
</ul>
<h3>Automatic TensorBoard</h3>
<p><a href="/20170430-tensorflows_queuerunner/">Like</a> an <code>input_producer</code>, an Estimator automatically writes information for TensorBoard. To check them out, point TensorBoard at the <code>model_dir</code> and browse to <code>localhost:6006</code>.</p>
<pre><code class="language-bash">$ tensorboard --logdir tflinreg</code></pre>

<p>For the example above, we get the model graph and two scalar summaries.</p>
<p>Here's what was was constructed in the TensorFlow graph for our <code>LinearRegressor</code>:</p>
<p><img alt="graph" src="img/graph.png"></p>
<p>In the scalar summaries, we get a measure of how fast the training process was running, in global steps per second:</p>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<p>The variation in speed shown here is not particularly meaningful.</p>
<p>And we get the training loss:</p>
<p><img alt="loss" src="img/loss.png"></p>
<p>We didn't really need to train for a full thousand steps.</p>
<p>By default, summaries are generated every 100 steps, but this can be set via <code>save_summary_steps</code> in a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>, along with several other settings.</p>
<p>Further customization, with support for additional metrics, validation on separate data, and even automatic early stopping, is available with <a href="https://www.tensorflow.org/get_started/monitors">ValidationMonitor</a>.</p>
<h3>Automatic Model Save/Restore</h3>
<p>After training for 1,000 steps above, TensorFlow saved the model to the <code>model_dir</code>. If we point to the same <code>model_dir</code> again in a new Python session, the model will be automatically restored from that checkpoint.</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>For more control over how often and when checkpoints are saved, see <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>.</p>
<h3>Using input functions</h3>
<p>Above, training data was provided via <code>x</code> and <code>y</code> arguments, which is like how scikit works, but not really what TensorFlow Estimators should use.</p>
<p>The appropriate mechanism is to make an <a href="https://www.tensorflow.org/get_started/input_fn">input function</a> that returns the equivalents to <code>x</code> and <code>y</code> when called. The function is passed as the <code>input_fn</code> argument to <code>model.fit()</code>, for example.</p>
<p>This approach is flexible and makes it easy to avoid, for example, keeping track of separate data structures for data and labels.</p>
<h3>Distributed Training</h3>
<p>Among the <code>tf.contrib.learn</code> <a href="https://www.tensorflow.org/api_guides/python/contrib.learn">goodies</a> is <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"><code>tf.contrib.learn.Experiment</code></a>, which works with an Estimator to help do distributed training. It looks like this one is still settling down, with a lot of deprecated bits at the moment. I'm interested to see more about this. For now, you could check out a Google Cloud ML <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/iris/trainer/task.py">example</a> that works with <code>learn_runner</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</link>
<guid>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</guid>
<pubDate>Sat, 06 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Dataset: US Presidential Party and GDP Growth</title>
<description><![CDATA[

<p>Here's a simple <a href="president_gdp.csv">dataset</a> for use in examples. It's taken from the <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">online appendix</a> to <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20140913">Presidents and the US Economy: An Econometric Exploration</a> by Blinder and Watson.</p>
<p><a href="president_gdp.csv"><code>president_gdp.csv</code></a></p>
<p>The fields are:</p>
<ul>
<li><code>term</code>: A short text description of the presidential term, like "Reagan 2".</li>
<li><code>party</code>: The political party of the presidency, either "D" for the <a href="https://en.wikipedia.org/wiki/Democratic_Party_(United_States)">Democratic Party</a> or "R" for the <a href="https://en.wikipedia.org/wiki/Republican_Party_(United_States)">Republican Party</a>.</li>
<li><code>growth</code>: The average annualized growth in US Gross Domestic Product (<a href="https://en.wikipedia.org/wiki/Gross_domestic_product">GDP</a>) for that presidential term, expressed as percentage points.</li>
</ul>
<p>For more details, please see the <a href="http://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.20140913">paper</a> and <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">appendix</a>.</p>
<p>The only changes I've made are to order the data chronologically and put it in the convenient CSV format.</p>
<p>For example, in <a href="https://www.r-project.org/">R</a>, you can do the following:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; lm(growth ~ party, data)
&gt; t.test(growth ~ party, data)</code></pre>

<p>Because the <a href="president_gdp.csv">dataset</a> is so small, I'll also display it as spaced-out text here:</p>
<pre><code>term,             party,  growth
Truman,               D,    6.57
Eisenhower 1,         R,    2.72
Eisenhower 2,         R,    2.26
Kennedy-Johnson,      D,    5.74
Johnson 2,            D,    4.95
Nixon 1,              R,    3.57
Nixon-Ford,           R,    1.97
Carter,               D,    3.56
Reagan 1,             R,    3.12
Reagan 2,             R,    3.89
G.H.W. Bush,          R,    2.05
Clinton 1,            D,    3.53
Clinton 2,            D,    4.00
G.W. Bush 1,          R,    2.78
G.W. Bush 2,          R,    0.54
Obama 1,              D,    1.98</code></pre>    
    ]]></description>
<link>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</link>
<guid>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</guid>
<pubDate>Fri, 05 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-trained Models with Keras in TensorFlow</title>
<description><![CDATA[

<p>With TensorFlow 1.1, <a href="https://github.com/fchollet/keras">Keras</a> is now at <code>tf.contrib.keras</code>. With TensorFlow 1.3, it should be at <code>tf.keras</code>. This is great for making new models, but we also get the pre-trained models of <a href="https://github.com/fchollet/keras/tree/master/keras/applications"><code>keras.applications</code></a> (<a href="https://github.com/fchollet/deep-learning-models">also seen elsewhere</a>). It's so easy to classify images!</p>
<pre><code class="language-python">import tensorflow as tf

model = tf.contrib.keras.applications.ResNet50()</code></pre>

<p>This will automatically download trained weights for a model based on <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>. The weights are cached below your home directory, in <code>~/.keras/models/</code>.</p>
<p>Convenient image tools are also included. Let's use an <a href="https://github.com/ajschumacher/imagen/blob/master/imagen/n01882714_4157_koala_bear.jpg">image</a> of a koala from the <a href="https://github.com/ajschumacher/imagen">imagen</a> ImageNet subset.</p>
<p><img alt="original koala" src="n01882714_4157_koala_bear.jpg"></p>
<pre><code class="language-python">filename = 'n01882714_4157_koala_bear.jpg'
image = tf.contrib.keras.preprocessing.image.load_img(
    filename, target_size=(224, 224))</code></pre>

<p>This model can take input images that are 224 pixels on a side, so we have to make our image that size. We're just doing it by squishing, in this case.</p>
<p><img alt="smaller koala" src="smaller_koala.jpg"></p>
<p>We'll make that into an array that the model can take as input.</p>
<pre><code class="language-python">import numpy as np

array = tf.contrib.keras.preprocessing.image.img_to_array(image)
array = np.expand_dims(array, axis=0)</code></pre>

<p>Now we can classify the image!</p>
<pre><code class="language-python">probabilities = model.predict(array)</code></pre>

<p>We have one thousand probabilities, one for each class the model knows about. To interpret the result, we can use another helpful function.</p>
<pre><code class="language-python">tf.contrib.keras.applications.resnet50.decode_predictions(probabilities)
## [[(u'n01882714', u'koala', 0.99466419),
##   (u'n02497673', u'Madagascar_cat', 0.0013330306),
##   (u'n01877812', u'wallaby', 0.00085774728),
##   (u'n02137549', u'mongoose', 0.00063530984),
##   (u'n02123045', u'tabby', 0.00056512095)]]</code></pre>

<p>Great success! The model is highly confident that it's looking at a koala. Not bad.</p>
<p>It's pretty fun that this kind of super-easy access to quite good pre-trained models is now available all within the TensorFlow package. Just <code>pip install</code> and go!</p>
<hr>
<p>The thousand ImageNet categories this model knows about include some things that are commonly associated with people, but not a "person" class. Still, just for fun, what will <code>ResNet50</code> say about me?</p>
<pre><code class="language-python">## [[(u'n02883205', u'bow_tie', 0.3144455),
##   (u'n03787032', u'mortarboard', 0.059674311),
##   (u'n02992529', u'cellular_telephone', 0.049916871),
##   (u'n04357314', u'sunscreen', 0.048197504),
##   (u'n04350905', u'suit', 0.03481029)]]</code></pre>

<p>I guess I'll take it?</p>
<p><img alt="Aaron" src="aaron.jpg"></p>
<hr>
<p><strong>Notes:</strong></p>
<p>The model may have been trained on the very koala picture we're testing it with. I'm okay with that. Feel free to test your own koala pictures!</p>
<p>There's also another function, <code>resnet50.preprocess_input</code>, which in theory should help the model work better, but my tests gave seemingly worse results when using that pre-processing. It would be used like this:</p>
<pre><code class="language-python">array = tf.contrib.keras.applications.resnet50.preprocess_input(array)</code></pre>

<p>Keras in TensorFlow also contains <code>vgg16</code>, <code>vgg19</code>, <code>inception_v3</code>, and <code>xception</code> models as well, along the same lines as <code>resnet50</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</link>
<guid>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</guid>
<pubDate>Tue, 02 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling ImageNet</title>
<description><![CDATA[

<p><a href="http://image-net.org/">ImageNet</a> is a standard image dataset. It's pretty big; just the IDs and URLs of the images take over a gigabyte of text. I collected a fun <a href="https://github.com/ajschumacher/imagen">sampling</a> for small-scale purposes.</p>
<hr>
<p>ImageNet is distributed primarily as a text file of <a href="http://image-net.org/download-imageurls">image URLs</a>. The compressed file is 334 megabytes. The unpacked file is 1.1 gigabytes.</p>
<pre><code class="language-bash">$ wget http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz
$ tar zxvf imagenet_fall11_urls.tgz
$ wc fall11_urls.txt
##  14197122 28414665 1134662781 fall11_urls.txt
$ head -3 fall11_urls.txt
## n00004475_6590   http://farm4.static.flickr.com/3175/2737866473_7958dc8760.jpg
## n00004475_15899  http://farm4.static.flickr.com/3276/2875184020_9944005d0d.jpg
## n00004475_32312  http://farm3.static.flickr.com/2531/4094333885_e8462a8338.jpg</code></pre>

<p>The first field is an image ID. The part before the underscore is a WordNet ID, so the first image is of <code>n00004475</code>. What's that?</p>
<p>The mapping from WordNet ID to a brief text label can be downloaded from a link on the ImageNet <a href="http://image-net.org/download-API">API page</a>.</p>
<pre><code class="language-bash">$ wget http://image-net.org/archive/words.txt
$ wc words.txt
##   82114  302059 2655750 words.txt
$ head -3 words.txt
## n00001740   entity
## n00001930   physical entity
## n00002137   abstraction, abstract entity</code></pre>

<p>There are 82,114 WordNet IDs. Now we can decode the one we're interested in.</p>
<pre><code class="language-bash">$ grep n00004475 words.txt
## n00004475    organism, being</code></pre>

<p>So the first picture in ImageNet is of an "organism, being". What does such a thing look like?</p>
<p><img alt="organism, being" src="img/n00004475_6590.jpg"></p>
<p>There are eight examples of "organism, being" and two of the others are cats.</p>
<p>I think 82,114 categories is too many to try to sample randomly from, for my purposes. I'll use the <a href="http://image-net.org/challenges/LSVRC/2017/browse-det-synsets">200 categories</a> specified for the <a href="http://image-net.org/challenges/LSVRC/2017/">ILSVRC2017</a> object detection <a href="http://image-net.org/challenges/LSVRC/2017/#det">challenge</a>.</p>
<pre><code class="language-bash">wget -O 200words.html http://image-net.org/challenges/LSVRC/2017/browse-det-synsets</code></pre>

<p>I used Emacs to pull out the 200 WordNet IDs and convenient extra-short descriptions, saved in <a href="200words.csv">200words.csv</a>. The script <a href="make_urls_subset.py">make_urls_subset.py</a> produces <a href="200words100urls.csv">200words100urls.csv</a> with 100 random URLs for each of the categories. Finally, <a href="get_fives.py">get_fives.py</a> downloads five working JPGs for each category. A couple came back with "missing" images, so I manually replaced those with others from the list.</p>
<p>The results are packaged up on <a href="https://github.com/">GitHub</a> at <a href="https://github.com/ajschumacher/imagen">ajschumacher/imagen</a> and feature such beauties as <a href="img/n02118333_27_fox.jpg">n02118333_27_fox.jpg</a>.</p>
<p><img alt="n02118333_27_fox.jpg" src="img/n02118333_27_fox.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20170430-sampling_imagenet/</link>
<guid>http://planspace.org/20170430-sampling_imagenet/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow's QueueRunner</title>
<description><![CDATA[

<p>A TensorFlow <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a> helps to feed a TensorFlow <a href="http://planspace.org/20170327-tensorflow_and_queues/">queue</a> using threads which are optionally managed with a TensorFlow <a href="http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/">Coordinator</a>. QueueRunner objects can be used directly, or via higher-level APIs, which also offer automatic TensorBoard summaries.</p>
<pre><code class="language-python">import tensorflow as tf
session = tf.Session()</code></pre>

<hr>
<h2>QueueRunner Directly</h2>
<p>To use a QueueRunner you need a TensorFlow queue and an op that puts a new thing in the queue every time that op is evaluated. Typically, such an op will itself involve a queue, which is a bit of a tease. To avoid that circularity, this example will use random numbers.</p>
<pre><code class="language-python">queue = tf.FIFOQueue(capacity=10, dtypes=[tf.float32])
random_value_to_enqueue = tf.random_normal([])  # shape=[] means a single value
enqueue_op = queue.enqueue(random_value_to_enqueue)
random_value_from_queue = queue.dequeue()</code></pre>

<p>At this point if you evaluate <code>random_value_from_queue</code> in the session it will block, because nothing has been put in the queue yet.</p>
<pre><code class="language-python">queue_runner = tf.train.QueueRunner(queue, [enqueue_op])</code></pre>

<p>Still nothing has been enqueued, but <code>queue_runner</code> stands ready to make threads that will do the enqueuing.</p>
<p>If you put more enqueue ops in the list, or the same one multiple times, more threads will be started when things get going.</p>
<pre><code class="language-python">coordinator = tf.train.Coordinator()
threads = queue_runner.create_threads(session, coord=coordinator, start=True)</code></pre>

<p>Using <code>start=True</code> means we won't have to call <code>.start()</code> for each thread ourselves.</p>
<pre><code class="language-python">&gt;&gt;&gt; len(threads)
## 2</code></pre>

<p>There are two threads running: one for handling coordinated shutdown, and one for the enqueue op.</p>
<p>Now at last we can get at random values from the queue!</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(random_value_from_queue)
## 0.69283932
&gt;&gt;&gt; session.run(random_value_from_queue)
## -0.53802371</code></pre>

<p>The feeding thread will try to keep the queue at capacity, which was set to 10, so there should always be more items available to dequeue.</p>
<p>Since we used a coordinator, we can shut the threads down nicely.</p>
<pre><code class="language-python">coordinator.request_stop()
coordinator.join(threads)</code></pre>

<hr>
<h2>QueueRunner with Higher-Level APIs</h2>
<p>It's possible to work with QueueRunner directly, as shown above, but it's easier to use higher-level TensorFlow APIs that themselves use QueueRunner.</p>
<p>It's common for TensorFlow queue-chains to start with a list of filenames (sometimes a list of just one filename) to read data from. The <code>string_input_producer</code> function makes a queue using provided strings.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'])
letter_from_queue = queue.dequeue()</code></pre>

<p>This is a <code>FIFOQueue</code>, just as before, but notice we don't have an enqueue op for it. Like many things in <code>tf.train</code>, here TensorFlow has already done some work for us. A QueueRunner has already been made, and it was added to the <code>queue_runners</code> collection.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.get_collection('queue_runners')
## [&lt;tensorflow.python.training.queue_runner_impl.QueueRunner object at 0x121ee2c10&gt;]</code></pre>

<p>You could access and run that QueueRunner directly, but <code>tf.train</code> makes things easier.</p>
<!--

Finally, here's a place where it would be useful to be
doing a session context manager!

-->

<pre><code class="language-python">coordinator.clear_stop()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p><code>tf.train.start_queue_runners</code> automatically starts the threads.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'</code></pre>

<p>By default, the queue will go through the original items multiple times, or multiple epochs, and shuffle the order of strings within an epoch.</p>
<p>Limiting the number of epochs uses a local variable, which must be initialized.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, num_epochs=1)
letter_from_queue = queue.dequeue()
initializer = tf.local_variables_initializer()
session.run(initializer)
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p>Now the QueueRunner will close the queue when there isn't anything more to put in it, so the dequeue op will eventually give an <code>OutOfRangeError</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## OutOfRangeError</code></pre>

<hr>
<h2>Automatic TensorBoard Summaries</h2>
<p>A single-epoch queue will be helpful for illustrating an interesting thing about <code>tf.train.string_input_producer</code>: it automatically adds a TensorBoard summary to the graph.</p>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<p>It's nice to have direct control over every detail of your program, but the conveniences of higher-level APIs are also pretty nice. The summary that gets added is a scalar summary representing the percent full that the queue is.</p>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg"></p>
<pre><code class="language-python">tf.reset_default_graph()  # Starting queue runners will fail if a queue is
session = tf.Session()    # closed, so we need to clear things out.
queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, capacity=4, num_epochs=1)
letter_from_queue = queue.dequeue()
summaries = tf.summary.merge_all()
summary_writer = tf.summary.FileWriter('logs')
initializer = tf.local_variables_initializer()
session.run(initializer)
coordinator = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)
for i in range(4):
    summary_writer.add_summary(session.run(summaries), i)
    session.run(letter_from_queue)
summary_writer.add_summary(session.run(summaries), 4)
summary_writer.flush()  # Ensure everything is written out.</code></pre>

<p>After opening up TensorBoard with <code>tensorboard --logdir=logs</code> and going to <code>http://localhost:6006/</code> and and turning off plot smoothing, you'll see this:</p>
<p><img alt="log" src="img/fraction_full.png"></p>
<p>This shows that the queue, with capacity four, started 100% full and then every time something was dequeued from it it became 25 percentage points less full until it was empty.</p>
<p>For this example, the queue wasn't being refilled, so we knew it would become less and less full. But if you're reading data into an input queue that you expect to keep full, it's a great diagnostic to be able to check how full it actually is while things are running, to find find out if loading data is a bottleneck.</p>
<p>The automatic TensorBoard logging here is also a nice first taste of all the things that happen with even higher-level TensorFlow APIs.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170430-tensorflows_queuerunner/</link>
<guid>http://planspace.org/20170430-tensorflows_queuerunner/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>From Behaviorist to Constructivist AI</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/B._F._Skinner">B. F. Skinner</a> might be satisfied that neural networks achieve intelligence when they perform tasks well. This behaviorist perspective leads to misunderstandings of current technology and limits development toward systems that think. Pervasive epistemological confusion about categories is one example. In general, a constructivist approach will become necessary for advanced machine learning and artificial intelligence.</p>
<p><img alt='bird in operant conditioning chamber or "Skinner box"' src="img/skinner_box.jpg"></p>
<p>Training a neural network by <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagating</a> from a loss function is a lot like <a href="https://en.wikipedia.org/wiki/Operant_conditioning">operant conditioning</a>. Error becomes punishment. The objective and result of eliciting particular behavior is the same whether you're doing object detection <a href="https://motherboard.vice.com/en_us/article/america-secretly-tried-to-destroy-totalitarianism-with-pigeons">with a pigeon</a> in a <a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber">Skinner box</a> or with a convolutional neural network.</p>
<p>Little could be less like real intelligence than blurting out a name for every object you see. Little could better epitomize behaviorist stimulus-response. This is the intelligence of the <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> classifiers that popularized deep learning.</p>
<p>These classifiers really shouldn't be anthropomorphized. A cat/dog classifier is not thinking about cats and dogs. An engineer designed the network with a cat neuron and a dog neuron, and got them to light them up as desired.</p>
<p>An image classifier has continuous input and categorical output. The categories are specified by design. This is clearly a limitation on the output side, and a very different limitation from any constraint on the input side in image resolution or color space. A cat/dog classifier cannot say anything other than cat and dog.</p>
<p>One could argue that this output is not strictly categorical because it might be read, for example, as 95% cat and 5% dog, but this does not undo the designed categories, and this sort of non-<a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac</a> enhancement is not generally provided when systems take categorical input.</p>
<p>Language models are frequently categorically constrained in both input and output. At the word level, this means a model can't deal with words it's never seen before. This leads to approaches like <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">Google's neural machine translation</a> falling back to <a href="https://arxiv.org/abs/1508.07909">subword units</a> for rare words. But even if a language model goes to the character level, this is still a categorical constraint, and a system trained on "e" could be perfectly blind to "&#233;."</p>
<p>Whether categories are imposed on the input or output side, they make it obvious that the system is limited. The system cannot handle categories not specified in the design.</p>
<p>Categorical input is also foreign to humans. It would be like having a separate sense for every category. Instead of feeling "warm" or "not-warm", for instance, you could feel "cat" or "not-cat" and "dog" or "not-dog," and however many more. But if you didn't have a sense for "aardvark," you would be congenitally blind to "aardvark."</p>
<p>Continuous sensor data, like images and audio, is much more interesting and analogous to the human experience. There are still limitations - for example, you don't really see the edges of your field of view, and you can't really imagine what sensing magnetism would be like - but unstructured input provides a starting point for forming gestalt perceptions.</p>
<p><img height="240" alt="ouija board" src="img/ouija_board.jpeg"></p>
<p><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a>, or word vectors, might seem to handle the categorical problem, but this is largely misdirection. Word embeddings are representations of words that have fixed dimensionality, so that a language system no longer needs a separate input for every possible word, but only a separate input for every dimension of the word embeddings. There might be 50,000 words, but only 200 or 300 dimensions for an embedding.</p>
<p>Using word embeddings doesn't solve the categorical problem; it just pushes the problem to the embeddings. A system can still only handle words that embeddings have been generated for.</p>
<p>Word embeddings are useful in that they are representations of words that tend to give good results when used as input to various algorithms. In the same way, convolutional classifiers learn representations of images that tend to give good results when used as input to other algorithms. In both cases, having a good representation is useful. In both cases, people may or may not find the representations interpretable.</p>
<p>There tends to be <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/3/yoshua-bengio-machines-that-dream">excitement</a> about the meaningfulness of word embeddings when people amuse themselves with arithmetic like <em>king - man + woman = queen</em>, or make visualizations with reasonable-seeming clusters. But the understanding that's happening here is happening in the people; having nice representations does not mean that a system has achieved understanding.</p>
<p>It seems important that an intelligent system should be able to develop internal concepts without those concepts being built into the system's design, so there was interest when Google <a href="https://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html">found</a> a cat neuron in a 2011 system.</p>
<p><img height="240" src="img/google_cat.jpg" alt="Google cat neuron"></p>
<p>The system was trained to take an image as input and generate the same image as output. This is easy for a computer to do just by copying, so to make it interesting you have to put restrictions on the flow of information from input to output. The restricted system learns good representations for the images. Then, by testing lots of images with and without cats, Google found one point in the system that tended to respond positively to cats and negatively to other things.</p>
<p>The temptation is to declare that the system formed an idea of cats. You could just as well say that a mold formed an idea of its cast.</p>
<p>Visual systems learn many useful internal representations. For example, they learn oriented edge detectors. Humans have these too. It should be clear that in neither case is there an idea of an oriented edge. The Google researchers were correct in their <a href="https://arxiv.org/abs/1112.6209">paper</a>'s title when they said that their system learned high-level features rather than concepts.</p>
<p>More recently, <a href="https://openai.com/">OpenAI</a>'s <a href="https://blog.openai.com/unsupervised-sentiment-neuron/">unsupervised sentiment neuron</a> is another case of humans interpreting neurons. The model is categorical at input and output on the character level, learning to predict the next character in text. OpenAI used all 4,096 dimensions of their learned representation, but their title comes from noticing that just one of those dimensions captured a lot of sentiment-related information.</p>
<p>Word embeddings are patterns of activation over perhaps 300 neurons. They are <a href="http://www.bcp.psych.ualberta.ca/~mike/Pearl_Street/Dictionary/contents/D/distributed.html">distributed representations</a>. A cat neuron or sentiment neuron, on the other hand, is in line with the implausible "<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">unit/value principle</a>" that a single neuron represents a single concept.</p>
<p>It is easy to think that words <em>are</em> categories, and so a distributed representation of a word is a distributed representation of a category. With images, it's clear that one picture of a cat is different from another picture of a cat. But words are not categories. The difference between words and images is principally one of cardinality (there are many more images than words) and composibility (images more easily contain many things). But just as a word vector for <em>cat</em> is close to a word vector for <em>kitty</em>, distributed representations for pictures of cats should also be close to one another, and the Google system could just as well have been mined for distributed representations. Perhaps it is the human desire to categorize that makes us comfortable with multi-dimensional representations when we've provided categories in advance, but look for single-dimensional representations when we haven't. (Or maybe it's just easier.)</p>
<p>Regardless of whether distributed or unit representations are better, having a representation does not imply thought. These representations flash through their networks, coming before the result like Pavlovian slobber. This is not to say these representations couldn't be used in a system that thinks, but that current usage is too limited. One thing that's missing is state that develops over time.</p>
<p>Sequence models (such as the sentiment neuron example) introduce a limited kind of time-awareness, and a kind of memory. There could be something here (<a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/9/ilya-sutskever-unsupervised-learning-attention-and-other-mysteries">attention</a> in particular is interesting) but most <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/10/oriol-vinyals-sequence-to-sequence-machine-learning">usage</a> still seems to be learning representations and doing encoding to and decoding from these representations.</p>
<p>To be clear: Good representations are useful, whether or not they are utilized for anything like higher-level thought. But it seems unlikely that conventional models used in supervised or unsupervised learning will spontaneously invent higher-level thought, no matter how good their representations are.</p>
<p>By its name, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> seems purely behavioristic, but it also recognizes the idea of internal models, as illustrated in <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>'s taxonomy of reinforcement learning agents. These models are generally something like an agent's internal conception of the world around it.</p>
<p><img alt="taxonomy of reinforcement learning agents" src="img/rl_taxonomy.png"></p>
<p>Lots of reinforcement learning is model-free, and where there are internal models they are often heavily specified in advance or quite distant from what we think of as mental models. The idea, if not current implementations, is key. The behaviorist perspective is one of only inputs and outputs. Model-based reinforcement learning suggests, at least in spirit, a missing piece. Humans certainly aren't model-free, for example.</p>
<p>What the constructivist perspective adds is that it isn't enough to simply have an internal model. Intelligence entails building and working with new models as a part of problem-solving.</p>
<p>One view of building different models for different situations might be selectively enlisting parts of a large system for a given task. <a href="https://arxiv.org/abs/1701.08734">PathNet</a> tries to "discover which parts of the network to re-use for new tasks." This is interesting, but the focus on selecting a subset of wiring seems distant from the imaginative process of building a mental model, likely drawing on representations which may be distributed.</p>
<p>It becomes important to understand how a representation behaves. Say a system does have a cat neuron; can it reason about cats? The cat neuron can be excited, but this seems more like experiencing the <a href="https://en.wikipedia.org/wiki/Qualia">quale</a> of cat-ness than like imagining a cat. Experiencing an emotion is even more clearly not an abstraction, so the case of a sentiment neuron makes this distinction even clearer.</p>
<p>Imagining a cat might be an algebraic operation, in the sense that it posits an entity, a variable, which has cat properties or is a cat. In <a href="https://mitpress.mit.edu/books/algebraic-mind">The Algebraic Mind</a>, Gary Marcus argues that connectionist (neural network) models lack this kind of kind of ability.</p>
<p><img alt="The Algebraic Mind" src="img/algebraic_mind_cover.jpg"></p>
<p>An intelligent system should be able not only to represent things but to build and manipulate models composed from these representations.</p>
<p>For example, whether you like Chomsky or not, understanding a sentence seems like an algebraic procedure in the sense of apprehending values for variables like subject, verb, and object. The plug and play composability of noun phrases and the like also suggests a constructive mental process.</p>
<p>Or take the example of number: can a system perceive that a picture has three cats, as opposed to two cats? There's some depth here, as a system could represent two or three entities all of which are cats individually, or it could represent number concepts explicitly. It's hard to find current models that do either in a meaningful way.</p>
<p>The Algebraic Mind was published in 2001. There have been many advances in machine learning since 2001, but they largely haven't been advances toward reasoning. In this sense, Marcus thinks artificial intelligence is <a href="https://www.technologyreview.com/s/603945/is-artificial-intelligence-stuck-in-a-rut/">in a rut</a>.</p>
<p>If you take the view that machine learning is a type of statistics, then you may not care about any of this. But for machine learning as artificial intelligence, it may be that algebraic (or symbolic) considerations will be necessary.</p>
<p>A fair criticism is that if it isn't clear how to make direct progress in the constructivist direction, time is better spent advancing what have become traditional techniques. There is certainly value in this kind of advancement.</p>
<p>There may also be some reason for hope in <a href="https://deepmind.com/blog/differentiable-neural-computers/">differentiable neural computers</a> (DNCs) and related work. In some ways, the linking of DNC memory locations is like Marcus's proposal for representing structured data, and the DNC memory itself is something like Marcus's idea of registers. But it looks like DNCs work with categorical input and output, relying on it for seemingly algebraic task performance.</p>
<p>It seems likely that artificial general intelligence will use a composite approach. It may process visual input with convolutions. It may use distributed representations for internal concepts. It may access memory along the lines of a differentiable neural computer. It may have an attention mechanism that allows it to focus on the state of the external world one moment and its internal world the next. Combining all these ideas into a system that can come up with its own ideas is an intriguing project.</p>
<blockquote>
<p>&#8220;There is no reason, as yet, to be confident that an intermediate symbolic representation will not be required for modeling higher cognitive processes.&#8221; (<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">Feldman and Ballard, 1982</a>)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</link>
<guid>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</guid>
<pubDate>Sat, 29 Apr 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
