<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan ➔ space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Highlights from ICML 2016</title>
<description><![CDATA[

<p>I was fortunate to attend the <a href="http://icml.cc/">International Conference on Machine Learning</a> in New York from June 19-24 this year. There were walls and walls of multi-arm bandit posters (etc.) but I mostly checked out neural net, deep learning things. Here are my highlights.</p>
<p>In general: People are doing so many interesting things! Wow. Too many things to try. Architectures are more complex and more flexibly experimented with, it seems. And everybody loves <a href="https://arxiv.org/abs/1502.03167">batch norm</a>. I didn't see anything that seemed like a huge conceptual breakthrough that would change the direction of the field, but maybe I'm wishing for too much, maybe the field doesn't need such a thing, or maybe it was there and I missed it!</p>
<p>For my breakdown of the conference <em>structure</em> itself, see <a href="/20160703-conferences_what_is_the_deal/">Conferences: What is the Deal?</a>.</p>
<hr>
<p>Flashiest paper award: <a href="https://arxiv.org/abs/1605.05396">Generative Adversarial Text to Image Synthesis</a> is a system where you put in text and the computer makes a picture.</p>
<p><a href="https://arxiv.org/abs/1605.05396"><img alt="birds" src="img/birds.png"></a></p>
<p>Good job, computer!</p>
<hr>
<p>Best talk award: <a href="http://www.slideshare.net/ajschumacher/machine-learning-applications-at-bell-labs-holmdel">Machine Learning Applications at Bell Labs, Holmdel</a> was Larry Jackel's history of neural nets, starting back in 1976. Great perspective and personal stories, presented as part of <a href="https://sites.google.com/site/nnb2tf/">Neural Nets Back to the Future</a>.</p>
<p><a href="http://www.slideshare.net/ajschumacher/machine-learning-applications-at-bell-labs-holmdel"><img alt="bets" src="img/bets.png"></a></p>
<p>Yes, Jackel's bets involve the <a href="https://en.wikipedia.org/wiki/Vladimir_Vapnik">V</a> of <a href="https://en.wikipedia.org/wiki/VC_dimension">VC Dimension</a>, the <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Le</a> of <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a>, and SGD-master <a href="https://en.wikipedia.org/wiki/L%C3%A9on_Bottou">Bottou</a>.</p>
<hr>
<p>The first <a href="http://icml.cc/2016/?page_id=97">tutorial</a> I heard was from <a href="http://kaiminghe.com/">Kaiming He</a> on <a href="http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf">Deep Residual Networks</a>. ResNets and their cousins (like <a href="http://people.idsia.ch/~rupesh/very_deep_learning/">highway networks</a>) succeed essentially because wiggling a little tends to be better than going somewhere entirely different.</p>
<p>The intuition: It's hard to train deeper networks. But they ought to be better! Well, if we take a net and add another layer that encodes the identity, that's deeper, and exactly as good! What if all the layers were like the identity plus a little bit that's learned?</p>
<p>I was also impressed that, for example, a 152-layer ResNet can have fewer parameters than a 16-layer VGG net. Models aren't always getting more complex in terms of number of parameters!</p>
<p>Slightly more: If you're careful where you put nonlinearities, it's like you're always <a href="https://twitter.com/planarrowspace/status/744525688829534208">adding</a> instead of multiplying! Stacking two convolutional layers before "reconnecting" works better in practice than doing one or three!</p>
<p>Boom: you can train a model with a thousand layers.</p>
<hr>
<p>The second tutorial I <a href="https://twitter.com/planarrowspace/status/744545675896033280">heard</a> was from <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a> on <a href="http://www.thespermwhale.com/jaseweston/icml2016/">Memory Networks for Language Understanding</a>. Interesting references:</p>
<ul>
<li><a href="http://arxiv.org/pdf/1511.08130v2.pdf">A Roadmap towards Machine Intelligence</a></li>
<li><a href="http://arxiv.org/pdf/1506.03340v3.pdf">Teaching Machines to Read and Comprehend</a></li>
<li><a href="https://arxiv.org/abs/1502.05698">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</a></li>
<li><a href="https://research.facebook.com/research/babi/">bAbI natural language tasks</a></li>
</ul>
<p>It was neat seeing what can and can't be done currently for the "toy tasks." And Weston was <a href="https://twitter.com/planarrowspace/status/744549249019437062">stylish</a>.</p>
<hr>
<p>The third tutorial I heard was <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>'s on <a href="http://hunch.net/~beygel/deep_rl_tutorial.pdf">Deep Reinforcement Learning</a> (with <a href="http://icml.cc/2016/tutorials/AlphaGo-tutorial-slides.pdf">bonus deck on AlphaGo</a>). Reinforcement learning is a big deal and getting bigger, it seems. I should really get into it more. Silver has <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course materials</a> on his site. Also, <a href="https://openai.com/">OpenAI</a> is into it, and they have a whole <a href="https://gym.openai.com/">OpenAI Gym</a> where you can play with things. Neat!</p>
<hr>
<p>I liked the problem statement and solution of CReLUs. Too briefly: With ReLUs you tend to get paired positive/negative filters because one filter can't represent both. So build pairing into your system, and things work better. Very cogent content. Paper: <a href="https://arxiv.org/abs/1603.05201">Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units</a></p>
<hr>
<p>On sizing layers, an <a href="https://twitter.com/planarrowspace/status/745602200366354432">SVD approach</a> based on statistical structure in the weights was interesting. Paper: <a href="https://arxiv.org/abs/1602.02285">A Deep Learning Approach to Unsupervised Ensemble Learning</a></p>
<hr>
<p><a href="https://arxiv.org/abs/1605.08283">Discrete Deep Feature Extraction: A Theory and New Architectures</a> is a little math-heavy but close to interesting ideas about understanding and using the internals of neural networks.</p>
<hr>
<p><a href="https://arxiv.org/abs/1512.09300">Autoencoding beyond pixels using a learned similarity metric</a> does neat things using the internals of a network for more network work - it might be metaphorically "reflective" - and comes up with some flashy visual results, too.</p>
<hr>
<p><a href="https://arxiv.org/abs/1606.01583">Semi-Supervised Learning with Generative Adversarial Networks</a> shows that a fairly natural idea for GANs does in fact work: you can use labels like "real_a, real_b, real_c, fake" instead of just "real, fake".</p>
<blockquote>
<p>We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.</p>
</blockquote>
<hr>
<p><a href="https://arxiv.org/abs/1511.06393">Fixed Point Quantization of Deep Convolutional Networks</a> was pretty neat, if you're into that kind of thing.</p>
<hr>
<p>I really liked the <a href="https://sites.google.com/site/nnb2tf/">Neural Nets Back to the Future</a> workshop.</p>
<p><img alt="Back to the Future" src="img/bttf.jpg"></p>
<p>I was so interested in the historical perspective from the experienced people presenting, for a while I was <a href="https://twitter.com/planarrowspace/status/745958455865933824">tweeting</a> every slide.</p>
<hr>
<p>From pictures I took (and <a href="https://twitter.com/planarrowspace/status/745959355724468224">tweeted</a>) I reconstructed <a href="http://www.slideshare.net/ajschumacher/machine-learning-applications-at-bell-labs-holmdel">slides</a> for "Machine Learning Applications at Bell Labs, Holmdel" from Larry Jackel, as mentioned above.</p>
<hr>
<p>I also <a href="https://twitter.com/planarrowspace/status/745972920040689664">tweeted</a> all through Gary Tesauro's talk, but thought it was of less general interest.</p>
<hr>
<p>Patrice Simard's <a href="http://papers.nips.cc/paper/833-backpropagation-without-multiplication">Backpropagation without Multiplication</a> was fascinating, and much better for his commentary. For example:</p>
<blockquote>
<p>Discretizing the gradient is potentially very dangerous. Convergence may no longer be guaranteed, learning may hecome prohibitively slow, and final performance after learning may be be too poor to be interesting,</p>
</blockquote>
<p>Simard said that quote from the paper isn't really true; you can just use his method without worrying about that. This is a quote I <a href="https://twitter.com/planarrowspace/status/746035221318012928">took down</a> as he was speaking:</p>
<blockquote>
<p>Gradient descent is so robust to errors in the gradient, your code probably has bugs, but you don't need to fix them.</p>
</blockquote>
<hr>
<p>During a panel discussion, Yann LeCun said max pooling makes adversarial training unstable, and so it's better to use strided convolution if you want to downsample.</p>
<p>Later I noticed related sentiment in Karpathy's <a href="http://cs231n.github.io/convolutional-networks/">notes</a>:</p>
<blockquote>
<p><em>Getting rid of pooling.</em>&#8203; Many people dislike the pooling operation and think that we can get away without it. For example, <a href="http://arxiv.org/abs/1412.6806">Striving for Simplicity: The All Convolutional Net</a> proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers.</p>
</blockquote>
<hr>
<p><a href="http://www.bcl.hamilton.ie/~barak/">Barak Pearlmutter</a> gave a <a href="https://sites.google.com/site/nnb2tf/abstracts#pearlmutter">talk</a> about automatic differentiation and a really fast system called vlad that I think runs on <a href="https://github.com/barak/stalin">stalin</a>. Pretty impressive speed! Following up later he did add:</p>
<blockquote>
<p>keep in mind that if all your computations are actually array
operations on the GPU, which takes all the time, then the overhead
of the AD system might not really matter since it's just scheduling
the appropriate derivative-computing array operations on the GPU.</p>
</blockquote>
<p>The next day <a href="https://twitter.com/ryan_p_adams">Ryan Adams</a> gave a talk on the same topic at the <a href="https://sites.google.com/site/automl2016/scientific-program">AutoML workshop</a> featured a Python <a href="https://github.com/HIPS/autograd">autograd</a> package that seemed to have some of the features (closure, for example) as vlad.</p>
<p>Everybody loves automatic differentiation!</p>
<hr>
<p>I was briefly at the <a href="https://sites.google.com/site/ondeviceintelligence/icml2016">Workshop on On-Device Intelligence</a> but ended up wandering.</p>
<hr>
<p>I visited the <a href="https://sites.google.com/site/dataefficientml/">Data-Efficient Machine Learning</a> workshop and saw <a href="http://andrewgelman.com/">Gelman</a>'s <a href="https://twitter.com/planarrowspace/status/746357503009689601">amusing</a> talk called Toward Routine Use of Informative Priors.</p>
<hr>
<p>I spent some time at the <a href="https://sites.google.com/site/mlsys2016/schedule">Machine Learning Systems</a> workshop.</p>
<p>Unfortunately I missed <a href="http://daggerfs.com/">Yangqing Jia</a>'s <a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5czIwMTZ8Z3g6NGM3ZTgxYmYyZDBiM2VjNA">Towards a better DL framework: Lessons from Brewing Caffe</a>.</p>
<p>I did see <a href="http://www.slideshare.net/justinbasilico/is-that-a-time-machine-some-design-patterns-for-real-world-machine-learning-systems">Design Patterns for Real-World Machine Learning Systems</a>, at which I was impressed at the <a href="https://twitter.com/planarrowspace/status/746394115240239105">number</a> of named machine learning design patterns at Netflix.</p>
<hr>
<p>I closed out the conference at the <a href="https://sites.google.com/site/automl2016/scientific-program">AutoML workshop</a>. Of possible interest in connection with this topic is a recent <a href="http://www.darpa.mil/news-events/2016-06-17">DARPA effort</a>.</p>    
    ]]></description>
<link>http://planspace.org/20160707-icml_2016/</link>
<guid>http://planspace.org/20160707-icml_2016/</guid>
<pubDate>Thu, 07 Jul 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Conferences: What is the Deal?</title>
<description><![CDATA[

<p>I was fortunate to attend all six days of the
<a href="http://icml.cc/">International Conference on Machine Learning</a> in New
York this year. I hadn't attended such a conference before. Here's
what I learned about how academic conferences like ICML go down.</p>
<h3>Conferences: The Deal</h3>
<p><em>Tutorials</em>: I usually think of tutorials as being mostly how-to
affairs, possibly with a hands-on component. But ICML tutorials are
just long talks. They run for two hours each, and provide fairly
general overviews of a given sub-field or problem type. They're good
for getting a feel for a broad area, with less focus on new work. The
first day of the conference (Sunday) was given over to these.</p>
<p><em>Receptions/Parties/etc.</em>: There's a ton of food and drink and people!
There are official conference events, and then there are also
company-sponsored affairs. There's a "job opportunities" email list
you can get on when you register for the conference, but the real
reason to subscribe is to get the invites for all the social events.
It's exhausting.</p>
<p><em>Main conference</em>: People give fifteen-minute talks about their new
papers in giant rooms with barely any electrical outlets. This goes on
for three days. Monday. Tuesday. Wednesday.</p>
<p><em>Poster sessions</em>: These are what you really want to go to during the
main conference days. You can quickly check out way more papers than
you could sit through the talks for, and you can usually start talking
to one of the authors immediately if you're interested. I found more
cool things this way. I suppose if you were really proactive you could
check out all the papers in advance online, but then why are you at
the conference anyway? (That's a good question in general, by the
way; see below.)</p>
<p><em>Workshops</em>: The workshops are like whole mini-conferences, each a day
long. These ran on Thursday and Friday. They succeed or fail with
their organizers. I went to a great one on Thursday, and I visited
four different ones on Friday. Many have their own poster sessions,
which are great if you can hit them.</p>
<h3>Conferences: A Good Deal?</h3>
<p>Maybe you should stay home! With papers on <a href="https://arxiv.org/">arXiv</a>
and a lot of video turning up online, it may not be worth the trip and
days of time, if all you want to do is learn about work in the field.
Of course if you want to meet collaborators or look for a job, your
incentives are quite different.</p>
<p>I'm sure other conferences are different. Even the conference that
might be a "competitor" with ICML, <a href="https://nips.cc/">NIPS</a>, has quite
a different feel, I'm led to understand.</p>    
    ]]></description>
<link>http://planspace.org/20160703-conferences_what_is_the_deal/</link>
<guid>http://planspace.org/20160703-conferences_what_is_the_deal/</guid>
<pubDate>Sun, 03 Jul 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Presenting “Hello, TensorFlow!”</title>
<description><![CDATA[

<p><em>A presentation version of my &#8220;Hello, TensorFlow!&#8221; <a href="https://www.oreilly.com/learning/hello-tensorflow">O'Reilly post</a>. Given at the <a href="http://www.meetup.com/TensorFlow-Washington-DC/">TensorFlow Washington DC</a> meetup on Wednesday June 29, 2016, &#8220;<a href="http://www.meetup.com/TensorFlow-Washington-DC/events/231860618/">Deep Dive into TensorFlow</a>&#8221; (<a href="https://www.eventbrite.com/e/washington-dc-meetup-deep-dive-into-tensorflow-tickets-26035651334">registration</a>). (<a href="big.html">slides</a>)</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Hello, TensorFlow! on O'Reilly" src="img/screenshot.png"></p>
<hr>
<p>This presentation started as a blog post I <a href="/20160619-writing_with_oreilly/">wrote</a> which was <a href="https://www.oreilly.com/learning/hello-tensorflow">published</a> on O'Reilly about a week and a half ago.</p>
<p>If you want you can follow along <a href="https://www.oreilly.com/learning/hello-tensorflow">there</a> and we'll stay pretty close to <a href="https://www.oreilly.com/learning/hello-tensorflow">that content</a>.</p>
<p>Also, please interrupt with comments and questions! We're still working on the <a href="http://www.oreilly.com/oriole/">Oriole</a> version, so your feedback can help others who eventually see that.</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>A lot of people have already contributed to this material. I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA) and have to especially thank John for his support, and all my colleagues there for their feedback. I'm also particularly indebted to the <a href="http://www.meetup.com/DC-Machine-Learning-Journal-Club/">DC Machine Learning Journal Club</a>, and of course the folks at O'Reilly.</p>
<hr>
<p><img alt="braided river" src="img/braided_river.jpg"></p>
<hr>
<p>This evening is billed as a &#8220;Deep Dive into TensorFlow&#8221;. For my part, you can think of it as a deep dive into the shallow end.</p>
<p>I'm going to try to show in a very hands-on way some of the details of how TensorFlow works. The idea is not to exercise all the features of TensorFlow but to really understand some of the important underlying concepts.</p>
<p>It should be a slightly more hands-on thorough exposition of topics Saba touched on <a href="http://blog.altoros.com/videos-from-washington-dc-tensorflow-meetup-march-9-2016.html">in March</a>.</p>
<p><em>Image from <a href="https://www.flickr.com/photos/alaskanps/8029733984">National Park Service, Alaska Region on Flickr</a>.</em></p>
<hr>
<ul>
<li>tensors</li>
<li>flows</li>
<li>pictures</li>
<li>servers</li>
</ul>
<hr>
<p>But first, a couple interesting things about TensorFlow.</p>
<p>I'm really only going to demonstrate the middle two, but let's mention them all.</p>
<hr>
<p><img alt="not neurons, tensors" src="img/not_neurons.png"></p>
<hr>
<p>First thing about TensorFlow: tensors.</p>
<p>TensorFlow hates neurons and it loves linear algebra.</p>
<p>When you design software you make choices that affect what you can do and how you can do it.</p>
<p>For neural nets, there has in the past been a fixation on the biological metaphor and the "neurons" in particular. If you've used software like R's <a href="https://cran.r-project.org/web/packages/neuralnet/">neuralnet</a> package or Python's <a href="http://pybrain.org/">PyBrain</a>, you've seen interfaces where you typically define the number of neurons or "hidden units" that you want in each layer, and the connections between them (and perhaps some other components) are set up automatically one way or another.</p>
<p>But if you're looking at the math you have to evaluate inside these neural net models, the "neurons" hardly exist at all. The weights on the connections certainly exist, and there are operations that connect values as they flow through the network, but in the end you don't really need "neurons" to be a dominant abstraction in the system.</p>
<p>If you go this route, everything is just matrix math - or, for arbitrary dimensions, tensor math. This is the choice TensorFlow makes. It makes TensorFlow really flexible and lets it achieve efficient computation that would be more difficult otherwise.</p>
<p>So TensorFlow can do more than "just" neural nets. As demonstrated in the <a href="https://www.udacity.com/course/deep-learning--ud730">Google/Udacity Deep Learning MOOC</a>, you can implement logistic regression easily with TensorFlow, for example.</p>
<p>Focusing on (typically fixed-dimension) tensors does arguably make it more awkward to attempt esoteric techniques like dynamically changing the architecture of your neural net by adding neurons, for example. (This comment was inspired by <a href="https://drive.google.com/file/d/0Bxf-khCt_eknWEF6aFJ1ZU4yQ00/view">Let your Networks Grow</a>, <a href="https://sites.google.com/site/nnb2tf/">Neural Nets Back to the Future</a> at <a href="http://icml.cc/2016/">ICML 2016</a>.)</p>
<p>The linear algebra you actually have to think about is not too tricky, but for our purposes I'm not going to use anything more than individual numbers, so there's no chance of being distracted by linear algebra.</p>
<p>The relevant point is that while some other software focuses on neurons and mostly ignores weights, in the case of TensorFlow we focus on weights and mostly ignore neurons as a top-level abstraction. In any event, it's all just numbers.</p>
<hr>
<p><img alt="simple computation graph" src="img/math_graph.png"></p>
<hr>
<p>Second thing about TensorFlow: flows.</p>
<p>TensorFlow loves graphs.</p>
<p>It's important to distinguish between graphs and pictures. Here we mean <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graphs</a> as in the abstract data type. Data flow graphs.</p>
<p>This is a picture of a data flow graph. You might prefer to think of it as \(y=mx+b\). This is what TensorFlow's graphs are like.</p>
<p>The advantage of graphs is that the software can manipulate them and "reason about" how to answer questions you're asking before it commits to particular execution paths.</p>
<p>Lots of software is moving toward working via graphs in this way. Often they're <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> but I don't care too much about whether that's always strictly the case. <a href="https://twitter.com/planarrowspace/status/569963882900561921">&#129370;</a></p>
<p>For example, <a href="http://deeplearning.net/software/theano/">Theano</a> works similarly enough to TensorFlow that <a href="http://keras.io/">Keras</a> can use either as its backend. <a href="http://torch.ch/">Torch</a> is also similar.</p>
<p>Outside of software mostly for deep learning, <a href="http://spark.apache.org/">Spark</a> works with a kind of computation graph to manage distributed processing. Tools like <a href="https://github.com/spotify/luigi">Luigi</a> and <a href="https://github.com/Factual/drake">Drake</a> manage graph pipelines of data. And many <a href="http://drivendata.github.io/cookiecutter-data-science/">recommend</a> that all data work be thought of as graph-like pipelines.</p>
<p>The graph concept is interesting and worth exploring in order to understand TensorFlow, so I'll spend some time with it today.</p>
<p><em>Image made with <a href="https://draw.io/">draw.io</a>.</em></p>
<hr>
<p><img alt="Visualization of a TensorFlow graph" src="img/graph_vis_animation.gif"></p>
<hr>
<p>Third thing about TensorFlow: pictures.</p>
<p>If you use Python's <a href="http://scikit-learn.org/">scikit-learn</a>, you likely also use a separate visualization tool, likely including <a href="http://matplotlib.org/">matplotlib</a>. If you've tried to <a href="/20151129-see_sklearn_trees_with_d3/">visualize decision tree rules</a>, you may have had a frustrating time. You may or may not use any explicit logging system at all.</p>
<p>The <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md">TensorBoard</a> tooling that comes with TensorFlow is a great logging and visualizing system that gives you access to a lot of internals that could otherwise be a lot of work to expose.</p>
<p>I think TensorBoard <em>by itself</em> is a compelling reason to use TensorFlow over other similar software.</p>
<p>TensorBoard is super great and I will be showing several sides of it today.</p>
<p><em>Image from <a href="https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/index.html">TensorBoard: Graph Visualization</a>.</em></p>
<hr>
<p><img alt="Life of a Servable" src="img/serving_architecture.svg"></p>
<hr>
<p>Fourth thing about TensorFlow: servers.</p>
<p>TensorFlow loves moving directly into production.</p>
<p>In addition to everything else, TensorFlow has a highly-engineered serving architecture. So if you develop a TensorFlow model on your laptop, you can (relatively) easily build a deployment system to serve that model in a serious way. It seems pretty neat, but I haven't used it and I'm not going to talk more about it.</p>
<p><em>Image from the <a href="https://tensorflow.github.io/serving/architecture_overview">TensorFlow Serving Architecture Overview</a>.</em></p>
<hr>
<p>demo</p>
<hr>
<p>From here to the end is just demo! The starting point is <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20160629-presenting_hello_tensorflow/demo_begin.ipynb">demo_start.ipynb</a> and the ending point (with output) is <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20160629-presenting_hello_tensorflow/demo_end.ipynb">demo_end.ipynb</a>.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20160629-presenting_hello_tensorflow/</link>
<guid>http://planspace.org/20160629-presenting_hello_tensorflow/</guid>
<pubDate>Wed, 29 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Hello, TensorFlow! (Just the Code)</title>
<description><![CDATA[

<p>This is the final code block from <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> I like it as
a cheat sheet for TensorFlow basics.</p>
<pre><code class="language-python">import tensorflow as tf

x = tf.constant(1.0, name='input')
w = tf.Variable(0.8, name='weight')
y = tf.mul(w, x, name='output')
y_ = tf.constant(0.0, name='correct_value')
loss = tf.pow(y - y_, 2, name='loss')
train_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)

for value in [x, w, y, y_, loss]:
    tf.scalar_summary(value.op.name, value)

summaries = tf.merge_all_summaries()

sess = tf.Session()
summary_writer = tf.train.SummaryWriter('log_simple_stats', sess.graph)

sess.run(tf.initialize_all_variables())
for i in range(100):
    summary_writer.add_summary(sess.run(summaries), i)
    sess.run(train_step)</code></pre>    
    ]]></description>
<link>http://planspace.org/20160620-hello_tensorflow_just_the_code/</link>
<guid>http://planspace.org/20160620-hello_tensorflow_just_the_code/</guid>
<pubDate>Mon, 20 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Writing with O'Reilly</title>
<description><![CDATA[

<p><a href="http://www.oreilly.com/">O'Reilly Media</a> is tech publishing brand best known for its iconic <a href="http://shop.oreilly.com/">books</a>. They're also the force behind a number of <a href="http://www.oreilly.com/conferences/">conferences</a> and they've been publishing more web content in their <a href="https://www.oreilly.com/ideas">Ideas</a> and <a href="https://www.oreilly.com/learning">Learning</a> sections. I'm very excited to now be able to contribute a small article which will join their site. "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>" should be live at 7am EDT on Monday June 20, 2016. It was a fun experience getting it there, and I thought I'd describe the process.</p>
<h3>Summary</h3>
<ul>
<li>Inception: Connections, Initiative, Luck</li>
<li>Editing: Markdown, Google Doc, Web Publishing</li>
<li>Publishing: Adventure!</li>
</ul>
<h3>Inception</h3>
<p>The post was born through a combination of connections, some initiative, and good fortune.</p>
<p>I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA). O'Reilly got in touch and I think it may have been Paco who originally proposed a discussion about DLA and O'Reilly doing something together. This led to a call between <a href="https://twitter.com/bigdata">Ben</a> and <a href="https://twitter.com/cmariebeau">Marie</a> from O'Reilly and John and I from DLA. I joined because I've done some blogging and teaching and am interested in such things. (It was surprisingly weird to be on a phone call with Ben after hearing his voice in my headphones during so many episodes of the <a href="http://radar.oreilly.com/tag/oreilly-data-show-podcast">O'Reilly Data Show</a>.) Nothing came directly of our initial call aside from establishing that there was  interest that could eventually take some real form.</p>
<p>Months later, I was starting to play with TensorFlow on my own time, collecting some notes as I went along, and starting to think about shaping them into a post for my personal blog. I guess I mentioned this to John, who suggested that we cultivate the content a little more and see if it would work as a post for O'Reilly. I was very fortunate in this connection to have the support of DLA to spend some more time writing the post, and to get initial feedback from a number of DLA colleagues.</p>
<p>After a draft was together, we sent it off to Marie to see if O'Reilly would be interested. Sure enough there was interest, and we moved on to editing. Sometimes O'Reilly republishes existing blog posts, but in this case it wasn't yet published anywhere and Marie wanted to do a round of editing, which I think has turned out to be a very good idea.</p>
<p>It is interesting how various publishers find things to publish. I've talked to "acquisitions editors" who are looking for people to write books, and I occasionally hear from people who want to republish a blog post or solicit new posts. Everybody wants content. The ease of publishing means anybody can write a book, and a lot of publishers seem to be going for long tail content, or even just <em>anything</em> new.</p>
<p>I think O'Reilly is the most discerning of the tech publishers. Aside from liking the people who work there, it's their good reputation that made putting an article on their site attractive to me. I mean, this is the house of <a href="http://shop.oreilly.com/product/9780596004927.do">the camel book</a>! That reputation also motivates linking DLA and O'Reilly, however indirectly. The apparent editorial policy of O'Reilly to not do sponsored content or other branding tie-ins preserves their status and makes them the more attractive as an ally.</p>
<h3>Editing</h3>
<p>I write generally pretty raw unedited posts (such as this one) for my blog, and I had heard that O'Reilly had cool processes in place for the editing and publishing process, so I was curious to see what it would be like working with them. The article went through three stages, first as markdown, then as a Google Doc, and then in the O'Reilly web publishing platform.</p>
<p>I almost always write text files with just a little bit of <a href="https://daringfireball.net/projects/markdown/">markdown</a> syntax (<a href="http://planspace.org/20160209-how_i_blog/">more details</a>). For the TensorFlow post, I got feedback on this form first from colleagues at <a href="http://www.deeplearninganalytics.com/">DLA</a>, which included <a href="https://slack.com/">slack</a> comments, email comments, and a pull request. Also, the local <a href="http://www.meetup.com/DC-Machine-Learning-Journal-Club/">machine learning journal club</a> has been working with TensorFlow, and they generously allowed me to present a version at one of their meetups. That experience led to a number of changes, such as clarifying the distinction between graphs and sessions.</p>
<p>Once I had the draft ready I sent it off as a zip file (including images) to Marie, O'Reilly's lead data editor. Marie put my content into a <a href="https://www.google.com/docs/about/">Google Doc</a>. I never saw Google Docs used so well. It helped that her edits were good, but she also made expert use of change tracking and commenting so that everybody knew what was going on, and I could confirm changes or discuss alternatives. I still prefer the peace of mind of git, but the ease of use of Google Docs combined with Marie's thoroughness was remarkably satisfying.</p>
<p>In terms of the actual content of the edits, I was incredibly pleased. It has been my experience in the past that collaborative document editing can be among the most painful of endeavors. One manifestation of this is when a collaborator makes changes that decrease the quality of the document. Fortunately my experience here was quite the reverse. Thanks Marie! As one example, my original title was "TensorFlow from the Plumbing Up." John had suggested "Hello TensorFlow Graph" or something like that. When Marie changed it to "Hello, TensorFlow!" it was clear it should never have been anything else.</p>
<p>We spent some time working with the Google Doc version, and then passed it off to Jenn, O'Reilly's online managing editor. (You may know Jenn's voice from <a href="http://radar.oreilly.com/tag/oreilly-radar-podcast">O'Reilly Radar</a>!) Jenn put the content into the O'Reilly web system, where it appeared to us in read-only preview form behind a login. After a couple rounds of emails for final adjustments, it was ready to go and the publish date was set. I think it looks really nice now - thanks Jenn!</p>
<p>There was a little bit of friction around the transitions from system to system (markdown to google doc to web publishing system). If I was the dictator of systems I guess I would keep everything markdown, but that probably wouldn't have all the features that the O'Reilly publishing system needs, and I don't know of any markdown applications that have all the collaboration features and ease of use that you get with Google Docs. It's really interesting to me the systems and processes that turn out existing in the world; often they seem to grow more than be built, and even for things that seem like they should be easy it often turns out that there are a lot of details in the way. The system we used worked for us, but it definitely required the expert handling of its masters.</p>
<h3>Publishing</h3>
<p>Everything was ready on Friday afternoon, but we agreed that it would be best to run the post on Monday morning. This is presumably because people are much more likely to read it when they're supposed to be working than when they have free time. (I understand this, but I still think it's funny.)</p>
<p>The last email we exchanged looped in three more O'Reilly folks who do marketing things, as I understand it. I'm curious to see what this will mean for a simple blog post like this.</p>
<p>The <a href="https://www.oreilly.com/learning">O'Reilly learning</a> site is really beautiful, with great fonts and all those things publishers care about. I'm super excited to see our post up there!</p>
<p>Dropping Monday June 20: <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> I hope it's not so awful that it irritates Google or other TensorFlow developers and users!</p>    
    ]]></description>
<link>http://planspace.org/20160619-writing_with_oreilly/</link>
<guid>http://planspace.org/20160619-writing_with_oreilly/</guid>
<pubDate>Sun, 19 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Your Favorite Class</title>
<description><![CDATA[

<p>I read a fairly bad <a href="/20160207-talk_like_ted/">book</a> which included this:</p>
<blockquote>
<p>When you think back to your favorite college class, there's a good
chance the professor you most enjoyed injected a fair amount of
humor into his or her presentations.</p>
</blockquote>
<p>That section was about humor in presentations, but the reference got
me thinking about my own favorite college class, which wasn't
particularly funny.</p>
<p>I couldn't name or even recognize the professor of the class I
remember most strongly. The class was memorable because the professor
wasn't doing the same things other professors did. The professor was
orchestrating the class so that <em>students</em> did things.</p>
<p>This was the first class I recall in which the professor would pose a
question and tell us to turn and talk about it with the people near us
for a solid five minutes, multiple times through a "lecture."</p>
<p>This class had a long reading list from which we chose what we wanted
to read. I read <a href="https://www.amazon.com/Agile-Gene-Nature-Turns-Nurture/dp/006000679X">The Agile Gene</a>. I probably thought more about that
book than anything else I read as an undergrad.</p>
<p>In this class we did a group project. My group decided to transcribe
<a href="https://en.wikipedia.org/wiki/Family_Guy">Family Guy</a> in the <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet">International Phonetic Alphabet</a>. It was so
much fun, I'm sure we learned more about the IPA and thought more
about regional dialects than we would have done with a more typical
assignment.</p>
<p>I learned more about teaching techniques like these, which I might
summarize as <a href="https://en.wikipedia.org/wiki/Student-centred_learning">student-centered</a>, when I studied
<a href="http://www.bard.edu/mat/">teaching at Bard</a>. They were so rare at <a href="http://www.wisc.edu/">my university</a> that it
was shocking when I encountered a class that didn't reduce perfectly
well to a series of videos.</p>
<p>If the most memorable thing about a class is the person teaching it,
that's an indictment of the class, not a recommendation. It certainly
doesn't hurt for a teacher to be charismatic, but it's sad for
students to go through school hoping to encounter super-teachers who
are expert entertainers, as if that's the best or only way to learn.</p>    
    ]]></description>
<link>http://planspace.org/20160609-your_favorite_class/</link>
<guid>http://planspace.org/20160609-your_favorite_class/</guid>
<pubDate>Thu, 09 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine to Foretell the Future?</title>
<description><![CDATA[

<p>In 1973 a collection of essays by <a href="https://en.wikipedia.org/wiki/E._F._Schumacher">E. F. Schumacher</a> was published as <a href="https://en.wikipedia.org/wiki/Small_Is_Beautiful">Small Is Beautiful: A Study of Economics As If People Mattered</a>. I don't always agree with him but I think there's a lot to consider in there, and it's an interesting view from the past&#8212;in turns prescient and archaic. The selection excerpted below includes an underrepresented viewpoint for people doing modern data work.</p>
<p><a href="http://www.amazon.com/dp/0061997765/"><img alt="Small is Beautiful (cover)" src="small_is_beautiful.jpg"></a></p>
<hr>
<h3>A Machine to Foretell the Future?</h3>
<p>The reason for including a discussion on predictability in this volume
is that it represents one of the most important metaphysical&#8212;and
therefore practical&#8212;problems with which we are faced. There have never
been so many futurologists, planners, forecasters, and model-builders
as there are today, and the most intriguing product of technological
progress, the computer, seems to offer untold new possibilities.
People talk freely about &#8220;machines to foretell the future.&#8221; Are not
such machines just what we have been waiting for? All men at all times
have been wanting to know the future.</p>
<p>The ancient Chinese used to consult the <em>I Ching</em>, also called <em>The
Book of Changes</em> and reputed to be the oldest book of mankind. Some of
our contemporaries do so even today. The <em>I Ching</em> is based on the
conviction that, while everything changes all the time, change itself
is unchanging and conforms to certain ascertainable metaphysical laws.
&#8220;To everything there is a season,&#8221; said Ecclesiastes, &#8220;and a time to
every purpose under heaven ... a time to break down and a time to
build up ... a time to cast away stones and a time to gather stones
together,&#8221; or, as we might say, a time for expansion and a time for
consolidation. And the task of the wise man is to understand the great
rhythms of the Universe and to gear in with them. While the Greeks&#8212;and
I suppose most other nations&#8212;went to living oracles, to their Pythias,
Casandras, prophets and seers, the Chinese, remarkably, went to a book
setting out the universal and necessary pattern of changes, the very
Laws of Heaven to which all nature conforms inevitably and to which
man will conform freely as a result of insight gained either from
wisdom or from suffering. Modern man goes to the computer.</p>
<p>Tempting as it may be to compare the ancient oracles and the modern
computer, only a comparison by contrast is possible. The former deal
exclusively with qualities; the latter, with quantities. The
inscription over the Delphic temple was &#8220;Know Thyself,&#8221; while the
inscription on an electronic computer is more likely to be: &#8220;Know Me,&#8221;
that is, &#8220;Study the Operating Instructions before Plugging in.&#8221; It
might be thought that the <em>I Ching</em> and the oracles are metaphysical
while the computer model is &#8220;real&#8221;; but the fact remains that a
machine to foretell the future is based on metaphysical assumptions of
a very definite kind. It is based on the implicit assumption that &#8220;the
future is already here,&#8221; that it exists already in a determinate form,
so that it requires merely good instruments and good techniques to get
it into focus and make it visible. The reader will agree that this is
a very far-reaching metaphysical assumption, in fact, a most
extraordinary assumption which seems to go against all direct personal
experience. It implies that human freedom does not exist or, in any
case, that it cannot alter the predetermined course of events. We
cannot shut our eyes to the fact, on which I have been insisting
throughout this book, that such an assumption, like all metaphysical
theses, whether explicit or implicit, has decisive practical
consequences. The question is simply: is it true or is it untrue?</p>
<p>When the Lord created the world and people to live in it&#8212;an enterprise
which, according to modern science, took a very long time&#8212;I could well
imagine that He reasoned with Himself as follows: &#8220;If I make
everything predictable, these human beings, whom I have endowed with
pretty good brains, will undoubtedly learn to predict everything, and
they will thereupon have no motive to do anything at all, because they
will recognise that the future is totally determined and cannot be
influenced by any human action. On the other hand, if I make
everything unpredictable: they will gradually discover that there is
no rational basis for any decision whatsoever and, as in the first
case, they will thereupon have no motive to do anything at all.
Neither scheme would make sense. I must therefore create a mixture of
the two. Let some things be predictable and let others be
unpredictable. They will then, amongst many other things, have the
very important task of finding out which is which.&#8221;</p>
<p>And this, indeed, is a very important task, particularly today, when
people try to devise machines to foretell the future. Before anyone
makes a prediction, he should be able to give a convincing reason why
the factor to which his prediction refers is inherently predictable.</p>
<p>Planners, of course, proceed on the assumption that the future is not
&#8220;already here,&#8221; that they are not dealing with a predetermined&#8212;and
therefore predictable&#8212;system, that they can determine things by their
own free will, and that their plans will make the future different
from what it would have been had there been no plan. And yet it is the
planners, more than perhaps anyone else, who would like nothing better
than to have a machine to foretell the future. Do they ever wonder
whether the machine might incidentally also foretell their own plans
before they have been conceived?</p>
<h5>Need for Semantics</h5>
<p>However this may be, it is clear that the question of predictability
is not only important but also somewhat involved. We talk happily
about estimating, planning, forecasting, budgeting, about surveys,
programmes, targets, and so forth, and we tend to use these terms as
if they were freely interchangeable and as if everybody would
automatically know what was meant. The result is a great deal of
confusion, because it is in fact necessary to make a number of
fundamental distinctions. The terms we use may refer to the past or to
the future; they may refer to acts or to events; and they may signify
certainty or uncertainty. The number of combinations possible where
there are three pairs of this kind is \(2^3\), or 8, and we really
ought to have eight different terms to be quite certain of what we are
talking about. Our language, however, is not as perfect as that. The
most important distinction is generally that between acts and events.
The eight possible cases may there fore be ordered as follows:</p>
<pre><code>1. Act           5. Event
   Past             Past
   Certain          Certain
2. Act           6. Event
   Future           Future
   Certain          Certain
3. Act           7. Event
   Past             Past
   Uncertain        Uncertain
4. Act           8. Event
   Future           Future
   Uncertain        Uncertain</code></pre>

<p>The distinction between acts and events is as basic as that between
active and passive or between &#8220;within my control&#8221; or &#8220;outside my
control.&#8221; To apply the word &#8220;planning&#8221; to matters outside the
planner's control is absurd. Events, as far as the planner is
concerned, simply happen. He may be able to forecast them and this may
well influence his plan; but they cannot possibly be part of the plan.</p>
<p>The distinction between the past and the future proved to be necessary
for our purpose, because, in fact, words like &#8220;plan&#8221; or &#8220;estimate&#8221; are
being used to refer to either. If I say: &#8220;I shall not visit Paris
without a plan,&#8221; this can mean: &#8220;I shall arm myself with a street plan
for orientation&#8221; and would therefore refer to case 5. Or it can mean:
&#8220;I shall arm myself with a plan which outlines in advance where I am
going to go and how I am going to spend my time and money&#8221;&#8212;case 2
or 4. If someone claims that &#8220;to have a plan is indispensable,&#8221; it is
not without interest to find out whether he means the former or the
latter. The two are <em>essentially</em> different.</p>
<p>Similarly, the word &#8220;estimate,&#8221; which denotes uncertainty, may apply
to the past or to the future. In an ideal world, it would not be
necessary to make estimates about things that had already happened.
But in the actual world, there is much uncertainty even about matters
which, in principle, could be fully ascertained. Cases 3, 4, 7, and 8
represent four different types of estimates. Case 3 relates to
something I have done in the past; case 7, to something that has
happened in the past. Case 4 relates to something I plan to do in the
future, while case 8 relates to something I expect to happen in the
future. Case 8, in fact, is a forecast in the proper sense of the term
and has nothing whatever to do with &#8220;planning.&#8221; How often, however,
are forecasts presented as if they were plans&#8212;and <em>vice versa!</em> The
British &#8220;National Plan&#8221; of 1965 provides an outstanding example and,
not surprisingly, came to nothing.</p>
<p>Can we ever speak of future acts or events as certain (cases 2 and 6)?
If I have made a plan with full knowledge of all the relevant facts,
being inflexibly resolve to carry it through&#8212;case 2&#8212;I may, in this
respect, consider my future actions as certain. Similarly, in
laboratory science, dealing with carefully isolated deterministic
systems, future events may he described as certain. The real world,
however, is not a deterministic system; we may be able to talk with
certainty about acts or events of the past&#8212;cases 1 or 5&#8212;but we can do
so <em>about future events only on the basis of assumptions.</em> In other
words, we can formulate conditional statements about the future, such
as: &#8220;<em>If</em> such and such a trend of events continued for another x
years, this is where it would take us.&#8221; This is not a forecast or
prediction, which must always be uncertain in the real world, but an
exploratory calculation, which, being conditional, has the virtue of
mathematical certainty.</p>
<p>Endless confusion results from the semantic muddle in which we find
ourselves today. As mentioned before, &#8220;plans&#8221; are put forward which
upon inspection turn out to relate to events totally outside the
control of the planner. &#8220;Forecasts&#8221; are offered which upon inspection
turn out to be conditional sentences, in other words, exploratory
calculations. The latter are misinterpreted as if they were forecasts
or predictions. &#8220;Estimates&#8221; are put forward which upon inspection turn
out to be plans. And so on and so forth. Our academic teachers would
perform a most necessary and really helpful task if they taught their
students to make the distinctions discussed above and developed a
terminology which fixed them in words.</p>
<h5>Predictability</h5>
<p>Let us now return to our main subject&#8212;predictability. Is prediction or
forecasting&#8212;the two terms would seem to be interchangeable&#8212;possible at
all? The future does not exist; how could there be knowledge about
something nonexistent? This question is only too well justified. In
the strict sense of the word, knowledge can only be about the past.
The future is always in the making, but it is being made <em>largely</em> out
of existing material, about which a great deal can be known. The
future, there, is <em>largely</em> predictable, if we have solid and
extensive knowledge of the past. <em>Largely</em>, but by no means wholly;
for into the making of the future there enters that mysterious and
irrepressible factor called human freedom. It is the freedom of a
being of which it has been said that it was made in the image of God
the Creator: the freedom of creativity.</p>
<p>Strange to say, under the influence of laboratory science many people
today seem to use their freedom only for the purpose of denying its
existence. Men and women of great gifts find their purest delight in
magnifying every &#8220;mechanism,&#8221; every &#8220;inevitability,&#8221; everything where
human freedom does not enter or does not appear to enter. A great
shout of triumph goes up whenever anybody has found some further
evidence&#8212;in physiology or psychology or sociology or economics or
politics&#8212;of unfreedom, some further indication that people cannot help
being what they are and doing what they are doing, no matter how
inhuman their actions might be. The denial of freedom, of course, is a
denial of responsibility: there are no acts, but only events;
everything simply happens; no one is responsible. And this is no doubt
the main cause of the semantic confusion to which I have referred
above. It is also the cause for the belief that we shall soon have a
machine to foretell the future.</p>
<p>To be sure, if everything simply happened, if there were no element of
freedom, choice, human creativity and responsibility, everything would
be perfectly predictable, subject only to accidental and temporary
limitations of knowledge. The absence of freedom would make human
affairs suitable for study by the natural sciences or at least by
their methods, and reliable results would no doubt quickly follow the
systematic observation of facts. Professor Phelps Brown, in his
presidential address to the Royal Economic Society, appears to adopt
precisely this point of view when talking about &#8220;The Underdevelopment
of Economics.&#8221; &#8220;Our own science,&#8221; he says, &#8220;has hardly yet reached its
seventeenth century.&#8221; Believing that economics is <em>metaphysically</em> the
same as physics, he quotes another economist, Professor Morgenstern,
approvingly, as follows:</p>
<blockquote>
<p>The decisive break which came in physics in the seventeenth century,
specifically in the field of mechanics, was possible only because of
previous developments in astronomy. It was backed by several
millennia of systematic, scientific, astronomical observation....
Nothing of this sort has occurred in economic science. It would have
been absurd in physics to have expected Kepler and Newton without
Tycho&#8212;and there is no reason to hope for an easier development in
economics.</p>
</blockquote>
<p>Professor Phelps Brown concludes therefore that we need many, many
more years of observations of behaviour. &#8220;<em>Until then, our
mathematisation is premature.</em>&#8221;</p>
<p>It is the intrusion of human freedom and responsibility that makes
economics metaphysically different from physics and makes human
affairs largely unpredictable. We obtain predictability, of course,
when we or others are acting according to a plan. But this is so
precisely because a plan is the result of an exercise in the freedom
of choice: the choice has been made; all alternatives have been
eliminated. If people stick to their plan, their behaviour is
predictable simply because they have chosen to surrender their freedom
to act otherwise than prescribed in the plan.</p>
<p>In principle, everything which is immune to the intrusion of human
freedom, like the movements of the stars, is predictable, and
everything subject to this intrusion is unpredictable. Does that mean
that all human actions are unpredictable? No, because most people,
most of the time, make no use of their freedom and act purely
mechanically. Experience shows that when we are dealing with large
numbers of people many aspects of their behaviour are indeed
predictable; for out of a large number, at any one time, only a tiny
minority are using their power of freedom, and they often do not
significantly affect the total outcome. Yet all really important
innovations and changes normally start from tiny minorities of people
who <em>do</em> use their creative freedom.</p>
<p>It is true that social phenomena acquire a certain steadiness and
predictability from the non-use of freedom, which means that the great
majority of people responds to a given situation in a way that does
not alter greatly in time, unless there are really overpowering new
causes.</p>
<p>We can therefore distinguish as follows:</p>
<ol>
<li>Full predictability (in principle) exists only in the absence of
    human freedom, <em>i.e.</em> in &#8220;sub-human&#8221; nature. The limitations of
    predictability are purely limitations of knowledge and technique.</li>
<li>Relative predictability exists with regard to the behaviour
    pattern of very large numbers of people doing &#8220;normal&#8221; things
    (routine).</li>
<li>Relatively full predictability exists with regard to human'
    actions controlled by a plan which eliminates freedom, <em>e.g.</em>
    railway timetable.</li>
<li>Individual decisions by individuals are in principle
    unpredictable.</li>
</ol>
<h5>Short-Term Forecasts</h5>
<p>In practice all prediction is simply extrapolation, modified by known
&#8220;plans.&#8221; But how do you extrapolate? How many years do you go back?
Assuming there is a record of growth, what precisely do you
extrapolate&#8212;the average rate of growth, or the increase in the rate of
growth, or the annual increment in absolute terms? As a matter of
fact. there are no rules: it is just a matter of &#8220;feel&#8221; or judgment.
(When there are seasonal or cyclical patterns, it is, of course,
necessary to go back by at least a year or a cycle; but it is a matter
of judgement to decide how many years or cycles.)</p>
<p>It is good to know of all the different possibilities of using the
same time series for extrapolations with very different results. Such
knowledge will prevent us from putting undue faith in any
extrapolation. At the same time, and by the same token, the
development of (what purport to be) better forecasting techniques can
become a vice. In short-term forecasting, say, for next year, a
refined technique rarely produces significantly different results from
those of a crude technique. After a year of growth&#8212;what can you
predict?</p>
<ol>
<li>that we have reached a (temporary) ceiling;</li>
<li>that growth will continue at the same, or a slower, or a faster
    rate;</li>
<li>that there will be a decline.</li>
</ol>
<p>Now, it seems clear that the choice between these three basic
alternative predictions cannot be made by &#8220;forecasting technique&#8221; but
only by informed judgment. It depends, of course, on what you are
dealing with. When you have something that is normally growing very
fast, like the consumption of electricity, your threefold choice is
between the same rate of growth, a faster rate, or a slower rate.</p>
<p>It is not so much forecasting technique, as a full understanding of
the current situation shat can help in the formation of a sound
judgment for the future. If the present level of performance (or rate
of growth) is known to be influenced by quite abnormal factors which
are unlikely to apply in the coming year, it is, of course, necessary
to take these into account. The forecast, &#8220;same as last year,&#8221; may
imply a &#8220;real&#8221; growth or a &#8220;real&#8221; decline on account of exceptional
factors being present this year, and this, of course, must be made
explicit by the forecaster.</p>
<p>I believe, therefore, that all effort needs to be put into
understanding the current situation, to identify and, if need be,
eliminate &#8220;abnormal&#8221; and non-recurrent factors from the current
picture. This having been done, the method of forecasting can hardly
be crude enough. No amount of refinement will help one come to the
fundamental judgment&#8212;is next year going to be the same as last year,
or better, or worse?</p>
<p>At this point, it may be objected that there ought to be great
possibilities of short-term forecasting with the help of electronic
computers, because they can very easily and quickly handle a great
mass of data and fit to them some kind of mathematical expression. By
means of &#8220;feedback&#8221; the mathematical expression can be kept up to date
almost instantaneously, and once you have a really good mathematical
fit, the the machine can predict the future.</p>
<p>Once again, we need to have a look at the metaphysical basis of such
claims. What is the meaning of a &#8220;good mathematical fit?&#8221; Simply that
a sequence of quantitative changes in the past has been elegantly
described in precise mathematical language. But the fact that I&#8212;or the
machine&#8212;have been able to describe this sequence so exactly by no
means establishes a presumption that the pattern will continue. It
could continue only if <em>(a)</em> there were no human freedom and <em>(b)</em>
there was no possibility of any change in the causes that have given
rise to the observed pattern.</p>
<p>I should accept the claim that a very clear and very strongly
established pattern (of stability, growth, or decline) can be expected
to continue for a little longer, unless there is definite knowledge of
the arrival of new factors likely to change it. But I suggest that for
the detection of such clear, strong and persistent patterns the
non-electronic human brain is normally cheaper, faster, and more
reliable than its electronic rival. Or to put it the other way round:
if it is really necessary to apply such highly refined methods of
mathematical analysis for the detection of a pattern that one needs an
electronic computer, the pattern is too weak and too obscure to be a
suitable basis for extrapolation in real life.</p>
<p>Crude methods of forecasting&#8212;after the current picture has been
corrected for abnormalities&#8212;are not likely to lead into the errors of
spurious verisimilitude and spurious detailing&#8212;the two greatest vices
of the statistician. Once you have a formula and an electronic
computer, there is an awful temptation to squeeze the lemon until it
is dry and to present a picture of the future which through its very
precision and verisimilitude carries conviction. Yet a man who uses an
imaginary map, thinking it a true one, is likely to be worse off than
someone with no map at all; for he will fail to inquire wherever he
can, to observe every detail on his way, and to search continuously
with all his senses and all his intelligence for indications of where
he should go.</p>
<p>The person who makes the forecasts may still have a precise
appreciation of the assumptions on which they are based. But the
person who uses the forecasts may have no idea at all that the whole
edifice, as is often the case, stands and falls with one single,
unverifiable assumption. He is impressed by the thoroughness of the
job done, by the fact that everything seems to &#8220;add up,&#8221; and so forth.
If the forecasts were presented quite artlessly, as it were, on the
back of an envelope, he would have a much better chance of
appreciating their tenuous character and the fact that, forecasts or
no forecasts, someone has to take an entrepreneurial decision about
the unknown future.</p>
<h5>Planning</h5>
<p>I have already insisted that a plan is something essentially different
from a forecast. It is a statement of intention, of what the
planners&#8212;or their masters&#8212;intend to do. Planning (as I suggest the
term should be used) is inseparable from power. It is natural and
indeed desirable that everybody wielding any kind of power should have
some sort of a plan, that is to say, that he should use power
deliberately and consciously, looking some distance ahead in time. In
doing so he must consider what other people are likely to do; in other
words, he cannot plan sensibly without doing a certain amount of
forecasting. This is quite straightforward as long as that which has
to be forecast is, in fact, &#8220;forecastable,&#8221; if it relates either to
matters into which human freedom does not enter, or to the routine
actions of a very large number of individuals, or to the established
plans of other people wielding power. Unfortunately, the matters to be
forecast very often belong to none of these categories but are
dependent on the individual decisions of single persons or small
groups of persons. In such cases forecasts are little more than
&#8220;inspired guesses,&#8221; and no degree of improvement in forecasting
technique can help. Of course, some people may turn out to make better
guesses than others, but this will not be due to their possessing a
better forecasting technique or better mechanical equipment to help
them in their computations.</p>
<p>What, then, could be the meaning of a &#8220;national plan&#8221; in a free I
society? It cannot mean the concentration of all power at one point,
because that would imply the end of freedom: genuine planning is
coextensive with power. It seems to me that the only intelligible
meaning of the words &#8220;a national plan&#8221; in a free society would be the
fullest possible statement of intentions by all people wielding
substantial economic power, such statements being collected and
collated by some central agency. The very inconsistencies of such a
composite &#8220;plan&#8221; might give valuable pointers.</p>
<h5>Long-term Forecasts and Feasibility Studies</h5>
<p>Let us now turn to long-term forecasting, by which I mean producing
estimates dive or more years ahead. It must be clear that, change
being a function of time, the longer-term future is even less
predictable than the short-term. In fact, all long-term forecasting is
somewhat presumptuous and absurd, unless it is of so general a kind
that it merely states the obvious. All the same, there is often a
practical necessity for &#8220;taking a view&#8221; on the future, as decisions
have to be taken and long-term commitments entered. Is there nothing
that could help?</p>
<p>Here I should like to emphasise again the distinction between
forecasts on the one hand and &#8220;exploratory calculations&#8221; or
&#8220;feasibility studies&#8221; on the other. In the one case I assert that this
or that will be the position in, say, twenty years' time. In the other
case I merely explore the long-term effect of certain assumed
tendencies. It is unfortunately true that in macro-economics
feasibility studies are very rarely carried beyond the most
rudimentary beginnings. People are content to rely on general
forecasts which are rarely worth the paper they are written on.</p>
<p>It may be helpful if I give a few examples. It is very topical these
days to talk about the development of underdeveloped countries and
countless &#8220;plans&#8221; (so-called) are being produced to this end. If we go
by the expectations that are being aroused all over the world, it
appears to be assumed that within a few decades most people the world
over are going to be able to live more or less as the western
Europeans are living today. Now, it seems to me, it would be very
instructive if someone undertook to make a proper, detailed
feasibility study of this project. He might choose the year 2000 as
the terminal date and work backwards from there. What would be the
required output of foodstuffs, fuels, metals, textile fibres, and so
forth? What would be the stock of industrial capital? Naturally, he
would have to introduce many new assumptions as he went along. Each
assumption could then become the object of a further feasibility
study. He might then find that he could not solve his equations unless
he introduced assumptions which transcended all bounds of reasonable
probability. This might prove highly instructive. It might conceivably
lead to the conclusion that, while most certainly there ought to be
substantial economic development throughout the countries where great
masses of people live in abject misery, there are certain choices
between alternative <em>patterns</em> of development that could be made, and
that some types of development would appear more feasible than others.</p>
<p>Long-term thinking, supported by conscientious feasibility studies,
would seem to be particularly desirable with regard to all
non-renewable raw materials of limited availability, that is to say,
mainly fossil fuels and metals. At present, for instance, there is a
replacement of coal or oil. Some people seem to assume that coal is on
the way out. A careful feasibility study, making use of all available
evidence of coal, oil, and natural gas reserves, proved as well as
merely assumed to exist, would be exceedingly instructive.</p>
<p>On the subject of population increase and food supplies, we have had
the nearest thing to feasibility studies so far, coming mainly from
United Nations organisations. They might be carried much further,
giving not only the totals of food production to be attained by 1980
or 2000, but also showing in much greater detail than has so far been
done the timetable of specific steps that would have to be taken in
the near future if these totals are to be attained.</p>
<p>In all this, the most essential need is a purely intellectual one: a
clear appreciation of the difference between a forecast and a
feasibility study. It is surely a sign of statistical illiteracy to
confuse the two. A long-term forecast, as I said, is presumptuous; but
a long- term feasibility study is a piece of humble and unpretentious
work which we shall neglect at our peril.</p>
<p>Again the question arises whether this work could be facilitated by
more mechanical aids such as electronic computers. Personally, I am
inclined to doubt it. It seems to me that the endless multiplication
of mechanical aids in fields which require judgment more than anything
else is one of the chief dynamic forces behind Parkinson's Law. Of
course, an electronic computer can work out a vast number of
permutations, employing varying assumptions, within a few seconds or
minutes, while it might take the non-electronic brain as many months
to do the same job. But the point is that the non-electronic brain
need never attempt to do that job. By the power of judgment it can
concentrate on a few decisive parameters which are quite sufficient to
outline the ranges of reasonable probability. Some people imagine that
it would be possible and helpful to set up a machine for long-range
forecasting into which current &#8220;news&#8221; could be fed continuously and
which, in response, would produce continual revisions of some
long-term forecasts. No doubt, this would be possible; but would it be
helpful? Each item of &#8220;news&#8221; has to be judged for its long-term
relevance, and a sound judgment is generally not possible immediately.
Nor can I see any value in the continual revision of long-term
forecasts, as a matter of mechanical routine. A forecast is required
only when a long-term decision has to be taken or reviewed, which is a
comparatively rare event even in the largest of businesses, and then
it is worth while deliberately and conscientiously to assemble the
best evidence, to judge each item in the light of accumulated
experience, and finally to come to a view which appears reasonable to
the best brains available. It is a matter of self-deception that this
laborious and uncertain process could be short-circuited by a piece of
mechanical apparatus.</p>
<p>When it comes to feasibility studies, as distinct from forecasts, it
may occasionally seem useful to have apparatus which can quickly test
the effect of variations in one's assumptions. But I have yet to be
convinced that a slide rule and a set of compound interest tables are
not quite sufficient for the purpose.</p>
<h5>Unpredictability and Freedom</h5>
<p>If I hold a rather negative opinion about the usefulness of
&#8220;automation&#8221; in matters of economic forecasting and the like, I do not
underestimate the value of electronic computers and similar apparatus
for other tasks, like solving mathematical problems or programming
production runs. These latter tasks belong to the exact sciences or
their applications. Their subject matter is non-human, or perhaps I
should say, sub-human. Their very exactitude is a sign of the absence
of human freedom, the absence of choice, responsibility and dignity.
As soon as human freedom enters, we are in an entirely different world
where there is great danger in any proliferation of mechanical
devices. The tendencies which attempt to obliterate the distinction
should be resisted with the utmost determination. Great damage to
human dignity has resulted from the misguided attempt of the social
sciences to adopt and imitate the methods of the natural sciences.
Economics, and even more so, applied economics, is not an exact
science; it is in fact, or ought to be, something much greater: a
branch of wisdom. Mr. Colin Clark once claimed &#8220;that long-period world
economic equilibrium develop themselves in their own peculiar manner,
entirely independently of political and social changes.&#8221; On the
strength of this metaphysical heresy he wrote a book, in 1941,
entitled <em>The Economics of 1960</em>. It would be unjust to say that the
picture he drew bears no resemblance to what actually came to pass;
there is, indeed, the kind of resemblance which simply stems from the
fact that man uses his freedom within an unchanged setting of physical
laws of nature. But the lesson from Mr. Clark's book is that his
metaphysical assumption is untrue; that, in fact, world economic
equilibria, even in the longer run, are highly dependent on political
and social changes; and that the sophisticated and ingenious methods
of forecasting employed by Mr. Clark merely served to produce a work
of spurious verisimilitude.</p>
<h5>Conclusion</h5>
<p>I thus come to the cheerful conclusion that life, including economic
life, is still worth living because it is sufficiently unpredictable
to be interesting. Neither the economist nor the statistician will get
it &#8220;taped.&#8221; Within the limits of the physical laws of nature, we are
still masters of our individual and collective destiny, for good or
ill.</p>
<p>But the know-how of the economist, the statistician, the natural
scientist and engineer, and even of the genuine philosopher can help
to clarify the limits within which our destiny is confined. The future
cannot be forecast, but it can be explored. Feasibility studies can
show us where we appear to be going, and this is more important today
than ever before, since &#8220;growth&#8221; has become a keynote of economics all
over the world.</p>
<p>In his urgent attempt to obtain reliable knowledge about his
essentially indeterminate future, the modern man of action may
surround himself by ever-growing armies of forecasters, by
ever-growing mountains of factual data to be digested by ever more
wonderful mechanical contrivances: I fear that the result is little
more than a huge game of make-believe and an ever more marvellous
vindication of Parkinson's Law. The best decisions will still be based
on the judgments of mature non-electronic brains possessed by men who
have looked steadily and calmly at the situation and seen it whole.
&#8220;Stop, look, and listen&#8221; is a better motto than &#8220;Look it up in the
forecasts.&#8221;</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20160531-a_machine_to_foretell_the_future/</link>
<guid>http://planspace.org/20160531-a_machine_to_foretell_the_future/</guid>
<pubDate>Tue, 31 May 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Old Article for the Cardinal: "UW computer models weather patterns"</title>
<description><![CDATA[

<p>Years ago, the better student newspaper of my alma mater, <a href="http://www.dailycardinal.com/">The Daily Cardinal</a>, decided to get science articles written by people who were in the <a href="https://nerdnite.com/">Nerd Nite</a> Madison <a href="https://www.facebook.com/groups/nerdnite.madison/">Facebook group</a>. I eventually agreed to write an article about a new computer the university was standing up, which ran on November 29, 2011. I happened to find an old copy of that paper recently, and I thought I'd include that old article here, not because it is good for the sake of completism.</p>
<p>I managed to find <a href="http://www.dailycardinal.com/article/2011/11/uw-computer-models-weather-patterns">the article still on the paper's web site</a>. I also found <a href="11292011.page04.pdf">a PDF of the page from the paper that the article ran on</a> thanks to Google's completist storage of all my old email. I'll put the text of the article below, because why not.</p>
<hr>
<h3>UW computer models weather patterns</h3>
<p>Tuesday, November 29, 2011 - Aaron Schumacher - The Daily Cardinal</p>
<p>If a butterfly flaps its wings in Brazil, will a hurricane destroy Florida? If a satellite sees clouds in Nevada, will it mean rain for crops or mud slides down mountains? Questions like these need a computer with real muscle, run by the best people, and that&#8217;s what UW-Madison now has.</p>
<p>The S4 supercomputer built on campus this year is now fully operational and the most powerful resource of its kind at the university&#8217;s disposal. It&#8217;s working to fully incorporate data from satellites in models of the Earth&#8217;s oceans and atmosphere&#8212;data that could help improve our understanding and forecasting of these complex systems.</p>
<p>Just how powerful is the new supercomputer? Well, it&#8217;s gotten harder to make computer processors faster, so engineers are placing more than one computer brain or &#8220;core&#8221; on each chip. Even your iPhone has two cores that work together to send text messages as quickly as possible. A new iMac has four cores on its single chip, for blazing fast Facebook rendering.</p>
<p>The processors in the new supercomputer at UW-Madison&#8217;s Space Science and Engineering Center (SSEC) each have 12 cores per chip, while a typical desktop has only one. There are four of these chips in each unit.</p>
<p>Oh, and there are a whopping 64 of these units working together in the S4, keeping track of a multitude of data and grid points over the Earth&#8217;s surface. In terms of RAM, the working memory of any computer, the S4&#8217;s processing units alone have over 2,000 times what you&#8217;d find in an iMac.</p>
<p>This speedy behemoth fills up five ceiling-high racks in the 1225 Dayton St. building. There are 26 more computers serving as the collective hard disk, providing a total of 456 terabytes in storage space.</p>
<p>To fund the construction of the S4 supercomputer, the SSEC received a grant of $1 million from the National Oceanic and Atmospheric Administration (NOAA) earlier this year. This decision was made in favor of UW-Madison largely because computer engineers on campus have the special skills to create such extreme computing systems.</p>
<p>The S4 supercomputer project extends a collaboration between the NOAA and UW-Madison&#8217;s SSEC. NOAA researchers, including many at UW-Madison, use the supercomputer for important computationally complex tasks.</p>
<p>The S4 supercomputer is a powerful new resource on campus. Its availability to researchers will help advance atmospheric and oceanic modeling techniques for national applications and will expand the capabilities of UW-Madison researchers working on many other projects.</p>    
    ]]></description>
<link>http://planspace.org/20160528-old_article_for_the_cardinal/</link>
<guid>http://planspace.org/20160528-old_article_for_the_cardinal/</guid>
<pubDate>Sat, 28 May 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple isn't Easy</title>
<description><![CDATA[

<p><a href="https://www.broadinstitute.org/bios/nick-patterson">Nick Patterson</a> of the <a href="https://www.broadinstitute.org/">Broad Institute</a> was on a recent <a href="http://www.thetalkingmachines.com/blog/2016/2/26/ai-safety-and-the-legacy-of-bletchley-park">episode</a> of the <a href="http://www.thetalkingmachines.com/">Talking Machines</a> podcast. He said some things that seemed right to me.</p>
<p>First, he said:</p>
<blockquote>
<p>I think the most important thing to do on data analysis is to do the simple things right.</p>
</blockquote>
<p>I agree. I think that in a field where people are excited about complicated algorithms and flashy visuals, the importance of "fundamentals" is too often neglected.</p>
<p>He also said:</p>
<blockquote>
<p>The smarter you are the less likely you are to make a stupid mistake.</p>
</blockquote>
<p>I think this is related to the above, but not necessarily only or even primarily about intelligence. I think it also has to do with humility, thoroughness, responsibility, and experience, at least.</p>    
    ]]></description>
<link>http://planspace.org/20160427-simple_isnt_easy/</link>
<guid>http://planspace.org/20160427-simple_isnt_easy/</guid>
<pubDate>Wed, 27 Apr 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Equivalences between Tables, Maps, Graphs, and Sets</title>
<description><![CDATA[

<p>By <em>tables</em>, I mean the relational database or well-used spreadsheet
system of data in rectangular grids of rows and columns.</p>
<pre><code>| property_a  | property_b  |
|-------------|-------------|
| value_a_one | value_b_one |
| value_a_two | value_b_two |</code></pre>

<p>By <em>maps</em>, I mean data in <a href="https://en.wikipedia.org/wiki/Associative_array">map</a> data structures. Maps are also known
as associative arrays or dictionaries. In JavaScript they're called
objects, and JavaScript Object Notation (<a href="https://en.wikipedia.org/wiki/JSON">JSON</a>) is a common format.</p>
<pre><code>[{property_a: value_a_one, property_b: value_b_one},
 {property_a: value_a_two, property_b: value_b_two}]</code></pre>

<p>By <em>graphs</em>, I mean nodes connected by edges. <a href="https://www.w3.org/RDF/">RDF</a>, for example,
has labels for both nodes and edges. One representation is <em>triples</em>
of the form <code>subject, predicate, object</code>, where <code>subject</code> and <code>object</code>
are nodes, and <code>predicate</code> is an edge.</p>
<pre><code>id_one, property_a, value_a_one
id_one, property_b, value_b_one
id_two, property_a, value_a_two
id_two, property_b, value_b_two</code></pre>

<p>A visual for this <em>triple graph</em> shows two components.</p>
<p><img alt="graph of triple data" src="triple_graph.png"></p>
<p>Graphs can also have unlabeled edges, in which case one representation
is <em>doubles</em> of the form <code>subject, object</code>.</p>
<pre><code>id_one, id_a_one
id_a_one, property_a
id_a_one, value_a_one
id_one, id_b_one
id_b_one, property_b
id_b_one, value_b_one
id_two, id_a_two
id_a_two, property_a
id_a_two, value_a_two
id_two, id_b_two
id_b_two, property_b
id_b_two, value_b_two</code></pre>

<p>A visual for one component of this <em>double graph</em> shows how the
additional identity nodes expand the representation.</p>
<p><img alt="graph of double data" src="double_graph.png"></p>
<p>In this formulation, with non-identity nodes always leaf nodes, double
graphs are equivalent to <a href="https://en.wikipedia.org/wiki/Set_(abstract_data_type)">sets</a>.</p>
<pre><code>[{{property_a, value_a_one}, {property_b, value_b_one}},
 {{property_a, value_a_two}, {property_b, value_b_two}}]</code></pre>

<p>There are a lot of close equivalences:</p>
<ul>
<li>sets are mostly equivalent to double graphs</li>
<li>maps are mostly equivalent to triple graphs</li>
<li>tables and the above are all mostly equivalent</li>
</ul>
<hr>
<p>Tabular data can be converted to map data. With <a href="https://www.python.org/">Python</a>'s
<a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html">pandas data frames</a> this is as easy as <code>dataframe.to_json()</code>.</p>
<p>Often maps can be complex in ways that don't correspond immediately to
tabular data, but still it's often possible for example to
<a href="http://konklone.io/json/">convert JSON to CSV</a>.</p>
<p>Graphs often work with doubles or triples directly as a storage
format.</p>
<p>Automatically converting tabular or map data to/from triples is less
common. I have a little toy <a href="https://github.com/ajschumacher/dd">data diff</a> code that does a simple
version of this. I think some parts of <a href="http://www.datomic.com/">datomic</a>'s internals do
related conversions.</p>
<p>Doubles are interesting but I don't know of any automatic converters
to or from other formats.</p>
<hr>
<p>Some considerations argue against the primacy of two-dimensional data
tables, and some of these concerns are valid, but it is not
insignificant that spreadsheet interfaces seem to work pretty well for
a lot of humans, and probably even if relying on a different
underlying structure tables should remain as an interface.</p>
<p>One complaint about spreadsheets in particular, though relational
databases are not necessarily much better, is that people can and do
choose layouts for their data that other people find appalling.</p>
<p>Proponents of "data cubes" might also suggest that two dimensions is
not enough for some types of data.</p>
<p>Wickham's <a href="http://vita.had.co.nz/papers/tidy-data.pdf">Tidy Data</a> paper provides responses to both of these
objections. In the first place, a wide range of table layouts are
equivalent and can be converted to a standard "tidy" format if needed.
There is no fundamental need to impose any given format in the name of
aesthetics. In the second place, an arbitrary number of dimensions can
be represented in a two-dimensional table, though such a
representation may not be ideal for human users. There remains a
question of how to provide a good interface.</p>
<p>It is awkward in a flat table to represent things that have multiple
values. For example, with one row per song and one "artist" field,
what do you do for a song that has two artists?</p>
<p>You might want to create a "second artist" field, which could be a
solution to a different problem. What we really want is to have two
entries each "in the same place" in the data.</p>
<p>Whatever the solution, there is a related interface issue: especially
with a tabular display, how do we show the user that there are two
values "in a cell"?</p>
<p>A technique common in relational databases hacks a graph approach into
the tabular framework. It uses one table for songs, one table for
artists, and a third table of "edges" connecting songs and artists
together. This means you have to know about a lot of the tables'
design. It can seem like overkill when the songs and artists are
really just strings. And there's not an obvious solution to the
interface problem.</p>
<p>You might want to use composite data types: instead of storing one
simple value in the song-artist position, for example, store a list of
values. This could be done with relational databases or spreadsheets,
but it generally isn't done. It is easy to do with pandas data frames
or in JSON.</p>
<p>Composite data values are probably worth having for some use cases.
This comes with a new interface issue, distinct from the issue of an
interface for multiple values. And it isn't a good solution to the
original problem of allowing multiple values. Aside from lists in
particular being ordered, there is a difference between something
having multiple values, and something being one list.</p>
<p>Composite data like lists also introduce complexity that could be
avoided. It is easy and natural with triple graphs (though not maps)
to represent multiple values.</p>
<pre><code>id_song_one, artist, a_jazz_band
id_song_one, artist, a_jazz_singer</code></pre>

<p>The other side of multiple values is missing values. Tables, when
realized as such, are "dense", meaning they have something at every
intersection of row and column. Since we don't always have a value, a
database might use <code>NULL</code>, or there could be special values like <code>NA</code>
in <a href="https://www.r-project.org/">R</a>. With triple graphs (and maps) these aren't necessary as you
always have a choice to not include a triple. Triples are naturally
"sparse" for missing data (distinct from sparsity for zeros).</p>
<pre><code>| property_a  | property_b  |
|-------------|-------------|
| value_a_one | NULL        |
| NULL        | value_b_two |</code></pre>

<pre><code>[{property_a: value_a_one},
 {property_b: value_b_two}]</code></pre>

<pre><code>id_one, property_a, value_a_one
id_two, property_b, value_b_two</code></pre>

<p>Spreadsheets are not relational databases. Products like Excel
implement a kind of discrete Cartesian quarter-plane within which
simple data types can be placed. The spacial nature of the spreadsheet
supports calculations like "add the value to the left of this one to
the value three above" and having multiple tables "next to" one
another.</p>
<p>A spreadsheet user may be thinking of tables, and some software
functionality may support this, but for the most part structure in
spreadsheets is hallucinated by the user. It is a testament to human
abilities, combining remarkable gestalt perception with careful
attention to detail, that anything useful is ever accomplished with
spreadsheets.</p>
<p>It seems like giving spreadsheet software well-defined tables rather
than vast expanses of empty cells would be a change worth making. Some
tools, such as <a href="http://app.editdata.org/">editdata</a>, do this.</p>
<p>It's awkward for people to think about things without physical
metaphor. Set literals have to be written in some order, for example,
despite not having any order intrinsically. In the same way, tables
have spacial baggage. Columns are always next to one another. Rows are
before or after one another.</p>
<p>If you change the order of rows or columns, there is some change, but
it isn't a very big change.</p>
<pre><code>| property_a  | property_b  |
|-------------|-------------|
| value_a_one | value_b_one |
| value_a_two | value_b_two |</code></pre>

<pre><code>| property_b  | property_a  |
|-------------|-------------|
| value_b_one | value_a_one |
| value_b_two | value_a_two |</code></pre>

<p>If you diff CSV files after re-ordering columns, every line of the
file has changed. But in some sense the data hasn't changed at all.</p>
<p>I think the right way to conceptualize this is to see even a simple
CSV file as a view of an underlying data set. While we usually don't
make it explicit, the ordering of columns and rows is characteristic
of the view and not the data.</p>
<p>With both tables and maps, sometimes the order of things has meaning
that should really be in another field.</p>
<p>Separate though sometimes conflated with ordering is the idea of a
unique identity for each row or map. By their distinctness they have
an implicit identity separate from any values.</p>
<p>Especially when you're interested in comparing two versions of a data
set, having an ID for each "row" is important. Otherwise you're in the
fraught position of having to determine what changed and what was
supposed to be the same, at the same time. This is impossible for at
least some changes.</p>
<p>The <a href="http://dat-data.com/">dat project</a> beta, for example, had an import <code>-key</code> option
(see <a href="https://github.com/maxogden/dat/blob/dd984adaa57c35fa08cd2c315e22186378d6f928/docs/cli-docs.md">beta docs</a>). The default behavior though was to automatically
generate a unique ID, which is also what <a href="https://www.mongodb.com/">MongoDB</a> does by default.</p>
<p>When specifying IDs by hand it is tempting to make choices that turn
out to be bad, like using a name as an ID and finding later both that
names can change and multiple things can have the same name.</p>
<p>Once there are unique IDs for rows/maps, comparison between versions
easily discovers which have been added and removed, and those that
remain can be compared to their partners.</p>
<p>Doubles and sets as a data representation are interesting but may not
be very useful. There is nothing to distinguish properties from
values. It could probably still be worked out by finding patterns of
value occurrences, but this would be expensive along the same lines as
matching up rows without IDs. It is a little poetic to think about
expressing things like <code>{smell, roses, raindrops}</code> but in the end the
sets and doubles don't directly convey all the same information as the
other formats. A while ago I thought that doubles were <a href="http://planspace.org/2012/06/27/doubles-are-sufficient-for-all-data/">sufficient</a>,
but I no longer think so.</p>
<p>My main interest in all this is dealing with the problem of versioning
data, and the related problems of diffing and merging data. Often,
only the most recent version of data is available or even stored
anywhere, which I <a href="http://planspace.org/2014/04/05/data-done-wrong-the-only-most-recent-data-model/">don't like</a>.</p>
<hr>
<p>For all the graphs here, I'm not considering duplicate edges.</p>
<p>I made the graph diagrams with <a href="https://www.draw.io/">draw.io</a>.</p>    
    ]]></description>
<link>http://planspace.org/20160417-equivalences_between_tables_maps_graphs_and_sets/</link>
<guid>http://planspace.org/20160417-equivalences_between_tables_maps_graphs_and_sets/</guid>
<pubDate>Sat, 16 Apr 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Mindstorms: Children, Computers, and Powerful Ideas</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/Mindstorms:_Children,_Computers,_and_Powerful_Ideas">Mindstorms</a> is a book from 1980. It inspired, among many things, <a href="https://en.wikipedia.org/wiki/Lego_Mindstorms">Lego Mindstorms</a>. It's been <a href="http://worrydream.com/LearnableProgramming/">called</a> &#8220;perhaps the greatest book ever written on learning in general&#8221;. And it contains <a href="/20151231-juggling_for_programmers/">a lesson on juggling</a>. Here are some quotes and notes I extracted.</p>
<p><a href="http://www.amazon.com/Mindstorms-Children-Computers-Powerful-Ideas/dp/0465046746"><img alt="Mindstorms: Children, Computers, and Powerful Ideas" src="mindstorms.jpg"></a></p>
<hr>
<p>One of the central themes of the book is "that children can learn to use computers in a masterful way" (p. vii) and one of the central questions I have is whether children today are doing so.</p>
<hr>
<p>From the introduction to the second edition (p. xv):</p>
<blockquote>
<p>"From the perspective of the 1990s, it appears bizarre or downright reactionary that <em>Mindstorms</em> makes no reference to gender or multiculturalism. I have become convinced that recognizing the <a href="https://en.wikipedia.org/wiki/Androcentrism">androcentric</a> nature of traditional ways of knowing will play a central role in producing change in education."</p>
<p>"... the ultimate theoretical task in advancing, for example, the learning of mathematics, is not producing a range of so-and-so-centric kinds of mathematical knowledge but rather finding ways of thinking about mathematical knowledge that will allow each individual to make what in <em>Mindstorms</em> I call a <a href="https://en.wiktionary.org/wiki/syntonic">syntonic</a> appropriation."</p>
</blockquote>
<p>I relate this to the later work by others in <a href="https://en.wikipedia.org/wiki/Where_Mathematics_Comes_From">Where Mathematics Comes From</a> which builds mathematical understanding on metaphor "syntonic with" human experience.</p>
<hr>
<p>In the preface (p. xxi):</p>
<blockquote>
<p>"The computer is the <a href="https://en.wikipedia.org/wiki/Proteus">Proteus</a> of machines."</p>
</blockquote>
<hr>
<p>On page 23:</p>
<blockquote>
<p>"... many children are held back in their learning because they have a model of learning in which you have either 'got it' or 'got it wrong.' But when you learn to program you almost never get it right the first time. <strong>Learning to be a master programmer is learning to become highly skilled at isolating and correcting 'bugs,'</strong> the parts that keep the program from working. The question to ask about the program is not whether it is right or wrong, but if it is fixable. <strong>If this way of looking at intellectual products were generalized to how the larger culture thinks about knowledge and its acquisition, we all might be less intimidated by our fears of 'being wrong.'</strong>" (Emphasis mine.)</p>
</blockquote>
<hr>
<p>On page 31:</p>
<blockquote>
<p>"... [meaningful use of computers] comes into head-on collision with the many aspects of school whose effect, if not whose intention, is to 'infantilize' the child."</p>
<p>"I see Piaget as the theorist of learning without curriculum and the theorist of the kind of learning that happens without deliberate teaching. To turn him into the theorist of a new curriculum is to stand him on his head."</p>
</blockquote>
<hr>
<p>On page 32:</p>
<blockquote>
<p>"... <strong>educational intervention means changing the culture</strong>, planting new constructive elements in it and eliminating noxious ones. This is a more ambitious undertaking than introducing a curriculum change, ..." (Emphasis mine.)</p>
<p>"The educator must be an anthropologist."</p>
</blockquote>
<hr>
<p>On page 37 Papert anticipates personal computers being a way to side-step the sluggishness of the traditional education system, leading to the sentence that closes the first chapter:</p>
<blockquote>
<p>"There might be a renaissance of thinking about education."</p>
</blockquote>
<hr>
<p>From page 39 to 40:</p>
<blockquote>
<p>"To my ear the word 'mathophobia' has two associations. One of these is a widespread fear of mathematics, which often has the intensity of a real phobia. The other comes from the meaning of the stem 'math.' In Greek it means 'learning' in a general sense. In our culture, fear of learning is no less endemic (although more frequently disguised) than fear of mathematics."</p>
</blockquote>
<hr>
<p>Page 50:</p>
<blockquote>
<p>"It is easy to understand why math and grammar fail to make sense to children when they fail to make sense to everyone around them and why helping children to make sense of them requires more than a teacher making the right speech or putting the right diagram on the board. I have asked many teachers and parents what they thought mathematics to be and why it was important to learn it. Few held a view of mathematics that was sufficiently coherent to justify devoting several thousand hours of a child's life to learning it, and children sense this. When a teacher tells a student that the reason for those many hours of arithmetic is to be able to check the change at the supermarket, the teacher is simply not believed. Children see such 'reasons' as one more example of adult double talk. The same effect is produced when children are told school math is 'fun' when they are pretty sure that teachers who say so spend their leisure hours on anything except the allegedly fun-filled activity. Nor does it help to tell them that they need math to become scientists - most children don't have such a plan. The children can see perfectly well that the teacher does not like math any more than they do and that the reason for doing it is simply that it has been inscribed into the curriculum. All of this erodes children's confidence in the adult world and the process of education. <em>And I think it introduces a deep element of dishonesty into the educational relationship.</em>" (Emphasis in original.)</p>
<p>"Children perceive the school's rhetoric about mathematics as double talk. In order to remedy the situation we must first acknowledge that the child's perception is fundamentally correct. The <em>kind of mathematics</em> foisted on children in schools is not meaningful, fun, or even very useful." (Emphasis in original.)</p>
</blockquote>
<p>I think this is an important point, and it sets off two trains of thought for me.</p>
<p>First, and more directly to Papert's point, why learn math? I agree it isn't to make change at a store, just have fun, or prepare for some specialized application. I side with those who hold that math is about thinking, and while it is not clearly present in learning long division, the kind of math that I think is worth spending time on is the distillation of good thinking. Mathematics is argument stripped of rhetoric. If logic is a more comfortable word, then use it, but don't worry about whether you can calculate a square root if you can't <a href="https://www.ets.org/gre/revised_general/prepare/analytical_writing/argument/prepare_task">analyze an argument</a>. One thing that math teaches you is that (at least in certain limited domains or ways) arguments actually can be 100% right and not subject to debate. There are real differences between being right and wrong and we can think about them.</p>
<p>Second, it is interesting to think about what "makes sense" and how we think about various topics. By this I mean that math, properly taught/learned, "makes sense" in that it has internal consistency and logic that binds its ideas together. On the other hand, English spelling makes very little sense except in very different ways, such as possibly a historical "way of knowing".</p>
<hr>
<p>On page 60:</p>
<blockquote>
<p>"They learn a general 'mathetic principle,' making components to favor modularity. And they learn to use the very powerful idea of 'state.'"</p>
</blockquote>
<hr>
<p>On page 61:</p>
<blockquote>
<p>"The programmer is encouraged to study the bug rather than forget the error."</p>
</blockquote>
<hr>
<p>On page 63:</p>
<blockquote>
<p>"Make sense of what you want to learn."</p>
</blockquote>
<hr>
<p>On page 71:</p>
<blockquote>
<p>"Of all ideas I have introduced to children, recursion stands out as the one idea that is particularly able to evoke an excited response."</p>
</blockquote>
<hr>
<p>On page 74:</p>
<blockquote>
<p>"In the LOGO environment new ideas are often acquired as a means of satisfying a personal need to do something one could not do before."</p>
</blockquote>
<hr>
<p>On page 76:</p>
<blockquote>
<p>"... what is important when we give children a theorem to use is not that they should memorize it. What matters most is that by growing up with a few very powerful theorems one comes to appreciate how certain ideas can be used as tools to think with over a lifetime. One learns to enjoy and to respect the power of powerful ideas. <strong>One learns that the most powerful idea of all is the idea of powerful ideas.</strong>" (Emphasis mine.)</p>
</blockquote>
<hr>
<p>On page 96:</p>
<blockquote>
<p>"An important component in the history of knowledge is the development of techniques that increase the potency of 'words and diagrams.'"</p>
</blockquote>
<hr>
<p>The idea of the child as epistemologist appears throughout.</p>
<hr>
<p>On page 100:</p>
<blockquote>
<p>"One might even say that computer science is wrongly so called: Most of it is not the science of computers, but the science of descriptions and descriptive languages."</p>
</blockquote>
<hr>
<p>On page 113:</p>
<blockquote>
<p>"<strong>We can learn more, and more quickly, by taking conscious control of the learning process, articulating and analyzing our behavior.</strong>" (Emphasis mine.)</p>
</blockquote>
<hr>
<p>On page 115:</p>
<blockquote>
<p>"Discovery cannot be a setup; invention cannot be scheduled."</p>
</blockquote>
<hr>
<p>From page 116 to 117:</p>
<blockquote>
<p>"A child (and, indeed, perhaps most adults) lives in a world in which everything is only partially understood; well enough perhaps, but never completely."</p>
</blockquote>
<hr>
<p>On page 120:</p>
<blockquote>
<p>"First, relate what is new and to be learned to something you already know. Second, take what is new and make it your own: ..."</p>
</blockquote>
<hr>
<p>On page 129:</p>
<blockquote>
<p>"... each of the microworlds we described can function as an explorable and manipulable environment."</p>
</blockquote>
<hr>
<p>From page 139 to 140:</p>
<blockquote>
<p>"Everyone is too busy following the cookbook. ... The computer is used to aggravate the already too-quantitative methodology of the physics classes."</p>
</blockquote>
<hr>
<p>Cool stuff from 142 to 145 (roughly) on the importance of working with and updating intuitions rather than discarding them for formalism alone.</p>
<hr>
<p>On page 155:</p>
<blockquote>
<p>"The cultural assimilation of the computer presence will give rise to a computer literacy. This phrase is often taken as meaning knowing how to program, or knowing about the varied uses made of computers. But true computer literacy is not just knowing how to make use of computers and computational ideas. It is knowing when it is appropriate to do so."</p>
</blockquote>
<hr>
<p>From page 158 to 159:</p>
<blockquote>
<p>"the idea of studying learning by focusing on the structure of what is learned"</p>
</blockquote>
<p>I think this idea is hugely important and too-often ignored. It is an empty pedagogy that spends all its effort in determining clever ways to teach the wrong things. The example in the text is of learning to ride a bike:</p>
<blockquote>
<p>"Thus learning to ride [a bike] does not mean learning to balance, it means learning not to unbalance, learning not to interfere."</p>
</blockquote>
<hr>
<p>The interpretation of <a href="https://en.wikipedia.org/wiki/Nicolas_Bourbaki">Bourbaki</a> on pages 159 and 160 is interesting but does not seem to be prominent today; by the wiki entry anyway, it seems like they were a good deal more formal than would be appropriate for early educational purposes.</p>
<hr>
<p>The idea of a "transitional object" (page 161) is interesting and connects possibly to ideas about using manipulatives in math education.</p>
<hr>
<p>On page 172:</p>
<blockquote>
<p>"Consider another example of how our images of knowledge can subvert our sense of ourselves as intellectual agents. Educators sometimes hold up an ideal of knowledge as having the kind of coherence defined by formal logic. But these ideals bear little resemblance to the way in which most people experience themselves. The subjective experience of knowledge is more similar to the chaos and controversy of competing agents than to the certitude and orderliness of <em>p</em>'s implying <em>q</em>'s. The discrepancy between our experience of ourselves and our idealizations of knowledge has an effect: It intimidates us, it lessens the sense of our own competence, and it leads us into counterproductive strategies for learning and thinking."</p>
</blockquote>
<hr>
<p>On page 177:</p>
<blockquote>
<p>"... a particular computer culture, a <a href="https://en.wikipedia.org/wiki/Mathetics">mathetic</a> one, that is, one that helps us not only to learn but to learn about learning."</p>
</blockquote>
<hr>
<p>On page 179:</p>
<blockquote>
<p>"In this book we have considered how mathematics might be learned in settings that resemble the Brazilian samba school, in settings that are real, socially cohesive, and where experts and novices are all learning."</p>
<p>"John Dewey expressed a nostalgia for earlier societies where the child becomes a hunter by real participation and by playful imitation."</p>
</blockquote>
<p><a href="http://www.amazon.com/Mindstorms-Children-Computers-Powerful-Ideas/dp/0465046746"><img alt="Mindstorms: Children, Computers, and Powerful Ideas" src="mindstorms.jpg"></a></p>    
    ]]></description>
<link>http://planspace.org/20160327-mindstorms/</link>
<guid>http://planspace.org/20160327-mindstorms/</guid>
<pubDate>Sun, 27 Mar 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Books for Programmers</title>
<description><![CDATA[

<p>Someone found my email address on <a href="https://github.com/">github</a> and wrote asking me to recommend books about software development. Well, here is one book on software development in general, and roughly one book each in connection with Python, JavaScript, Clojure, R, and git.</p>
<hr>
<h3>Software development in general</h3>
<p>Some years ago a mentor lent me a copy of <a href="https://pragprog.com/book/tpp/the-pragmatic-programmer">The Pragmatic Programmer</a>. It is a very useful collection of The Right Way to do things. It's a set of ground rules for doing good work that can make you and the people you work with more effective.</p>
<p><a href="https://pragprog.com/book/tpp/the-pragmatic-programmer"><img alt="The Pragmatic Programmer" src="prag_prog.jpg"></a></p>
<p>For more books on software development "in general", it looks like <a href="http://blog.codinghorror.com/recommended-reading-for-developers/">Atwood's list</a> is quite good. I should read some of those. He even lists Tufte!</p>
<hr>
<h3>Python</h3>
<p>I encountered <a href="http://greenteapress.com/wp/think-python-2e/">Think Python</a> and the rest of Downey's books while teaching some data science courses. It's a nice intro to computer science, programming, and Python, and it's available for free online.</p>
<p><a href="http://greenteapress.com/wp/think-python-2e/"><img alt="Think Python" src="think_python.jpg"></a></p>
<hr>
<h3>JavaScript</h3>
<p>If you want an intro to programming and JavaScript, I think everyone agrees that <a href="http://eloquentjavascript.net/">Eloquent JavaScript</a> is the right book. If you know how to program and want the dense low-down on JavaScript, the classic is <a href="http://www.amazon.com/JavaScript-Good-Parts-Douglas-Crockford/dp/0596517742">JavaScript: The Good Parts</a>. There's probably more modern stuff because now you can start a whole company and even convince people to fund you, all with JavaScript. But here's a book I actually read:</p>
<p><a href="http://shop.oreilly.com/product/0636920028857.do">Functional JavaScript</a> covers interesting functional programming techniques with JavaScript and the <a href="http://underscorejs.org/">underscore</a> library. I liked the content, but I also liked the writing so much that I looked for more by the author, which is what led me to the Clojure book below.</p>
<p><img alt="Functional JavaScript" src="functional_js.gif"></p>
<hr>
<h3>Clojure</h3>
<p><a href="https://clojure.org/">Clojure</a> is a neat language, and <a href="http://www.joyofclojure.com/">The Joy of Clojure</a> is a neat book. There is a ton to learn. It's "a Clojure book", but as the subtitle, "Thinking the Clojure Way", indicates, the language and the book are not bashful about philosophy, and it's a different philosophy from a lot of other languages and ways of thinking. Use and appreciate functional programming with immutable data structures.</p>
<p><a href="http://www.joyofclojure.com/"><img alt="The Joy of Clojure" src="joy_of_clojure.png"></a></p>
<hr>
<h3>R</h3>
<p>Years back I loved <a href="https://www.nostarch.com/artofr.htm">The Art of R Programming</a> for acknowledging that you need to program in R. Some books treat R like it's a statistics GUI that, for reasons unknown, you happen to have to type to use.</p>
<p>Nowadays I think Hadley's freely available <a href="http://adv-r.had.co.nz/">Advanced R</a> is by far the best R book. Despite the title, it does also cover introductory material, but the pace is quicker and the depth is greater than The Art of R Programming.</p>
<p><a href="http://adv-r.had.co.nz/"><img alt="Advanced R" src="advanced_r.jpg"></a></p>
<hr>
<h3>git</h3>
<p><a href="https://git-scm.com/">git</a> is not a language, it's a version control tool, but I'm including it here because there's an obvious book for it which is free online. It's called <a href="https://git-scm.com/book/en/v2">Pro Git</a>. Newcomers to git sometimes find git confusing, and there's a simple solution: read the book. Read the whole book. Just read it.</p>
<p><a href="https://git-scm.com/book/en/v2"><img alt="Pro Git" src="pro_git.jpg"></a></p>
<hr>
<p>See also <a href="/20160320-books_for_professionals/">Books for Professionals</a>.</p>    
    ]]></description>
<link>http://planspace.org/20160322-books_for_programmers/</link>
<guid>http://planspace.org/20160322-books_for_programmers/</guid>
<pubDate>Tue, 22 Mar 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Books for Professionals</title>
<description><![CDATA[

<p>My sister started her first post-graduate job last year. She asked me to recommend books on how to be a professional.</p>
<p>In my opinion, "being a professional" has at least two meanings:</p>
<ol>
<li>In general, (a) <strong>working effectively</strong> (b) <strong><em>with people</em></strong>.</li>
<li>In particular, being an expert in your specific field.</li>
</ol>
<p>Here are four picks regarding general professionalism.</p>
<hr>
<h3>(a) <strong>working effectively</strong></h3>
<p>I recommend <a href="https://en.wikipedia.org/wiki/How_to_Live_on_24_Hours_a_Day">How to Live on 24 Hours a Day</a> by Arnold Bennett. You can read it for free <a href="http://www.gutenberg.org/ebooks/2274">online via Project Gutenberg</a>.</p>
<p><img alt="How to Live on 24 Hours a Day" src="how_to_live_on_24_hours_a_day.jpeg"></p>
<p>I also highly recommend <a href="http://bitliteracy.com/">Bit Literacy</a>, which is a little old now but still relevant especially regarding how to handle email. You can read it for free <a href="http://itunes.apple.com/us/book/isbn9780979368103">in the iBookstore</a>.</p>
<p><img alt="Bit Literacy" src="bit_literacy.gif"></p>
<hr>
<h3>(b) <em>with people</em></h3>
<p>If you've never read <a href="https://en.wikipedia.org/wiki/How_to_Win_Friends_and_Influence_People">How to Win Friends and Influence People</a>, you should. I don't think it would be very surprising to you, since you're already so well-friended and influential, but it is a sort of classic and worth checking out. Looks like they're still trying to <a href="http://www.amazon.com/How-Win-Friends-Influence-People/dp/0671027034">sell it</a>.</p>
<p><img alt="How to Win Friends and Influence People" src="how_to_win_and_influence.jpg"></p>
<p>I also recently (well, last November) read <a href="http://shop.oreilly.com/product/0636920042372.do">Debugging Teams</a> and thought it was pretty good. I wrote <a href="http://planspace.org/20151118-debugging_teams/">some stuff</a> about it and pulled out quotes. The book is <a href="http://www.amazon.com/Debugging-Teams-Productivity-through-Collaboration/dp/1491932058">for sale</a>.</p>
<p><img alt="Debugging Teams" src="debugging_teams.png"></p>
<hr>
<p>Where is the line between competency that should be expected of everyone and area of specialist knowledge? This may be particularly confused by so-called "business books", some of which are about being effective in any role, some of which are specifically about "management", and all of which want to sell as many copies as possible at the airport. I suspect that most are weakly rehashing some of the above titles.</p>
<hr>
<p>See also <a href="/20160322-books_for_programmers/">Books for Programmers</a>.</p>    
    ]]></description>
<link>http://planspace.org/20160320-books_for_professionals/</link>
<guid>http://planspace.org/20160320-books_for_professionals/</guid>
<pubDate>Sun, 20 Mar 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Moments of Impact</title>
<description><![CDATA[

<p>This is a well-wrought business book about how to lead workshops.</p>
<p><a href="http://www.amazon.com/Moments-Impact-Strategic-Conversations-Accelerate/dp/1451697627"><img alt="Moments of Impact" src="cover.png"></a></p>
<p>When I got it, I thought this book was about negotiation: how to have
conversations that advance your aims. Really by "strategic
conversations" the authors mean "conversations about strategy" and
care mostly about getting business executives to work together.</p>
<p>In addition to open-ended "let's find a new direction" meetings, the
authors spend a lot of time on what are really questions of education.
Their guidance is similar to that for running an effective class
discussion, setting up small group work, or orchestrating experiential
learning.</p>
<p>It would be interesting to know what the effect would be of devoting
similar resources toward educating children as toward educating
executives.</p>
<p>The subtitle, "How to design strategic conversations that accelerate
change", includes the main term the authors are trying to advance:
"strategic conversations". I think that in a field with little in the
way of new ideas, authors of business books thrive to the extent that
they get their phrases to stick. The authors even focus on this kind
of phrase-finding in the contents of the book, and I think there's
something to it.</p>
<p>The book follows its outline very tightly, and its core outline is this:</p>
<blockquote>
<p><strong>Core principles and key practices</strong></p>
<ol>
<li>Define your purpose<ul>
<li>Seize your moment</li>
<li>Pick one purpose</li>
<li>Go slow to go fast</li>
</ul>
</li>
<li>Engage multiple perspectives<ul>
<li>Assemble a dream team</li>
<li>Create a common platform</li>
<li>Ignite a controlled burn</li>
</ul>
</li>
<li>Frame the issues<ul>
<li>Stretch (don't break) mind-sets</li>
<li>Think inside <em>different</em> boxes</li>
<li>Choose a few key frames</li>
</ul>
</li>
<li>Set the scene<ul>
<li>Make your space</li>
<li>Get visual</li>
<li><em>Do</em> sweat the small stuff</li>
</ul>
</li>
<li>Make it an experience<ul>
<li>Discover, don't tell</li>
<li>Engage the whole person</li>
<li>Create a narrative arc</li>
</ul>
</li>
</ol>
</blockquote>
<p>They go through that outline in book form in the first 170 pages, and
then again in a "starter kit" (think "CliffsNotes") in the next 60
pages. Frustratingly, very occasionally it seems like things appear in
the "starter kit" but not the long form text. Together, the two parts
make for a very pleasantly-dimensioned physical book.</p>
<p>A couple quotes:</p>
<blockquote>
<p>It's not always easy to point out the obvious so that people will
see it. (p. 92)</p>
<p>"Desire, not goal-directedness," [Professor Jeanne Lidtka of the University of Virginia's Darden School of Business] <a href="https://hbr.org/product/beyond-strategic-thinking-strategy-as-experienced/ROT127-PDF-ENG">writes</a>, "is the true driver of behavioral change." (p. 138)</p>
<p>Org charts are a hoax (p. 142)</p>
<p>... experience is not just the best teacher - it's the only one. (p. 163)</p>
</blockquote>
<p>The book is really quite nicely designed by <a href="http://minesf.com/">Mine</a>. Mine has a <a href="https://www.youtube.com/embed/vdGuEwZb378">bboy demo reel</a>.</p>    
    ]]></description>
<link>http://planspace.org/20160215-moments_of_impact/</link>
<guid>http://planspace.org/20160215-moments_of_impact/</guid>
<pubDate>Mon, 15 Feb 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>How I Blog, or, Colophon</title>
<description><![CDATA[

<p>I recently received a friendly email, including:</p>
<blockquote>
<p>... tell me what your development stack is for your blog, especially
your backend language and the frontend framework that you used. I
find your code snippets awesome. How did you embed them in your
blog?</p>
</blockquote>
<p>Thanks for asking!</p>
<p>Everything that is my blog, and I do mean everything, lives in
<a href="https://github.com/ajschumacher/ajschumacher.github.io/">one git repo</a>
and is (in the large) entirely custom.</p>
<p>In the past, I had blogs on <a href="https://www.blogger.com/">Blogger</a> and
then <a href="https://wordpress.com/">WordPress.com</a>. My domain name used to
point to my WordPress site.</p>
<p>I wanted more flexibility and control, and I didn't want to be tied to
any specific provider. I didn't want my writing to be floating around
in precarious databases; I wanted it all in version control.</p>
<p>My blog is static. It has no database or backend language, and can be
served by any conventional HTTP server. It is currently served via
<a href="https://pages.github.com/">GitHub Pages</a>.</p>
<p>I looked at a couple standard
<a href="https://www.staticgen.com/">static site generators</a> and decided that
I hated them all. My main complaint was that I didn't want to put what
<a href="http://jekyllrb.com/">Jekyll</a> calls "front matter" in my posts.</p>
<p>I wanted to write just
<a href="https://daringfireball.net/projects/markdown/">markdown</a>, usually. I
wanted as much as possible to not be tied to a particular static site
generator. And I like the idea of having a single and simple source of
truth. I even considered <a href="https://github.com/Xeoncross/jr">Jr</a>, the
"static static site generator", because it doesn't even generate HTML
files, but there you have to put a <code>&lt;script&gt;</code> tag in every post, which
is annoying and sort of an ugly hack anyway. (View source at the
<a href="http://xeoncross.github.io/jr/">Jr demo</a> to see what I mean.)</p>
<p>More or less, I thought about how I wanted to write a blog post, and
then wrote a Python script to make it work.</p>
<p>Here's my blogging process. I go to my local blog repo directory,
start a local web server, and make a new subdirectory:</p>
<pre><code class="language-bash">cd ~/ajschumacher.github.io/
python -m SimpleHTTPServer
mkdir 20160209-how_i_blog
cd 20160209-how_i_blog</code></pre>

<p>Everything for a post goes in the post directory. That includes
(usually) a markdown file and a generated HTML file, any images that
appear in the post, and (more rarely) other HTML, JavaScript, and so
on. I really like having <em>everything</em> together in that directory. I
don't want to have to coordinate across multiple directories when I'm
writing a post, and I want each post to be self-contained when it's
finished.</p>
<p>I put the date of the post in the directory name. My Python script
finds that date and inserts it into the generated HTML, which will be
<code>index.html</code> in the post's directory.</p>
<p>It's an HTTP server convention to serve <code>index.html</code> when you request
the directory; this mechanism satisfies my preference for having
semantic URLs without artifacts of an underlying technology. Think
about how you feel when you're at a <code>.aspx</code> URL. Gross.</p>
<pre><code class="language-bash">emacs index.md</code></pre>

<p>I edit <code>index.md</code> in the post's directory. The only rule is that the
first line is the title. My Python script looks for that line to make
into the headline in <code>index.html</code>, above the date.</p>
<p>My blog is put together mostly by one script, <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/make_page.py">make_page.py</a>. It uses a <a href="https://pypi.python.org/pypi/Markdown">Python implementation of markdown</a>, plus a custom hack to support <a href="https://help.github.com/articles/creating-and-highlighting-code-blocks/">GitHub-style fenced code blocks</a>, which I like. I also hack around to support <a href="https://www.latex-project.org/">LaTeX</a>-style equations via <a href="https://www.mathjax.org/">MathJax</a>, so I can write \(e^{i\pi}+1=0\) as easily as <code>\\(e^{i\pi}+1=0\\)</code>.</p>
<p>For presentations, I wanted to be able to write a single markdown file and then have both an article for reading and slides for presenting. So if <code>make_page.py</code> sees any lines with just five hyphens, it recognizes them as demarcating a slide, and then generates a <a href="https://github.com/tmcw/big">big</a> HTML/JS-based presentation in <code>big.html</code>. For example, you can <a href="http://planspace.org/20141117-well_used_simple_tools/">read this</a>, but if I'm giving it as a presentation I can <a href="http://planspace.org/20141117-well_used_simple_tools/big.html">show this</a>. Spoiler alert: If you type the <a href="http://code.snaptortoise.com/konami-js/">Konami code</a> from the <a href="http://planspace.org/20141117-well_used_simple_tools/">post</a>, it automatically takes you to the <a href="http://planspace.org/20141117-well_used_simple_tools/big.html">slides version</a>.</p>
<p>The styling for my blog is a hodge-podge of CSS, with main colors the inverse of the <a href="https://github.com/bbatsov/zenburn-emacs">Zenburn</a> color scheme. I use <a href="https://highlightjs.org/">highlight.js</a> to make code blocks looks nice. I like highlight.js a lot, except that it colors some things I wish it wouldn't, like the numbers in the code block above. Spoiler alert: I use <a href="https://elrumordelaluz.github.io/csshake/">CSShake</a> to make the arrow at the top of my <a href="/">root page</a> move around when you mouse over it.</p>
<p>I eventually wrote <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/make_rss.py">make_rss.py</a>, which generates an <code>rss.xml</code> file. Somebody told me they liked RSS. I'm not completely sure it totally works with images and relative paths and so on, or that anybody uses RSS any more.</p>
<p>I guess I'll mention that I use <a href="https://www.google.com/analytics/">Google Analytics</a> so I can see if anybody goes to my blog. Some people do. It gets more visitors than my old <a href="http://www.naldaramjui.com/">naldaramjui</a> project, but fewer pageviews. Just a few posts generate the bulk of the visits, and usually not the best ones.</p>
<p>Oh, remember those old Blogger and WordPress blogs I had? I exported them to XML and then wrote scripts to convert that content to work with my current system. It was annoying, but I was able to preserve the URLs from the WordPress blog, so I was pretty happy about that.</p>
<p>I usually write most of a post, and then revise while iteratively building it and looking at how it looks in a browser. This is sort of too much work, but I feel like the distance I get by switching from editor to browser helps me to see what I'm writing with fresh eyes, to some extent. So I'll run this many times, switching to a browser and then back to editor between runs:</p>
<pre><code class="language-bash">../make_page.py</code></pre>

<p>Since switching to my current blog system, updates to the <a href="/">index</a>
listing all my posts are always done manually. This is, again, more
work than it really needs to be, but it gives me very explicit control
over what appears in that index and how. (Not that I'm very creative
about it.)</p>
<p>So after all the edits to markdown files are complete, the release
process for a new post goes like this:</p>
<pre><code class="language-bash">../make_page.py
git add index.md index.html  # and any other files for the post
cd ..
./make_page.py
git add index.md index.html  # this is the root index
./make_rss.py
git add rss.xml
git commit -m 'new post about how my blog works'
git push</code></pre>

<p>And that's all there is to it! I do use <a href="https://github.com/ajschumacher/ajschumacher.github.io/issues">github issues</a> to keep track of post and other blog enhancement ideas, but I also put a lot of things there and never return to them.</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20160209-how_i_blog/</link>
<guid>http://planspace.org/20160209-how_i_blog/</guid>
<pubDate>Tue, 09 Feb 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Talk Like TED</title>
<description><![CDATA[

<p>There are some useful points in this book, but I don't like the book as a whole very much.</p>
<p><a href="http://gallocommunications.com/books/talk-like-ted-2/"><img alt="Talk Like TED" src="cover.jpg"></a></p>
<p>I think <a href="https://www.ted.com/">TED</a> talks are now recognized as being more consistently well-presented than truly profound, though both aspects have been diluted by time and <a href="https://www.ted.com/about/programs-initiatives/tedx-program">TEDx</a>. The book is about how to present, and not how to construct a sound argument or make a meaningful discovery, so I shouldn't be as critical as I feel. It's rare that I read something so far from my natural position on the analytical/emotional spectrum, and it wasn't totally pleasant.</p>
<p>My reaction started from the cover blurbs, with the unmeasured hyperbole in one from Guy Kawasaki: "The premise of this book&#8212;that TED talks provide great examples and lessons&#8212;is a magnificent insight." I would wonder whether it was intended as a sarcastic jab at TED and the book, but Kawasaki is a marketer of some repute. Has he learned that referring to occasionally valid observations as magnificent insights is useful?</p>
<p>A repeated opening argument from the author is that "Ideas are the true currency of the twenty-first century. So, in order to succeed you need to be able to sell your ideas and yourself persuasively." Those familiar with currency will note that you don't generally need to convince anyone to take it; the book's argument here is ridiculous, but it may be that the illogical presentation is more palatable than a more honest phrasing: "Most people have very little sense, so a practical way to succeed is to focus a lot on selling your ideas and yourself persuasively, regardless of the quality of you and your ideas."</p>
<p>The second chapter is called "Master the Art of Storytelling". I have to agree that using stories in speaking is a good idea for connecting to audiences and making points emotionally, but I also object because anecdotes are almost never useful grounds for drawing conclusions and as a person who looks at data a lot, a common reaction to a story as part of someone's argument is the feeling that they are probably wrong, deliberately lying, or both. But as the author says, "Abstractions are difficult for most people to process. Stories turn abstract concepts into tangible, emotional, and memorable ideas."</p>
<p>The author helpfully presents Aristotle's persuasion framework: ethos (credibility), logos (logic), and pathos (emotion). My usual bias is toward logic, and credibility only as much as it measures a person's historical record on logic. But the author is correct that emotion and other kinds of credibility are significant, and I shouldn't ignore them when communicating.</p>
<p>So I read this book in a state of displeased agreement, disliking the author from the start, whether he was playing fast and loose with propositions and conclusions, advertising his consulting business, or just titling all his books like BuzzFeed listicles. But I realized that his advice was frequently advice I could benefit from following. I just couldn't stand sentences like this one on page 27: "Academically, if you can't measure something you cannot quantify what it actually does." At some point, I gave up counting things like that.</p>
<p>I was very interested to learn of the <a href="http://m.ragan.com/Main/Articles/Speakers_follow_the_10_TED_Commandments_48603.aspx">Ten TED Commandments</a> that are sent to presenters while they're preparing:</p>
<ol>
<li>Thou shalt not simply trot out thy usual shtick.</li>
<li>Thou shalt dream a great dream, or show forth a wondrous new thing, or share something thou hast never shared before.</li>
<li>Thou shalt reveal thy curiosity and thy passion.</li>
<li>Thou shalt tell a story.</li>
<li>Thou shalt freely comment on the utterances of other speakers for the sake of blessed connection and exquisite controversy.</li>
<li>Thou shalt not flaunt thine ego. Be thou vulnerable; speak of thy failure as well as thy success.</li>
<li>Thou shalt not sell from the stage: neither thy company, thy goods, thy writings, not thy desperate need for funding; lest thou be cast aside into outer darkness.</li>
<li>Thou shalt remember all the while: Laughter is good.</li>
<li>Thou shalt not read thy speech.</li>
<li>Thou shalt not steal the time of them that follow thee.</li>
</ol>
<p><em>Talk Like TED</em> doesn't actually include the Ten TED Commandments; I had to go find them online. I imagine they weren't included because the author didn't want them to clash with his own list. Here's an annotated <em>Talk Like TED</em> table of contents, with its three parts with three chapters each. As a fun exercise, map the Ten TED Commandments onto <em>Talk Like TED</em>'s chapters!</p>
<ol>
<li>Emotional<ol>
<li><strong>Unleash the Master Within</strong> I was optimistic about this chapter because it comes close to saying "Talk about something you actually know about." It focuses a lot on passion.</li>
<li><strong>Master the Art of Storytelling</strong> People like stories. People remember stories. Tell stories.</li>
<li><strong>Have a Conversation</strong> This chapter is actually about having your presentation be technically perfect: well practiced, well delivered, etc. It's really astonishingly far from advocating engaging the audience in real conversation. The book is about one-directional communication, and no surprise; that's the TED format and the format of most things that are thought of as presentations.</li>
</ol>
</li>
<li>Novel<ol>
<li><strong>Teach Me Something New</strong> Again I like the idea of this chapter as it relates to choosing your content, but a lot of this chapter is also about giving your talk a Twitter-friendly headline.</li>
<li><strong>Deliver Jaw-Dropping Moments</strong> Be creative and do interesting things! But also speak in soundbites and be quotable on Twitter.</li>
<li><strong>Lighten Up</strong> This is the chapter on using humor. People like humor!</li>
</ol>
</li>
<li>Memorable<ol>
<li><strong>Stick to the 18-Minute Rule</strong> No strong rationale for 18 minutes exactly, but the point of being brief is a good one. There's also a lot in here about following the rule of three.</li>
<li><strong>Paint a Mental Picture with Multisensory Experiences</strong> This probably could have been a section in the storytelling chapter.</li>
<li><strong>Stay in Your Lane</strong> Be authentic.</li>
</ol>
</li>
</ol>
<p>Here are a couple other things I particularly liked:</p>
<p>Quoting Matthieu Ricard on page 26: "It is essential to inspire hope and confidence, since it is what we lack most and need most in our times."</p>
<p>For a while I've been a fan of separating what a presenter says and what appears on a projector: "Since we're all sick of 'Death by PowerPoint,' it's time to kill it permanently. Let me be clear&#8212;I'm not advocating the end of PowerPoint as a tool, but the end of traditional PowerPoint design cluttered with text and bullet points. ... The old style of PowerPoint is an anachronism on the modern corporate battlefield." <a href="https://en.wikipedia.org/wiki/Edward_Tufte">Tufte</a> has written more convincingly in <a href="http://users.ha.uth.gr/tgd/pt0501/09/Tufte.pdf">The Cognitive Style of PowerPoint</a> about the evils of bullet points as a medium.</p>
<p>Quoting John Medina on page 213: "To put it bluntly, research shows that <em>we can't multitask</em>. We are biologically of processing attention-rich inputs simultaneously." (Emphasis in original.) I sometimes hear people claiming that they can productively multitask, and I appreciate an excuse to not believe them.</p>
<p>In the end, <em>Talk Like TED</em> could be summarized as follows: people are irrational, but there are ways you can convince them of whatever you want. I am not so impressed with Carneades's ability to argue both sides of any issue and I identify more with Socrates in Gorgias: Truth is worth pursuing, and is fundamentally more important than the artifices of rhetoric. That said, I do not deny the utility of presenting good ideas well rather than poorly.</p>    
    ]]></description>
<link>http://planspace.org/20160207-talk_like_ted/</link>
<guid>http://planspace.org/20160207-talk_like_ted/</guid>
<pubDate>Sun, 07 Feb 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Juggling for Programmers</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/Seymour_Papert">Seymour Papert</a> wrote a book called <a href="http://www.amazon.com/Mindstorms-Children-Computers-Powerful-Ideas/dp/0465046746">Mindstorms: Children, Computers, And Powerful Ideas</a>. <a href="http://worrydream.com/">Bret Victor</a> says in <a href="http://worrydream.com/LearnableProgramming/">Learnable Programming</a> that Mindstorms is &#8220;perhaps the greatest book ever written on learning in general&#8221;.</p>
<p>Nestled in Mindstorms is juggling lesson which, it is claimed, reduces the time required to learn to juggle three balls continuously &#8220;often to as little as twenty or thirty minutes.&#8221; It took me about 40 minutes, but I was using ping pong balls, which are too light, small, and bouncy to be ideal. I have no other excuses.</p>
<p>The next section is how to juggle, from <a href="http://www.amazon.com/Mindstorms-Children-Computers-Powerful-Ideas/dp/0465046746">Mindstorms</a>.</p>
<hr>
<p>There are many different kinds of juggling. When most people think of juggling, they are thinking about a procedure that is called &#8220;showers juggling.&#8221; In showers juggling balls move one behind the other in a &#8220;circle&#8221; passing from left to right at the top and from right to left at the bottom (or vice versa). This takes two kinds of throws: a short, low throw to get the balls from hand to the other at the bottom of the &#8220;circle&#8221; (near the hands) and a long, high throw to get the balls to go around the top of the circle. (See Figure 11.)</p>
<p><img alt="Figure 11: Two Forms of Juggling" src="figure_11.png"></p>
<p>Cascade juggling has a simpler structure. There is no bottom of the circle; balls travel in both directions over the upper arc. There is only one kind of toss: a long and high one. (See Figure 11.) Its simplicity makes it a better route into juggling as well as a better example for our argument. Our guiding question is this: Will someone who wishes to learn cascade juggling be helped or hindered by a verbal, analytic description of how to do it? The answer is: It all depends. It depends on what materials the learner has for making analytic descriptions. We use cascade juggling to show how good computational models can help construct &#8220;people procedures&#8221; that improve performance of skills and how reflection on those people procedures can help us learn to program and to do mathematics. But, of course, <em>some</em> verbal descriptions will confuse more than they will help. Consider, for example, the description:</p>
<ol>
<li>Start with balls 1 and 2 in the left hand and ball 3 in the right.</li>
<li>Throw ball 1 in a high parabola to the right hand.</li>
<li>When ball 1 is at the vertex throw ball 3 over to the left hand in a similar high parabola, but take care to toss ball 3 under the trajectory of ball 1.</li>
<li>When ball 1 arrives at the right hand and ball 3 is at the vertex, catch ball 1 and throw ball 2 in a trajectory under that of ball 3, and so on.</li>
</ol>
<p>This description is basically a brute-force straight-line program. It is not a useful description for the purpose of learning. People outside the computer culture might say it is too much like a computer program, &#8220;just one instruction after another.&#8221; It is like certain programs, for example Keith's first <code>MAN</code> program. But we have seen that stringing instructions together without good internal structure is not a good model for computer programming either, and we shall see that the techniques of structured programming that <em>are</em> good for writing programs are also good as mathetic descriptions of juggling.</p>
<p>Our goal is to create a people procedure: <code>TO JUGGLE</code>. As a first step toward defining this procedure we identify and name subprocedures analogous to their role to the subprocedures Keith used in drawing his stick figure (<code>TO VEE</code>, <code>TO HEAD</code>, <code>TO LINE</code>). In the case of juggling, a natural pair of subprocedures is what we call <code>TOSSRIGHT</code> and <code>TOSSLEFT</code>. Just as the command <code>VEE</code> was defined functionally by the fact that it causes the computer to place a certain V-shaped figure on the screen, the command <code>TOSSLEFT</code> given to our apprentice juggler should &#8220;cause&#8221; him to throw a ball, which we assume he is holding in his left hand, over to the right hand.</p>
<p>But there is an important difference between programming <code>TO MAN</code> and programming <code>TO JUGGLE</code>. The programmer of <code>TO MAN</code> need not worry about timing, but in setting up the procedure for juggling we <em>must</em> worry about it. The juggler must perform the actions <code>TOSSRIGHT</code> and <code>TOSSLEFT</code> at appropriate moments in a cycle, and the two actions will have to overlap in time. Since we have chosen to include the catching phase in the same subprocedure as the throwing phase, the procedure <code>TOSSRIGHT</code> is meant to include catching the ball when it comes over to the left hand. Similarly, <code>TOSSLEFT</code> is a command to throw a ball from the left hand over to the right and catch it when it arrives.</p>
<p>Since most people can perform these actions, we shall take <code>TOSSLEFT</code> and <code>TOSSRIGHT</code> as given and concentrate on how they can be combined to form a new procedure we shall call <code>TO JUGGLE</code>. Putting them together is different in one essential way from the combination of subprocedures <code>TO VEE</code> and <code>TO HEAD</code> to make the procedure <code>TO MAN</code>. <code>TOSSLEFT</code> might have to be initiated before the action initiated by the previous <code>TOSSRIGHT</code> is completed. In the language of computer science, this is expressed by saying that we are dealing with <em>parallel</em> processes as opposed to the strictly <em>serial</em> processes used in drawing the stick figure.</p>
<p>To describe the combination of the subprocedures we introduce a new element of programming: The concept of a &#8220;<code>WHEN DEMON</code>.&#8221; This is illustrated by the instruction: <code>WHEN HUNGRY EAT</code>. In one version of LOGO this would mean: Whenever the condition called <code>HUNGRY</code> happens, carry out the action called <code>EAT</code>. The metaphor of a &#8220;demon&#8221; expresses the idea that the command creates an autonomous entity within the computer system, one that remains dormant until a certain kind of event happens, and then, like a demon, it pounces out to perform its action. The juggling act will use two such <code>WHEN DEMONS</code>.</p>
<p>Their definitions will be something like:</p>
<pre><code>WHEN something TOSSLEFT
WHEN something TOSSRIGHT</code></pre>

<p>To fill the blanks, the &#8220;somethings,&#8221; we describe two conditions, or recognizable states of the system, that will trigger the tossing action.</p>
<p>At a key moment in the cycle the balls are disposed about like this (Figure 12):</p>
<p><img alt="Figure 12" src="figure_12.png"></p>
<p>But this diagram of the state of the system is incomplete since it fails to show in which direction the top ball is flying. To complete it we add arrows to indicate a direction (Figure 13a) and obtain two state descriptions (Figures 13b and 13c).</p>
<p><img alt="Figure 13a" src="figure_13a.png"></p>
<p><img alt="Figure 13b: TOPRIGHT: The ball is at the top and is moving to the right" src="figure_13b.png"></p>
<p><img alt="Figure 13c: TOPLEFT: The ball is at the top and is moving to the left" src="figure_13c.png"></p>
<p>If we assume, reasonably, that the juggler can recognize these two situations, the following formalism should be self-explanatory:</p>
<pre><code>TO KEEP JUGGLING
WHEN TOPRIGHT TOSSRIGHT
WHEN TOPLEFT TOSSLEFT</code></pre>

<p>or even more simply:</p>
<pre><code>TO KEEP JUGGLING
WHEN TOPX TOSSX</code></pre>

<p>which declares that when the state <code>TOPRIGHT</code> occurs, the right hand should initiate a toss and when <code>TOPLEFT</code> occurs, the left hand should initiate a toss. A little thought will show that this is a complete description: The juggling process will continue in a self-perpetuating way since each toss creates a state of the system that triggers the next toss.</p>
<p>How can this model that turned juggling into a <em>people procedure</em> be applied as a teaching strategy? First, note that the model of juggling made several assumptions:</p>
<ol>
<li>that the learner can perform <code>TOSSRIGHT</code> and <code>TOSSLEFT</code></li>
<li>that she can recognize the trigger states <code>TOPLEFT</code> and <code>TOPRIGHT</code></li>
<li>that she can combine these performance abilities according to the definitions of the procedure <code>TO KEEP JUGGLING</code></li>
</ol>
<p>Now, we translate our assumptions and our people procedures into a teaching strategy.</p>
<p>STEP 1: Verify that the learner <em>can</em> toss. Give her one ball, ask her to toss it over into the other hand. This usually happens smoothly, but we will see later that a minor refinement is often needed. The spontaneous procedure has a bug.</p>
<p>STEP 2: Verify that the learner can combine tosses. Try with two balls with instructions:</p>
<pre><code>TO CROSS
TOSSLEFT
WHEN TOPRIGHT TOSSRIGHT
END</code></pre>

<p>This is intended to exchange the balls between left and right hands. Although it appears to be a simple combination of <code>TOSSLEFT</code> and <code>TOSSRIGHT</code>, it usually does not work immediately.</p>
<p>STEP 3: Look for bugs in the toss procedures. Why doesn't <code>TO CROSS</code> work? Typically we find that the learner's ability to toss is not really as good as it seemed in step 1. The most common deviation or &#8220;bug&#8221; in the toss procedure is following the ball with the eyes in doing a toss. Since a person has only one pair of eyes, their engagement in the first toss makes the second toss nearly impossible and thus usually ends in disaster with the balls on the floor.</p>
<p>STEP 4: Debugging. Assuming that the bug was following the first ball with the eyes, we debug by returning our learner to tossing with one ball without following it with her eyes. Most learners find (to their amazement) that very little practice is needed to be able to perform a toss while fixing the eyes around the expected apex of the parabola made by the flying ball. When the single toss is debugged, the learner again tries to combine two tosses. Most often this now works, although there may still be another bug to eliminate.</p>
<p>STEP 5: Extension to three balls. Once the learner can smoothly execute the procedure we called <code>CROSS</code>, we go on to three balls. To do this beings with two balls in one hand and one in the other (Figure 14).</p>
<p>Ball 2 is tossed as if executing <code>CROSS</code>, ignoring ball 1. The <code>TOSSRIGHT</code> in <code>CROSS</code> brings the three balls into a state that is ready for <code>KEEP JUGGLING</code>. The transition from <code>CROSS</code> to <code>KEEP JUGGLING</code> presents a little difficulty for some learners, but this is easily overcome. Most people can learn to juggle in less than half an hour by using this strategy.</p>
<p><img alt="Figure 14: Cascade Juggling" src="figure_14.png"></p>
<p>Variants of this teaching strategy have been used by many LOGO teachers and studied in detail by one of them, Howard Austin, who took the analysis of juggling as the topic of his Ph.D. thesis. There is no doubt that the strategy is very effective and little doubt as to the cause: The use of programming concepts as a descriptive language facilitates debugging.</p>
<hr>
<p>I think this, and all of <a href="http://www.amazon.com/Mindstorms-Children-Computers-Powerful-Ideas/dp/0465046746">Mindstorms</a>, is pretty cool. If nothing else, there is no longer any reason for anyone not to learn to juggle!</p>    
    ]]></description>
<link>http://planspace.org/20151231-juggling_for_programmers/</link>
<guid>http://planspace.org/20151231-juggling_for_programmers/</guid>
<pubDate>Thu, 31 Dec 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Hearing Data with Sonic Histograms</title>
<description><![CDATA[

<p>You can see an iris, and you can smell an iris, and you can measure the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">length and width of 150 iris sepals and petals</a>... but can you <em>hear</em> the iris data?</p>
<p><audio controls preload="auto" autobuffer>
    <source src="yes.mp3" type="audio/mpeg"></source>
    <source src="yes.ogg" type="audio/ogg"></source>
</audio></p>
<p><a href="https://en.wikipedia.org/wiki/Iris_flower_data_set"><img alt="iris" src="iris.jpg"></a></p>
<p><a href="https://en.wikipedia.org/wiki/Histogram">Histograms</a> are a great way to get a sense for the distribution of a set of values. The view you get depends on how wide your bars are.</p>
<p><img alt="two histograms" src="two_histograms.png"></p>
<p>Both those histograms look pretty good, but audio isn't so boxy. A cousin of the histogram, <a href="https://en.wikipedia.org/wiki/Kernel_density_estimation">kernel density estimation</a> (KDE) seems more like audio. Like a histogram, it can be more or less "smooth".</p>
<p><img alt="two KDEs" src="two_kdes.png"></p>
<p>Stealing the idea of KDE, we can adapt it for audio. We'll just add up audio pulses that are positioned in time to represent values.</p>
<p>We can use "wide" audio points <audio controls preload="auto" autobuffer><source src="sonic_point_wide.mp3" type="audio/mpeg"></source><source src="sonic_point_wide.ogg" type="audio/ogg"></source></audio> or "narrow" audio points <audio controls preload="auto" autobuffer><source src="sonic_point_narrow.mp3" type="audio/mpeg"></source><source src="sonic_point_narrow.ogg" type="audio/ogg"></source></audio>.</p>
<p>Combining a bunch of wide audio points, we get a "smoother" audio histogram. <audio controls preload="auto" autobuffer><source src="sonic_sepal_length_wide.mp3" type="audio/mpeg"></source><source src="sonic_sepal_length_wide.ogg" type="audio/ogg"></source></audio> And if we combine narrow audio points, we get a more "discrete" sounding histogram. <audio controls preload="auto" autobuffer><source src="sonic_sepal_length_narrow.mp3" type="audio/mpeg"></source><source src="sonic_sepal_length_narrow.ogg" type="audio/ogg"></source></audio></p>
<p>Adding an audio axis label and some sonic tick marks, we get serviceable audio histograms that you can compare to their visible counterparts.</p>
<p><audio controls preload="auto" autobuffer><source src="sonic_sepal_length.mp3" type="audio/mpeg"></source><source src="sonic_sepal_length.ogg" type="audio/ogg"></source></audio> <img alt="sepal length histogram" src="sepal_length.png"></p>
<p><audio controls preload="auto" autobuffer><source src="sonic_sepal_width.mp3" type="audio/mpeg"></source><source src="sonic_sepal_width.ogg" type="audio/ogg"></source></audio> <img alt="sepal width histogram" src="sepal_width.png"></p>
<p><audio controls preload="auto" autobuffer><source src="sonic_petal_length.mp3" type="audio/mpeg"></source><source src="sonic_petal_length.ogg" type="audio/ogg"></source></audio> <img alt="petal length histogram" src="petal_length.png"></p>
<p><audio controls preload="auto" autobuffer><source src="sonic_petal_width.mp3" type="audio/mpeg"></source><source src="sonic_petal_width.ogg" type="audio/ogg"></source></audio> <img alt="petal width histogram" src="petal_width.png"></p>
<p>So yes, it kind of works! You can certainly hear bimodality, and even differentiate between the first two distributions if you listen carefully. I probably won't switch from visualizations to sonifications, but it's a fun things to explore!</p>
<hr>
<p>Thanks to <a href="https://www.spotify.com/">Spotify</a>'s <a href="http://monthlymusichackathon.org/">Monthly Music Hackathon NYC</a> (<a href="https://twitter.com/musichackathon">@musichackathon</a>) <a href="http://monthlymusichackathon.org/post/133438271112/viz-son">Sound Visualization &amp; Data Sonification Hackathon</a> for providing the push to do this. There's <a href="http://livestream.com/accounts/5176069/events/4582831">video on Livestream</a> (I'm on from about 5:30 to 10:30) but ironically the audio doesn't seem to be working. Thanks also to <a href="https://thomaslevine.com/">Thomas Levine</a> for showing me the <a href="https://cran.r-project.org/web/packages/tuneR/index.html">tuneR</a> library for <a href="https://www.r-project.org/">R</a> which let me jump in and start making sounds really quickly.</p>
<hr>
<p>This code was hacked together quickly and is not what you'd call "production grade". It uses the Mac <code>say</code> and also <code>ffmpeg</code> via <code>system</code>, but it's otherwise <a href="play.R">R that might work for you</a> if you want to try it.</p>
<pre><code class="language-r"># install.packages("tuneR")  # install if not installed
library("tuneR")
setWavPlayer('/usr/bin/afplay')  # on Mac

point_at &lt;- function(value,  # data value
                     lowest,  # data range min
                     highest,  # data range max
                     duration,  # seconds
                     point_width,  # seconds for +/- 3 SD (points are normal)
                     point_freq=440, sample_rate=44100) {
  point_width_samples &lt;- point_width * sample_rate
  point &lt;- sine(point_freq, point_width_samples, stereo=TRUE)
  filter &lt;- dnorm(seq_along(point), mean = length(point)/2, sd = length(point)/6)
  filter &lt;- filter / max(filter)
  point &lt;- point * filter
  duration &lt;- duration * sample_rate
  data_range &lt;- highest - lowest
  left_offset &lt;- value - lowest
  peak_position &lt;- (left_offset / data_range) * duration
  left_padding_duration &lt;- peak_position - length(point)/2
  if (left_padding_duration &gt; 0) {
    result &lt;- bind(silence(left_padding_duration, stereo=TRUE), point)
  } else {
    result &lt;- point[abs(left_padding_duration):length(point)]
  }
  if (length(result) &lt; duration) {
    result &lt;- bind(result, silence(duration - length(result), stereo=TRUE))
  } else {
    result &lt;- result[1:duration]
  }
  result
}

points_at &lt;- function(values,  # data values
                      lowest,  # data range min
                      highest,  # data range max
                      duration,  # seconds
                      point_width,  # seconds for +/- 3 SD (points are normal)
                      point_freq=440, sample_rate=44100) {
  result &lt;- silence(duration * sample_rate, stereo = TRUE)
  for (value in values) {
    result &lt;- result + point_at(value, lowest, highest, duration, point_width,
                                point_freq, sample_rate)
  }
  result
}

low_high &lt;- function(values) {
  breakpoints &lt;- pretty(values)
  lowest &lt;- breakpoints[1]
  highest &lt;- breakpoints[length(breakpoints)]
  c(lowest, highest)
}

normalized_sonic_hist_content &lt;- function(values,  # data values
                                          duration=4,  # seconds
                                          point_width=0.2,  # seconds for +/1 3 SD (points are normal)
                                          point_freq=440, sample_rate=44100) {
  breakpoints &lt;- low_high(values)
  lowest &lt;- breakpoints[1]
  highest &lt;- breakpoints[2]
  content &lt;- points_at(values, lowest, highest, duration, point_width,
                       point_freq, sample_rate)
  normalize(content)
}

sonic_hist &lt;- function(values,  # data values
                       main,  # "title" of variable
                       duration=4,  # seconds
                       point_width=0.2,  # seconds for +/1 3 SD (points are normal)
                       point_freq=440,
                       legend=TRUE,
                       units="",
                       edge=TRUE, edge_freq=880, edge_duration=1000,
                       sample_rate=44100) {
  if (missing(main)) {
    main &lt;- deparse(substitute(values))
  }
  breakpoints &lt;- low_high(values)
  lowest &lt;- breakpoints[1]
  highest &lt;- breakpoints[2]
  content &lt;- normalized_sonic_hist_content(values, duration, point_width, point_freq, sample_rate)
  edge_sound &lt;- sine(edge_freq, duration=edge_duration, stereo=TRUE)
  if (edge) {
    content &lt;- bind(edge_sound, content, edge_sound)
  }
  if (legend) {
    system(paste("say", main, "in", duration, "seconds from", lowest, "to", highest, units, "-o t"))
    system("ffmpeg -i t.aiff -ar 44100 t.wav")
    legend_mono &lt;- normalize(readWave("t.wav"), pcm=FALSE)
    legend_stereo &lt;- stereo(legend_mono, legend_mono)
    system("rm t.aiff t.wav")
    content &lt;- bind(legend_stereo, content)
  }
  content
}

triple_save &lt;- function(some_wav) {
  name_prefix &lt;- deparse(substitute(some_wav))
  wav_name &lt;- paste(name_prefix, ".wav", sep='')
  mp3_name &lt;- paste(name_prefix, ".mp3", sep='')
  ogg_name &lt;- paste(name_prefix, ".ogg", sep='')
  writeWave(some_wav, wav_name)
  system(paste("ffmpeg -i", wav_name, mp3_name))
  system(paste("ffmpeg -i", wav_name, ogg_name))
  system(paste("rm", wav_name))
}

data("iris")

iris$Sepal.Length


png('two_histograms.png', width=800, height=380)
par(mfrow=c(1, 2))
hist(iris$Sepal.Length, main="sepal length histogram, default breaks")
hist(iris$Sepal.Length, breaks=100, xlim=c(4, 8), main="sepal length histogram, 100 breaks")
dev.off()

png('two_kdes.png', width=800, height=380)
par(mfrow=c(1, 2))
plot(density(iris$Sepal.Length), main="sepal length kernel density, default binwidth")
plot(density(iris$Sepal.Length, bw=0.01), main="sepal length kernel density, 0.01 binwidth")
dev.off()

sonic_point_wide &lt;- point_at(value=4, lowest=0, highest=8, duration=2, point_width=0.5)
triple_save(sonic_point_wide)
sonic_point_narrow &lt;- point_at(value=4, lowest=0, highest=8, duration=2, point_width=0.05)
triple_save(sonic_point_narrow)

sonic_sepal_length_wide &lt;- sonic_hist(iris$Sepal.Width, legend=F, edge=F, point_width=0.4)
triple_save(sonic_sepal_length_wide)
sonic_sepal_length_narrow &lt;- sonic_hist(iris$Sepal.Width, point_width=0.05, legend=F, edge=F)
triple_save(sonic_sepal_length_narrow)

sonic_tick_mark &lt;- sine(880, duration=1000, stereo=TRUE)
triple_save(sonic_tick_mark)

png("sepal_length.png", height=380, width=480)
hist(iris$Sepal.Length, main="sepal length, centimeters")
dev.off()
sonic_sepal_length &lt;- sonic_hist(iris$Sepal.Length, main="sepal length", units="centimeters")
triple_save(sonic_sepal_length)

png("sepal_width.png", height=380, width=480)
hist(iris$Sepal.Width, main="sepal width, centimeters")
dev.off()
sonic_sepal_width &lt;- sonic_hist(iris$Sepal.Width, main="sepal width", units="centimeters")
triple_save(sonic_sepal_width)

png("petal_length.png", height=380, width=480)
hist(iris$Petal.Length, main="petal length, centimeters")
dev.off()
sonic_petal_length &lt;- sonic_hist(iris$Petal.Length, main="petal length", units="centimeters")
triple_save(sonic_petal_length)

png("petal_width.png", height=380, width=480)
hist(iris$Petal.Width, main="petal width, centimeters")
dev.off()
sonic_petal_width &lt;- sonic_hist(iris$Petal.Width, main="petal width", units="centimeters")
triple_save(sonic_petal_width)</code></pre>

<hr>
<p>If you're having any problems with your sonic histograms, you can adjust them with a <a href="https://en.wikipedia.org/wiki/Sonic_screwdriver">sonic screwdriver</a>.</p>
<p><img alt="sonic screwdriver" src="sonic_screwdriver.png"></p>    
    ]]></description>
<link>http://planspace.org/20151214-hearing_data_with_sonic_histograms/</link>
<guid>http://planspace.org/20151214-hearing_data_with_sonic_histograms/</guid>
<pubDate>Mon, 14 Dec 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>How to Eat Computers</title>
<description><![CDATA[

<p><em>A ten-minute talk about &#8220;my experiences with computer science&#8221; for <a href="https://hourofcode.com/">Hour of Code</a>. Given for first, second, and third-graders at the <a href="http://www.lemanmanhattan.org/">L&#233;man Manhattan Preparatory School</a>'s <a href="http://www.lemanmanhattan.org/academics/lower">Lower School</a> on Friday December 4, 2015, and for seventh and eighth-graders at <a href="http://schools.nyc.gov/SchoolPortals/13/K113/">NYC MS 113 Ronald Edmonds Learning Center</a> on Tuesday December 8. (<a href="big.html">slides</a>)</em></p>
<p>I started with a cute demo of the <code>say</code> functionality at the Mac command line. It's much easier to get the computer to <code>say hello</code> than it used to be!</p>
<pre><code class="language-bash">say Hello!
say I am a computer.
say Please do not eat me.</code></pre>

<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img height="1000%" src="img/aaron.jpg" title="young Aaron"></p>
<hr>
<p>This is me at the beginning of my academic career.</p>
<p>Back then, my family didn't have a computer at home.</p>
<p>So how did I learn about computers?</p>
<hr>
<p>friends and family</p>
<hr>
<p>One way I was lucky to learn some things was through friends and family.</p>
<hr>
<p><img src="img/grandparents.jpg" title="grandparents"></p>
<hr>
<p>These are my grandparents.</p>
<p>My grandfather used to program back when that meant plugging cables into different plugs.</p>
<p>He taught me some of the first things I learned about computers.</p>
<hr>
<p>binary</p>
<hr>
<p>One thing my grandpa taught me was binary.</p>
<p>Binary is fun and cool!</p>
<hr>
<p><code>&#160;&#160;0</code></p>
<hr>
<p>This is zero.</p>
<hr>
<p><code>&#160;&#160;1</code></p>
<hr>
<p>This is one.</p>
<p>Easy, right?</p>
<hr>
<p><code>&#160;10</code></p>
<hr>
<p>This is two.</p>
<hr>
<p><code>&#160;11</code></p>
<hr>
<p>This is three.</p>
<hr>
<p><code>100</code></p>
<hr>
<p>So what's this?</p>
<p>That's right, it's four!</p>
<p>So now you know binary.</p>
<p>Everything inside the computer is binary.</p>
<p>To get the computer to say &#8220;Hello&#8221;, it takes quite a lot of binary!</p>
<hr>
<p>school</p>
<hr>
<p>Another good way to learn things is at school.</p>
<hr>
<p><img src="img/old_apple.png" title="old Apple ]["></p>
<hr>
<p>At my old school, we had some computers that looked like this.</p>
<hr>
<p>typing</p>
<hr>
<p>Probably the most frequently useful thing I learned on those old computers was typing.</p>
<p>Typing is so important!</p>
<p>And I don't mean typing on a phone or a tablet. Touch screens are for babies.</p>
<p>You need to be able to touch type on a physical keyboard.</p>
<hr>
<p>internet</p>
<hr>
<p>Another source for learning materials is the internet.</p>
<hr>
<p>typing.com</p>
<hr>
<p>For example, you can learn and practice typing for free at <a href="https://www.typing.com/">typing.com</a>.</p>
<p>If you can't type 80 words per minute, go to <a href="https://www.typing.com/">typing.com</a> and keep practicing until you can!</p>
<hr>
<p><img height="1000%" src="img/cat_typing_animated.gif" title="cat typing"></p>
<hr>
<p>If you're lucky, a very large part of your future will involve typing.</p>
<hr>
<p>reading</p>
<hr>
<p>Reading is yet another really great way to learn things.</p>
<hr>
<p><img height="1000%" src="img/basic.jpg" title="basic manual"></p>
<hr>
<p>When I was young, I read a book like this which let me start to program on those old computers at my school.</p>
<p>This book is a bit out of date now, but there are lots of great books that you should read!</p>
<hr>
<p><img height="1000%" src="img/lauren_ipsum.png" title="Lauren Ipsum"></p>
<hr>
<p>This <a href="http://www.laurenipsum.org/">Lauren Ipsum</a> book is a really fun story that introduces computer science ideas in the context of a cool adventure story. Read it!</p>
<hr>
<p><img height="1000%" src="img/think_python.jpg" title="Think Python"></p>
<hr>
<p>This <a href="http://www.greenteapress.com/thinkpython/">Think Python</a> book is a more technical introduction to computer science and the Python programming language.</p>
<p>It's also super cool, and you can read the whole thing for free online! Just <a href="https://www.google.com/#q=Think%20Python">google 'Think Python'</a>.</p>
<hr>
<p><img height="1000%" src="img/cat_reading.jpg" title="reading cat"></p>
<hr>
<p>Cool cats read.</p>
<hr>
<p>build things</p>
<hr>
<p>You should also be learning things by building things.</p>
<hr>
<p><img height="1000%" src="img/ti_82.png" title="TI-82 graphing calculator"></p>
<hr>
<p>When I was school we had these huge calculators with tons of buttons.</p>
<p>They're kind of clunky, but the great thing was that you could write programs directly on them, any time.</p>
<p>We made so many cool things!</p>
<hr>
<p><span id="js">JavaScript</span></p>
<hr>
<p>Nowadays, computers with browsers are everywhere, which means YOU can program everywhere, with JavaScript inside the browser!</p>
<p>On my computer, with the Chrome browser, I can open up a JavaScript console with <code>option-command-j</code>.</p>
<p>By typing <code>document.getElementById('js').innerHTML = '&amp;hearts;'</code> here, I can change what my presentation says right here!</p>
<p>In the same way, JavaScript lets you play with programming and the web, any time!</p>
<hr>
<p>take things apart</p>
<hr>
<p>Another great way to learn about things is to take them apart.</p>
<hr>
<p><img height="1000%" src="img/cat_screwdriver.jpg" title="cat building computer"></p>
<hr>
<p>It's not always as easy to take computers apart these days, but it's still a fun thing to do if you can.</p>
<p>Taking computers apart, and building new ones yourself too, are great ways to understand how computers work.</p>
<hr>
<p>try things</p>
<hr>
<p>Really a lot of learning is just about trying things.</p>
<hr>
<p>concessionist, sandwich artist, computer salesperson, box cutter, clerk, assistant system administrator, experimentalist, brand manager, math teacher, English teacher, analyst, senior data services specialist, data science expert in residence, consultant, data scientist, data science teacher, senior data scientist, senior data scientist and software engineer</p>
<hr>
<p>For example, I've tried a lot of jobs.</p>
<p>These are most of the job titles I've had since I was 15.</p>
<p>I've tried a lot of things, and I'm happy to say that my current job is the best ever. I do fun things with fun people, and I work from home with a hoodie on.</p>
<hr>
<p><img src="img/cat_reclining.png" title="cat reclining with keyboard"></p>
<hr>
<p>So this is what I look like at work.</p>
<hr>
<p><img height="1000%" src="img/cat_at_mini_desk.jpg" title="cat at mini desk"></p>
<hr>
<p>Sometimes I look like this.</p>
<hr>
<p><img height="1000%" src="img/cat_night.jpg" title="cat hacking at night"></p>
<hr>
<p>Sometimes I look like this.</p>
<hr>
<p><img height="1000%" src="img/cat_raging.jpg" title="enthusiastic cat"></p>
<hr>
<p>Sometimes I look like this.</p>
<hr>
<p><img height="1000%" src="img/cat_watching.jpg" title="cat watching screen"></p>
<hr>
<p>Sometimes I look like this.</p>
<hr>
<p><img src="img/cat_pouncing.jpg" title="cat pouncing at keyboard"></p>
<hr>
<p>And sometimes I look like this.</p>
<hr>
<p>What do I do?</p>
<hr>
<p>What do I actually do?</p>
<p>Good question!</p>
<hr>
<p><img src="img/xkcd_programming.png" title="xkcd programming"></p>
<hr>
<p>A big part of what I do is programming. I'm programming one way or another every day. Designing and building programs explains the "software engineer" part of my job title.</p>
<p>(image from <a href="https://xkcd.com/722/">xkcd 722</a>)</p>
<hr>
<p><img height="1000%" src="img/cat_science.jpg" title="science cat"></p>
<hr>
<p>My job title also includes data scientist, which mostly means doing experiments with data to find the best solutions to the problems we're dealing with.</p>
<hr>
<p><code>machine learning</code></p>
<hr>
<p>Data science includes working with what's called "machine learning".</p>
<p>Computers are stupid.</p>
<p>They're <em>so</em> stupid, that we have to tell them how to learn things, and then we have to try to get them to learn something by looking at some data.</p>
<hr>
<p><img src="img/wheres_waldo.jpg" title="Where's Waldo?"></p>
<hr>
<p>The main problem I'm working on right now is a lot like "Where's Waldo".</p>
<p>Can we get a computer to tell us where in the picture Waldo is?</p>
<p>Could the computer identify all the interesting things in the picture?</p>
<hr>
<p><a href="/20150907-interactive_perceptron_training_toy/">demo</a></p>
<hr>
<p>I don't have a good demo for the full problem I'm currently working on, but I do have my <a href="/20150907-interactive_perceptron_training_toy/">Interactive Perceptron Training Toy</a>, which is different from the "Where's Waldo" example, and simpler, and shows a lot of the moving parts.</p>
<p>(Proceed to demo with a few points and training on the perceptron toy.)</p>
<hr>
<p>here we go!</p>
<hr>
<p>(This slide flashes in presentation mode...)</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20151206-how_to_eat_computers/</link>
<guid>http://planspace.org/20151206-how_to_eat_computers/</guid>
<pubDate>Sun, 06 Dec 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>See sklearn trees with D3</title>
<description><![CDATA[

<p>The <a href="http://scikit-learn.org/stable/modules/tree.html">decision trees</a> from <a href="http://scikit-learn.org/">scikit-learn</a> are very easy to train and predict with, but it's not easy to see the rules they learn. The code below makes it easier to see inside <code>sklearn</code> classification trees, enabling visualizations that look like this:</p>
<p><a href="http://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f"><img alt="partial tree view" src="partial_tree_view.png"></a></p>
<p>This shows, for example, that all the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">irises</a> with <code>petal length (cm)</code> less than 2.45 were <code>setosa</code>.</p>
<p>The ability to interpret the rules of a decision tree is often considered a strength of the algorithm, and in <a href="https://www.r-project.org/">R</a> you can usually <code>summary()</code> and <code>plot()</code> a tree fit to see the rules. In <a href="https://www.python.org/">Python</a> with <code>sklearn</code>, there is <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"><code>export_graphviz</code></a>, but it isn't terribly convenient. It shouldn't be so hard to see what's going on inside a tree.</p>
<p>The following <a href="http://www.json.org/">JSON</a> format is simple and works with common <a href="http://d3js.org/">D3</a> tree graphing code, so let's target this format:</p>
<pre><code class="language-json">{name: "container thing",
 children: [{name: "leaf thing one"},
            {name: "leaf thing two"}]}</code></pre>

<p>Each <code>name</code> will describe a true/false decision rule for an inner node or the distribution of training example labels for a leaf node. The first of a pair of <code>children</code> is where the rule is true, and the second is where the rule is false. (These are binary trees.)</p>
<p>The way <code>sklearn</code> trees store their rules internally is described <a href="https://github.com/scikit-learn/scikit-learn/blob/0.16.1/sklearn/tree/_tree.pyx#L2956-L3008">in <code>_tree.pyc</code></a>. The <code>rules</code> function here examines a fit <code>sklearn</code> decision tree to generate a Python dictionary (with structure like the above) representing the decision tree's rules:</p>
<pre><code class="language-python">def rules(clf, features, labels, node_index=0):
    """Structure of rules in a fit decision tree classifier

    Parameters
    ----------
    clf : DecisionTreeClassifier
        A tree that has already been fit.

    features, labels : lists of str
        The names of the features and labels, respectively.

    """
    node = {}
    if clf.tree_.children_left[node_index] == -1:  # indicates leaf
        count_labels = zip(clf.tree_.value[node_index, 0], labels)
        node['name'] = ', '.join(('{} of {}'.format(int(count), label)
                                  for count, label in count_labels))
    else:
        feature = features[clf.tree_.feature[node_index]]
        threshold = clf.tree_.threshold[node_index]
        node['name'] = '{} &gt; {}'.format(feature, threshold)
        left_index = clf.tree_.children_left[node_index]
        right_index = clf.tree_.children_right[node_index]
        node['children'] = [rules(clf, features, labels, right_index),
                            rules(clf, features, labels, left_index)]
    return node</code></pre>

<p>How is this used? Let's get a quick example decision tree and take a look:</p>
<pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

data = load_iris()

clf = DecisionTreeClassifier(max_depth=3)
clf.fit(data.data, data.target)

rules(clf, data.feature_names, data.target_names)</code></pre>

<p>The <code>rules</code> function returns the following Python dictionary, formatted for readability here:</p>
<pre><code class="language-python">{'name': 'petal length (cm) &gt; 2.45000004768',
 'children': [
     {'name': 'petal width (cm) &gt; 1.75',
      'children': [
         {'name': 'petal length (cm) &gt; 4.85000038147',
          'children': [
              {'name': '0 of setosa, 0 of versicolor, 43 of virginica'},
              {'name': '0 of setosa, 1 of versicolor, 2 of virginica'}]},
         {'name': 'petal length (cm) &gt; 4.94999980927',
          'children': [
              {'name': '0 of setosa, 2 of versicolor, 4 of virginica'},
              {'name': '0 of setosa, 47 of versicolor, 1 of virginica'}]}]},
     {'name': '50 of setosa, 0 of versicolor, 0 of virginica'}]}</code></pre>

<p>This is pretty readable, but now we can also write the result out to a file and visualize it with D3:</p>
<pre><code class="language-python">import json

r = rules(clf, data.feature_names, data.target_names)
with open('rules.json', 'w') as f:
    f.write(json.dumps(r))</code></pre>

<p>Check out the <a href="http://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f">interactive view</a>! Once again, a partially expanded view looks like this:</p>
<p><a href="http://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f"><img alt="partial tree view" src="partial_tree_view.png"></a></p>    
    ]]></description>
<link>http://planspace.org/20151129-see_sklearn_trees_with_d3/</link>
<guid>http://planspace.org/20151129-see_sklearn_trees_with_d3/</guid>
<pubDate>Sun, 29 Nov 2015 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
