<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Word Vectors and SAT Analogies</title>
<description><![CDATA[

<p>In 2013, <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> popularized <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">word vectors</a> and <em>king - man + woman = queen</em>.</p>
<p><a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"><img alt="king queen etc." src="img/king_queen.png"></a></p>
<p>This reminded me of <a href="https://en.wikipedia.org/wiki/SAT">SAT</a> analogy questions, which <a href="http://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them">disappeared from the SAT in 2005</a>, but looked like this:</p>
<pre><code>PALTRY : SIGNIFICANCE ::

A. redundant : discussion
B. austere : landscape
C. opulent : wealth
D. oblique : familiarity
E. banal : originality</code></pre>

<p>The king/queen example is not difficult, and I don't know whether it was tested or discovered. A better evaluation would use a set of challenging pre-determined questions.</p>
<p>There is a Google set of analogy questions, but all the relationships are grammatical, geographical, or by gender. Typical: "fast : fastest :: old : oldest." (<a href="http://download.tensorflow.org/data/questions-words.txt">dataset</a>, <a href="https://arxiv.org/abs/1301.3781">paper</a>, <a href="https://aclweb.org/aclwiki/Google_analogy_test_set_(State_of_the_art)">context</a>)</p>
<p>SAT questions are more interesting. Selecting from fixed answer choices provides a nice guessing baseline (1/5 is 20%) and using a human test means it's easier to get human performance levels (average US college applicant is 57%; human voting is 81.5%).</p>
<p>Michael Littman and Peter Turney have made available <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">a set of 374 SAT analogy questions</a> since 2003. You have to email Turney to get them, and I appreciate that he helped me out.</p>
<p>Littman and Turney <a href="http://cogprints.org/4518/1/NRC-48273.pdf">used</a> a vector-based approach on their dataset back in 2005. They achieved 47% accuracy (state of the art at the time) which is a nice benchmark.</p>
<p>They made vectors for each word pair using web data. To get one value for "banal : originality" they would search <a href="https://en.wikipedia.org/wiki/AltaVista">AltaVista</a> for "banal and not originality" and take the log of the number of hits. With a list of 64 connectives they made vectors with 128 components.</p>
<p>I'm using <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> and word2vec word vectors that are per-word and based on various text corpora directly. Since the vectors are not specific to particular pairings, they may at a relative disadvantage for the SAT task. To get a vector for a word pair, I just subtract.</p>
<p>Stanford provides a variety of GloVe vectors pre-trained on three different corpora:</p>
<ul>
<li>Twitter (<code>glove.twitter</code>)<ul>
<li>2B tweets, 27B tokens, 1.2M vocab, uncased</li>
<li>as each of 25d, 50d, 100d, &amp; 200d vectors</li>
</ul>
</li>
<li>Wikipedia 2014 + Gigaword 5 (<code>glove.6B</code>)<ul>
<li>6B tokens, 400K vocab, uncased</li>
<li>as each of 50d, 100d, 200d, &amp; 300d vectors</li>
</ul>
</li>
<li>Common Crawl<ul>
<li>uncased (<code>glove.42B</code>) 42B tokens, 1.9M vocab, 300d vectors</li>
<li>cased (<code>glove.840B</code>) 840B tokens, 2.2M vocab, 300d vectors</li>
</ul>
</li>
</ul>
<p>I have one word2vec model: <code>GoogleNews-vectors</code> trained on a Google News dataset, providing 300d vectors.</p>
<p><img src="img/accuracy.png" width="480"></p>
<ul>
<li>The best word vectors for the SAT analogies task (<code>glove.840B</code>) achieve 49% accuracy. This outperforms the 2005 47% result, but is still within the confidence bounds of 42.2% to 52.5% that they report.</li>
<li>Holding the training set constant and varying the size of the vectors affects performance on the SAT analogies task: bigger vectors work better, though performance may be plateauing around 300d.</li>
<li>Twitter data does markedly worse on the SAT analogies task. This is consistent with Twitter's limitations and reputation for being less than erudite.</li>
</ul>
<p>Trained word vectors provide a fixed vocabulary. When a word was missing, I used a vector of zeros. Most embeddings only missed one to eight words from the 374 questions, but <code>glove.twitter</code> missed 105. I also checked accuracies when excluding questions that had unsupported words for a set of embeddings, and the results were remarkably close, even with the Twitter embeddings. So the accuracies are due to the embeddings and not just gaps in the vocabularies.</p>
<p>It is pretty impressive that word vectors work as well as they do on this task, but nobody should consider word vectors a solution to natural language understanding. Problems with word vectors have been pointed out (<a href="http://www.aclweb.org/anthology/N15-1098">source</a>, <a href="http://www.aclweb.org/anthology/C/C16/C16-1262.pdf">source</a>, <a href="http://anthology.aclweb.org/W16-2503">source</a>, <a href="https://arxiv.org/abs/1705.11168">source</a>).</p>
<p>Still, the idea of word vectors (translating sparse data to a learned dense representation) is super useful, and not just for words. See implementations <a href="https://keras.io/layers/embeddings/">in Keras</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embedding_column">in TensorFlow</a>.</p>
<p>These methods are not the best-performing non-human technique for these SAT analogy questions. Littman and Turney <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">report</a> several. Latent Relational Analysis comes in at 56% accuracy, against the average US college applicant at 57%.</p>
<p>This is a case in which "the average human" is not a good bar for measuring AI success. The average human has a remarkably small vocabulary relative to what a computer should easily handle, not to mention that the average human would not be admitted to most colleges you could name.</p>
<hr>
<p>I'm grateful to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for helpful discussions, and particularly Shabnam Tafreshi and Dmitrijs Milajevs for sharing references at <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/events/237114317/">the January 26, 2017 meeting</a>. Thanks also to <a href="https://twitter.com/metasemantic">Travis Hoppe</a> of <a href="http://dc.hackandtell.org/">DC Hack and Tell</a> for always doing cool things with NLP. Thanks to Peter Turney for providing the dataset and commenting on distances.</p>
<p>Notes:</p>
<ul>
<li>An <a href="https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/">example</a> of word2vec popularization.</li>
<li>My code is available at <a href="https://github.com/ajschumacher/sat_analogies">ajschumacher/sat_analogies</a>.</li>
<li>All but 20 of the 374 questions have five designed answer choices; those 20 have four and "no : choice" as the last option. To maintain comparability I kept those questions in, though it means a human guessing baseline should be slightly over 20%.</li>
<li>For efficiency, I used GloVe values as 2-byte floats, which is lossy. I also tested with 4-byte floats, and the results varied by at most one question, while being much slower to generate.</li>
<li>Cosine distance is more effective than Euclidean distance for this task, and the advantage increases with dimensionality.</li>
</ul>
<p><img src="img/cosine_advantage.png" width="480"></p>    
    ]]></description>
<link>http://planspace.org/20170705-word_vectors_and_sat_analogies/</link>
<guid>http://planspace.org/20170705-word_vectors_and_sat_analogies/</guid>
<pubDate>Wed, 05 Jul 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Questions from TensorFlow APIs Webinar</title>
<description><![CDATA[

<p>I did <a href="/20170524-more_and_better_new_tensorflow_apis/">More and Better: The New TensorFlow APIs</a> as a <a href="https://www.altoros.com/blog/event/more-and-better-the-new-tensorflow-apis/">webinar</a>. <a href="https://www.altoros.com/">Altoros</a> ran it using <a href="https://www.gotomeeting.com/webinar">GoToWebinar</a>, which reported all questions from attendees. Questions and answers:</p>
<hr>
<h3>What should I learn?</h3>
<blockquote>
<p>"New to DL and TF... several people have said to not waste time w/Keras, etc, but go deep into TF to really learn it.  From what you said, it seems like TF changes a lot, though.  Recommendations in terms of sequence/path for learning DL concepts and the tools?"</p>
</blockquote>
<p>First, regarding the rate of change of TensorFlow: The low-level API should really be quite stable now. It's the high-level APIs that are still settling down a little bit. And even there, it looks like things are getting pretty set. I wouldn't expect Keras to change a lot, in particular.</p>
<p>The question of whether to use a high-level API or a low-level API depends on your goals. I don't think it's a waste of time to use Keras. In fact, I think it would be easier to argue that it's "a waste of time" to dig into low-level TensorFlow implementation details, because there's a lot you can get done much more quickly by using Keras.</p>
<p>It depends on your goals. Do you want to understand every step in the internals of your system? I've heard people like Matt Zeiler suggest that you should write your own deep learning framework from scratch in order to understand all the details of backprop, etc. How far do you want to go?</p>
<p>For most people, I think higher-level interfaces are a good choice. Sometimes they can actually make it easier to learn the concepts that are really important, because you don't have to deal with all the details that can complicate things.</p>
<p>That's not to say that it isn't valuable to get into the low-level details as well. Sometimes you really do need to make changes or understand phenomena that are down at that level. But you don't necessarily need to start at the low level; you can also start at higher levels of abstraction and then learn lower levels later.</p>
<p>As for learning DL concepts and tools, I'm currently a big advocate of <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a> by Fran&#231;ois Chollet, the author of Keras. It's still in pre-release, but the chapters that are already available are quite good. I think it's a great book especially for people who are newer to deep learning because it explains a lot of fundamental things in approachable ways.</p>
<hr>
<h3>Keras outside or inside TensorFlow?</h3>
<blockquote>
<p>"What is the link between keras and tf that is being shown? I have used Keras and I simply indicate that it should use tensorflow in the backend and my calls do not include tf.xxx.keras.xxx... One of the goals of Keras is to be framework independent. To call from Tensorflow doesn&#8217;t really maintain that spirit."</p>
</blockquote>
<p>You can still use Keras directly with TensorFlow as the backend, maintaining framework independence. As far as I know, Keras will continue to be developed separately from TensorFlow as well as within the TensorFlow codebase. And for many things, using Keras as it appears inside TensorFlow will work just the same way as using independent Keras with the TensorFlow backend.</p>
<p>Having Keras inside TensorFlow is a convenience, for one, but it should also enable some tighter integrations. I'm expecting the layers API will become even more unified and allow for mixing and matching even more design possibilities. More importantly, I think, Keras models in TensorFlow will be easily convertible to TensorFlow Estimators, which should be pretty neat.</p>
<hr>
<h3>C? C++?</h3>
<blockquote>
<p>"how come [TensorFlow is] written in C++ but recommended interfaces should talk to the C part?"</p>
</blockquote>
<p>I suppose there are at least two reasons. One is that it standardizes the interface, making it very clear what is stable and safe to build an API to. Another is that as a practical matter, lots of languages have established standard ways to connect to a C API. To read more about TensorFlow's take, check out the <a href="https://www.tensorflow.org/extend/language_bindings">language support documentation</a>.</p>
<hr>
<h3>On-graph? Off-graph?</h3>
<blockquote>
<p>"Can you remind folks about what 'on-graph' is?"</p>
</blockquote>
<p>TensorFlow's core way of working is by defining a graph of operations and then running them; anything that runs that way is "on-graph."</p>
<p>Normal Python code doesn't work that way; you don't define a graph and then run it, you just run your Python code right away.</p>
<p>Even within the TensorFlow Python API, not everything works by putting things on the graph. For example, <code>tf.python_io.tf_record_iterator</code> doesn't use the TensorFlow graph.</p>
<p>I wrote some more about this distinction in <a href="/20170428-everything_in_the_graph_even_glob/">Everything in the Graph? Even Glob?</a>. An example of TensorFlow functionality that exists in both off-graph and on-graph versions is TFRecords processing. I have a post that uses the <a href="/20170323-tfrecords_for_humans/">off-graph API</a> and two posts that use the on-graph API: <a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">reading from disk</a> and <a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">parsing TFRecords</a>.</p>
<hr>
<h3>Python 2? Python 3?</h3>
<blockquote>
<p>"Are you using Python 2.x?  If so why?"</p>
</blockquote>
<p>I use Python 2.7 a lot because projects I work on use it. I usually recommend that new projects start up with Python 3.</p>
<p>The inertia of Python 2.7 is really incredible, but maybe the tide is turning at last. Under a year ago, this quote <a href="https://blog.openai.com/infrastructure-for-deep-learning/">appeared</a> on OpenAI's blog:</p>
<blockquote>
<p>"Like much of the deep learning community, we use Python 2.7."</p>
</blockquote>
<p>More recently though, another <a href="https://blog.openai.com/openai-baselines-dqn/">post</a> from OpenAI indicated at least some work is now on Python 3:</p>
<blockquote>
<p>"We use Python 3 and TensorFlow."</p>
</blockquote>
<hr>
<h3>Hyperparameter tuning?</h3>
<blockquote>
<p>"I would like to know if there is a recommended way to make hyperparameter tuning (number of layers, number of nodes in each layer, learning rate, number of epochs)"</p>
</blockquote>
<p>For number of epochs, you can evaluate your model multiple times during training to get train and eval performance curves which should help in making that choice.</p>
<p>For architectural choices like number of layers and number of nodes in each layer, and optimization parameters like learning rate, you can parameterize your code so that choices become command-line arguments, which makes it easier to run experiments and determine what works best.</p>
<p>Making choices based on your previous experience and what's worked for others can save you a lot of time; usually you don't really want to search over <em>all</em> the things you <em>could</em> vary.</p>
<p>When you're really not sure, you can search over all your possible parameters. You may be able to do better than grid search: even <a href="http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization">random search</a> can be better, and <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a> could be better yet. Google has <a href="https://cloud.google.com/ml-engine/docs/how-tos/using-hyperparameter-tuning">productized hyperparameter tuning</a>, which may be worth checking out.</p>
<hr>
<h3>Visualization for very large models?</h3>
<blockquote>
<p>"I have a question, can we also visualize distributed tensorflow on very large models with billion parameters with tensorboard?"</p>
</blockquote>
<p>The answer is probably "yes" but there are multiple aspects to this question.</p>
<p>First, when running  distributed TensorFlow training following the standard pattern, probably you have your chief worker writing out TensorBoard summaries. So that centralizes the "where" of TensorBoard summary writing.</p>
<p>Second, even with small models, it's possible to generate unwieldy amounts of TensorBoard data. You probably want to think carefully about what you want to save during training. For example, you probably don't need to log things every iteration, and you probably don't need to log every parameter value if a histogram will do the job.</p>
<p>Third, some of the things you likely want to visualize for a large model are the same as things you want to visualize for a small model, like loss progression through training. There might not be any changes necessary when model size is different.</p>
<p>If you just want to visualize the model structure (the model graph) then you can take advantage of TensorBoard's grouping functionality so that you can see and reason about components across multiple scales.</p>
<p>In the end, it will depend on exactly what you want to visualize. TensorBoard has a lot of neat features, but it won't be the right choice for every possible problem.</p>
<hr>
<h3>What about serving?</h3>
<blockquote>
<p>How does TensorFlow Serving fit in currently?</p>
</blockquote>
<p><a href="https://tensorflow.github.io/serving/">TensorFlow Serving</a> is largely distinct from the APIs that I showed, except that Estimators can easily <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel">export</a> the SavedModel format that you want to use for serving.</p>    
    ]]></description>
<link>http://planspace.org/20170526-questions_from_tensorflow_apis_webinar/</link>
<guid>http://planspace.org/20170526-questions_from_tensorflow_apis_webinar/</guid>
<pubDate>Fri, 26 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>More and Better: The New TensorFlow APIs</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://www.altoros.com/blog/event/more-and-better-the-new-tensorflow-apis/">webinar</a> given Wednesday May 24, 2017. (<a href="big.html">slides</a>) (<a href="https://altoros.wistia.com/medias/e5su4b1vtz">video</a>) The Jupyter notebook and supporting files for code demos are available in a <a href="https://github.com/ajschumacher/tf_api_20170524">repository</a> on GitHub.</em></p>
<hr>
<p>Thank you!</p>
<hr>
<p>Before starting, I want to take a moment to notice how great it is to have a webinar. Thank you for checking it out! We are all really lucky. If this isn't nice, I don't know what is.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work for a company called Deep Learning Analytics. You can find out more about DLA at <a href="https://www.deeplearninganalytics.com/">deeplearninganalytics.com</a>. We work with a number of frameworks, and I've been keeping an eye on TensorFlow.</p>
<p>I do <em>not</em> work for Google, and I'm not a core TensorFlow developer. I'm just talking about my view of things, which is not authoritative or official.</p>
<hr>
<p>motivation</p>
<hr>
<p>Why this talk? What is it about, and who should care?</p>
<hr>
<p>"a low-level library, meaning you&#8217;ll be multiplying matrices and vectors." (2015)</p>
<hr>
<p>This is a description of TensorFlow from an <a href="http://fastml.com/what-you-wanted-to-know-about-tensorflow/">article</a> that came out the same month TensorFlow was released, in November of 2015. It was pretty true then.</p>
<p>Some people thought this was fine. It was like <a href="http://deeplearning.net/software/theano/">Theano</a>.</p>
<p>Some people thought it was not so fine, and so they may have decided TensorFlow was not for them.</p>
<p>But while Theano hasn't radically expanded its API to include tons of higher-level ways of using it, TensorFlow has. TensorFlow is no longer only a low-level library.</p>
<hr>
<pre><code class="language-python">...TensorFlow 1.2.0rc0</code></pre>

<hr>
<p>TensorFlow wasn't "finished" when it was released as open source. It only hit version 1.0 in February of this year. And that means the low-level API is largely stabilized. But high-level APIs are being added at a rapid pace.</p>
<p>I gave a <a href="/20170509-building_tensorflow_systems_from_components/">workshop</a> on TensorFlow at <a href="https://conferences.oreilly.com/oscon/">OSCON</a> two weeks ago, and one of the APIs I'll talk about today is new since then. I'll also mention things that aren't going to be in TensorFlow for at least a few weeks yet.</p>
<p>(I'm referring to the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data"><code>Dataset</code> API</a>, which entered <code>contrib</code> in 1.2, and the Keras model <code>get_estimator</code> method, respectively.)</p>
<hr>
<p>"I don't know which API to use." (2017)</p>
<hr>
<p>This is a sentiment I encountered in talking to people at the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a>.</p>
<p>There are now so many ways of using TensorFlow, it can be overwhelming, and you might not know where to start. If you do jump in, you might only see one part of the API and not know what else you're missing.</p>
<hr>
<p>a tour of TensorFlow APIs</p>
<hr>
<p>This talk is not for people who are expert TensorFlow developers or people who have been following TensorFlow development super closely.</p>
<p>This talk is for people who are new to TensorFlow, or people who may have tried TensorFlow in the past but haven't been following more recent developments.</p>
<p>The hope is that you'll understand better the options you have for using TensorFlow and be able to choose what to explore.</p>
<hr>
<ul>
<li>language survey</li>
<li>Python survey</li>
<li>Python focus</li>
</ul>
<hr>
<p>We'll start very broad and then narrow in on a few high-level APIs for TensorFlow.</p>
<p>First, considering the way TensorFlow works with multiple programming languages can illuminate the core of TensorFlow's low-level design. This introduces the low-level Python API, and the key distinction between implementations on and off the TensorFlow graph.</p>
<p>We're not going to cover them all equally, but we'll survey a lot of Python API options and how they relate.</p>
<p>Finally, we'll focus on aspects of the core TensorFlow Python APIs, especially Keras and the Estimator API, with a little bit more detail and code examples.</p>
<p>(The following section is based on <a href="/20170320-tensorflow_apis_for_various_languages/">TensorFlow APIs for Various Languages</a>.)</p>
<hr>
<p>What even is TensorFlow?</p>
<hr>
<p>Here's one way to think about TensorFlow.</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>The beating heart of TensorFlow is the Distributed Execution Engine, or runtime.</p>
<p>One way to think of it is as a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">virtual machine</a> whose language is the TensorFlow graph.</p>
<p>That core is written in C++, but lots of TensorFlow functionality lives in the frontends. In particular, there's a lot in the Python frontend.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<p>Python is the richest frontend for TensorFlow. It's what we'll use today.</p>
<p>You should remember that not everything in the Python TensorFlow API touches the graph. Some parts are just Python.</p>
<hr>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>R has an unofficial <a href="https://rstudio.github.io/tensorflow/">TensorFlow API</a> which is kind of interesting in that it just wraps the Python API. It's really more like an R API to the Python API to TensorFlow. So when you use it, you write R, but Python also runs. This is not how TensorFlow encourages languages to implement TensorFlow APIs, but it's out there.</p>
<hr>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<p>The way TensorFlow encourages API development is via TensorFlow's C bindings.</p>
<hr>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<p>You could use C++, of course.</p>
<hr>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<p>Also there's Java.</p>
<hr>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<p>And there's Go.</p>
<p>Python, C++, Java, and Go are the only languages with official Google-blessed TensorFlow APIs. But there are projects offering APIs that interface with TensorFlow via the C bindings.</p>
<hr>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<p>There's Rust.</p>
<hr>
<p><img alt="C#" src="img/c_sharp.png"></p>
<hr>
<p>And there's C#.</p>
<hr>
<p><img alt="Julia" src="img/julia.png"></p>
<hr>
<p>And there's Julia.</p>
<hr>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<p>And there's even Haskell!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Currently, a rough summary is that languages other than Python have TensorFlow support that is close to the runtime. Basically make your ops and run them.</p>
<p>This is likely to be enough support to deploy a TensorFlow system doing inference in whatever language you like, but if you're developing and training systems you probably still want to use Python.</p>
<hr>
<p>graph or not graph?</p>
<hr>
<p>So this is a distinction to think about: Am I using the TensorFlow graph, or not?</p>
<p>The distinction will be pretty clear when using the low-level API. It becomes less obvious when using higher-level APIs, but the situation there is that you can safely assume they're using the graph.</p>
<hr>
<p>Python APIs</p>
<hr>
<p>So, what's this about multiple Python APIs?</p>
<p>TensorFlow started with a low-level API, and it's always been possible and even a good idea to build higher-level APIs on top of it. Plenty of people have done so.</p>
<p>Let's start by checking out some APIs that are entirely outside of TensorFlow.</p>
<p>(The following section is based on <a href="/20170321-various_tensorflow_apis_for_python/">Various TensorFlow APIs for Python</a>.)</p>
<hr>
<p><img alt="Bucksstar Coffee" src="img/bucksstar.png" height="100%"></p>
<hr>
<p>It seems like lots of projects got names involving one or the other of "tensor" or "flow". It doesn't mean they're part of TensorFlow, and it doesn't mean they're good.</p>
<p>Of course, maybe they are; some of these could be good for you, for all I know.</p>
<hr>
<p><img alt="TFLearn logo" src="img/tflearn.png" height="100%"></p>
<hr>
<p>The confusingly named <a href="https://github.com/tflearn/tflearn">TFLearn</a> (no space; perhaps more clearly identified as <a href="(http://tflearn.org/)">TFLearn.org</a>) is not at all the same thing as the TF Learn (with a space) that appears in TensorFlow at <code>tf.contrib.learn</code>. TFLearn is a <a href="http://stackoverflow.com/questions/38859354/what-is-the-difference-between-tf-learn-aka-scikit-flow-and-tflearn-aka-tflea">separate</a> Python package that uses TensorFlow. It seems like it aspires to be like Keras. Even the TFLearn logo looks like a knock-off.</p>
<hr>
<p><img alt="TensorLayer logo" src="img/tensorlayer.png"></p>
<hr>
<p>Another one with a confusing name is <a href="https://github.com/zsdonghao/tensorlayer/">TensorLayer</a>. This is a separate package from TensorFlow and it's different from TensorFlow's layers API.</p>
<hr>
<p>ImageFlow</p>
<hr>
<p><a href="https://github.com/HamedMP/ImageFlow">ImageFlow</a> was supposed to make it easier to do image work with TensorFlow. It looks like it's abandoned.</p>
<hr>
<p>Pretty Tensor</p>
<hr>
<p><a href="https://github.com/google/prettytensor">Pretty Tensor</a> is still a Google project, so it might be worth checking out if you really like <a href="https://en.wikipedia.org/wiki/Fluent_interface">fluent interfaces</a> with lots of chaining, like <a href="https://d3js.org/">d3</a>.</p>
<hr>
<p><img alt="Sonnet" src="img/sonnet.png"></p>
<hr>
<p>Google's <a href="https://deepmind.com/">DeepMind</a> group <a href="https://deepmind.com/blog/open-sourcing-sonnet/">released</a> their <a href="https://github.com/deepmind/sonnet">Sonnet</a> library, which is used in the code for their <a href="https://github.com/deepmind/learning-to-learn">learning to learn</a> and <a href="https://github.com/deepmind/dnc">Differentiable Neural Computer</a> projects. Sonnet isn't part of TensorFlow (it builds on top) but it is a Google project. Sonnet has a modular approach compared to that of <a href="https://github.com/torch/nn">Torch/NN</a>. Also they have a cool logo.</p>
<hr>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<hr>
<p>The previous APIs exist outside of TensorFlow. Let's now talk about APIs <em>inside</em> TensorFlow.</p>
<p>We'll trace the development of the stack of available APIs before looking at some code examples.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p>Python Frontend (op level)</p>
<hr>
<p>You can continue to use TensorFlow's low-level APIs.</p>
<p>There are ops, like <code>tf.matmul</code> and <code>tf.nn.relu</code>, which you might use to build a neural network architecture in full detail. To do really novel things, you may want this level of control. But you may also prefer to work with larger building blocks. The other other APIs below will mostly specialize in this kind of application.</p>
<p>There are also ops like <code>tf.image.decode_jpeg</code> (and many others) which may be necessary but don't necessarily relate to what is usually considered the architecture of neural networks. Some higher-level APIs wrap some of this functionality, but they usually stay close to the building of network architectures and the training of such networks once defined.</p>
<hr>
<p>Layers</p>
<hr>
<p>The "layer" abstraction is a natural way to think about deep neural network design.</p>
<p>This <code>layers</code> API provides a first higher level of abstraction over writing things out by individual ops. For example, <code>tf.layers.conv2d</code> implements a convolution layer that involves multiple individual ops.</p>
<hr>
<p><img alt="slim... shady?" src="img/slim_shady.png"></p>
<hr>
<p>You may have heard of <a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html">TF-Slim</a>. TF-Slim has a number of <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">components</a> but it looks like essentially we're seeing the following: <code>tf.contrib.slim.layers</code> became <code>tf.contrib.layers</code> becomes <code>tf.layers</code>.</p>
<p>Sergio Guadarrama, one of the TF-Slim authors, generously confirmed that TensorFlow is converging on a layers API and implementation along these lines, but warns that there can still be some differences. He points out that <code>tf.contrib.layers</code> have <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/arg_scope"><code>arg_scope</code></a>, while <code>tf.layers</code> don't.</p>
<p>Other parts of TF-Slim are likely also worth using, and there is a collection of <a href="https://github.com/tensorflow/models/tree/master/slim">models that use TF-Slim</a> in the <a href="https://github.com/tensorflow/models">TensorFlow models repository</a>.</p>
<p>Historical note: It looks like before calling them layers, TF-Slim overloaded the word "op" for their layer concept (see <a href="https://github.com/tensorflow/models/tree/master/inception/inception/slim">earlier documentation</a>).</p>
<p>TF-Slim is in the TensorFlow codebase as <code>tf.contrib.slim</code>.</p>
<hr>
<p><img alt="Keras" src="img/keras.jpg" height="100%"></p>
<hr>
<p><a href="https://keras.io/">Keras</a> was around before TensorFlow. It was always a high-level API for neural nets, originally running with <a href="http://www.deeplearning.net/software/theano/">Theano</a> as its backend.</p>
<p>After the release of TensorFlow, Keras moved to also work with TensorFlow as a backend. And now TensorFlow is absorbing at least some aspects of the Keras project into the TensorFlow codebase, though <a href="https://twitter.com/fchollet">Fran&#231;ois Chollet</a> seems likely to continue championing Keras as a very fine project in its own right. (See also his response to a <a href="https://www.quora.com/What-will-Keras-do-with-TensorFlow-Slim">Quora question</a> about any possible relationship between TF-Slim and Keras.)</p>
<p>Keras is appearing in the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/keras">tf.contrib.keras</a> and should move to <code>tf.keras</code> at TensorFlow 1.3, at which point it will also be possible to turn Keras models directly into Estimators, which we'll see next.</p>
<hr>
<p><img alt="scikit-learn logo" src="img/sklearn.png" height="100%"></p>
<hr>
<p>Distinct from TensorFlow, the <a href="http://scikit-learn.org/">scikit-learn</a> project makes a lot of machine learning models conveniently available in Python.</p>
<p>A key design element of scikit is that many different models offer the same simple API: they are all implemented as <em>estimators</em>. So regardless of whether the model is linear regression or a GBM (Gradient Boosting Machine), you get the same simple interface.</p>
<p>The estimators API started as <a href="https://github.com/tensorflow/skflow">skflow</a> ("scikit-flow") before moving into the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">tf.contrib.learn</a> ("TF Learn") and now the base estimator code is getting situated in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator">tf.estimator</a> and <a href="/20170521-navigating_tensorflow_estimator_documentation/">documentation</a> is accumulating.</p>
<hr>
<p>Estimators</p>
<hr>
<p>The Estimators API really just defines an interface. You can design models that fit into the Estimator system, or you can use Estimator-based models that are already "set up" to varying degrees.</p>
<hr>
<p>Canned Estimators</p>
<hr>
<p>Canned estimators are concrete pre-defined models that follow the estimator conventions. Currently there are a bunch right in <code>tf.contrib.learn</code>, such as <code>LinearRegressor</code> and <code>DNNClassifier</code>. There are some elsewhere. For example, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/g3doc/kmeans.md">kmeans</a> is in <code>tf.contrib.factorization</code>. It isn't clear to me exactly where all the canned estimators will eventually settle down in the API; some may eventually move out of <code>contrib</code>.</p>
<p>"Canned Estimators" are a lot like scikit-learn in the level at which they make functionality available to programmers.</p>
<hr>
<ul>
<li>automatic checkpoints</li>
<li>automatic logging</li>
<li>separate train/eval/pred</li>
<li>easily train distributed</li>
</ul>
<hr>
<p>What do you get with Estimators? A bunch of things!</p>
<hr>
<p><img alt="data pipeline" src="img/tf_data_pipeline.gif"></p>
<hr>
<p>One more thing: loading data. A lot of people have gotten hung up on the complicated multi-thread, multi-queue, queue-runner design that TensorFlow has espoused for loading data.</p>
<hr>
<p>Datasets</p>
<hr>
<p>TensorFlow developers <a href="https://github.com/tensorflow/tensorflow/issues/7951">listened</a>, and they came up with <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data">Datasets</a>, which is a new API that wraps all the complexity in a pretty nice interface.</p>
<p>This is new as of about a week ago.</p>
<hr>
<p><img alt="latest diagram" src="img/io17_tiers.png"></p>
<hr>
<p>So this is the latest diagram from Google I/O. It adds XLA, which I won't talk about.</p>
<p>(Image from <a href="https://www.youtube.com/watch?v=OzAdKMPgUt4">TensorFlow Frontiers</a> at Google I/O '17.)</p>
<hr>
<p>code!</p>
<hr>
<p>Let's look at some code, shall we?</p>
<p>Go to a notebook (<a href="https://github.com/ajschumacher/tf_api_20170524/blob/master/APIs_start.ipynb">start</a>/<a href="https://github.com/ajschumacher/tf_api_20170524/blob/master/APIs_finish.ipynb">finish</a>) in the <a href="https://github.com/ajschumacher/tf_api_20170524">tf_api_20170524</a> repo.</p>
<hr>
<p>more</p>
<hr>
<p>For lots more, there are plenty of places you can go.</p>
<p>My <a href="/">blog</a> is one place.</p>
<p><a href="https://www.tensorflow.org/">TensorFlow</a>'s documentation is another.</p>
<p>The latest <a href="https://www.youtube.com/watch?v=OzAdKMPgUt4">video</a>s from Google is another. The I/O 2017 videos include a few TensorFlow ones, and there's also still the February TensorFlow DevFest video collection.</p>
<hr>
<p><img alt="Deep Learning with Python book cover" src="img/chollet_dlp.png"></p>
<hr>
<p>I want to also mention this book by the author of Keras: <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a>. It's pretty good. If you're looking to understand deep learning, this is now the book I recommend. It's currently in pre-release, and the available chapters are pretty great already. It focuses on Keras rather than TensorFlow per se, but especially since Keras is now <em>in</em> TensorFlow, any Keras thing is also a TensorFlow thing.</p>
<p>The first chapter is free.</p>
<hr>
<p><img alt="necessary qualities" src="img/qualities.png"></p>
<hr>
<p>I want to close with a thought that is not so much about programming. This is a question I got on LinkedIn, and it made me think.</p>
<p>It is eventually necessary to have some proficiency with particular tools, but the qualities I find most important are not really particular tool skills.</p>
<p>What problem do you need to solve? Can you identify it, articulate it, frame it in a way that makes it tractable? Can you work with what's available to come up with a solution?</p>
<p>I hope knowing more about TensorFlow is helpful to you as you go forward with your work, but I also think it's important to remember that TensorFlow is just a tool. Your success comes from what you do.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170524-more_and_better_new_tensorflow_apis/</link>
<guid>http://planspace.org/20170524-more_and_better_new_tensorflow_apis/</guid>
<pubDate>Wed, 24 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating TensorFlow Estimator Documentation</title>
<description><![CDATA[

<p>The TensorFlow documentation keeps improving, but it can still be hard to find what you're looking for. Here's a way of organizing <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">Estimator</a>s or <a href="https://www.tensorflow.org/api_guides/python/contrib.learn"><code>tf.contrib.learn</code></a> documentation, from high-level to low-level:</p>
<ul>
<li>Using Estimators (already made or "canned")<ul>
<li><a href="https://www.tensorflow.org/get_started/tflearn"><code>DNNClassifier</code> on iris dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide"><code>LinearClassifier</code> (logistic) on census income dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide_and_deep"><code>DNNLinearCombinedClassifier</code> (extends census income example)</a></li>
</ul>
</li>
<li>Aspects of using Estimators<ul>
<li><a href="https://www.tensorflow.org/tutorials/linear">Feature columns (in context of linear models)</a></li>
<li><a href="https://www.tensorflow.org/get_started/input_fn">Using <code>input_fn</code>; <code>DNNRegressor</code> on Boston housing dataset</a></li>
<li><a href="https://www.tensorflow.org/get_started/monitors">Logging etc. with <code>ValidationMonitor</code> (extends iris example)</a></li>
</ul>
</li>
<li>Making your own Estimator<ul>
<li><a href="https://www.tensorflow.org/extend/estimators">With <code>layers</code> in a <code>model_fn</code> on the abalones dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/layers">With <code>layers</code> in a <code>model_fn</code> on MNIST</a></li>
</ul>
</li>
<li>Lower-level API than Estimators<ul>
<li><a href="https://www.tensorflow.org/get_started/mnist/pros#build_a_multilayer_convolutional_network">Use the lower-level API to make a system for MNIST</a></li>
</ul>
</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</link>
<guid>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</guid>
<pubDate>Sun, 21 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Code Reading Questions at OSCON</title>
<description><![CDATA[

<p><a href="https://www.oreilly.com/">O'Reilly</a>'s 2017 <a href="https://conferences.oreilly.com/oscon/">OSCON</a> had a "<a href="https://conferences.oreilly.com/oscon/oscon-tx/public/content/game">code game</a>" with ten questions covering nine different languages. It was supposed to get people engaged in the expo hall, but the quiz-like gamification content reminded me of my old <a href="/20150616-code_reading_question/">code reading question</a> idea. The questions are available in a <a href="https://cdn.oreillystatic.com/en/assets/1/event/214/oscon2017_code_game.pdf">PDF</a> (<a href="oscon2017_code_game.pdf">mirror</a>). I'll put the questions here as well.</p>
<hr>
<h3>1. Rust</h3>
<pre><code class="language-rust">use std::collections::HashSet;
use std::io::{BufRead, Result};

fn f&lt;I: BufRead&gt;(input: &amp;mut I) -&gt; Result&lt;usize&gt; {
    Ok(input.lines()
       .map(|r| r.expect(&#8220;ara ara&#8221;))
       .flat_map(|l| l.split_whitespace()
                     .map(str::to_owned)
                     .collect::&lt;Vec&lt;_&gt;&gt;())
       .collect::&lt;HashSet&lt;_&gt;&gt;()
       .len())
}</code></pre>

<p>This function reads input. What else does it do?</p>
<ul>
<li>A. Counts the number of white-space-separated &#8220;words&#8221;</li>
<li>B. Counts the number of distinct &#8220;words&#8221;</li>
<li>C. Finds the &#8220;word&#8221; that appears most frequently</li>
<li>D. Finds the longest line</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>2. JavaScript</h3>
<pre><code class="language-javascript">function search(values, target) {
  for(var i = 0; i &lt; values.length; ++i){
    if (values[i] == target) { return i; }
  }
  return -1;
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Depth-first search</li>
<li>B. Binary search</li>
<li>C. Merge search</li>
<li>D. Linear search</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>3. Go</h3>
<pre><code class="language-go">func function(s []float64) float64 {
        var sum float64 = 0.0
        for _, n := range s {
                sum += n
        }
        return sum / float64(len(s))
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Sums the contents of a slice</li>
<li>B. Finds the maximum value in a slice</li>
<li>C. Averages the contents of a slice</li>
<li>D. Appends values to a slice</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>4. Perl 5</h3>
<pre><code class="language-perl">sub mystery {
    return @_ if @_ &lt; 2;
    my $p = pop;
    mystery(grep $_ &lt; $p, @_), $p,
    mystery(grep $_ &gt;= $p, @_);
}</code></pre>

<p>What does the mystery subroutine do?</p>
<ul>
<li>A. Binary search</li>
<li>B. Merge sort</li>
<li>C. Removes items that are too large or too small</li>
<li>D. Quick sort</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>5. Java 8</h3>
<pre><code class="language-java">static void function(int[] ar)
 {
   Random rnd = ThreadLocalRandom.current();
   for (int i = ar.length - 1; i &gt; 0; i--)
   {
     int index = rnd.nextInt(i + 1);
     int a = ar[index];
     ar[index] = ar[i];
     ar[i] = a;
   }
 }</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Merge sort</li>
<li>B. Shuffle</li>
<li>C. Increases size of array</li>
<li>D. Decreases size of array</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>6. Ruby</h3>
<pre><code class="language-ruby">def f(hash)
  prs = hash.inject({}) do |hsh, pr|
    k, v = yield pr
    hsh.merge(k =&gt; v)
  end
  Hash[prs]
end</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Reverses an array</li>
<li>B. Administers a booster shot</li>
<li>C. Enters a freeway safely</li>
<li>D. Transforms a hash</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>7. Python</h3>
<pre><code class="language-python">def function(list):
     return [x for x in list if x == x[::-1]]</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Finds and returns all palindromes within the given list</li>
<li>B. Reverses all strings in the given list</li>
<li>C. Swaps the first and last letter in each word in the given list</li>
<li>D. Returns a list of anagrams for each word in the given list</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>8. Scala</h3>
<pre><code class="language-scala">object Op {
  val r1: Regex = &#8220;&#8221;&#8221;([^aeiouAEIOU\d\s]+)([^\d\s]*)$&#8221;&#8221;&#8221;.r
  val r2: Regex = &#8220;&#8221;&#8221;[aeiouAEIOU][^\d\s]*$&#8221;&#8221;&#8221;.r
  val s1:String = &#8220;\u0061\u0079&#8221;
  val s2:String = &#8220;\u0077&#8221; + s1
  def apply(s: String): String = {
    s.toList match {
      case Nil =&gt; &#8220;&#8221;
      case _ =&gt; s match {
        case r1(c, r) =&gt; r ++ c ++ s1 case r2(_*) =&gt; s ++ s2
        case _ =&gt; throw new
RuntimeException(&#8220;Sorry&#8221;)
      }
    }
  }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Converts a given String to a JavaScript-based String</li>
<li>B. Converts a Unix/MacOSX String format into a Windows String format</li>
<li>C. Converts a word into the children&#8217;s language equivalent called &#8220;Pig Latin&#8221;</li>
<li>D. Converts a given String to IPV6 format since IP numbers are running out</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>9. Swift</h3>
<pre><code class="language-swift">import Foundation

let i = &#8220;Hell&#248;, Swift&#8221;

let t = i.precomposedStringWithCanonicalMapping

let c = t.utf8.map({UnicodeScalar($0+2)})
let j = i.utf8.map({UnicodeScalar($0+1)}).count / 2

var d = String(repeating: String(describing: c[j]), count: j)

d.append(Character(&#8220;&#127462;&#127482;&#127482;&#127480;&#8221;))

let result = &#8220;\(d): \(d.characters.count)&#8221;</code></pre>

<p>What is the value of &#8220;result&#8221;?</p>
<ul>
<li>A. ......&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>B. ......&#127462;&#127482;&#127482;&#127480;: 8</li>
<li>C. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>D. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 8</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>10. JavaScript</h3>
<pre><code class="language-javascript">function thing (n) {
    for (var i = 0; i &lt; n; i++) {
        setTimeout(function () {console.log(i);}, 0);
    }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Prints numbers 0 through n</li>
<li>B. Prints n n time</li>
<li>C. Prints 0 n times</li>
<li>D. Prints nothing</li>
</ul>
<!-- Correct: B -->

<hr>
<p>View HTML source to see the answers.</p>
<p>I tried to keep the above close to what appeared on the physical
cards. I haven't tried to correct the little mistakes and bizarre
indentation throughout.</p>
<p>It's fun!</p>    
    ]]></description>
<link>http://planspace.org/20170517-code_reading_questions_at_oscon/</link>
<guid>http://planspace.org/20170517-code_reading_questions_at_oscon/</guid>
<pubDate>Wed, 17 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Caffe and TensorFlow at Deep Learning Analytics</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">presentation</a> given Wednesday May 10, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a> as part of <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/full/tensorflow-day">TensorFlow Day</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<p>Thank you!</p>
<hr>
<p>Thanks to all the OSCON organizers and participants, and especially to the Google folks organizing TensorFlow Day, and everyone coming out for TensorFlow events! What fun!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png" width="100%"></p>
<hr>
<p>I work at a company called <a href="https://www.deeplearninganalytics.com/">Deep Learning Analytics</a>, or DLA.</p>
<p>The advantage of a name like Deep Learning Analytics is that, assuming you've heard of deep learning, you immediately know something about what we do.</p>
<hr>
<p>deeplearninganalytics.com</p>
<hr>
<p>The disadvantage of a name like Deep Learning Analytics is that our URL is really long.</p>
<hr>
<p><img alt="some of the Deep Learning Analytics team" src="img/dla_team.jpg" width="100%"></p>
<hr>
<p>This is most of us at our Arlington location, just outside DC. We do a mix of government contracting and commercial work.</p>
<hr>
<p><img alt="DarLA" src="img/darla.png" height="100%"></p>
<hr>
<p>We have a cute robot mascot called DarLA.</p>
<p>DarLA the robot could be a metaphor for any machine learning system. You have to build the arms and the legs, and that's a good deal of work. There's also generally a lot of work in getting data to train the system with. But you also need a brain, and that's inevitably sort of the interesting part.</p>
<p>So what do you use for the brain?</p>
<hr>
<p>warning: opinions</p>
<hr>
<p>I'm going to comment on a few aspects of a few systems, based on what I've seen. Everybody involved in this space is incredibly smart, and probably correct in some ways even when I disagree.</p>
<hr>
<ul>
<li>Caffe?</li>
<li>TensorFlow?</li>
</ul>
<hr>
<p>The two systems I'll talk about are Caffe and TensorFlow.</p>
<p>I'm aware of Caffe 2 but I don't have anything to say about it today, aside from that it seems a lot like Caffe.</p>
<hr>
<p><img src="img/nvidia_comparison.jpg" alt="DL framework comparison" height="100%"></p>
<hr>
<p>This is a picture of a slide from a talk some NVIDIA folks gave a couple weeks ago. I will now agree and disagree with it.</p>
<p>Caffe easy to start? I disagree, and I'll talk about why.</p>
<p>Caffe easy to develop? I disagree, and I'll talk about why.</p>
<p>Caffe limited capability? I agree, and I'll talk about why.</p>
<p>TensorFlow portable and nice documentation? I mostly agree.</p>
<p>TensorFlow slow? I think to the extent this is true, it doesn't matter, and I'll talk about some related considerations.</p>
<p>My colleagues have done some work in Torch as well, but I'm not going to talk about these others.</p>
<hr>
<p>Easy to start?</p>
<hr>
<p>So which framework makes it easier to get started?</p>
<hr>
<p><code>pip install:</code><br>
&#10008; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>This is a convenience, but it's also really nice. There are advantages to compiling things yourself too. But for quickly getting started, this is great.</p>
<p>Even if you're happy to compile an optimized build for your system, having easy installation options available can be nice for setting up test environments, for example.</p>
<hr>
<p>"Easy" models:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>By "easy" models, I mean models that are either completely pre-specified and pre-trained, or very easily specified by a very high-level API.</p>
<p>The Caffe model zoo might have been the first recognized collection of pre-trained models and model architecture specifications. One or two years ago, it might have been the best way to start working with an interesting deep net without going through the whole training process yourself.</p>
<p>Now, though, I think TensorFlow is really passing Caffe on this. You can get a number of pre-trained models already with just one line of Python, using <code>tf.contrib.keras</code>. Hopefully others will be able to match that ease of redistribution.</p>
<p>Further, TensorFlow's "canned models" in <code>tf.contrib.learn</code> provide a scikit-learn-like interface that Caffe never attempts.</p>
<hr>
<p>Easy to develop?</p>
<hr>
<p>What about ease of development?</p>
<hr>
<p>API level:</p>
<ul>
<li>high? middle? low?</li>
</ul>
<hr>
<p>For development, it matters what level you want to work at. You can do a lot quickly if you can stay at a high level. But sometimes you need to get low-level to control details or do something slightly non-standard.</p>
<p>I think TensorFlow now is doing a better job than Caffe of providing API surfaces across a range of levels.</p>
<p>Caffe's API, at the protocol buffer text format level you have to eventually get to, is sort of a middle-low level. The vocabulary is more limited than what you get with TensorFlow ops.</p>
<p>You can build higher-level APIs with Caffe, and DLA has an in-house library that we use to make Caffe easier to work with.</p>
<p>TensorFlow has Keras.</p>
<hr>
<p>Python integration:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>Both Caffe and TensorFlow are written with C++, but interfacing with Caffe can feel like interfacing with the separate free-standing program, whereas the TensorFlow interface is seamless.</p>
<p>If there's a way to use Caffe without at some point writing protobuf text files to disk and then having Caffe read them, I don't know it.</p>
<hr>
<p>modular design</p>
<hr>
<p>I'll show an example of a Caffe layer in order to talk about some of the design issues.</p>
<hr>
<pre><code>layer {name: "data"
       type: "Data"
       top: "data"
       top: "label"
       transform_param {crop_size: 227}
       data_param {source: "train_lmdb_path"
                   batch_size: 256
                   backend: LMDB}}</code></pre>

<hr>
<p>I condensed a Caffe data layer spec a bit.</p>
<p>Caffe likes reading data from LMDB databases. So this layer includes a path to a location on disk, where an LMDB database of a particular form needs to be. So this is a layer that reads from disk.</p>
<p>But wait - this layer spec also specifies the training batch size.</p>
<p>Does it do anything else? What's that "<code>crop_size</code>"?</p>
<p>This layer also implicitly takes random crops from the images it reads, 227 pixels by 227 pixels. So the images in the database should be at least that big.</p>
<p>That's a lot happening in one layer!</p>
<p>On the one hand, this is pretty neat. Caffe does a lot for you. And once you've made a lot of choices, you can optimize the implementation, which is part of how Caffe gets pretty fast.</p>
<p>On the other hand, Caffe is making a lot of decisions for us, and it isn't particularly happy if we want to change those decisions. Caffe can feel more like final application code than framework code.</p>
<p>What if we want non-square crops? What if we want to introduce random distortions? Or if we want to read data from JPG files instead of LMDB? None of these are supported by this LMDB layer, and these changes aren't easily composable. We can switch to a memory data layer to get training batches in, but then we have to independently implement cropping if we want it, and so on.</p>
<p>In contrast, TensorFlow doesn't have such tight integrations. The closest analog in TensorFlow might be reading from TFRecords files, but that functionality is isolated, and doesn't bundle in choices about batch size or cropping.</p>
<p>You get a more modular system with TensorFlow, so you shouldn't find yourself having to pick apart tightly-coupled functionality later on.</p>
<p>At the same time, TensorFlow does include the higher-level APIs mentioned earlier, so it isn't completely a choice between low-level and high-level either.</p>
<hr>
<p>multi-GPU</p>
<hr>
<p>How do we get multi-GPU systems?</p>
<hr>
<p><img src="img/alexnet.png" alt="AlexNet diagram" width="100%"></p>
<hr>
<p>This is the diagram from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>, the 2012 paper that popularized deep learning. It has over 11,000 citations now.</p>
<p>This was a multi-GPU implementation!</p>
<p>I want to point out how crazy it is. Alex Krizhevsky split individual convolutional layers between two GPUs. Whoa.</p>
<hr>
<p>Caffe</p>
<hr>
<p>So how do we do multi-GPU with Caffe?</p>
<hr>
<p><img alt="multi-GPU Caffe docs" src="img/caffe_multi.png"></p>
<hr>
<p>For Caffe, there's this page of documentation. As far as I know there is no link to this page of documentation. I found it <a href="https://github.com/BVLC/caffe/blob/master/docs/multigpu.md">via</a> the GitHub repo that backs the website.</p>
<p>There are some limitations in Caffe's multi-GPU support.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>Well, how do we do multi-GPU with TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same way we do single-GPU! The API stays the same, and you just say where you want things to run. This is also more flexible in terms of the ways you can parallelize things. You could have two copies of a model on two GPUs, or one really big model across two GPUs, for example.</p>
<hr>
<p><img alt="Jenn" src="img/jenn.jpg"></p>
<hr>
<p>My colleague Jenn parallelized some GAN (Generative Adversarial Network) code to use multiple GPUs. I was super curious to see what she had to do. She was really puzzled about why I was so curious, because TensorFlow made it so easy.</p>
<p>Also, if you're curious about GANs, you should really check out Jenn's <a href="https://github.com/jennsleeman/introtogans_dcdatascience_2017">talk on the subject</a>.</p>
<hr>
<p>multi-machine</p>
<hr>
<p>What if we want to train across multiple machines now?</p>
<hr>
<p>Caffe</p>
<hr>
<p>How about Caffe?</p>
<hr>
<p><img src="img/trivial_mpi.png" alt="trivial MPI" width="100%"></p>
<hr>
<p>Caffe doesn't do it, but Yangqing helpfully <a href="https://github.com/BVLC/caffe/issues/876">points out</a> that you could make your own distributed Caffe system by adding MPI. Some people have actually <a href="https://software.intel.com/en-us/articles/caffe-training-on-multi-node-distributed-memory-systems-based-on-intel-xeon-processor-e5">done</a> this. I'm not sure it's trivial.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>And TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same API! You just put computation on various machines!</p>
<p>There is a little bit more to it than that, but I think it's easier than MPI.</p>
<hr>
<p><img alt="infrastructure" src="img/infr.png"></p>
<hr>
<p>Another thing we're excited about is possibilities for scaling out our work with TensorFlow. OpenAI <a href="https://blog.openai.com/infrastructure-for-deep-learning/">published</a> information and code on their system, and it seems like a pretty great model.</p>
<hr>
<p>responsiveness</p>
<hr>
<p>I'm going to talk about one anecdotal measures of developer responsiveness. It isn't totally fair, because Caffe development is basically over as Caffe 2 emerges, but it may still be interesting.</p>
<p>I occasionally submit very minor pull requests, for things like ensuring a variable is the correct type, or fixing typos in documentation. These are simple PRs that don't generally require discussion or alteration; they just get merged in.</p>
<p>So how quickly does that happen with Caffe and with TensorFlow?</p>
<hr>
<p>merge time:</p>
<ul>
<li>TensorFlow: 12 hours</li>
<li>Caffe: 329 days</li>
</ul>
<hr>
<p>TensorFlow developers seem more responsive.</p>
<p>(<a href="https://github.com/tensorflow/tensorflow/pull/9451">TensorFlow PR</a>)</p>
<p>(<a href="https://github.com/BVLC/caffe/pull/4182">Caffe PR</a>)</p>
<hr>
<p><img alt="TensorFlow logo" src="img/tf.jpg" height="100%"></p>
<hr>
<p>And I think that, along with many other things, bodes well for the future of TensorFlow.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>    
    ]]></description>
<link>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</link>
<guid>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</guid>
<pubDate>Wed, 10 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Building TensorFlow Systems from Components</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">workshop</a> given Tuesday May 9, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<h3>Workshop participant materials</h3>
<p>These are things you can use during the workshop. Use them when you need them.</p>
<ul>
<li>Focus one: <em>getting data in</em><ul>
<li>Download notebook: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
<li><a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a></li>
</ul>
</li>
<li>Focus two: <em>distributed programs</em><ul>
<li><a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/"><code>mapreduce_with_tensorflow</code> code and data</a></li>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
</li>
<li>Focus three: <em>high-level ML APIs</em><ul>
<li>Download notebook: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
</li>
</ul>
<p>The whole slide deck and everything follows, and it's long.</p>
<hr>
<p>Thank you!</p>
<hr>
<p>Before starting, I want to take a moment to notice how great it is to have a conference. Thank you for being here! We are all really lucky. If this isn't nice, I don't know what is.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<p>If you want to participate in the workshop, <em>really go</em> to <code>planspace.org</code> and pull up these materials!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work for a company called Deep Learning Analytics.</p>
<p>I'll talk more about DLA tomorrow <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">as part of TensorFlow Day</a>. Hope to see you there as well!</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>You'll want to have TensorFlow version 1.1 installed. TensorFlow 1.1 was only officially released on April 20, and the API really has changed.</p>
<p>The <em>core documented</em> API, mostly the low-level API, is frozen as of TensorFlow 1.0, but a lot of higher-level stuff is still changing. TensorFlow 1.1 brings in some really neat stuff, like Keras, which we'll use later.</p>
<hr>
<p><img src="img/hello_tensorflow.png" alt="Hello, TensorFlow! on O'Reilly" height="100%"></p>
<hr>
<p>As an example: I wrote <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> about a year ago. It used TensorFlow 0.8.</p>
<p>After TensorFlow 1.0 came out, I went and looked at the summary code block at the end of "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>". It had 15 lines of TensorFlow code. Of that, 5 lines no longer worked. I had to update 33% of the lines of my simple TensorFlow example code because of API changes.</p>
<hr>
<p><img alt="one third" src="img/one_third.jpg"></p>
<hr>
<p>I was surprised that it was one third.</p>
<p>I have an <a href="/20160620-hello_tensorflow_just_the_code/">updated code snippet</a>, but we're not doing "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>" today.</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>So please have TensorFlow 1.1 installed, is the point of that whole story.</p>
<p>The stable bits of TensorFlow really are stable, but there's a lot of exciting new stuff, and I don't want you to miss out today!</p>
<hr>
<p>THE BIG IDEA</p>
<hr>
<p>Okay! What is this workshop about?</p>
<hr>
<p>use what you need</p>
<hr>
<p>TensorFlow works for you! Use it where it does things that help you.</p>
<p>TensorFlow is a tool. It's a very general tool, on the one hand. It's also a tool with lots of pieces. You can use <a href="/20170312-use_only_what_you_need_from_tensorflow/">some</a> of the pieces. Or you can decide not to use TensorFlow altogether!</p>
<p>We'll look at several specific aspects of TensorFlow. Maybe you'll want to use them. Maybe you won't. The hope is that you'll be more comfortable with what's available and able to decide what to apply when.</p>
<p>If you like <a href="https://www.infoq.com/presentations/Simple-Made-Easy">Rich Hickey words</a>, maybe I'm trying to <em>decomplect</em> the strands within TensorFlow so they can be understood individually.</p>
<hr>
<p>THE PLAN</p>
<hr>
<p>Here's the plan for this workshop.</p>
<hr>
<ul>
<li>one short work</li>
<li>three longer works</li>
</ul>
<hr>
<p>I'll talk about some things, but the hope of the workshop is that you do some good work.</p>
<p>I hope that you don't finish all the things you think of, but want to keep working after the workshop is over.</p>
<p>Also, there's a break from 10:30 to 11:00 on the official schedule, but I don't really like that. If we happen to be working during the break, fine. Feel free to take a break whenever you need a break.</p>
<hr>
<p>Do what you want!</p>
<hr>
<p>TensorFlow is really big, and not every part will be interesting or important to you.</p>
<p>I'll have very specific things for you to work on for each of the works, but if you think of something better to do, you better do it!</p>
<hr>
<p>Intro by Logo</p>
<hr>
<p>To get creative juices flowing a little, let's explore some logo history.</p>
<hr>
<p><img alt="LOGO turle" src="img/logo_turtle.jpg" height="100%"></p>
<hr>
<p>Not that sort of <a href="https://en.wikipedia.org/wiki/Logo_(programming_language)">Logo</a>!</p>
<p>Fine to think about, though.</p>
<hr>
<p><img alt="G&#246;del, Escher, Bach" src="img/geb.jpg" height="100%"></p>
<hr>
<p>In the beginning, there was <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">G&#246;del, Escher, Bach: An Eternal Golden Braid</a> by Douglas Hofstadter.</p>
<p>This "metaphorical fugue on minds and machines in the spirit of Lewis Carroll" includes the <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> idea and related discussion of consciousness and formal systems. Hofstadter's influence can be seen, for example, <a href="https://cs.illinois.edu/news/strange-loop-conference">in the name</a> of the <a href="http://www.thestrangeloop.com/">Strange Loop</a> tech conference.</p>
<p>It seems likely that many people working in artificial intelligence and machine learning have encountered G&#246;del, Escher, Bach.</p>
<hr>
<p><img src="img/chernin.jpg" alt="Chernin Entertainment" height="100%"></p>
<hr>
<p>Of course, there's also <a href="https://en.wikipedia.org/wiki/Chernin_Entertainment">Chernin Entertainment</a>, the production company.</p>
<p>So we shouldn't rule out the possibility that Google engineers are fans of <a href="http://www.imdb.com/company/co0286257/">Chernin's work</a>. <a href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden Figures</a> is quite good. And I guess a lot of people like <a href="https://en.wikipedia.org/wiki/New_Girl">New Girl</a>?</p>
<hr>
<p><img alt="TensorFlow logo - old?" src="img/tf-old-big.png" height="100%"></p>
<hr>
<p>In any event, somehow we get to this TensorFlow logo:</p>
<p>If you look carefully, does it seem like the right side of the "T" view is too short?</p>
<hr>
<p><img alt="Issue 1922" src="img/issue_1922.png"></p>
<hr>
<p>This very serious concern appears as <a href="https://github.com/tensorflow/tensorflow/issues/1922">issue #1922</a> on the TensorFlow github, and <a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XhO1sqp4l4g">on the TensorFlow mailing list</a> complete with ASCII art illustration.</p>
<p>The consensus response seemed to be some variant of "won't fix" (it wouldn't look as cool, anyway) until...</p>
<hr>
<p><img alt="TensorFlow logo - new?" src="img/tf-new.jpg" height="100%"></p>
<hr>
<p>As of around the 1.0 release of TensorFlow, which was around the first <a href="https://events.withgoogle.com/tensorflow-dev-summit/">TensorFlow Dev Summit</a>, this logo variant seems to be in vogue. It removes the (possibly contentious) shadows, and adds additional imagery in the background.</p>
<p>I want to suggest that the image in the background can be a kind of Rorschach test for at least three ways you might be thinking about TensorFlow.</p>
<ul>
<li>Is it a diagram of connected neurons? Maybe you're interested in TensorFlow because you want to make neural networks.</li>
<li>Is it a diagram of a computation graph? Maybe you're interested in TensorFlow for general calculation, possibly using GPUs.</li>
<li>Is it a diagram of multiple computers in a distributed system? Maybe you're interested in TensorFlow for its distributed capabilities.</li>
</ul>
<p>There can be overlap among those interpretations as well, but I hope the point is not lost that TensorFlow can be different things to different people.</p>
<hr>
<p>short work</p>
<hr>
<p>I want to get you thinking about systems without further restriction. The idea is to imagine and start fleshing out a system that might involve TensorFlow.</p>
<p>Later on we'll follow the usual workshop pattern of showing you something you can do and having you do it. But it's much more realistic and interesting to start from something you want to do and then try to figure out how to do it. So let's start from the big picture.</p>
<hr>
<p>draw a system!</p>
<ul>
<li>block diagram</li>
<li>add detail</li>
<li>pseudocode?</li>
</ul>
<hr>
<p>You can draw a system you've already made, or something you're making, or something you'd like to make. It could be something you've heard about, or something totally unique.</p>
<p>You don't have to know how to build everything in the system. You don't need to know how TensorFlow fits in. Feel free to draw what you <em>wish</em> TensorFlow might let you do.</p>
<p>Keep adding more detail. If you get everything laid out, start to think about what functions or objects you might need to have, and start putting pseudocode together.</p>
<hr>
<p>short work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p>Let's do hard AI!</p>
<hr>
<p>I'm going to walk through an example and talk about how TensorFlow can fit in.</p>
<p>Doing hard AI, or Artificial General Intelligence (AGI), is intended as a joke, but it's increasingly less so, with Google DeepMind and OpenAI both explicitly working on getting to AGI.</p>
<hr>
<p>(build)</p>
<hr>
<p>I've got 35 slides of sketches, so they're in <a href="build.pdf">a separate PDF</a> to go through.</p>
<hr>
<p><img alt="system" src="img/system.jpg"></p>
<hr>
<p>Here's the final system that we built to.</p>
<p>We're not going to do everything in there today.</p>
<hr>
<ul>
<li>getting data in</li>
<li>distributed programs</li>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>These are the three focuses for today.</p>
<p>The <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">original description</a> for this workshop also mentioned serving, but I'm not covering it today. Sorry. Not enough time.</p>
<hr>
<p>What even is TensorFlow?</p>
<hr>
<p>Here's one way to think about TensorFlow.</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>The beating heart of TensorFlow is the Distributed Execution Engine, or runtime.</p>
<p>One way to think of it is as a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">virtual machine</a> whose language is the TensorFlow graph.</p>
<p>That core is written in C++, but lots of TensorFlow functionality lives in the frontends. In particular, there's a lot in the Python frontend.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<p>Python is the richest frontend for TensorFlow. It's what we'll use today.</p>
<p>You should remember that not everything in the Python TensorFlow API touches the graph. Some parts are just Python.</p>
<hr>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>R has an unofficial <a href="https://rstudio.github.io/tensorflow/">TensorFlow API</a> which is kind of interesting in that it just wraps the Python API. It's really more like an R API to the Python API to TensorFlow. So when you use it, you write R, but Python runs. This is not how TensorFlow encourages languages to implement TensorFlow APIs, but it's out there.</p>
<hr>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<p>The way TensorFlow encourages API development is via TensorFlow's C bindings.</p>
<hr>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<p>You could use C++, of course.</p>
<hr>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<p>Also there's Java.</p>
<hr>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<p>And there's Go.</p>
<hr>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<p>And there's Rust.</p>
<hr>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<p>And there's even Haskell!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Currently, languages other than Python have TensorFlow support that is very close to the runtime. Basically make your ops and run them.</p>
<p>This is likely to be enough support to deploy a TensorFlow system in whatever language you like, but if you're developing and training systems you probably still want to use Python.</p>
<hr>
<p>graph or not graph?</p>
<hr>
<p>So this is a distinction to think about: Am I using the TensorFlow graph, or not?</p>
<hr>
<ul>
<li>getting data in</li>
</ul>
<hr>
<p>So here we are at the first focus area.</p>
<p>We'll do two sub-parts.</p>
<hr>
<ul>
<li>getting data in<ul>
<li>to the graph</li>
<li>TFRecords</li>
</ul>
</li>
</ul>
<hr>
<p>First, a quick review of the the graph and putting data into it.</p>
<p>Second, a bit about TensorFlow's TFRecords format.</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
</ul>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>work with data!</p>
<ul>
<li>your own system?</li>
<li>planspace.org: <code>mystery.tfrecords</code></li>
<li>move to graph</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Almost certainly there's some data input that needs to happen. How are you going to read that data, and possibly get it into TensorFlow?</p>
<p>If you want to stay really close to the TensorFlow stuff just demonstrated, here's a fun little puzzle for you: What's in <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a>?</p>
<p>That could be hard, or it could be easy. If you want to extend it, migrate the reading and parsing of the TFRecords file into the TensorFlow graph. The demonstration in the notebook worked with TFRecords/Examples without using the TensorFlow graph. The links on the <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a> page can help with this.</p>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="data" src="img/data_data.jpg" width="100%"></p>
<hr>
<p>As a wrap-up: Working directly with data, trying to get it into the right shape, cleaning it, etc., may not be the most fun, but it's got to be done. Here's a horrible pie chart <a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/">from some guy on Forbes</a>, the point of which is that people spend a good deal of time fighting with data.</p>
<hr>
<ul>
<li>distributed programs</li>
</ul>
<hr>
<p>We arrive at the second focus area. TensorFlow has some pretty wicked distributed computing capabilities.</p>
<hr>
<ul>
<li>distributed programs<ul>
<li>command-line arguments</li>
<li>MapReduce example</li>
</ul>
</li>
</ul>
<hr>
<p>I'm putting a bit about command-line arguments in here because I think it's interesting. It doesn't necessarily fit in with distributed computing, although you might well have a distributed program that takes command-line arguments. Google Cloud ML can use command-line arguments for hyperparameters, for example.</p>
<p>Then we'll get to a real distributed example, in which we implement a distributed MapReduce word count in 50 lines of Python TensorFlow code.</p>
<hr>
<p>command-line arguments</p>
<hr>
<p>So let's take a look at some ways to do command-line arguments.</p>
<p>This section comes from the post <a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a>.</p>
<hr>
<pre><code class="language-bash">$ python script.py --color red
a red flower</code></pre>

<hr>
<p>I'll show eight variants that all do the same thing. You provide a <code>--color</code> argument, and it outputs (in text) a flower of that color.</p>
<hr>
<pre><code class="language-python">import sys

def main():
    assert sys.argv[1] == '--color'
    print('a {} flower'.format(sys.argv[2]))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This is a bare-bones <code>sys.argv</code> method. Arguments become elements of the <code>sys.argv</code> list, and can be accessed as such. This has limitations when arguments get more complicated.</p>
<hr>
<pre><code class="language-python">import sys
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main():
    flags.FLAGS._parse_flags(args=sys.argv[1:])
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This <code>FLAGS</code> API is <a href="/20170313-tensorflow_use_of_google_technologies/">familiar to Googlers</a>, I think. It's interesting to me where some Google-internal things peak out from the corners of TensorFlow.</p>
<hr>
<pre><code class="language-python">import sys
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main():
    gflags.FLAGS(sys.argv)
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>You could also install the <code>gflags</code> module, which works much the same way.</p>
<hr>
<pre><code class="language-python">import sys
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args(sys.argv[1:])
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>Here's Python's own standard <a href="https://docs.python.org/3/library/argparse.html"><code>argparse</code></a>, set up to mimic the <code>gflags</code> example, still using <code>sys.argv</code> explicitly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

def main(args):
    assert args[1] == '--color'
    print('a {} flower'.format(args[2]))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>Using <code>tf.app.run()</code>, another Google-ism, frees us from accessing <code>sys.argv</code> directly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main(args):
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>We can combine <code>tf.app.run()</code> with <code>tf.app.flags</code>.</p>
<hr>
<pre><code class="language-python">import google.apputils.app
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main(args):
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    google.apputils.app.run()</code></pre>

<hr>
<p>To see the equivalent outside the TensorFlow package, we can combine <code>gflags</code> and <code>google.apputils.app</code>.</p>
<hr>
<pre><code class="language-python">import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args()
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This has all been fun, but here's what you should really do. Just use <code>argparse</code>.</p>
<hr>
<p>Whew!</p>
<hr>
<p>That was a lot of arguing.</p>
<p>It may be worth showing all these because you'll encounter various combinations as you read code out there in the world. More recent examples are tending to move to <code>argparse</code>, but there are some of the other variants out there as well.</p>
<hr>
<p>MapReduce example</p>
<hr>
<p>Now to the MapReduce example!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Recall that the core of TensorFlow is a <em>distributed</em> runtime. What does that mean?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>Behold, the power of <code>tf.device()</code>!</p>
<hr>
<p></p><pre><code>with tf.device('/cpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local CPU.</p>
<hr>
<p></p><pre><code>with tf.device('/gpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local GPU.</p>
<hr>
<p></p><pre><code>with tf.device('/job:ps/task:0'):
    # Do something.</code></pre>
<hr>
<p>In exactly the same way, you can specify that work happen on <em>a different computer</em>!</p>
<p>This is pretty amazing. I think of it as sort of <a href="/20170423-tensorflow_as_automatic_mpi/">declarative MPI</a>.</p>
<hr>
<p><img alt="distributing a TensorFlow graph" src="img/distributed_graph.png"></p>
<hr>
<p>TensorFlow automatically figures out when it needs to send information between devices, whether they're on the same machine or on different machines. So cool!</p>
<hr>
<p><img alt="oh map reduce..." src="img/oh_map_reduce.png"></p>
<hr>
<p>MapReduce is often associated with Hadoop. It's just divide and conquer.</p>
<p>So let's do it with TensorFlow!</p>
<hr>
<p>(demo)</p>
<hr>
<p>It's time to see how this looks in practice! (Well, or at least to see how it looks in a cute little demo.)</p>
<p>The demo uses the contents of the <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">mapreduce_with_tensorflow</a> repo on GitHub. For more explanation, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a> and <a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a>.</p>
<hr>
<p>(code)</p>
<hr>
<p>Walking through some details inside <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/blob/master/count.py"><code>count.py</code></a>.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>different distributed functionality?</li>
<li>add command-line?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Maybe command-line args are relevant to you. Maybe running a system across multiple machines is relevant for you. Or maybe not.</p>
<p>If you want to exercise the things just demonstrated, you could add a command-line argument to the distributed word-count program. For example, you could make it count only a particular word, or optionally count characters, or something else. Or you could change the distributed functionality without any command-line fiddling. (You could make it a distributed neural net training program, for example.)</p>
<p>Here are some links that might be helpful:</p>
<ul>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="oh kubernetes..." src="img/oh_kubernetes.jpg"></p>
<hr>
<p>I should probably say that you don't really want to start all your distributed TensorFlow programs by hand. <a href="https://www.docker.com/">Containers</a> and <a href="https://kubernetes.io/">Kubernetes</a> and all that.</p>
<hr>
<ul>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>Let's to some machine learning!</p>
<hr>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<hr>
<p>New and exciting things are being added in TensorFlow Python land, building up the ladder of abstraction.</p>
<p>This material comes from <a href="/20170321-various_tensorflow_apis_for_python/">Various TensorFlow APIs for Python</a>.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="slim... shady?" src="img/slim_shady.png" height="100%"></p>
<hr>
<p>The "layer" abstractions largely from TF-Slim are now appearing at <code>tf.layers</code>.</p>
<hr>
<p><img alt="scikit-learn logo" src="img/sklearn.png" height="100%"></p>
<hr>
<p>The Estimators API now at <code>tf.estimator</code> is drawn from <code>tf.contrib.learn</code> work, which is itself heavily inspired by scikit-learn.</p>
<hr>
<p><img alt="Keras" src="img/keras.jpg" height="100%"></p>
<hr>
<p>And Keras is entering TensorFlow first as <code>tf.contrib.keras</code> and soon just <code>tf.keras</code> with version 1.2.</p>
<hr>
<ul>
<li>high-level ML APIs<ul>
<li>training an Estimator</li>
<li>pre-trained Keras</li>
</ul>
</li>
</ul>
<hr>
<p>So let's try this out!</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
</ul>
<p>There's also a TensorBoard demo baked in there. A couple backup slides follow.</p>
<hr>
<p><img alt="graph" src="img/tensorboard_graph.png"></p>
<hr>
<p>This is what the graph should look like in TensorBoard.</p>
<hr>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<hr>
<p>Steps per second should look something like this.</p>
<hr>
<p><img alt="loss" src="img/loss.png"></p>
<hr>
<p>Loss should look like this.</p>
<hr>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg" height="100%"></p>
<hr>
<p>I just enjoy this image too much not to share it.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>flip regression to classification?</li>
<li>classify some images?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Do you need a model of some kind in your system? Maybe you can use TensorFlow's high-level machine learning APIs.</p>
<p>If you want to work with the stuff just shown some more, that's also totally cool! Instead of simple regression, maybe you want to flip the presidential GDP problem around to be logistic regression. TensorFlow has <code>tf.contrib.learn.LinearClassifier</code> for that. And many more variants!</p>
<p>Or maybe you want to classify your own images, or start to poke around the model some more. Also good! If you want more example images, there's <a href="https://github.com/ajschumacher/imagen">this set</a>.</p>
<p>Here are some links that could be helpful:</p>
<ul>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<hr>
<p>Oh my! Are we out of time already?</p>
<hr>
<p>What else?</p>
<hr>
<p>There are a lot of things we haven't covered.</p>
<hr>
<p>debugging, optimizing (XLA, low-precision, etc.), serving, building custom network architectures, embeddings, recurrent, generative, bazel, protobuf, gRPC, queues, threading...</p>
<hr>
<p>Here's a list of the first things that came to mind.</p>
<p>I hope you continue to explore!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170509-building_tensorflow_systems_from_components/</link>
<guid>http://planspace.org/20170509-building_tensorflow_systems_from_components/</guid>
<pubDate>Tue, 09 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Regression with a TensorFlow Estimator</title>
<description><![CDATA[

<p>With TensorFlow 1.1, the <a href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators">Estimator</a> API is now at <code>tf.estimator</code>. A number of "canned estimators" are <a href="https://www.tensorflow.org/extend/estimators">at</a> <code>tf.contrib.learn</code>. This higher-level API bakes in some best practices and makes it much easier to do a lot quickly with TensorFlow, similar to using APIs available in other languages.</p>
<hr>
<h2>Data</h2>
<p>This example will use the very simple <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/">US Presidential Party and GDP Growth dataset</a>: <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/president_gdp.csv">president_gdp.csv</a>.</p>
<p>The regression problem will be to predict annualized percentage GDP growth from presidential party.</p>
<hr>
<h2>R</h2>
<p><a href="https://www.r-project.org/">R</a> is made for problems such as this, with an API that makes it quite easy:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; model = lm(growth ~ party, data)
&gt; predict(model, data.frame(party=c('R', 'D')))
##        1        2
## 2.544444 4.332857</code></pre>

<p>The dataset is very small, and we won't introduce a train/test split. Linear regression is just a way of calculating means: we expect our model to predict the mean GDP growth conditional on party. Annual GDP growth during Republican presidents has been about 2.5%, and during Democratic presidents about 4.3%.</p>
<hr>
<h2>sklearn</h2>
<p>Moving into Python, let's first read in the data and get it ready, using <a href="http://www.numpy.org/">NumPy</a> and <a href="http://pandas.pydata.org/">Pandas</a>.</p>
<pre><code class="language-python">import numpy as np
import pandas as pd

data = pd.read_csv('president_gdp.csv')
party = data.party == 'D'
party = np.expand_dims(party, axis=1)
growth = data.growth</code></pre>

<p>With R, we relied on automatic handling of categorical variables. Here we explicitly change the strings 'R' and 'D' to be usable in a model: Boolean values will become zeros and ones. We also adjust the <code>party</code> data shape to be one row per observation.</p>
<p>Tracking <a href="/20170321-various_tensorflow_apis_for_python/">TensorFlow Python APIs</a>, the Estimator API comes from TF Learn, which is inspired by <a href="http://scikit-learn.org/">scikit-learn</a>. Here's the regression with scikit:</p>
<pre><code class="language-python">import sklearn.linear_model

model = sklearn.linear_model.LinearRegression()
model.fit(X=party, y=growth)
model.predict([[0], [1]])
## array([ 2.54444444,  4.33285714])</code></pre>

<hr>
<h2>TensorFlow</h2>
<p>This will abuse the API a little to maximize comparability to the examples above; you'll see warnings when you run the code, which will be addressed in the next section.</p>
<pre><code class="language-python">import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col])</code></pre>

<p>Unlike with scikit, we need to specify the structure of our regressors when we instantiate the model object. This is done with FeatureColumns. There are several <a href="https://www.tensorflow.org/tutorials/wide#selecting_and_engineering_features_for_the_model">options</a>; <code>real_valued_column</code> is probably the simplest but others are useful for general categorical data, etc.</p>
<p>We're providing that data as a simple matrix, so it's important that we use the empty string <code>''</code> for <code>column_name</code>. If there is a substantial <code>column_name</code>, we'll have to provide data in dictionaries with column names as keys.</p>
<pre><code class="language-python">model.fit(x=party, y=growth, steps=1000)
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>TensorFlow needs to be told how many steps of gradient descent to run, or it will keep going indefinitely, without additional configuration. A thousand iterations gets very close to the results achieved with R and with scikit.</p>
<p>There are a lot of things that <code>LinearRegressor</code> takes care of. In this code, we did not have to explicitly:</p>
<ul>
<li>Create any TensorFlow variables.</li>
<li>Create any Tensorflow ops.</li>
<li>Choose an optimizer or learning rate.</li>
<li>Create a TensorFlow session.</li>
<li>Run ops in a session.</li>
</ul>
<p>This API also does a lot more than the R or scikit examples above, and allows for even more extensions.</p>
<hr>
<h2>TensorFlow Extensions</h2>
<p>The Estimator API does a lot by default, and allows for a lot more optionally.</p>
<p>First, there is a <code>model_dir</code>. Above, TensorFlow automatically used a temporary directory. It's nicer to explicitly choose a <code>model_dir</code>.</p>
<pre><code class="language-python">model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')</code></pre>

<p>The <code>model_dir</code> is used for two main purposes:</p>
<ul>
<li>Saving TensorBoard summaries (log info)</li>
<li>Saving model checkpoints</li>
</ul>
<h3>Automatic TensorBoard</h3>
<p><a href="/20170430-tensorflows_queuerunner/">Like</a> an <code>input_producer</code>, an Estimator automatically writes information for TensorBoard. To check them out, point TensorBoard at the <code>model_dir</code> and browse to <code>localhost:6006</code>.</p>
<pre><code class="language-bash">$ tensorboard --logdir tflinreg</code></pre>

<p>For the example above, we get the model graph and two scalar summaries.</p>
<p>Here's what was was constructed in the TensorFlow graph for our <code>LinearRegressor</code>:</p>
<p><img alt="graph" src="img/graph.png"></p>
<p>In the scalar summaries, we get a measure of how fast the training process was running, in global steps per second:</p>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<p>The variation in speed shown here is not particularly meaningful.</p>
<p>And we get the training loss:</p>
<p><img alt="loss" src="img/loss.png"></p>
<p>We didn't really need to train for a full thousand steps.</p>
<p>By default, summaries are generated every 100 steps, but this can be set via <code>save_summary_steps</code> in a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>, along with several other settings.</p>
<p>Further customization, with support for additional metrics, validation on separate data, and even automatic early stopping, is available with <a href="https://www.tensorflow.org/get_started/monitors">ValidationMonitor</a>.</p>
<h3>Automatic Model Save/Restore</h3>
<p>After training for 1,000 steps above, TensorFlow saved the model to the <code>model_dir</code>. If we point to the same <code>model_dir</code> again in a new Python session, the model will be automatically restored from that checkpoint.</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>For more control over how often and when checkpoints are saved, see <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>.</p>
<h3>Using input functions</h3>
<p>Above, training data was provided via <code>x</code> and <code>y</code> arguments, which is like how scikit works, but not really what TensorFlow Estimators should use.</p>
<p>The appropriate mechanism is to make an <a href="https://www.tensorflow.org/get_started/input_fn">input function</a> that returns the equivalents to <code>x</code> and <code>y</code> when called. The function is passed as the <code>input_fn</code> argument to <code>model.fit()</code>, for example.</p>
<p>This approach is flexible and makes it easy to avoid, for example, keeping track of separate data structures for data and labels.</p>
<h3>Distributed Training</h3>
<p>Among the <code>tf.contrib.learn</code> <a href="https://www.tensorflow.org/api_guides/python/contrib.learn">goodies</a> is <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"><code>tf.contrib.learn.Experiment</code></a>, which works with an Estimator to help do distributed training. It looks like this one is still settling down, with a lot of deprecated bits at the moment. I'm interested to see more about this. For now, you could check out a Google Cloud ML <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/iris/trainer/task.py">example</a> that works with <code>learn_runner</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</link>
<guid>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</guid>
<pubDate>Sat, 06 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Dataset: US Presidential Party and GDP Growth</title>
<description><![CDATA[

<p>Here's a simple <a href="president_gdp.csv">dataset</a> for use in examples. It's taken from the <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">online appendix</a> to <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20140913">Presidents and the US Economy: An Econometric Exploration</a> by Blinder and Watson.</p>
<p><a href="president_gdp.csv"><code>president_gdp.csv</code></a></p>
<p>The fields are:</p>
<ul>
<li><code>term</code>: A short text description of the presidential term, like "Reagan 2".</li>
<li><code>party</code>: The political party of the presidency, either "D" for the <a href="https://en.wikipedia.org/wiki/Democratic_Party_(United_States)">Democratic Party</a> or "R" for the <a href="https://en.wikipedia.org/wiki/Republican_Party_(United_States)">Republican Party</a>.</li>
<li><code>growth</code>: The average annualized growth in US Gross Domestic Product (<a href="https://en.wikipedia.org/wiki/Gross_domestic_product">GDP</a>) for that presidential term, expressed as percentage points.</li>
</ul>
<p>For more details, please see the <a href="http://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.20140913">paper</a> and <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">appendix</a>.</p>
<p>The only changes I've made are to order the data chronologically and put it in the convenient CSV format.</p>
<p>For example, in <a href="https://www.r-project.org/">R</a>, you can do the following:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; lm(growth ~ party, data)
&gt; t.test(growth ~ party, data)</code></pre>

<p>Because the <a href="president_gdp.csv">dataset</a> is so small, I'll also display it as spaced-out text here:</p>
<pre><code>term,             party,  growth
Truman,               D,    6.57
Eisenhower 1,         R,    2.72
Eisenhower 2,         R,    2.26
Kennedy-Johnson,      D,    5.74
Johnson 2,            D,    4.95
Nixon 1,              R,    3.57
Nixon-Ford,           R,    1.97
Carter,               D,    3.56
Reagan 1,             R,    3.12
Reagan 2,             R,    3.89
G.H.W. Bush,          R,    2.05
Clinton 1,            D,    3.53
Clinton 2,            D,    4.00
G.W. Bush 1,          R,    2.78
G.W. Bush 2,          R,    0.54
Obama 1,              D,    1.98</code></pre>    
    ]]></description>
<link>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</link>
<guid>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</guid>
<pubDate>Fri, 05 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Pre-trained Models with Keras in TensorFlow</title>
<description><![CDATA[

<p>With TensorFlow 1.1, <a href="https://github.com/fchollet/keras">Keras</a> is now at <code>tf.contrib.keras</code>. With TensorFlow 1.3, it should be at <code>tf.keras</code>. This is great for making new models, but we also get the pre-trained models of <a href="https://github.com/fchollet/keras/tree/master/keras/applications"><code>keras.applications</code></a> (<a href="https://github.com/fchollet/deep-learning-models">also seen elsewhere</a>). It's so easy to classify images!</p>
<pre><code class="language-python">import tensorflow as tf

model = tf.contrib.keras.applications.ResNet50()</code></pre>

<p>This will automatically download trained weights for a model based on <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>. The weights are cached below your home directory, in <code>~/.keras/models/</code>.</p>
<p>Convenient image tools are also included. Let's use an <a href="https://github.com/ajschumacher/imagen/blob/master/imagen/n01882714_4157_koala_bear.jpg">image</a> of a koala from the <a href="https://github.com/ajschumacher/imagen">imagen</a> ImageNet subset.</p>
<p><img alt="original koala" src="n01882714_4157_koala_bear.jpg"></p>
<pre><code class="language-python">filename = 'n01882714_4157_koala_bear.jpg'
image = tf.contrib.keras.preprocessing.image.load_img(
    filename, target_size=(224, 224))</code></pre>

<p>This model can take input images that are 224 pixels on a side, so we have to make our image that size. We're just doing it by squishing, in this case.</p>
<p><img alt="smaller koala" src="smaller_koala.jpg"></p>
<p>We'll make that into an array that the model can take as input.</p>
<pre><code class="language-python">import numpy as np

array = tf.contrib.keras.preprocessing.image.img_to_array(image)
array = np.expand_dims(array, axis=0)</code></pre>

<p>Now we can classify the image!</p>
<pre><code class="language-python">probabilities = model.predict(array)</code></pre>

<p>We have one thousand probabilities, one for each class the model knows about. To interpret the result, we can use another helpful function.</p>
<pre><code class="language-python">tf.contrib.keras.applications.resnet50.decode_predictions(probabilities)
## [[(u'n01882714', u'koala', 0.99466419),
##   (u'n02497673', u'Madagascar_cat', 0.0013330306),
##   (u'n01877812', u'wallaby', 0.00085774728),
##   (u'n02137549', u'mongoose', 0.00063530984),
##   (u'n02123045', u'tabby', 0.00056512095)]]</code></pre>

<p>Great success! The model is highly confident that it's looking at a koala. Not bad.</p>
<p>It's pretty fun that this kind of super-easy access to quite good pre-trained models is now available all within the TensorFlow package. Just <code>pip install</code> and go!</p>
<hr>
<p>The thousand ImageNet categories this model knows about include some things that are commonly associated with people, but not a "person" class. Still, just for fun, what will <code>ResNet50</code> say about me?</p>
<pre><code class="language-python">## [[(u'n02883205', u'bow_tie', 0.3144455),
##   (u'n03787032', u'mortarboard', 0.059674311),
##   (u'n02992529', u'cellular_telephone', 0.049916871),
##   (u'n04357314', u'sunscreen', 0.048197504),
##   (u'n04350905', u'suit', 0.03481029)]]</code></pre>

<p>I guess I'll take it?</p>
<p><img alt="Aaron" src="aaron.jpg"></p>
<hr>
<p><strong>Notes:</strong></p>
<p>The model may have been trained on the very koala picture we're testing it with. I'm okay with that. Feel free to test your own koala pictures!</p>
<p>There's also another function, <code>resnet50.preprocess_input</code>, which in theory should help the model work better, but my tests gave seemingly worse results when using that pre-processing. It would be used like this:</p>
<pre><code class="language-python">array = tf.contrib.keras.applications.resnet50.preprocess_input(array)</code></pre>

<p>Keras in TensorFlow also contains <code>vgg16</code>, <code>vgg19</code>, <code>inception_v3</code>, and <code>xception</code> models as well, along the same lines as <code>resnet50</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</link>
<guid>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</guid>
<pubDate>Tue, 02 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling ImageNet</title>
<description><![CDATA[

<p><a href="http://image-net.org/">ImageNet</a> is a standard image dataset. It's pretty big; just the IDs and URLs of the images take over a gigabyte of text. I collected a fun <a href="https://github.com/ajschumacher/imagen">sampling</a> for small-scale purposes.</p>
<hr>
<p>ImageNet is distributed primarily as a text file of <a href="http://image-net.org/download-imageurls">image URLs</a>. The compressed file is 334 megabytes. The unpacked file is 1.1 gigabytes.</p>
<pre><code class="language-bash">$ wget http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz
$ tar zxvf imagenet_fall11_urls.tgz
$ wc fall11_urls.txt
##  14197122 28414665 1134662781 fall11_urls.txt
$ head -3 fall11_urls.txt
## n00004475_6590   http://farm4.static.flickr.com/3175/2737866473_7958dc8760.jpg
## n00004475_15899  http://farm4.static.flickr.com/3276/2875184020_9944005d0d.jpg
## n00004475_32312  http://farm3.static.flickr.com/2531/4094333885_e8462a8338.jpg</code></pre>

<p>The first field is an image ID. The part before the underscore is a WordNet ID, so the first image is of <code>n00004475</code>. What's that?</p>
<p>The mapping from WordNet ID to a brief text label can be downloaded from a link on the ImageNet <a href="http://image-net.org/download-API">API page</a>.</p>
<pre><code class="language-bash">$ wget http://image-net.org/archive/words.txt
$ wc words.txt
##   82114  302059 2655750 words.txt
$ head -3 words.txt
## n00001740   entity
## n00001930   physical entity
## n00002137   abstraction, abstract entity</code></pre>

<p>There are 82,114 WordNet IDs. Now we can decode the one we're interested in.</p>
<pre><code class="language-bash">$ grep n00004475 words.txt
## n00004475    organism, being</code></pre>

<p>So the first picture in ImageNet is of an "organism, being". What does such a thing look like?</p>
<p><img alt="organism, being" src="img/n00004475_6590.jpg"></p>
<p>There are eight examples of "organism, being" and two of the others are cats.</p>
<p>I think 82,114 categories is too many to try to sample randomly from, for my purposes. I'll use the <a href="http://image-net.org/challenges/LSVRC/2017/browse-det-synsets">200 categories</a> specified for the <a href="http://image-net.org/challenges/LSVRC/2017/">ILSVRC2017</a> object detection <a href="http://image-net.org/challenges/LSVRC/2017/#det">challenge</a>.</p>
<pre><code class="language-bash">wget -O 200words.html http://image-net.org/challenges/LSVRC/2017/browse-det-synsets</code></pre>

<p>I used Emacs to pull out the 200 WordNet IDs and convenient extra-short descriptions, saved in <a href="200words.csv">200words.csv</a>. The script <a href="make_urls_subset.py">make_urls_subset.py</a> produces <a href="200words100urls.csv">200words100urls.csv</a> with 100 random URLs for each of the categories. Finally, <a href="get_fives.py">get_fives.py</a> downloads five working JPGs for each category. A couple came back with "missing" images, so I manually replaced those with others from the list.</p>
<p>The results are packaged up on <a href="https://github.com/">GitHub</a> at <a href="https://github.com/ajschumacher/imagen">ajschumacher/imagen</a> and feature such beauties as <a href="img/n02118333_27_fox.jpg">n02118333_27_fox.jpg</a>.</p>
<p><img alt="n02118333_27_fox.jpg" src="img/n02118333_27_fox.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20170430-sampling_imagenet/</link>
<guid>http://planspace.org/20170430-sampling_imagenet/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow's QueueRunner</title>
<description><![CDATA[

<p>A TensorFlow <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a> helps to feed a TensorFlow <a href="http://planspace.org/20170327-tensorflow_and_queues/">queue</a> using threads which are optionally managed with a TensorFlow <a href="http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/">Coordinator</a>. QueueRunner objects can be used directly, or via higher-level APIs, which also offer automatic TensorBoard summaries.</p>
<pre><code class="language-python">import tensorflow as tf
session = tf.Session()</code></pre>

<hr>
<h2>QueueRunner Directly</h2>
<p>To use a QueueRunner you need a TensorFlow queue and an op that puts a new thing in the queue every time that op is evaluated. Typically, such an op will itself involve a queue, which is a bit of a tease. To avoid that circularity, this example will use random numbers.</p>
<pre><code class="language-python">queue = tf.FIFOQueue(capacity=10, dtypes=[tf.float32])
random_value_to_enqueue = tf.random_normal([])  # shape=[] means a single value
enqueue_op = queue.enqueue(random_value_to_enqueue)
random_value_from_queue = queue.dequeue()</code></pre>

<p>At this point if you evaluate <code>random_value_from_queue</code> in the session it will block, because nothing has been put in the queue yet.</p>
<pre><code class="language-python">queue_runner = tf.train.QueueRunner(queue, [enqueue_op])</code></pre>

<p>Still nothing has been enqueued, but <code>queue_runner</code> stands ready to make threads that will do the enqueuing.</p>
<p>If you put more enqueue ops in the list, or the same one multiple times, more threads will be started when things get going.</p>
<pre><code class="language-python">coordinator = tf.train.Coordinator()
threads = queue_runner.create_threads(session, coord=coordinator, start=True)</code></pre>

<p>Using <code>start=True</code> means we won't have to call <code>.start()</code> for each thread ourselves.</p>
<pre><code class="language-python">&gt;&gt;&gt; len(threads)
## 2</code></pre>

<p>There are two threads running: one for handling coordinated shutdown, and one for the enqueue op.</p>
<p>Now at last we can get at random values from the queue!</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(random_value_from_queue)
## 0.69283932
&gt;&gt;&gt; session.run(random_value_from_queue)
## -0.53802371</code></pre>

<p>The feeding thread will try to keep the queue at capacity, which was set to 10, so there should always be more items available to dequeue.</p>
<p>Since we used a coordinator, we can shut the threads down nicely.</p>
<pre><code class="language-python">coordinator.request_stop()
coordinator.join(threads)</code></pre>

<hr>
<h2>QueueRunner with Higher-Level APIs</h2>
<p>It's possible to work with QueueRunner directly, as shown above, but it's easier to use higher-level TensorFlow APIs that themselves use QueueRunner.</p>
<p>It's common for TensorFlow queue-chains to start with a list of filenames (sometimes a list of just one filename) to read data from. The <code>string_input_producer</code> function makes a queue using provided strings.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'])
letter_from_queue = queue.dequeue()</code></pre>

<p>This is a <code>FIFOQueue</code>, just as before, but notice we don't have an enqueue op for it. Like many things in <code>tf.train</code>, here TensorFlow has already done some work for us. A QueueRunner has already been made, and it was added to the <code>queue_runners</code> collection.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.get_collection('queue_runners')
## [&lt;tensorflow.python.training.queue_runner_impl.QueueRunner object at 0x121ee2c10&gt;]</code></pre>

<p>You could access and run that QueueRunner directly, but <code>tf.train</code> makes things easier.</p>
<!--

Finally, here's a place where it would be useful to be
doing a session context manager!

-->

<pre><code class="language-python">coordinator.clear_stop()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p><code>tf.train.start_queue_runners</code> automatically starts the threads.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'</code></pre>

<p>By default, the queue will go through the original items multiple times, or multiple epochs, and shuffle the order of strings within an epoch.</p>
<p>Limiting the number of epochs uses a local variable, which must be initialized.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, num_epochs=1)
letter_from_queue = queue.dequeue()
initializer = tf.local_variables_initializer()
session.run(initializer)
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p>Now the QueueRunner will close the queue when there isn't anything more to put in it, so the dequeue op will eventually give an <code>OutOfRangeError</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## OutOfRangeError</code></pre>

<hr>
<h2>Automatic TensorBoard Summaries</h2>
<p>A single-epoch queue will be helpful for illustrating an interesting thing about <code>tf.train.string_input_producer</code>: it automatically adds a TensorBoard summary to the graph.</p>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<p>It's nice to have direct control over every detail of your program, but the conveniences of higher-level APIs are also pretty nice. The summary that gets added is a scalar summary representing the percent full that the queue is.</p>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg"></p>
<pre><code class="language-python">tf.reset_default_graph()  # Starting queue runners will fail if a queue is
session = tf.Session()    # closed, so we need to clear things out.
queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, capacity=4, num_epochs=1)
letter_from_queue = queue.dequeue()
summaries = tf.summary.merge_all()
summary_writer = tf.summary.FileWriter('logs')
initializer = tf.local_variables_initializer()
session.run(initializer)
coordinator = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)
for i in range(4):
    summary_writer.add_summary(session.run(summaries), i)
    session.run(letter_from_queue)
summary_writer.add_summary(session.run(summaries), 4)
summary_writer.flush()  # Ensure everything is written out.</code></pre>

<p>After opening up TensorBoard with <code>tensorboard --logdir=logs</code> and going to <code>http://localhost:6006/</code> and and turning off plot smoothing, you'll see this:</p>
<p><img alt="log" src="img/fraction_full.png"></p>
<p>This shows that the queue, with capacity four, started 100% full and then every time something was dequeued from it it became 25 percentage points less full until it was empty.</p>
<p>For this example, the queue wasn't being refilled, so we knew it would become less and less full. But if you're reading data into an input queue that you expect to keep full, it's a great diagnostic to be able to check how full it actually is while things are running, to find find out if loading data is a bottleneck.</p>
<p>The automatic TensorBoard logging here is also a nice first taste of all the things that happen with even higher-level TensorFlow APIs.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170430-tensorflows_queuerunner/</link>
<guid>http://planspace.org/20170430-tensorflows_queuerunner/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>From Behaviorist to Constructivist AI</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/B._F._Skinner">B. F. Skinner</a> might be satisfied that neural networks achieve intelligence when they perform tasks well. This behaviorist perspective leads to misunderstandings of current technology and limits development toward systems that think. Pervasive epistemological confusion about categories is one example. In general, a constructivist approach will become necessary for advanced machine learning and artificial intelligence.</p>
<p><img alt='bird in operant conditioning chamber or "Skinner box"' src="img/skinner_box.jpg"></p>
<p>Training a neural network by <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagating</a> from a loss function is a lot like <a href="https://en.wikipedia.org/wiki/Operant_conditioning">operant conditioning</a>. Error becomes punishment. The objective and result of eliciting particular behavior is the same whether you're doing object detection <a href="https://motherboard.vice.com/en_us/article/america-secretly-tried-to-destroy-totalitarianism-with-pigeons">with a pigeon</a> in a <a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber">Skinner box</a> or with a convolutional neural network.</p>
<p>Little could be less like real intelligence than blurting out a name for every object you see. Little could better epitomize behaviorist stimulus-response. This is the intelligence of the <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> classifiers that popularized deep learning.</p>
<p>These classifiers really shouldn't be anthropomorphized. A cat/dog classifier is not thinking about cats and dogs. An engineer designed the network with a cat neuron and a dog neuron, and got them to light them up as desired.</p>
<p>An image classifier has continuous input and categorical output. The categories are specified by design. This is clearly a limitation on the output side, and a very different limitation from any constraint on the input side in image resolution or color space. A cat/dog classifier cannot say anything other than cat and dog.</p>
<p>One could argue that this output is not strictly categorical because it might be read, for example, as 95% cat and 5% dog, but this does not undo the designed categories, and this sort of non-<a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac</a> enhancement is not generally provided when systems take categorical input.</p>
<p>Language models are frequently categorically constrained in both input and output. At the word level, this means a model can't deal with words it's never seen before. This leads to approaches like <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">Google's neural machine translation</a> falling back to <a href="https://arxiv.org/abs/1508.07909">subword units</a> for rare words. But even if a language model goes to the character level, this is still a categorical constraint, and a system trained on "e" could be perfectly blind to "&#233;."</p>
<p>Whether categories are imposed on the input or output side, they make it obvious that the system is limited. The system cannot handle categories not specified in the design.</p>
<p>Categorical input is also foreign to humans. It would be like having a separate sense for every category. Instead of feeling "warm" or "not-warm", for instance, you could feel "cat" or "not-cat" and "dog" or "not-dog," and however many more. But if you didn't have a sense for "aardvark," you would be congenitally blind to "aardvark."</p>
<p>Continuous sensor data, like images and audio, is much more interesting and analogous to the human experience. There are still limitations - for example, you don't really see the edges of your field of view, and you can't really imagine what sensing magnetism would be like - but unstructured input provides a starting point for forming gestalt perceptions.</p>
<p><img height="240" alt="ouija board" src="img/ouija_board.jpeg"></p>
<p><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a>, or word vectors, might seem to handle the categorical problem, but this is largely misdirection. Word embeddings are representations of words that have fixed dimensionality, so that a language system no longer needs a separate input for every possible word, but only a separate input for every dimension of the word embeddings. There might be 50,000 words, but only 200 or 300 dimensions for an embedding.</p>
<p>Using word embeddings doesn't solve the categorical problem; it just pushes the problem to the embeddings. A system can still only handle words that embeddings have been generated for.</p>
<p>Word embeddings are useful in that they are representations of words that tend to give good results when used as input to various algorithms. In the same way, convolutional classifiers learn representations of images that tend to give good results when used as input to other algorithms. In both cases, having a good representation is useful. In both cases, people may or may not find the representations interpretable.</p>
<p>There tends to be <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/3/yoshua-bengio-machines-that-dream">excitement</a> about the meaningfulness of word embeddings when people amuse themselves with arithmetic like <em>king - man + woman = queen</em>, or make visualizations with reasonable-seeming clusters. But the understanding that's happening here is happening in the people; having nice representations does not mean that a system has achieved understanding.</p>
<p>It seems important that an intelligent system should be able to develop internal concepts without those concepts being built into the system's design, so there was interest when Google <a href="https://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html">found</a> a cat neuron in a 2011 system.</p>
<p><img height="240" src="img/google_cat.jpg" alt="Google cat neuron"></p>
<p>The system was trained to take an image as input and generate the same image as output. This is easy for a computer to do just by copying, so to make it interesting you have to put restrictions on the flow of information from input to output. The restricted system learns good representations for the images. Then, by testing lots of images with and without cats, Google found one point in the system that tended to respond positively to cats and negatively to other things.</p>
<p>The temptation is to declare that the system formed an idea of cats. You could just as well say that a mold formed an idea of its cast.</p>
<p>Visual systems learn many useful internal representations. For example, they learn oriented edge detectors. Humans have these too. It should be clear that in neither case is there an idea of an oriented edge. The Google researchers were correct in their <a href="https://arxiv.org/abs/1112.6209">paper</a>'s title when they said that their system learned high-level features rather than concepts.</p>
<p>More recently, <a href="https://openai.com/">OpenAI</a>'s <a href="https://blog.openai.com/unsupervised-sentiment-neuron/">unsupervised sentiment neuron</a> is another case of humans interpreting neurons. The model is categorical at input and output on the character level, learning to predict the next character in text. OpenAI used all 4,096 dimensions of their learned representation, but their title comes from noticing that just one of those dimensions captured a lot of sentiment-related information.</p>
<p>Word embeddings are patterns of activation over perhaps 300 neurons. They are <a href="http://www.bcp.psych.ualberta.ca/~mike/Pearl_Street/Dictionary/contents/D/distributed.html">distributed representations</a>. A cat neuron or sentiment neuron, on the other hand, is in line with the implausible "<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">unit/value principle</a>" that a single neuron represents a single concept.</p>
<p>It is easy to think that words <em>are</em> categories, and so a distributed representation of a word is a distributed representation of a category. With images, it's clear that one picture of a cat is different from another picture of a cat. But words are not categories. The difference between words and images is principally one of cardinality (there are many more images than words) and composibility (images more easily contain many things). But just as a word vector for <em>cat</em> is close to a word vector for <em>kitty</em>, distributed representations for pictures of cats should also be close to one another, and the Google system could just as well have been mined for distributed representations. Perhaps it is the human desire to categorize that makes us comfortable with multi-dimensional representations when we've provided categories in advance, but look for single-dimensional representations when we haven't. (Or maybe it's just easier.)</p>
<p>Regardless of whether distributed or unit representations are better, having a representation does not imply thought. These representations flash through their networks, coming before the result like Pavlovian slobber. This is not to say these representations couldn't be used in a system that thinks, but that current usage is too limited. One thing that's missing is state that develops over time.</p>
<p>Sequence models (such as the sentiment neuron example) introduce a limited kind of time-awareness, and a kind of memory. There could be something here (<a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/9/ilya-sutskever-unsupervised-learning-attention-and-other-mysteries">attention</a> in particular is interesting) but most <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/10/oriol-vinyals-sequence-to-sequence-machine-learning">usage</a> still seems to be learning representations and doing encoding to and decoding from these representations.</p>
<p>To be clear: Good representations are useful, whether or not they are utilized for anything like higher-level thought. But it seems unlikely that conventional models used in supervised or unsupervised learning will spontaneously invent higher-level thought, no matter how good their representations are.</p>
<p>By its name, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> seems purely behavioristic, but it also recognizes the idea of internal models, as illustrated in <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>'s taxonomy of reinforcement learning agents. These models are generally something like an agent's internal conception of the world around it.</p>
<p><img alt="taxonomy of reinforcement learning agents" src="img/rl_taxonomy.png"></p>
<p>Lots of reinforcement learning is model-free, and where there are internal models they are often heavily specified in advance or quite distant from what we think of as mental models. The idea, if not current implementations, is key. The behaviorist perspective is one of only inputs and outputs. Model-based reinforcement learning suggests, at least in spirit, a missing piece. Humans certainly aren't model-free, for example.</p>
<p>What the constructivist perspective adds is that it isn't enough to simply have an internal model. Intelligence entails building and working with new models as a part of problem-solving.</p>
<p>One view of building different models for different situations might be selectively enlisting parts of a large system for a given task. <a href="https://arxiv.org/abs/1701.08734">PathNet</a> tries to "discover which parts of the network to re-use for new tasks." This is interesting, but the focus on selecting a subset of wiring seems distant from the imaginative process of building a mental model, likely drawing on representations which may be distributed.</p>
<p>It becomes important to understand how a representation behaves. Say a system does have a cat neuron; can it reason about cats? The cat neuron can be excited, but this seems more like experiencing the <a href="https://en.wikipedia.org/wiki/Qualia">quale</a> of cat-ness than like imagining a cat. Experiencing an emotion is even more clearly not an abstraction, so the case of a sentiment neuron makes this distinction even clearer.</p>
<p>Imagining a cat might be an algebraic operation, in the sense that it posits an entity, a variable, which has cat properties or is a cat. In <a href="https://mitpress.mit.edu/books/algebraic-mind">The Algebraic Mind</a>, Gary Marcus argues that connectionist (neural network) models lack this kind of kind of ability.</p>
<p><img alt="The Algebraic Mind" src="img/algebraic_mind_cover.jpg"></p>
<p>An intelligent system should be able not only to represent things but to build and manipulate models composed from these representations.</p>
<p>For example, whether you like Chomsky or not, understanding a sentence seems like an algebraic procedure in the sense of apprehending values for variables like subject, verb, and object. The plug and play composability of noun phrases and the like also suggests a constructive mental process.</p>
<p>Or take the example of number: can a system perceive that a picture has three cats, as opposed to two cats? There's some depth here, as a system could represent two or three entities all of which are cats individually, or it could represent number concepts explicitly. It's hard to find current models that do either in a meaningful way.</p>
<p>The Algebraic Mind was published in 2001. There have been many advances in machine learning since 2001, but they largely haven't been advances toward reasoning. In this sense, Marcus thinks artificial intelligence is <a href="https://www.technologyreview.com/s/603945/is-artificial-intelligence-stuck-in-a-rut/">in a rut</a>.</p>
<p>If you take the view that machine learning is a type of statistics, then you may not care about any of this. But for machine learning as artificial intelligence, it may be that algebraic (or symbolic) considerations will be necessary.</p>
<p>A fair criticism is that if it isn't clear how to make direct progress in the constructivist direction, time is better spent advancing what have become traditional techniques. There is certainly value in this kind of advancement.</p>
<p>There may also be some reason for hope in <a href="https://deepmind.com/blog/differentiable-neural-computers/">differentiable neural computers</a> (DNCs) and related work. In some ways, the linking of DNC memory locations is like Marcus's proposal for representing structured data, and the DNC memory itself is something like Marcus's idea of registers. But it looks like DNCs work with categorical input and output, relying on it for seemingly algebraic task performance.</p>
<p>It seems likely that artificial general intelligence will use a composite approach. It may process visual input with convolutions. It may use distributed representations for internal concepts. It may access memory along the lines of a differentiable neural computer. It may have an attention mechanism that allows it to focus on the state of the external world one moment and its internal world the next. Combining all these ideas into a system that can come up with its own ideas is an intriguing project.</p>
<blockquote>
<p>&#8220;There is no reason, as yet, to be confident that an intermediate symbolic representation will not be required for modeling higher cognitive processes.&#8221; (<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">Feldman and Ballard, 1982</a>)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</link>
<guid>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</guid>
<pubDate>Sat, 29 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Everything in the Graph? Even Glob?</title>
<description><![CDATA[

<p>Programming with TensorFlow is largely building TensorFlow graphs. It can be like using Python to write a program for another computer: the TensorFlow runtime. If you want all your computation to be in the graph, you need to be able to express everything with TensorFlow ops.</p>
<p>It's not necessarily bad to do work outside the TensorFlow graph. Maybe there's a package that does exactly what you need for one part of your program. It could make sense to use TensorFlow for some things, and then pull off the graph and use the other package.</p>
<p>Advantages of staying inside the graph might include better performance, as you aren't moving data into and out of the TensorFlow runtime's internal formats. And if you're developing in Python but want to take your graph and ship it using TensorFlow serving, or perhaps use another language, anything outside the graph will need special attention.</p>
<p>The desire to stay in-graph leads to lots of ops existing that you might not think are essential for a machine learning library. This includes functionality for file formats like PNG and JPEG, queues and queue management, and even the lowly <code>glob</code>.</p>
<p><a href="https://en.wikipedia.org/wiki/Glob_(programming)"><code>glob</code></a> refers to patterns used for matching filenames. For example, in a shell, you might use a <code>glob</code> to list all the text files in a directory.</p>
<pre><code class="language-bash">$ ls *.txt
## one.txt        two.txt</code></pre>

<p>In Python, the standard library includes <a href="https://docs.python.org/3/library/glob.html"><code>glob</code></a> functionality.</p>
<pre><code class="language-python">&gt;&gt;&gt; import glob
&gt;&gt;&gt; glob.glob('*.txt')
## ['one.txt', 'two.txt']</code></pre>

<p>TensorFlow has a similar operation: <a href="https://www.tensorflow.org/api_docs/python/tf/matching_files"><code>tf.matching_files</code></a>.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; glob = tf.matching_files('*.txt')
&gt;&gt;&gt; session.run(glob)
## array(['./one.txt', './two.txt'], dtype=object)</code></pre>

<p>TensorFlow's globbing is limited in that it doesn't support wildcards in directory names, and it doesn't support the recursive globbing found in the <code>glob</code> of Python 3.5+.</p>
<p>Every time a <code>tf.matching_files</code> op is evaluated, it goes and reads filenames from disk. What's often seen instead is <a href="https://www.tensorflow.org/api_docs/python/tf/train/match_filenames_once"><code>tf.train.match_filenames_once</code></a>, which introduces an extra level of lazy evaluation in that it will execute <code>tf.matching_files</code> the first time it's evaluated, and then it caches the results, returning the same list of files when evaluated again even if the files on disk become different. Because this relies on a local variable for the cache (<a href="https://github.com/tensorflow/tensorflow/blob/a5b1fb8e56ceda0ee2794ee05f5a7642157875c5/tensorflow/python/training/input.py#L55">source</a>) you'll have to initialize.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; glob = tf.train.match_filenames_once('*.txt')
&gt;&gt;&gt; initializer = tf.local_variables_initializer()
&gt;&gt;&gt; session.run(initializer)
&gt;&gt;&gt; session.run(glob)
## array(['./one.txt', './two.txt'], dtype=object)</code></pre>

<p>With functionality like this, TensorFlow really subsumes a lot of things that could at least in theory be handled separately. The choice of whether to put any particular piece of computation into the TensorFlow graph or not remains up to developer.</p>
<p><img alt="glob" src="glob.png"></p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170428-everything_in_the_graph_even_glob/</link>
<guid>http://planspace.org/20170428-everything_in_the_graph_even_glob/</guid>
<pubDate>Fri, 28 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Tensors and TFRecords</title>
<description><![CDATA[

<p>When a matrix, array, or tensor has lots of values that are zero, it can be called <em>sparse</em>. You might want to represent the zeros implicitly with a <em>sparse representation</em>. TensorFlow has support for this, and the support extends to its TFRecords <code>Example</code> format.</p>
<p>Here is a sparse one-dimensional tensor:</p>
<pre><code class="language-python">[0, 7, 0, 0, 8, 0, 0, 0, 0]</code></pre>

<p>The tensor is sparse, in that it has a lot of zeros, but the representation is dense, in that all those zeros are represented explicitly.</p>
<p>A sparse representation of the same tensor will focus only on the non-zero values.</p>
<pre><code class="language-python">values = [7, 8]</code></pre>

<p>We have to also remember where those values occur, by their indices:</p>
<pre><code class="language-python">indices = [1, 5]</code></pre>

<p>The one-dimensional <code>indices</code> form will work with some methods, for this one-dimensional example, but in general indices have multiple dimensions, so it will be more consistent (and work everywhere) to represent <code>indices</code> like this:</p>
<pre><code class="language-python">indices = [[1], [5]]</code></pre>

<p>With <code>values</code> and <code>indices</code>, we don't have quite enough information yet. How many zeros are there to the right of the last value? We have to represent the dense shape of the tensor.</p>
<pre><code class="language-python">dense_shape = [9]</code></pre>

<p>These three things together, <code>values</code>, <code>indices</code>, and <code>dense_shape</code>, are a sparse representation of the tensor.</p>
<p>TensorFlow accepts lists of values and NumPy arrays to define dense tensors, and it returns NumPy arrays when dense tensors are evaluated. But what to do with sparse tensors? SciPy has <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html">several</a> sparse matrix representations, but not a good match for TensorFlow's general sparse tensor form. So for sparse tensors, instead of reusing an existing Python class, TensorFlow provides <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensorValue"><code>tf.SparseTensorValue</code></a>. These are values that exist outside the TensorFlow graph, so they can be made without a <code>tf.Session</code>, for example.</p>
<pre><code class="language-python">tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[1], [5]], values=[7, 8], dense_shape=[9])</code></pre>

<p>Using <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensor"><code>tf.SparseTensor</code></a> puts that in the TensorFlow graph.</p>
<pre><code class="language-python">tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
## &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x11a4e0c10&gt;</code></pre>

<p>That <code>tf.SparseTensor</code> will be constant, since we specified all the pieces of it, and if you run it in a session, you'll get back the equivalent <code>tf.SparseTensorValue</code>.</p>
<p>TensorFlow has <a href="https://www.tensorflow.org/api_guides/python/sparse_ops">operations</a> specifically for working with sparse tensors, such as <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_matmul"><code>tf.sparse_matmul</code></a>. And you can change a sparse matrix to a dense one with <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_to_dense"><code>tf.sparse_tensor_to_dense</code></a>. These operations live in the graph, so they have to be run to see a result.</p>
<pre><code class="language-python">sparse = tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
dense = tf.sparse_tensor_to_dense(sparse)
session.run(dense)
## array([0, 7, 0, 0, 0, 8, 0, 0, 0], dtype=int32)</code></pre>

<p>Going from dense to sparse seems a little less <a href="http://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow">straightforward</a> at the moment, so let's continue assuming we already have the components of our sparse representation.</p>
<p>Going to more dimensions is quite natural. Here's a two-dimensional tensor with three non-zero values:</p>
<pre><code class="language-python">[[0, 0, 0, 0, 0, 7],
 [0, 5, 0, 0, 0, 0],
 [0, 0, 0, 0, 9, 0],
 [0, 0, 0, 0, 0, 0]]</code></pre>

<p>This can be represented in sparse form as:</p>
<pre><code class="language-python">indices = [[0, 5],
           [1, 1],
           [2, 4]]

values = [7, 5, 9]

dense_shape = [4, 6]

tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[0, 5], [1, 1], [2, 4]], values=[7, 5, 9], dense_shape=[4, 6])</code></pre>

<p>Now, to represent this in a TFRecords <code>Example</code> requires a little bit of transformation. <a href="/20170323-tfrecords_for_humans/">TFRecords</a> only support lists of integers, floats, and bytestrings. The values are easily represented in one <code>Feature</code>, but to represent the <code>indices</code>, each dimension will need its own <code>Feature</code> in the <code>Example</code>. The <code>dense_shape</code> isn't represented at all; that's left to be specified at parsing.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'index_0': tf.train.Feature(int64_list=tf.train.Int64List(value=[0, 1, 2])),
    'index_1': tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 1, 4])),
    'values': tf.train.Feature(int64_list=tf.train.Int64List(value=[7, 5, 9]))
}))
my_example_str = my_example.SerializeToString()</code></pre>

<p>This TFRecord sparse representation can then be <a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">parsed inside the graph</a> as a <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a>.</p>
<pre><code class="language-python">my_example_features = {'sparse': tf.SparseFeature(index_key=['index_0', 'index_1'],
                                                  value_key='values',
                                                  dtype=tf.int64,
                                                  size=[4, 6])}
serialized = tf.placeholder(tf.string)
parsed = tf.parse_single_example(serialized, features=my_example_features)
session.run(parsed, feed_dict={serialized: my_example_str})
## {'sparse': SparseTensorValue(indices=array([[0, 5], [1, 1], [2, 4]]),
##                              values=array([7, 5, 9]),
##                              dense_shape=array([4, 6]))}</code></pre>

<p>Support for multi-dimensional sparse features seems to be new in TensorFlow 1.1, and TensorFlow gives this warning when you use <code>SparseFeature</code>:</p>
<pre><code>WARNING:tensorflow:SparseFeature is a complicated feature config
                   and should only be used after careful consideration
                   of VarLenFeature.</code></pre>

<p><code>VarLenFeature</code> doesn't support real sparsity or multi-dimensionality though; it only supports "ragged edges" as in the case when one example has three elements and the next has seven, for example.</p>
<p>It is a little awkward to put together a sparse representation for TFRecords, but it does give you a lot of flexibility. To put a point on it, I don't know what you can do with a <code>SequenceExample</code> that you can't do with a regular <code>Example</code> using all of <code>FixedLenFeature</code>, <code>VarLenFeature</code>, and <code>SparseFeature</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170427-sparse_tensors_and_tfrecords/</link>
<guid>http://planspace.org/20170427-sparse_tensors_and_tfrecords/</guid>
<pubDate>Thu, 27 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Parsing TFRecords inside the TensorFlow Graph</title>
<description><![CDATA[

<p>You can parse TFRecords using the standard protocol buffer <code>.FromString</code> method, but you can also parse them inside the TensorFlow graph.</p>
<p>The examples here assume you have in memory the serialized Example <code>my_example_str</code> and SequenceExample <code>my_seq_ex_str</code> from <a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a>. You could create them, or read them from <a href="/20170323-tfrecords_for_humans/my_example.tfrecords">my_example.tfrecords</a> and <a href="/20170323-tfrecords_for_humans/my_seq_ex.tfrecords">my_seq_ex.tfrecords</a>. That loading could be via <code>tf.python_io.tf_record_iterator</code> or via <code>tf.TFRecordReader</code> following the pattern shown in <a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a>.</p>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/parse_single_example"><code>tf.parse_single_example</code></a> decoder works like <code>tf.decode_csv</code>: it takes a string of raw data and turns it into structured data, based on the options it's created with. The structured data it turns it into is <em>not</em> a protocol buffer message object, but a dictionary that is hopefully easier to work with.</p>
<pre><code class="language-python">import tensorflow as tf

serialized = tf.placeholder(tf.string)

my_example_features = {'my_ints': tf.FixedLenFeature(shape=[2], dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session = tf.Session()

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': array([5, 6]),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>The <code>shape</code> parameter is part of the schema we're defining. A <code>shape</code> of <code>[]</code> means a single element, so the result returned won't be in an array, as for <code>my_float</code>. The <code>shape</code> of <code>[1]</code> means an array containing one element, like for <code>my_bytes</code>. Within a Feature, things are always listed, so the choice of how to get a single element back out is decided by the choice of <code>shape</code> argument. A <code>shape</code> of <code>[2]</code> means a list of two elements, naturally enough, and there's no alternative.</p>
<p>The <code>dtype=object</code> is how NumPy works with strings.</p>
<p>When some feature might have differing numbers of values across records, they can all be read with <code>tf.VarLenFeature</code>. This distinction is made only when parsing. Records are made with however many values you put in; you don't specify <code>FixedLen</code> or <code>VarLen</code> when you're making an <code>Example</code>. So the <code>my_ints</code> feature just parsed as <code>FixedLen</code> can also be parsed as <code>VarLen</code>.</p>
<pre><code class="language-python">my_example_features = {'my_ints': tf.VarLenFeature(dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': SparseTensorValue(indices=array([[0], [1]]),
##                               values=array([5, 6]),
##                               dense_shape=array([2])),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>When parsing as a <code>VarLenFeature</code>, the result is a sparse representation. This can seem a little silly, because features here will always be dense from left to right. Early versions of TensorFlow <a href="https://github.com/tensorflow/tensorflow/issues/976">didn't</a> have the current behavior. But this sparseness is a mechanism by which TensorFlow can support non-rectangular data, for example when forming batches from multiple variable length features, or as seen next with a <code>SequenceExample</code>:</p>
<pre><code class="language-python">my_context_features = {'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_sequence_features = {'my_ints': tf.VarLenFeature(shape=[2], dtype=tf.int64)}
my_seq_ex = tf.parse_single_sequence_example(
                serialized,
                context_features=my_context_features,
                sequence_features=my_sequence_features)

result = session.run(my_seq_ex, feed_dict={serialized: my_seq_ex_str})
result
## ({'my_bytes': array(['data'], dtype=object)},
##  {'my_ints': SparseTensorValue(
##                  indices=array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 2]]),
##                  values=array([5, 6, 7, 8, 9]),
##                  dense_shape=array([2, 3]))})</code></pre>

<p>The result is a tuple of two dicts: the context data and the sequence data.</p>
<p>Since the <code>my_ints</code> sequence feature is parsed as a <code>VarLenFeature</code>, it's returned as a sparse tensor. This example has to be parsed as a <code>VarLenFeature</code>, because the two entries in <code>my_ints</code> are of different lengths (<code>[5, 6]</code> and <code>[7, 8, 9]</code>).</p>
<p>The way the <code>my_ints</code> values get combined into one sparse tensor is the same as the way it would be done when making a batch from multiple records each containing a <code>VarLenFeature</code>.</p>
<p>To make it clearer what's going on, we can look at the sparse tensor in dense form:</p>
<pre><code class="language-python">session.run(tf.sparse_tensor_to_dense(result[1]['my_ints']))
## array([[5, 6, 0],
##        [7, 8, 9]])</code></pre>

<p>The other option for parsing sequence features is <code>tf.FixedLenSequenceFeature</code>, which will work if each entry of the sequence feature is the same length. The result then is a dense tensor.</p>
<p>To parse multiple <code>Example</code> records in one op, there's <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/parse_example"><code>tf.parse_example</code></a>. This returns a dict with the same keys you'd get from parsing a single <code>Example</code>, with the values combining the values from all the parsed examples, in a batch-like fashion. There isn't a corresponding op for <code>SequenceExample</code> records.</p>
<p>More could be said about sparse tensors and TFRecords. The <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_merge">tf.sparse_merge</a> op is one way to combine sparse tensors, similar to the combination that happened for <code>my_ints</code> in the <code>SequenceExample</code> above. And there's <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a> for parsing out general sparse features directly from TFRecords (better documentation in <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parsing_ops.py">source</a>).</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 26 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>mystery.tfrecords</title>
<description><![CDATA[

<p>Here's a practical puzzle: what's in the file <a href="mystery.tfrecords"><code>mystery.tfrecords</code></a>?</p>
<hr>
<p>You don't need any more extra information specific to that file. Here are a few posts about the format and related issues, some of which might be helpful:</p>
<ul>
<li><a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a></li>
<li><a href="/20170330-tfrecords_via_proto/">TFRecords via Protocol Buffer Definitions</a></li>
<li><a href="/20170403-images_and_tfrecords/">Images and TFRecords</a></li>
<li><a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a></li>
<li><a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">Parsing TFRecords inside the TensorFlow Graph</a></li>
</ul>
<!--

Welcome to hidden additional notes! Finding this is a way of solving
the puzzle, I suppose...

Here's how `mystery.tfrecords` was made:

<pre><code class="language-python">import tensorflow as tf

with open('success.jpg') as f:
    success = f.read()

example = tf.train.Example(features=tf.train.Features(feature={
    'jpg': tf.train.Feature(bytes_list=tf.train.BytesList(value=[success]))
}))

example_str = example.SerializeToString()

with tf.python_io.TFRecordWriter('mystery.tfrecords') as writer:
    writer.write(example_str)</code></pre>

Here's one way to get the contents back out:

<pre><code class="language-python">reader = tf.python_io.tf_record_iterator('mystery.tfrecords')

examples = [tf.train.Example().FromString(example_str)
            for example_str in reader]
# Using `SequenceExample` rather than `Example` also works.

len(examples)  # 1
# So we know there's just one example in there.

example = examples[0]

example.features.feature.keys()  # 'jpg'
# If parsed as a SequenceExample, this would instead be:
# `example.context.feature.keys()`

# It should be clear from the 'jpg' key, but you can also check:
len(example.features.feature['jpg'].int64_list.value)  # 0
len(example.features.feature['jpg'].float_list.value)  # 0

len(example.features.feature['jpg'].bytes_list.value)  # 1

jpg = example.features.feature['jpg'].bytes_list.value[0]

# If you don't trust the key, you can check the magic number:
jpg[:2]  # '\xff\xd8'
# That's the JPG magic number, FFD8.

with open('success.jpg', 'wb') as f:
    f.write(jpg)</code></pre>

Success!

-->

<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170425-mystery.tfrecords/</link>
<guid>http://planspace.org/20170425-mystery.tfrecords/</guid>
<pubDate>Tue, 25 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow as Automatic MPI</title>
<description><![CDATA[

<p>TensorFlow raises the level of abstraction in distributed programs from message-passing to data structures and operations directly on those data structures. The difference is analogous to the difference between programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a> and programming in a high-level language. TensorFlow may not have every possible high-performance feature for cluster computing, but what it offers is compelling.</p>
<hr>
<h2>Message-Passing</h2>
<p>Leaving aside shared filesystems or directly accessed shared memory, systems that run across multiple computers have to communicate by sending messages to one another. A low-level approach might use <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> sockets directly. Some convenience could be obtained by using Remote Procedure Calls (<a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a>).</p>
<p>The Message-Passing Interface (<a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>) was created in the early 1990s to make distributed programming easier within the High Performance Computing (HPC) community. It standardized interfaces like <code>MPI_Send</code> and <code>MPI_Recv</code> to facilitate exchanging data between processes of a distributed system.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Actor_model">actor model</a> also focuses on message-passing, as for example in <a href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">Erlang</a> or with <a href="http://akka.io/">Akka</a> in Java or Scala.</p>
<p>Message-passing gives you fine-grained control over how nodes communicate. You can use message-passing to build up a variety of algorithms and new systems. For example, <a href="http://spark.apache.org/">Spark</a> was built in part with Akka agents for some time, before switching to an RPC-based implementation. TensorFlow also builds its own abstractions using message-passing.</p>
<hr>
<h2>Automatic Message-Passing with TensorFlow</h2>
<p>TensorFlow uses message-passing, but it's largely invisible to TensorFlow users. The TensorFlow API lets you say where data and operations should live, and TensorFlow automatically handles any necessary messaging.</p>
<p>Programming with explicit messages is like the low-level memory shifting necessary when programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a>. For example, here is some assembly pseudo-code:</p>
<pre><code>copy value from position `a` to register 1
copy value from position `b` to register 2
add register 1 and 2
copy result to position `c`</code></pre>

<p>In a high-level language, this could be:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>Similarly, with TensorFlow, you focus on the computation you want performed, and don't worry about how values may need to be moved around to make it happen. A distributed analog to the assembly above might look like this:</p>
<pre><code>send value of `a` from machine A to machine D
send value of `b` from machine B to machine D
add values on machine D
send result to machine C</code></pre>

<p>And with TensorFlow:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>At least, that's the ideal. TensorFlow may still need explicit direction on where things should be placed (<a href="https://www.tensorflow.org/api_docs/python/tf/device">with tf.device</a>) especially when multiple machines are involved. But that direction is more succinct and declarative than the full imperative detail of message-passing.</p>
<hr>
<h2>Faster Messages and Collective Communications</h2>
<p>TensorFlow's user API is essentially message-less, but TensorFlow still uses messages under the hood. The contents of the messages are serialized protocol buffers sent via <a href="http://www.grpc.io/">gRPC</a>, which uses <a href="https://en.wikipedia.org/wiki/HTTP/2">HTTP/2</a>.</p>
<p>A message-based approach can be made faster by using a faster transport, and by using faster algorithms. TensorFlow has some early support for both, but other frameworks may offer more. You may or may not care, as the simplicity of TensorFlow's approach may or may not outweigh possible performance boosts.</p>
<p>One easy transport speedup could come in hardware with NVIDIA's <a href="http://www.nvidia.com/object/nvlink.html">NVLink</a> interconnect, which is faster than <a href="https://en.wikipedia.org/wiki/PCI_Express">PCIe</a>. As far as I can tell NVLink won't require code changes, but it looks like it will be pretty rare: it may be that only some supercomputers (<a href="https://www.olcf.ornl.gov/summit/">Summit</a>, <a href="https://asc.llnl.gov/coral-info">Sierra</a>) will use NVLink, with IBM's <a href="https://en.wikipedia.org/wiki/IBM_POWER_microprocessors">POWER</a> processors.</p>
<p>Some clusters connect machines with <a href="https://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a> rather than <a href="https://en.wikipedia.org/wiki/Ethernet">ethernet</a>. InfiniBand Remote Direct Memory Access (<a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">RDMA</a>) is supposed to be pretty fast.</p>
<p>Jun Shi at Yahoo contributed an <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md">implementation</a>, <a href="https://github.com/tensorflow/tensorflow/pull/8943">merged</a> four days ago, that begins to let TensorFlow use RDMA for transport via IB verbs, in addition to gRPC.</p>
<p>MPI implementations often support InfiniBand, and an advantage of using MPI is that you can let that implementation worry about InfiniBand support without writing your own RDMA code. There's an <a href="https://github.com/tensorflow/tensorflow/pull/7710">open pull request</a> to give TensorFlow this kind of MPI integration.</p>
<p>On the algorithm side, there are <a href="https://computing.llnl.gov/tutorials/mpi/#Collective_Communication_Routines">collective communication</a> operations that can be optimized, and MPI implementations tend to be good at these. For example, <a href="http://research.baidu.com/bringing-hpc-techniques-deep-learning/">ring allreduce</a> is more efficient for syncing up model parameters than sending them all to a central parameter server and then sending them all back out. Baidu has a <a href="https://github.com/baidu-research/tensorflow-allreduce">fork</a> of TensorFlow that adds their <a href="https://github.com/baidu-research/baidu-allreduce">implementation</a> of ring allreduce using MPI at <a href="https://github.com/baidu-research/tensorflow-allreduce/tree/master/tensorflow/contrib/mpi"><code>tf.contrib.mpi</code></a>.</p>
<p>Outside the MPI world, the NVIDIA Collective Communications Library (<a href="https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/">NCCL</a>) works with multiple GPUs on the same machine. TensorFlow has some support for this via <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/nccl"><code>tf.contrib.nccl</code></a>.</p>
<p>Quite distinct from TensorFlow, the Facebook Incubator project <a href="https://github.com/facebookincubator/gloo">Gloo</a> focuses on collective communication algorithms and supports transport via TCP/IP and InfiniBand without MPI. Gloo is used by <a href="https://caffe2.ai/">Caffe2</a>; the Caffe2 example <a href="https://github.com/caffe2/caffe2/blob/master/caffe2/python/examples/resnet50_trainer.py">resnet50 trainer</a> uses Gloo's ring allreduce.</p>
<!--

Just for fun, here's an Akka implementation of allreduce that I found:
https://github.com/brianmartin/akka-allreduce
I believe that's more of a tree allreduce than a ring allreduce.

-->

<hr>
<h2>Single-Machine Parallelism</h2>
<p>The HPC community has an approach called "MPI everywhere" which means running one single-threaded MPI processes per CPU core. The simplicity of writing single-threaded programs with all parallelism via MPI is attractive, but it's not necessarily the best way to use a machine with a lot of cores and possibly GPUs.</p>
<p>The hybrid approach is to have parallelism both across multiple machines and within each individual machine, usually via threads. HPC folks seem to like to use <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> for multithreading.</p>
<p>TensorFlow supports this kind of within-process parallelism. TensorFlow uses threads pretty freely internally, and you can add your own additional parallelism with Python's <a href="https://docs.python.org/3/library/threading.html">threading</a> or <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a> libraries, among many possible options. Just as with MPI though, when programs are run by a cluster manager, resources may be restricted.</p>
<hr>
<h2>Code organization</h2>
<p>It isn't strictly required for either, but it seems most common to write MPI programs and distributed TensorFlow programs with one entry point that quickly specializes based on the particular role in the cluster for that node. As this implies, every invocation of the program has to know something about the cluster and its place in it.</p>
<hr>
<h2>Cluster Topology</h2>
<p>MPI implementations provide a mechanism in starting a group of processes by which every process knows how many there are in its group and its position or <em>rank</em> in the group. The rank 0 process, called the <em>root</em>, is generally special.</p>
<p>TensorFlow leaves the starting of distributed processes to a cluster manager like Kubernetes. It's common to have several task groups like <code>ps</code> and <code>worker</code>, with a given process having a task type and a number within that group.</p>
<p>Gloo is interestingly different in that individual processes may not start with knowledge of the cluster topology, but only with a reference to a central broker which could be a file on a shared filesystem or a Redis server. Each process checks in with and gets information about other members of the cluster via the central broker. Gloo calls this process <em>rendezvous</em>. As with MPI, processes find out about just one group of processes, and in fact Gloo can use MPI for its initial rendezvous setup.</p>
<hr>
<h2>See also</h2>
<ul>
<li><a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">TensorFlow as a Distributed Virtual Machine</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
</ul>
<hr>
<p>Thanks to <a href="https://twitter.com/danfujita123">Dan Fujita</a> for suggesting more connections between TensorFlow and MPI, and sharing some MPI code that was helpful. Thanks also to <a href="https://twitter.com/ljdursi">Jonathan Dursi</a>, whose <a href="https://www.dursi.ca/">blog</a> I read with interest. And thanks to <a href="https://github.com/cliffwoolley">Cliff Woolley</a> of NVIDIA for <a href="https://github.com/NVIDIA/nccl/issues/86">clarifying</a> the meaning of NCCL.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</link>
<guid>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</guid>
<pubDate>Sun, 23 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Reading from Disk inside the TensorFlow Graph</title>
<description><![CDATA[

<p>As I've <a href="/20170312-use_only_what_you_need_from_tensorflow/">noted</a>, the TensorFlow input pipeline misleadingly <a href="https://www.tensorflow.org/programmers_guide/reading_data">described</a> as "reading from files" is far more complicated than many people need or want to deal with. Indeed, TensorFlow developers realize this and are <a href="https://github.com/tensorflow/tensorflow/issues/7951">thinking about</a> adding alternative interfaces for getting data in to TensorFlow programs.</p>
<p>You can still use any method you like for loading data in Python and then put it in your TensorFlow graph via the <code>feed_dict</code> method. But say you do want to do your file reading with ops that live inside the TensorFlow graph. How does it work?</p>
<p>TensorFlow does have <a href="https://www.tensorflow.org/api_docs/python/tf/read_file"><code>tf.read_file</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/write_file"><code>tf.write_file</code></a>, which let you read and write whole files at once, something like regular Python <code>file.read()</code> and <code>file.write()</code>. But you likely want something a little more helpful.</p>
<p>In Python, you can read lines from a file like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; reader = open('filename.txt'):
&gt;&gt;&gt; for line in reader:
&gt;&gt;&gt;    print(line)</code></pre>

<p>TensorFlow has readers that generalize this idea of getting multiple things, like lines, from files. The immediate example is the <a href="https://www.tensorflow.org/api_docs/python/tf/TextLineReader"><code>tf.TextLineReader</code></a>. There are also <a href="https://www.tensorflow.org/api_guides/python/io_ops#Readers">readers</a> that read records from TFRecords files, or just fixed number of bytes at a time from arbitrary files. Reading a whole file at a time is sort of degenerate case.</p>
<p>Instead of taking a filename, however, TensorFlow readers take a queue of filenames. This can be useful:</p>
<ul>
<li>You may have lots of files that you want to read from, possibly using multiple machines.</li>
<li>You may want to read through one or more files multiple times, for multiple epochs of training.</li>
</ul>
<p>It's a little awkward to use a queue when you just want to read from one file, but that's how TensorFlow works.</p>
<p>TensorFlow's file reading gets tangled up with its threading <code>QueueRunner</code> because they're often shown together, but this is not necessary. We <a href="/20170327-tensorflow_and_queues/">can</a> set up a quick and dirty one-item queue like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; filename_queue = tf.FIFOQueue(capacity=1, dtypes=[tf.string])
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(filename_queue.enqueue('limerick.txt'))
&gt;&gt;&gt; session.run(filename_queue.close())</code></pre>

<p>We make a reader and tell it to read from files in the queue. There are two outputs that update every time you evaluate either of them:</p>
<ul>
<li>A <code>key</code>, based on the current filename, which you may not need and don't need to explicitly evaluate.</li>
<li>A <code>value</code>, which is the bit of data we're probably interested in.</li>
</ul>
<pre><code class="language-python">&gt;&gt;&gt; reader = tf.TextLineReader()
&gt;&gt;&gt; key, value = reader.read(filename_queue)
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:1', 'This limerick goes in reverse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:2', "Unless I'm remiss"]
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:3', 'The neat thing is this:']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:4', 'If you start from the bottom-most verse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:5', "This limerick's not any worse"]
&gt;&gt;&gt; session.run([key, value])
# raises `OutOfRangeError`</code></pre>

<p>The <a href="limerick.txt">limerick</a> is <a href="http://www.smbc-comics.com/?id=3201">due to</a> Zach Weinersmith.</p>
<p>If we hadn't closed the filename queue, that last <code>session.run</code> would block, waiting for somebody to add another filename to the filename queue.</p>
<p>If we had added more filenames to the filename queue, the reader would continue happily reading from the next, and the next.</p>
<p>The records here are lines of simple ASCII text, so we can immediately see them clearly in the REPL. But for many types of data you'll need a decoder.</p>
<p>To read CSV files, you would read text lines just as above, and then use the <a href="https://www.tensorflow.org/api_docs/python/tf/decode_csv"><code>tf.decode_csv</code></a> decoder on  <code>value</code>. The <a href="https://www.tensorflow.org/api_docs/python/tf/decode_raw"><code>tf.decode_raw</code></a> decoder turns raw bytes into standard TensorFlow datatypes. And you can parse TFRecords examples.</p>
<p>Boom! Reading files inside the graph!</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 12 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed MapReduce with TensorFlow</title>
<description><![CDATA[

<p>Using many computers to count words is a tired Hadoop example, but might be unexpected with TensorFlow. In 50 lines, a TensorFlow program can implement not only map and reduce steps, but a whole MapReduce system.</p>
<hr>
<h2>Set up the cluster</h2>
<p>The design will have three roles, or jobs. There will be one task in the <code>files</code> job, distributing units of work. There will be one task for the <code>reduce</code> role, keeping track of results. There could be arbitrarily many tasks doing the <code>map</code> job, but for this example there will be two.</p>
<p>TensorFlow programs often use <code>ps</code> (parameter server) and <code>worker</code> tasks, but this is largely a convention. The program here won't follow the convention.</p>
<p>The four tasks can run on four computers, or on fewer. To stay local, here's a cluster definition that runs them all on your local machine.</p>
<pre><code class="language-python">cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}</code></pre>

<p>I have a <a href="make_configs.py"><code>make_configs.py</code></a> script that produces four shell scripts (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>). Each should be sourced (like <code>source config_map_0.sh</code>) in a separate shell. This setting of environment variables is work that could be handled by a cluster manager like <a href="https://kubernetes.io/">Kubernetes</a>, but these scripts will get it done.</p>
<p>Every task will run <a href="count.py"><code>count.py</code></a>. The first few lines establish the cluster.</p>
<pre><code class="language-python">import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)</code></pre>

<p>For more on TensorFlow clusters, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a>.</p>
<p>To avoid lots of indentation, I'm not using <code>tf.Session</code> as a context manager, which is fine for this example. I'll similarly avoid <code>with</code> blocks when reading files later.</p>
<hr>
<h2>Set up the data</h2>
<p>For distributed data processing to make sense, you likely want a distributed file system like HDFS providing a way for workers to grab chunks of data to work on. You might have hundred-megabyte <a href="/20170323-tfrecords_for_humans/">TFRecords files</a> prepared, for example.</p>
<p>For this example, we'll use <a href="/20170331-on_tyranny/">a dataset of 22 small text files</a>. We'll generate a <a href="filenames.txt">list of filenames</a>. Then, assuming every member of the cluster can access the file system, we can give a worker a filename and have it read the file.</p>
<pre><code class="language-bash">$ wget http://planspace.org/20170331-on_tyranny/on_tyranny.tar.gz
$ tar zxvf on_tyranny.tar.gz
$ find on_tyranny -type f &gt; filenames.txt</code></pre>

<hr>
<h2>Make a filename distributor</h2>
<p>The <code>files</code> task will host a <a href="/20170327-tensorflow_and_queues/">queue</a> of filenames.</p>
<pre><code class="language-python">with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')</code></pre>

<p>This code will be executed by every member of the cluster, but it won't make multiple queues. The <code>tf.device</code> context specifies that the queue lives on the <code>files</code> task machine, and the <code>shared_name</code> uniquely identifies this queue. So just one queue gets made, but every member of the cluster can refer to it with the Python variable name <code>filename_queue</code>.</p>
<p>The next part of the code only runs on the <code>files</code> task:</p>
<pre><code class="language-python">if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()</code></pre>

<p>Device assignment here is left up to TensorFlow, which is fine.</p>
<p>The <code>file</code> task uses normal Python file reading to get filenames from <a href="filenames.txt"><code>filenames.txt</code></a> and put them in the queue.</p>
<p>I'm loading the queue explicitly to avoid getting into <code>Coordinator</code> and <code>QueueRunner</code>; you could also use a <code>string_input_producer</code>, for example.</p>
<p>Then the <code>files</code> task runs <code>server.join()</code>, which keeps it running so that the queue doesn't disappear. We'll have to kill the process eventually because it won't know when to stop. This is another thing a cluster manager could be responsible for.</p>
<hr>
<h2>Make a reduce node</h2>
<p>There's just going to be one variable storing the total word count.</p>
<pre><code class="language-python">with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')</code></pre>

<p>Like the code defining the queue, every task in the cluster will run this code, so every task in the cluster can refer to this variable. Variables in specific places are "de-duplicated" using <code>name</code> instead of <code>shared_name</code>.</p>
<p>There's very little code that only the <code>reduce</code> task runs.</p>
<pre><code class="language-python">if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)</code></pre>

<p>The <code>reduce</code> task initializes and then displays the current value of <code>total_word_count</code> every two seconds.</p>
<p>It would be a bit more like Hadoop MapReduce to have the reducer explicitly receive data emitted from mappers, perhaps via another queue. Then the reducer would have to run some code to reduce down data from that queue.</p>
<p>The absence of any reducing code in the <code>reduce</code> task demonstrates the way distribution works in TensorFlow. The <code>reduce</code> task owns a variable, but we can add to that variable from another machine.</p>
<p>Like the <code>files</code> task, the <code>reduce</code> task doesn't have any way of knowing when the counting process is done and then shutting down, which I think is okay for this example.</p>
<hr>
<h2>Make map nodes</h2>
<p>The <code>map</code> task has already run code establishing the filename queue and the total word count variable. Here's what each <code>map</code> task does:</p>
<pre><code class="language-python">if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign_add(total_word_count,
                                 word_count_to_add,
                                 use_locking=True)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>Each <code>map</code> task pulls a filename from the queue, reads the file, counts its words, and then adds its count to the total.</p>
<p>A <code>map</code> task will run until the queue is empty, and then it will die with an <code>OutOfRangeError</code>. This would be a little more careful:</p>
<pre><code class="language-python">        try:
            filename = session.run(filename_from_queue)
        except tf.errors.OutOfRangeError:
            break</code></pre>

<p>Inside the <code>map</code> task, the actual work that's done is not part of the TensorFlow graph. We can execute arbitrary Python here.</p>
<p>There's a five second pause in the loop so that things don't happen too fast to watch.</p>
<p>The total word count is stored off in the <code>reduce</code> task, possibly on a different computer, but that doesn't matter. This is part of what's cool about TensorFlow.</p>
<hr>
<h2>Run</h2>
<p>To execute, we run <a href="count.py"><code>count.py</code></a> in four places with the appropriate environment variables set. That's it. We've counted words with many computers.</p>
<hr>
<h2>So what?</h2>
<p>Programming a cluster with TensorFlow is just like programming a single computer with TensorFlow. This is pretty neat, and it makes a lot of things possible beyond just distributed stochastic gradient descent.</p>
<p>The example above demonstrates a distributed queue. If you can do that, do you need a separate queueing system like <a href="https://www.rabbitmq.com/">RabbitMQ</a>? Maybe not in every situation.</p>
<p>The example above sends filenames to workers, which is a pretty general model. What if you feel comfortable sending executable filenames, or some representation of code? You might implement something like <a href="http://www.celeryproject.org/">Celery</a> pretty quickly.</p>
<p>The example I've shown can probably fail in more ways than I even realize. It would be more work to make it robust. It would be again more work to make it more general. But it's pretty exciting to be able to write something like this at all.</p>
<p>And while even the typical TensorFlow distributed training is itself a kind of MapReduce process, TensorFlow is general enough that it could be used for wildly different architectures. TensorFlow is an amazing tool.</p>
<hr>
<h2>Code</h2>
<p>Here's <a href="make_configs.py"><code>make_configs.py</code></a>:</p>
<pre><code class="language-python">import json

cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}
for task_type, addresses in cluster.items():
    for index in range(len(addresses)):
        tf_config = {'cluster': cluster,
                     'task': {'type': task_type,
                              'index': index}}
        tf_config_string = json.dumps(tf_config, indent=2)
        with open('config_{}_{}.sh'.format(task_type, index), 'w') as f:
            f.write("export TF_CONFIG='{}'\n".format(tf_config_string))
            # GPUs won't be needed, so prevent accessing GPU memory.
            f.write('export CUDA_VISIBLE_DEVICES=-1\n')</code></pre>

<p>The files produced by <a href="make_configs.py"><code>make_configs.py</code></a> (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>) look like this:</p>
<pre><code class="language-json">export TF_CONFIG='{
  "cluster": {
    "files": [
      "localhost:2222"
    ], 
    "map": [
      "localhost:2224", 
      "localhost:2225"
    ], 
    "reduce": [
      "localhost:2223"
    ]
  }, 
  "task": {
    "index": 0, 
    "type": "map"
  }
}'
export CUDA_VISIBLE_DEVICES=-1</code></pre>

<p>And here's <a href="count.py"><code>count.py</code></a> all together:</p>
<pre><code class="language-python">from __future__ import print_function

import time
import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)

with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')

if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()

with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')

if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)

if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign(total_word_count,
                             total_word_count + word_count_to_add)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>All the files needed to demo this are together in a <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">repo on GitHub</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</link>
<guid>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</guid>
<pubDate>Tue, 11 Apr 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
