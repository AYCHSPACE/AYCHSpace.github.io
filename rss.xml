<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Designing Your Life</title>
<description><![CDATA[

<p>I heard about <a href="http://designingyour.life/the-book/">Designing Your Life</a> in an <a href="https://www.npr.org/2017/08/28/546716951/you-2-0-how-silicon-valley-can-help-you-get-unstuck">episode</a> of <a href="https://en.wikipedia.org/wiki/Shankar_Vedantam">Shankar Vedantam</a>'s <a href="https://www.npr.org/series/423302056/hidden-brain">Hidden Brain</a>.  The book roughly follows a <a href="http://designingyour.life/resources-authorized/">workshop</a> format of a <a href="http://lifedesignlab.stanford.edu/dyl">Stanford class</a>. It's quick to read&#8212;slower to do&#8212;and full of fun examples and good reminders.</p>
<p><img alt="Designing Your Life" src="designing_your_life_cover.jpg"></p>
<p>There are five core recommendations:</p>
<ol>
<li>be curious (curiosity)</li>
<li>try stuff (bias to action)</li>
<li>reframe problems (reframing)</li>
<li>know it's a process (awareness)</li>
<li>ask for help (radical collaboration)</li>
</ol>
<p>The authors identify two concepts to be wary of:</p>
<ul>
<li>gravity problems: things that can't be productively addressed directly, just as eliminating the force of gravity isn't possible</li>
<li>anchor problems: things that prevent progress because of overcommitment to one issue, solution path, etc.</li>
</ul>
<p>Both of these "problems" may involve "dysfunctional beliefs" that may be better reframed.</p>
<p>The bulk of the text is around a number of <a href="http://designingyour.life/resources-authorized/">activities</a>:</p>
<ul>
<li>Evaluate your life along dimensions of love, play, work, and health.</li>
<li>Write out a "workview" and "lifeview" and think about how they interact.</li>
<li>Keep track of what you actually enjoy doing ("good time journal").</li>
<li>Use mind mapping and brainstorming.</li>
<li>Take incremental steps to learn/try/change things ("prototyping").</li>
<li>Imagine multiple possible futures ("three odyssey plans") not just one "right" path.</li>
<li>Network by meeting people ("life design interviews") don't just apply for listed jobs.</li>
<li>Log and reflect on failures to find growth areas ("failure log").</li>
</ul>
<p>The chapter on "How not to get a job" is very sensible.</p>
<p>Toward the end, the book is somewhat philosophical. It references <a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games">Finite and Infinite Games</a> and talks about a "failure immunity" point of view (positive psychology).</p>
<p>The Hidden Brain episode was called "Getting Unstuck." That describes the book's utility. The ideas are not revolutionary, but it is nice to be reminded, especially when in a rut, to be positive, think creatively, make small improvements, and try new things. The activities in the book help to operationalize these recommendations.</p>    
    ]]></description>
<link>http://planspace.org/20171229-designing_your_life/</link>
<guid>http://planspace.org/20171229-designing_your_life/</guid>
<pubDate>Fri, 29 Dec 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning</title>
<description><![CDATA[

<p><em>This is a presentation given for <a href="https://www.meetup.com/Data-Science-DC/">Data Science DC</a> on <a href="https://www.meetup.com/Data-Science-DC/events/244145151">Tuesday November 14, 2017</a>.</em></p>
<ul>
<li><em><a href="deep_rl.pdf">PDF slides</a></em></li>
<li><em><a href="deep_rl.pptx">PPTX slides</a></em></li>
</ul>
<p>Further resources up front:</p>
<ul>
<li><a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> (paper)</li>
<li>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> (blog post)</li>
<li><a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> (textbook)</li>
<li>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> (videos and slides)</li>
<li><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> (videos, slides, and labs)</li>
<li>OpenAI <a href="https://github.com/openai/gym">gym</a> / <a href="https://github.com/openai/baselines">baselines</a> (software)</li>
<li><a href="http://nationalgocenter.org/">National Go Center</a> (physical place)</li>
<li><a href="http://dc.hackandtell.org/">Hack and Tell</a> (fun meetup)</li>
</ul>
<p>The following goes through all the content of the talk:</p>
<hr>
<p>Aaron Schumacher</p>
<ul>
<li>planspace.org has these slides</li>
</ul>
<hr>
<p>Hi! I'm Aaron. All these slides, and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA). We do deep learning work for government and commercial customers. DLA is a great place to work, and one of the ways it's great is that it sends us to conferences and such things.</p>
<hr>
<p><img alt="Deep RL Bootcamp" src="img/deep_rl_bootcamp.png"></p>
<hr>
<p>DLA sent me to the first UC Berkeley <a href="https://www.deepbootcamp.io/">Deep RL Bootcamp</a> organized by Abbeel, Duan, Chen, and Karpathy. It was a great experience and it largely inspired this talk.</p>
<p>I have a separate <a href="/20170830-berkeley_deep_rl_bootcamp/">summary write-up</a> about my experience at the bootcamp, and they've since put up all the <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">videos, slides, and labs</a>, so you can see everything that was covered there.</p>
<hr>
<p><img alt="Sutton and Barto's textbook" src="img/sutton_and_barto.jpg"></p>
<hr>
<p>The other major source for this talk is Sutton and Barto's textbook, which I like a lot.</p>
<p>The picture shows <a href="https://mitpress.mit.edu/books/reinforcement-learning">the first edition</a>, which is not what you want. The second edition is <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">available free online</a>, and was last updated about a week ago (November 5, 2017).</p>
<p>Sutton and Barto are major figures in reinforcement learning, and they do not follow any <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">no original research rules</a>, making their book really fairly exciting, if you're not put off by the length (over 400 pages).</p>
<p>(The diagrams on the cover are not neural nets, but backup diagrams.)</p>
<hr>
<p>Plan</p>
<ul>
<li>applications: what<ul>
<li>theory</li>
</ul>
</li>
<li>applications: how</li>
<li>onward</li>
</ul>
<hr>
<p>The plan for today is to first mention four successful applications of reinforcement learning. Then we'll go through a core of theory. This will let us then understand pretty completely how each of those applications is achieved. Finally, we'll wrap up, looking at a few other applications and thoughts about how things are going.</p>
<hr>
<p>applications: what</p>
<hr>
<p>The applications here are all going to be games, not because reinforcement learning is only applicable to games, but because games are fun, and these examples are well known and cover a good range of techniques.</p>
<hr>
<p><img alt="backgammon" src="img/backgammon.png"></p>
<hr>
<p>First up, backgammon.</p>
<hr>
<p><img alt="Atari Breakout" src="img/breakout.png"></p>
<hr>
<p>Next, Atari. A lot of Atari games are well played by RL now. The ones shown (Video Pinball, Boxing, Breakout) are some of the ones that RL does the best on.</p>
<hr>
<p><img alt="tetris" src="img/tetris.png"></p>
<hr>
<p>I'm also including Tetris, mostly because it's a chance to talk about an interesting technique.</p>
<hr>
<p><img alt="go" src="img/go.jpg"></p>
<hr>
<p>And in the last two years, Go has been pretty much conquered by RL, so we'll talk about that.</p>
<hr>
<p>theory</p>
<hr>
<p>Let's start to build up the theory of reinforcement learning.</p>
<p>This is going to start very gradually, but I promise that by the end we'll be moving fast.</p>
<hr>
<p>Yann LeCun's cake</p>
<ul>
<li>cake: unsupervised learning</li>
<li>icing: supervised learning</li>
<li>cherry: reinforcement learning</li>
</ul>
<p><img alt="cake" src="img/cake.jpg"></p>
<hr>
<p>Yann LeCun <a href="https://drive.google.com/file/d/0BxKBnD5y2M8NREZod0tVdW5FLTQ/view">introduced</a> this cake idea for relating three main varieties of machine learning. It's largely based on one view of how much information is used at each training step.</p>
<p>I'm going to use it to build up and relate these three kinds of learning, while introducing reinforcement learning notation.</p>
<hr>
<p>unsupervised learning</p>
<ul>
<li>\( s \)</li>
</ul>
<hr>
<p>In unsupervised learning, we have a collection of states, where each individual state can be referred to with \( s \).</p>
<p>I'm using "state" without distinguishing "state" from "observation". You could also call these "examples" or "<a href="https://en.wikipedia.org/wiki/Covariate">covariates</a>" or "data points" or whatever you like.</p>
<p>The symbol "x" is commonly used.</p>
<hr>
<p>state \( s \)</p>
<ul>
<li>numbers</li>
<li>text (as numbers)</li>
<li>image (as numbers)</li>
<li>sound (as numbers)</li>
</ul>
<hr>
<p>States can be anything as long as it can be expressed numerically. So that includes text, images, and sound. Really anything.</p>
<hr>
<p>unsupervised (?) learning</p>
<ul>
<li>given \( s \)</li>
<li>learn \( s \rightarrow \text{cluster_id} \)</li>
<li>learn \( s \rightarrow s \)</li>
</ul>
<hr>
<p>So say we have a set of a thousand images. Each image is an \( s \).</p>
<p>We want to learn something, and that tends to mean unsupervised learning starts to resemble supervised learning.</p>
<p>At two ends of a spectrum we have clustering and <a href="https://en.wikipedia.org/wiki/Autoencoder">autoencoders</a>, and all kinds of dimensionality reduction in between.</p>
<p>Unsupervised learning is sort of the <a href="https://en.wikipedia.org/wiki/Dark_matter">dark matter</a> of machine learning. Even <a href="https://www.youtube.com/watch?v=vdWPQ6iAkT4">Yann LeCun</a> says "We are missing the principles for unsupervised learning."</p>
<hr>
<p>deep unsupervised learning</p>
<ul>
<li>\( s \) with deep neural nets</li>
</ul>
<hr>
<p>Deep unsupervised learning is whenever we do unsupervised learning and somewhere there's a deep neural net.</p>
<hr>
<p>supervised learning</p>
<ul>
<li>\( s \rightarrow a \)</li>
</ul>
<hr>
<p>Up next is supervised learning. We're introducing a new entity \( a \), which I'll call an "action". It's common to call it a "label" or a "target" and to use the symbol "y". Same thing.</p>
<hr>
<p>action a</p>
<ul>
<li>numbers</li>
<li>"cat"/"dog"</li>
<li>"left"/"right"</li>
<li>17.3</li>
<li>[2.0, 11.7, 5]</li>
<li>4.2V</li>
</ul>
<hr>
<p>Whatever you call it, the action is again a numeric thing. It could be anything that \( s \) could be, but it tends to be lower-dimensional.</p>
<p>The cat/dog classifier is a popular example, and a left/right classifier is just the same, but those might feel more like actions.</p>
<hr>
<p>supervised learning</p>
<ul>
<li>given \( s, a \)</li>
<li>learn \( s \rightarrow a \)</li>
</ul>
<hr>
<p>In supervised learning you have a training set of state-action pairs, and you try to learn a function to produce the correct action based on the state alone.</p>
<p>Supervised learning can blur into imitation learning, which can be taken as a kind of reinforcement learning. For example, <a href="https://arxiv.org/abs/1604.07316">NVIDIA's end-to-end self-driving car</a> is based on learning human driving behaviors. (Sergey Levine <a href="https://www.youtube.com/watch?v=Pw1mvoOD-3A&amp;list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm&amp;index=13">explains</a> in some depth.) But I'm not going to talk more about imitation learning, and supervised learning will stand alone.</p>
<p>You can learn this function with linear regression or support vector machines or whatever you like.</p>
<hr>
<p>deep supervised learning</p>
<ul>
<li>\( s \rightarrow a \) with deep neural nets</li>
</ul>
<hr>
<p>Deep supervised learning is whenever we do supervised learning and somewhere there's a deep neural net.</p>
<p>There's also semi-supervised learning, when you have some labeled data and some unlabeled data, which can connect to active learning, which has some relation to reinforcement learning, but that's all I'll say about that.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\(r, s \rightarrow a\)</li>
</ul>
<hr>
<p>Finally, we reach reinforcement learning.</p>
<p>We're adding a new thing \( r \), which is reward.</p>
<hr>
<p>reward \( r \)</p>
<ul>
<li>-3</li>
<li>0</li>
<li>7.4</li>
<li>1</li>
</ul>
<hr>
<p>Reward is a scalar, and we like positive rewards.</p>
<hr>
<p>optimal control / reinforcement learning</p>
<hr>
<p>I'll mention that <a href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a> is closely related to reinforcement learning. It has its own parallel notation and conventions, and I'm going to ignore all that.</p>
<hr>
<p>\( r, s \rightarrow a \)</p>
<hr>
<p>So here's the reinforcement learning setting.</p>
<p>We get a reward and a state, and the agent chooses an action.</p>
<hr>
<p>tick</p>
<hr>
<p>Then, time passes. We're using discrete time, so this is a "tick".</p>
<hr>
<p>\( r', s' \rightarrow a' \)</p>
<hr>
<p>Then we get a new reward and state, which depend on the previous state and action, and the agent chooses a new action.</p>
<hr>
<p>tick</p>
<hr>
<p>And so on.</p>
<hr>
<p><img alt="standard diagram" src="img/standard_diagram.png"></p>
<hr>
<p>This is the standard reinforcement learning diagram, showing the agent and environment. My notation is similar.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>"given" \( r, s, a, r', s', a', ... \)</li>
<li>learn "good" \( s \rightarrow a \)</li>
</ul>
<hr>
<p>So here's reinforcement learning.</p>
<p>"Given" is in quotes because the rewards and states that we see depend on the actions we choose.</p>
<p>"Good" is in quotes because we haven't defined "good" beyond that we like positive rewards.</p>
<hr>
<p>Question:</p>
<p>Can you formulate supervised learning as reinforcement learning?</p>
<hr>
<p>Here's a question to think about.</p>
<hr>
<p>supervised as reinforcement?</p>
<ul>
<li>reward 0 for incorrect, reward 1 for correct<ul>
<li>beyond binary classification?</li>
</ul>
</li>
<li>reward deterministic</li>
<li>next state random</li>
</ul>
<hr>
<p>You can re-cast supervised learning as reinforcement learning, but it isn't necessarily as efficient: it's better to be told what the right answer is than to just be told that you're wrong. This is one sense in which Yann LeCun means that reinforcement learning is using fewer bits of information per example.</p>
<hr>
<p>Question:</p>
<p>Why is the Sarsa algorithm called Sarsa?</p>
<hr>
<p>Here's a question that you can already figure out!</p>
<p>(Sarsa is an on-policy TD control algorithm due to Sutton, 1996.)</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\( r, s \rightarrow a \)</li>
</ul>
<hr>
<p>Sarsa: State, Action, Reward, State, Action.</p>
<p>We'll see the closely related Q-learning algorithm in some detail later on.</p>
<hr>
<p>deep reinforcement learning</p>
<ul>
<li>\( r, s \rightarrow a \) with deep neural nets</li>
</ul>
<hr>
<p>Deep reinforcement learning is whenever we do reinforcement learning and somewhere there's a deep neural net.</p>
<p>The one twist is that in reinforcement learning, the network(s) may not be in the obvious place. We'll need to develop a few more ideas to see more places to put a neural net.</p>
<hr>
<p>Markov property</p>
<ul>
<li>\( s \) is enough</li>
</ul>
<hr>
<p>We're going to assume <a href="https://en.wikipedia.org/wiki/Markov_property">the Markov property</a>, which means that the state we observe tells us as much as we can know about what will happen next.</p>
<p>This is frequently not true, but we'll assume it anyway.</p>
<hr>
<pre><code>                   Make choices?
                   no                        yes
Completely    no   Markov chain              MDP: Markov Decision Process
observable?   yes  HMM: Hidden Markov Model  POMDP: Partially Observable MDP</code></pre>

<hr>
<p>This chart relates a bunch of Markov things, to hopefully help you orient yourself.</p>
<p>We're working with Markov Decision Processes (MDPs). Even though things are frequently not really completely observable, we'll usually just ignore that. Often this is fine (think about blackjack, for example).</p>
<p>It's often easiest to think about finite spaces, but this isn't always necessary.</p>
<p>(The chart is from <a href="http://www.pomdp.org/faq.html">POMDP.org</a>.)</p>
<hr>
<p>usual RL problem elaboration</p>
<hr>
<p>We'll introduce some more notation and a few ideas that help us work on reinforcement learning problems.</p>
<hr>
<p>policy \( \pi \)</p>
<ul>
<li>\( \pi: s \rightarrow a \)</li>
</ul>
<hr>
<p>This thing that takes a state and produces an action we'll call a "policy" \( \pi \). This effectively <em>is</em> our reinforcement learning agent, and the name of the game is figuring out how to get a good policy.</p>
<p>But how do we even know whether a policy is good?</p>
<p>(If you don't like spelling in Greek, you can imagine it's a little walker!)</p>
<hr>
<p>return</p>
<ul>
<li>\( \sum{r} \)<ul>
<li>into the future (episodic or ongoing)</li>
</ul>
</li>
</ul>
<hr>
<p>We want a policy that maximizes "return" \( \sum{r} \). Return is just the sum of all the rewards we'll get into the future. The symbol "G" is also used, but I'll keep writing the sum.</p>
<p>Reinforcement learning can consider finite-length episodes that always come to an end, or the ongoing setting in which the agent keeps on going. (You can see that there could be a problem with infinity in the ongoing setting.)</p>
<hr>
<p><img alt="gridworld" src="img/gridworld.png"></p>
<hr>
<p>Here's a typical toy example for understanding reinforcement learning: a gridworld.</p>
<p>There are sixteen states, each uniquely identifiable to the agent.</p>
<p>There are four actions: go up, down, left, right.</p>
<p>We want the agent to learn to go from the start to the goal.</p>
<p>We'll say this is episodic, with each episode ending after the agent makes it to the goal.</p>
<p>So how do we set up the rewards?</p>
<hr>
<p><img alt="gridworld reward" src="img/gridworld_reward.png"></p>
<hr>
<p>Here's a natural way to set up reward. You get a reward of one when you complete the task. What does the return at each state look like, then?</p>
<hr>
<p><img alt="gridworld return" src="img/gridworld_return.png"></p>
<hr>
<p>The return, unfortunately, is just one everywhere. And this means that there isn't any reason to go directly to the goal. We have all the time in the world, so we could just as well meander randomly until we happen to hit the goal. This isn't very good.</p>
<hr>
<p>Question:</p>
<p>How do we keep our agent from dawdling?</p>
<hr>
<p>How can we set things up so that going directly to the goal position is incentivized?</p>
<p>Consider changing rewards, or changing how return is calculated, or anything else.</p>
<p>There are at least three approaches that are commonly used.</p>
<hr>
<p>negative reward at each time step</p>
<hr>
<p>If we get a negative reward at each time step, the only way to maximize return is to minimize episode length, so we're motivated to end the episode as soon as possible. This certainly makes sense for the episodic setting.</p>
<p>(It's like existing is pain; the agent is like <a href="http://rickandmorty.wikia.com/wiki/Mr._Meeseeks">Mr. Meeseeks</a>!)</p>
<hr>
<p>discounted rewards</p>
<ul>
<li>\( \sum{\gamma^tr} \)</li>
</ul>
<hr>
<p>It's very common to use a "discount factor" \( \gamma \), which might be 0.9, for example. Then a reward soon is worth more than a reward later, and again the agent is motivated to do things more directly.</p>
<hr>
<p>average rate of reward</p>
<ul>
<li>\( \frac{\sum{r}}{t} \)</li>
</ul>
<hr>
<p>Average rate of reward is also used in some settings, where it can make things more stable.</p>
<hr>
<p>value functions</p>
<ul>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>I already snuck in a value function when we saw green values for every state in the gridworld.</p>
<p>The state value function \( v \) tells us the return from any state, and the state-action value function \( q \) gives the return from any state-action combination. It might not yet be clear why the two are importantly different, so let's get into that.</p>
<hr>
<p>Question:</p>
<p>Does a value function specify a policy (if you want to maximize return)?</p>
<hr>
<p>Here's the relevant question to consider.</p>
<hr>
<p>trick question?</p>
<ul>
<li>\( v_\pi \)</li>
<li>\( q_\pi \)</li>
</ul>
<hr>
<p>Is it a trick question? Really, values depend on what you do, on your policy. But let's assume I just give you a value function and you trust it. Can you use it to induce a policy?</p>
<hr>
<p>value functions</p>
<ul>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>The state-action value function \( q \) certainly implies a policy. For whatever state we're in, we check the return for all available actions, and choose the action that's best.</p>
<p>For the state value function \( v \), we can get a policy in a similar way, but only if we know what state we'll be in after taking an action. That may or may not be true.</p>
<hr>
<p>environment dynamics</p>
<ul>
<li>\( s, a \rightarrow r', s' \)</li>
</ul>
<hr>
<p>The next state and reward depend on the previous state and action, and that's determined by the environment. We don't necessarily know what's going on inside the environment.</p>
<hr>
<p><img alt="gridworld" src="img/gridworld.png"></p>
<hr>
<p>When you, a smart human with lots of ideas about physics, look at this picture of a gridworld, you assume that going "right" will do what you expect.</p>
<p>But in general, we don't know in advance how the environment behaves. Maybe going "right" always takes you to the upper left state, for example.</p>
<p>Sometimes you do know the environment dynamics, like with games with well-defined rules. But sometimes you don't. You could try to learn the environment dynamics with a model.</p>
<hr>
<p>model</p>
<ul>
<li>\( s, a \rightarrow r', s' \)</li>
</ul>
<hr>
<p>Reinforcement learning uses "model" to refer to a model that you learn for the environment dynamics. (Often, just the next state is predicted.)</p>
<p>This model is something that the agent has to learn, and depending on whether the agent is learning a model determines whether you're said to be doing model-based vs. model-free reinforcement learning.</p>
<p>(In adaptive control this "model learning" is called "system identification".)</p>
<hr>
<p>learning and acting</p>
<hr>
<p>Everything to this point has been elaborating the problem setup for reinforcement learning. Now we get into how we actually learn and go forth with RL.</p>
<hr>
<p>learn model of dynamics</p>
<ul>
<li>from experience</li>
<li>difficulty varies</li>
</ul>
<hr>
<p>As mentioned, you could try to learn your environment's dynamics. If you can do this well, it's great, but it may not be easy. Model-based RL is an exciting area of research.</p>
<hr>
<p>learn value function(s) with planning</p>
<ul>
<li>assuming we have the environment dynamics or a good model already</li>
</ul>
<hr>
<p>To introduce value function learning, let's consider the situation where you know nothing yet but you have the environment dynamics, so you can make decisions by looking ahead. This is a familiar thought process, conceptually.</p>
<hr>
<p><img alt="planning 0" src="img/planning0.png"></p>
<hr>
<p>You're in state \( s_1 \) and you have to choose between \( a_1 \) and \( a_2 \).</p>
<p>Because you have the environment dynamics, you can see what would happen next.</p>
<hr>
<p><img alt="planning 1" src="img/planning1.png"></p>
<hr>
<p>Looking one time step ahead, you can already start to evaluate your action choices by the rewards you'd get immediately. This is the state-action value view, \( q \): you're evaluating the value of the <em>actions</em> directly.</p>
<hr>
<p><img alt="planning 2" src="img/planning2.png"></p>
<hr>
<p>Looking two time steps ahead, you can similarly start to evaluate how good the states you would get to are. This is the state value view, \( v \).</p>
<p>This example is simple and assumes states and actions never recur. We also haven't introduced a couple more wrinkles. You can think about differences between thinking in terms of \( v \) and \( q \) later, for example in the case of backgammon.</p>
<p>This kind of planning can happen continuously in the background, or it can be done on demand at decision time. The search space can get quite large.</p>
<hr>
<p>connections</p>
<ul>
<li>Dijkstra's algorithm</li>
<li>A* algorithm</li>
<li>minimax search<ul>
<li>two-player games not a problem</li>
</ul>
</li>
<li>Monte Carlo tree search</li>
</ul>
<hr>
<p>The graph structure on the previous slide might make you think of a range of algorithms that you could already be familiar with.</p>
<p>You can think of these as smart ways of exploring the possibly very large branching structures that can spring up. <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a> is particularly clever, and it will appear again later.</p>
<hr>
<p><img alt="rollout diagram" src="img/rollout_diagram.png"></p>
<hr>
<p>One way of dealing with all the possible tree paths is just to sample from it and try to make estimates on the basis of these.</p>
<hr>
<p><img alt="backgammon roll" src="img/rollout.jpg"></p>
<hr>
<p>I thought it was interesting to learn that the term "roll-out" comes from work on backgammon, where you literally roll dice to move a game along.</p>
<hr>
<p>everything is stochastic</p>
<hr>
<p>Backgammon has randomness in the environment, from the dice, but randomness can enter all over the place.</p>
<hr>
<p><img alt="stochastic s'" src="img/stochastic_s.png"></p>
<hr>
<p>The next state can be random.</p>
<hr>
<p><img alt="stochastic r'" src="img/stochastic_r.png"></p>
<hr>
<p>And the next reward can be random, as in the case of <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandits</a>.</p>
<hr>
<p>exploration vs. exploitation</p>
<ul>
<li>\( \epsilon \)-greedy policy<ul>
<li>usually do what looks best</li>
<li>\( \epsilon \) of the time, choose random action</li>
</ul>
</li>
</ul>
<hr>
<p>This can always be an issue in unknown environments, but especially with randomness, we encounter the issue of exploration vs. exploitation. The \( \epsilon \)-greedy approach is one well-known way to ensure that an agent keeps exploring.</p>
<hr>
<p>Question:</p>
<p>How does randomness affect \( \pi \), \( v \), and \( q \)?</p>
<ul>
<li>\( \pi: s \rightarrow a \)</li>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>Now acknowledging that there's randomness all over the place, I can show another way that my notation is shorthand.</p>
<hr>
<p>Question:</p>
<p>How does randomness affect \( \pi \), \( v \), and \( q \)?</p>
<ul>
<li>\( \pi: \mathbb{P}(a|s) \)</li>
<li>\( v: s \rightarrow \mathbb{E} \sum{r} \)</li>
<li>\( q: s, a \rightarrow \mathbb{E} \sum{r} \)</li>
</ul>
<hr>
<p>I haven't been hiding too much. A policy is a probability distribution over possible actions, and value functions give expectations. No problem.</p>
<hr>
<p>Monte Carlo returns</p>
<ul>
<li>\( v(s) = ? \)</li>
<li>keep track and average</li>
<li>like "planning" from experienced "roll-outs"</li>
</ul>
<hr>
<p>Here's our first pass at a model-free learning algorithm. We interact with the environment, and use our experiences in the past just like model-based roll-outs.</p>
<hr>
<p>non-stationarity</p>
<ul>
<li>\( v(s) \) changes over time!</li>
</ul>
<hr>
<p>The environment may change, and we want our methods to be able to deal with this.</p>
<hr>
<p>moving average</p>
<ul>
<li>new mean = old mean + \( \alpha \)(new sample - old mean)</li>
</ul>
<hr>
<p>This kind of moving average is used all over the place.</p>
<p>The parameter \( \alpha \) is a learning rate, and the whole thing can be seen to be a case of stochastic gradient descent, which is a nice early connection to neural nets which start turning up.</p>
<p>(Sutton and Barto use \( \alpha \) for learning rate, not the otherwise popular \( \eta \). This is a sensible choice for them especially since they sometimes have multiple learning rates in the same algorithm, and the second learning rate can then be \( \beta \).)</p>
<hr>
<p><img alt="MDP diagram" src="img/mdp_diagram.png"></p>
<hr>
<p>We can think about this question, which is not a trick: the difference is \( r' \).</p>
<hr>
<p>Bellman equation</p>
<ul>
<li>\( v(s) = r' + v(s') \)</li>
</ul>
<hr>
<p>That relation gives rise to a bunch of Bellman equations, which are quite useful.</p>
<p><a href="https://en.wikipedia.org/wiki/Richard_E._Bellman">Bellman</a> came up with a couple neat things. <a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programming</a> (despite being <a href="http://arcanesentiment.blogspot.com/2010/04/why-dynamic-programming.html">poorly named</a>) is great for lots of problems, including those involving Bellman equations.</p>
<p>Bellman also introduced the phrase "<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>".</p>
<hr>
<p>temporal difference (TD)</p>
<hr>
<p>Bellman equations give rise to temporal difference (TD) learning (Sutton, 1988).</p>
<p>It's sometimes described as "bootstrap" learning.</p>
<hr>
<p>Q-learning</p>
<ul>
<li>\( \text{new } q(s, a) = q(s, a) + \alpha (r' + \gamma \max_{a} q(s', a) - q(s, a)) \)</li>
</ul>
<hr>
<p>Q-learning is a temporal difference method that combines a lot of the things we've just seen.</p>
<p>You can see in the equation that Sarsa is closely related.</p>
<hr>
<p>on-policy / off-policy</p>
<hr>
<p>Reinforcement learning differentiates between on-policy and off-policy learning. On-policy learning is when you're learning about the policy that you're currently following, and it's generally easier than off-policy learning.</p>
<p>Importance sampling is one way to achieve off-policy learning.</p>
<p>Q-learning is off-policy, which is nice. You can look at the equation and see why.</p>
<hr>
<p>estimate \( v \), \( q \)</p>
<ul>
<li>with a deep neural network</li>
</ul>
<hr>
<p>We've sort of implicitly been doing what's called tabular learning, where each state or state-action has its own estimated return. But we can plug in any kind of supervised learning algorithm, including deep learning. We'll see this in applications.</p>
<hr>
<p>back to the \( \pi \)</p>
<ul>
<li>parameterize \( \pi \) directly</li>
<li>update based on how well it works</li>
<li>REINFORCE<ul>
<li>REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility</li>
</ul>
</li>
</ul>
<hr>
<p>Stepping back from value functions, we can work with a parameterized policy directly.</p>
<p>REINFORCE is not necessarily a great acronym.</p>
<hr>
<p>policy gradient</p>
<ul>
<li>\( \nabla \log (\pi(a|s))\)</li>
</ul>
<hr>
<p>This is the gradient that you use.</p>
<p>You may see the connection to score functions and the so-called <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">log-derivative trick</a>, but we can interpret without all that.</p>
<p>If an action contributes to high return, you can encourage it, and if it contributes to low return, you can discourage it.</p>
<hr>
<p>actor-critic</p>
<ul>
<li>\( \pi \) is the actor</li>
<li>\( v \) is the critic</li>
<li>train \( \pi \) by policy gradient to encourage actions that work out better than \( v \) expected</li>
</ul>
<hr>
<p>Actor-critic algorithms combine policy gradient and value function methods to reduce variance, and are pretty popular.</p>
<hr>
<p>applications: how</p>
<hr>
<p>We now know enough to understand a lot of the details of big reinforcement learning applications!</p>
<hr>
<p>TD-gammon (1992)</p>
<ul>
<li>\( s \) is custom features</li>
<li>\( v \) with shallow neural net</li>
<li>\( r \) is 1 for a win, 0 otherwise</li>
<li>\( TD(\lambda) \)<ul>
<li>eligibility traces</li>
</ul>
</li>
<li>self play</li>
<li>shallow forward search</li>
</ul>
<hr>
<p>Backgammon was dealt with pretty completely about 25 years ago already.</p>
<p>We know what state the board will be in after any move, and the state of the board is what matters, so it makes sense to use a state value function \( v \) here.</p>
<hr>
<p>DQN (2015)</p>
<ul>
<li>\( s \) is four frames of video</li>
<li>\( v \) with deep convolutional neural net</li>
<li>\( r \) is 1 if score increases, -1 if decreases, 0 otherwise</li>
<li>Q-learning<ul>
<li>usually-frozen target network</li>
<li>clipping update size, etc.</li>
</ul>
</li>
<li>\( \epsilon \)-greedy</li>
<li>experience replay</li>
</ul>
<hr>
<p>Deep Q-Networks (DQN) is a well-known algorithm that works well for many Atari games. See:</p>
<ul>
<li><a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> (2013)</li>
<li><a href="https://deepmind.com/research/dqn/">Human-level control through Deep Reinforcement Learning</a> (2015)</li>
</ul>
<hr>
<p>evolution (2006)</p>
<ul>
<li>\( s \) is custom features</li>
<li>\( \pi \) has a simple parameterization</li>
<li>evaluate by final score</li>
<li>cross-entropy method</li>
</ul>
<hr>
<p>Surprise! Tetris is well-solved by evolutionary methods, which I didn't mention at all. These methods are sneaky that way. They can work better than you'd think. See:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Learning Tetris Using the Noisy Cross-Entropy Method</a> (2006)</li>
<li><a href="https://blog.openai.com/evolution-strategies/">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></li>
</ul>
<hr>
<p>AlphaGo (2016)</p>
<ul>
<li>\( s \) is custom features over geometry<ul>
<li>big and small variants</li>
</ul>
</li>
<li>\( \pi_{SL} \) with deep convolutional neural net, supervised training</li>
<li>\( \pi_{rollout} \) with smaller convolutional neural net, supervised training</li>
<li>\( r \) is 1 for a win, -1 for a loss, 0 otherwise</li>
<li>\( \pi_{RL} \) is \( \pi_{SL} \) refined with policy gradient peer-play reinforcement learning</li>
<li>\( v \) with deep convolutional neural net, trained based on \( \pi_{RL} \) games</li>
<li>asynchronous policy and value Monte Carlo tree search<ul>
<li>expand tree with \( \pi_{SL} \)</li>
<li>evaluate positions with blend of \( v \) and \( \pi_{rollout} \) rollouts</li>
</ul>
</li>
</ul>
<hr>
<p>AlphaGo surprised a lot of people by reaching super-human levels of Go play. It was a bit complicated. See:</p>
<ul>
<li><a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">Mastering the game of Go with deep neural networks and tree search</a></li>
</ul>
<hr>
<p>AlphaGo Zero (2017)</p>
<ul>
<li>\( s \) is simple features over time and geometry</li>
<li>\( \pi, v \) with deep residual convolutional neural net</li>
<li>\( r \) is 1 for a win, -1 for a loss, 0 otherwise</li>
<li>Monte Carlo tree search for self-play training and play</li>
</ul>
<hr>
<p>AlphaGo Zero improved on AlphaGo in performance and by requiring no human training data. It reaches incredibly good performance with only its self-play learning method. See:</p>
<ul>
<li><a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">Mastering the game of Go without human knowledge</a><ul>
<li>Interesting quote on reach/limitations: "Our approach is most directly applicable to Zero-sum games of perfect information."</li>
</ul>
</li>
</ul>
<p>(AlphaGo Zero can only play legal moves, but stupid moves (like filling eyes) aren't forbidden, so it is less constrained than the original AlphaGo.)</p>
<hr>
<p>onward</p>
<hr>
<p>Let's look at some more applications to paint out more of the area that reinforcement learning can influence.</p>
<hr>
<p>Neural Architecture Search (NAS)</p>
<hr>
<p>Neural Architecture Search (NAS) uses policy gradient where the actions design a neural net. The reward is validation set performance. See:</p>
<ul>
<li><a href="https://research.googleblog.com/2017/11/automl-for-large-scale-image.html">AutoML for large scale image classification and object detection</a></li>
<li><a href="https://arxiv.org/abs/1707.07012">Learning Transferable Architectures for Scalable Image Recognition</a></li>
<li><a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning</a></li>
</ul>
<hr>
<p><img alt="Amazon Echo" src="img/echo.jpg"></p>
<hr>
<p>A lot of people are interested in natural language processing. A lot of chatbots are still big nests of <code>if</code> statements, but there is interest in using RL for language. See:</p>
<ul>
<li><a href="https://arxiv.org/abs/1709.02349">A Deep Reinforcement Learning Chatbot</a><ul>
<li><a href="https://developer.amazon.com/alexaprize">Amazon Alexa Prize</a></li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1703.04908">Emergence of Grounded Compositional Language in Multi-Agent Populations</a></li>
</ul>
<p>(In particular in the multi-agent setting, there might be some connection to <a href="https://en.wikipedia.org/wiki/Evolutionary_game_theory">evolutionary game theory</a> and the work of <a href="https://en.wikipedia.org/wiki/Peyton_Young">Peyton Young</a>.)</p>
<hr>
<p><img alt="robot" src="img/robot.jpg"></p>
<hr>
<p>Lots of people think using RL for robot control is pretty neat.</p>
<p>(left to right: Chelsea Finn, Pieter Abbeel, <a href="http://www.willowgarage.com/pages/pr2/overview">PR2</a>, Trevor Darrell, Sergey Levine)</p>
<p>Pieter Abbeel, Peter Chen, Rocky Duan, and Tianhao Zhang <a href="https://nyti.ms/2hLYbGQ">founded Embodied Intelligence</a> to work on robot stuff, it seems.</p>
<hr>
<p>self-driving cars</p>
<ul>
<li>interest</li>
<li>results?</li>
</ul>
<hr>
<p>People seem to want to use RL for self-driving cars.</p>
<hr>
<p><img alt="OpenAI DotA 2" src="img/openai_dota2.jpg"></p>
<hr>
<p>OpenAI has <a href="https://blog.openai.com/dota-2/">beaten</a> top human players in one-on-one DotA 2. This is pretty neat. They haven't released details of their methods as they say they're working on the full five-on-five game. We'll see!</p>
<hr>
<p>conclusion</p>
<hr>
<p>Wrapping up!</p>
<hr>
<p><img alt="bootcamp diagram" src="img/annotated.jpg"></p>
<hr>
<p>There are <a href="/20170830-berkeley_deep_rl_bootcamp/">a lot of RL algorithms</a> with horrible acronyms.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\( r,s \rightarrow a \)</li>
<li>\( \pi: s \rightarrow a\)</li>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s,a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>But the core ideas are pretty concise, and lots of work in the field can be quickly understood to a first approximation by relating it back to these core ideas.</p>
<hr>
<p>network architectures for deep RL</p>
<ul>
<li>feature engineering can still matter</li>
<li>if using pixels<ul>
<li>often simpler than state-of-the-art for supervised</li>
<li>don't pool away location information if you need it</li>
</ul>
</li>
<li>consider using multiple/auxiliary outputs</li>
<li>consider phrasing regression as classification</li>
<li>room for big advancements</li>
</ul>
<hr>
<p>Lots of exciting developments in RL are coming from the use of deep neural nets, and I wanted to say a few things specific to doing these applications of deep learning.</p>
<hr>
<p>the lure and limits of RL</p>
<ul>
<li>seems like AI (?)</li>
<li>needs so much data</li>
</ul>
<hr>
<p>There's progress in RL now. The limits of that progress are not yet known.</p>
<p>One hot take <a href="https://twitter.com/dennybritz/status/925028640001105920">from Denny Britz</a>: "Ironically, the major advances in RL over the past few years all boil down to making RL look less like RL and more like supervised learning."</p>
<hr>
<p>Question</p>
<ul>
<li>Should you use reinforcement learning?</li>
</ul>
<hr>
<p>It's for you to decide!</p>
<hr>
<p>Thank you!</p>
<hr>
<p>Thank you for coming!</p>
<p>Thanks again to <a href="https://www.deeplearninganalytics.com/">DLA</a> for supporting me in learning more about reinforcement learning, and in being the audience for the earliest version of this talk, providing lots of valuable feedback.</p>
<p>Thanks to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for working through the second iteration of this talk with me, providing even more good feedback.</p>
<p>Thanks to <a href="https://twitter.com/EricHaengel">Eric Haengel</a> for help with Go and in thinking about the <a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">AlphaGo Zero paper</a>.</p>
<hr>
<p>further resources</p>
<hr>
<ul>
<li><a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> (paper)</li>
<li>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> (blog post)</li>
<li><a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> (textbook)</li>
<li>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> (videos and slides)</li>
<li><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> (videos, slides, and labs)</li>
<li>OpenAI <a href="https://github.com/openai/gym">gym</a> / <a href="https://github.com/openai/baselines">baselines</a> (software)</li>
<li><a href="http://nationalgocenter.org/">National Go Center</a> (physical place)</li>
<li><a href="http://dc.hackandtell.org/">Hack and Tell</a> (fun meetup)</li>
</ul>
<p>Here are my top resources for learning more about deep reinforcement
learning (and having a little fun).</p>
<p>The content of <a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> is similar to this talk. It does a quick intro to a lot of deep reinforcement learning.</p>
<p>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> is a readable introduction as well, focused on policy gradient, with readable code.</p>
<p>If you can afford the time, I recommend all of Sutton and Barto's <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a>. The authors are two major reinforcement learning pioneers, to the extent that they sometimes introduce new concepts in the pages of their textbook.</p>
<p>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> is quite good. It largely follows Sutton and Barto's text, and also references Csaba Szepesv&#225;ri's <a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning</a>.</p>
<p>The <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> that inspired this talk has lots of materials online, so you can get the full treatment if you like!</p>
<p>If you want to get into code, <a href="https://openai.com/">OpenAI</a>'s <a href="https://github.com/openai/gym">gym</a> and <a href="https://github.com/openai/baselines">baselines</a> could be nice places to get started.</p>
<p>If you want to have fun, you can play Go with humans at the <a href="http://nationalgocenter.org/">National Go Center</a>, and I always recommend the <a href="http://dc.hackandtell.org/">Hack and Tell</a> meetup for a good time nerding out with people over a range of fun projects.</p>
<p>Juergen Schmidhuber's <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Deep Learning in Neural Networks: An Overview</a> didn't make my top list, but it has an interesting section specific to using LSTMs in RL.</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20171114-deep_rl/</link>
<guid>http://planspace.org/20171114-deep_rl/</guid>
<pubDate>Tue, 14 Nov 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Mounting Disks on AWS</title>
<description><![CDATA[

<p>I don't hand-mount disks much these days, but sometimes I want to spin up a quick <a href="https://aws.amazon.com/ec2/">EC2</a> instance to hammer something out, and I want to use the fast <a href="https://en.wikipedia.org/wiki/Solid-state_drive">SSD</a> storage that comes with some instances, and those disks don't auto-mount.</p>
<p>Amazon has <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html#making-instance-stores-available-on-your-instances">some documentation</a> but it isn't quite enough for me by itself. Here are commands that have served me well on Ubuntu:</p>
<pre><code class="language-bash">df -h  # see what's already mounted
lsblk  # find out the /dev/ paths of devices, like say /dev/xvdb
mkfs.ext4 /dev/xvdb  # format a disk using the Ext4 filesystem
mkdir disk  # make a mount point
mount /dev/xvdb disk  # mount the disk
df -h  # check for success and size of the new disk
chown ubuntu disk1  # make the disk accessible to the admin account</code></pre>

<p>Some of those will need <code>sudo</code>.</p>
<p>I don't really know that Ext4 is the best filesystem to use; let me know if some other one is better!</p>
<p>Some instances come with multiple SSDs, in which case it might be fun to use them in a <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_0">RAID-0</a> configuration for even more speed. I haven't tried this yet, but there is some <a href="http://www.tldp.org/HOWTO/Software-RAID-HOWTO-5.html#ss5.5">documentation</a> to start from. Has anyone tried this?</p>    
    ]]></description>
<link>http://planspace.org/20171022-mounting_disks_on_aws/</link>
<guid>http://planspace.org/20171022-mounting_disks_on_aws/</guid>
<pubDate>Sun, 22 Oct 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Science as Mountaineering</title>
<description><![CDATA[

<p>In <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a> addresses "textbook science" that ironically distances students from science.</p>
<p>For example, a textbook might use <a href="https://en.wikipedia.org/wiki/Foucault_pendulum">Foucault's pendulum</a> as evidence that the Earth rotates. The issue with "experiments" like this, Kuhn says, is that "By the time they were performed no scientist still needed to be convinced of the validity of the theory their outcome is now used to demonstrate. Those decisions had long since been made on the basis of significantly more equivocal evidence."</p>
<p>Textbook science becomes a series of just-so stories, sanitized of the actual messy scientific process. Brief clear explanations are not necessarily bad, but they may not prepare a student for the very different mode of reading (and doing) research. Textbooks can be trusted, for the most part, but new work may be relatively poorly explained, not yet clearly correct, or even just incorrect.</p>
<p>I like the comparison to mountaineering. Science teachers want to get students up the mountain, so they rely on well-trod trails. Discoverers may have originally gone by entirely different paths. Some routes were only evident when looking down from higher ground.</p>
<p>Students take stairs up quickly to very steep terrain. If they miss the opportunity to learn how to climb on the easier hills, they face a difficult transition if they want to do work without guard rails.</p>
<p>I think this analysis isn't specific to science; the focus could be seen as critical thinking, evaluating evidence, and so on. Even within the <a href="https://www.nextgenscience.org/">Next Generation Science Standards</a>, however, the skills I think are important seem surprisingly minimized, as the "science and engineering practices" focus on "engineering design" rather than figuring out what's true.</p>
<p>Some of <a href="http://modeling.asu.edu/R&amp;E/ModelingThryPhysics.pdf">Hestene's work</a> is in the direction of teaching more appropriate skills, and I know many science teachers emphasize the scientific method. I think in both cases practices that could be helpful even outside of science can seem isolated and academic, and I wonder what other approaches might help more people handle themselves on the mountain.</p>    
    ]]></description>
<link>http://planspace.org/20170917-science_as_mountaineering/</link>
<guid>http://planspace.org/20170917-science_as_mountaineering/</guid>
<pubDate>Sun, 17 Sep 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Problems with ImageNet and its Solutions</title>
<description><![CDATA[

<p>The <a href="http://www.image-net.org/">ImageNet</a> challenges play an important role in the development of computer vision. The great <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">success</a> of neural nets on ImageNet has contributed to general fervor around artificial intelligence. While the applied breakthroughs are real, issues with ImageNet and modern networks indicate gaps between current practice and intelligent perception.</p>
<p>For this investigation I used a <a href="/20170430-sampling_imagenet/">subset</a> of ImageNet consisting of 5 images from each of 200 classes and a <a href="https://arxiv.org/abs/1512.03385">ResNet</a>50 network pre-trained on the 1,000-class ImageNet recognition task, as included with <a href="https://keras.io/">Keras</a>. The overlap is 150 classes, and on those the network has 92.3% top-5 accuracy, while it can't ever be correct for the other 50 subset classes. I also averaged the last layer's activations for each set of five examples to make class centroids, inducing a nearest-neighbor model.</p>
<p><img alt="not a croquet ball" src="img/n03134739_1193_croquet_ball.jpg"></p>
<p>The least important problem with ImageNet is that sometimes the ground truth labels are bad. My favorite example is an image labeled "<a href="https://en.wikipedia.org/wiki/Croquet">croquet</a> ball," which the centroid model more correctly labels "tennis ball." Perhaps the original labeler saw what looks like a <a href="https://en.wikipedia.org/wiki/Polo">Polo</a> <a href="https://en.wikipedia.org/wiki/Ralph_Lauren_Corporation">brand</a> wristband and thought it was close enough to call croquet.</p>
<p>Neural nets are pretty robust to dirty training labels, so this is not the end of the world for model performance, but the occasional obviously incorrect label does highlight that "ground truth" is not absolute.</p>
<p>Apart from correctness, using a single label for an object is also a choice of representation that may not be optimal. ImageNet is labeled with <a href="https://wordnet.princeton.edu/">WordNet</a>, which is hierarchical. It might be beneficial for a system to know that croquet balls and tennis balls are both balls, for example, but this kind of information is not usually used.</p>
<p><img alt="occluded unicycle(s)?" src="img/n04509417_4503_unicycle.jpg"></p>
<p>Another interesting example is an image which the centroid model labels "person" but which is supposed to be "unicycle." The unicycles aren't clearly visible in the image, so the only way to get "unicycle" is by context.</p>
<p><img alt="fish, cat, goblet" src="img/n01443537_4691_goldfish.jpg"></p>
<p>Images frequently include multiple distinct entities, as in an image that is labeled "goldfish" but which also includes a cat in a glass bowl. The ResNet model's best guess is "goblet," which is not entirely baseless. Should "cat" be incorrect?</p>
<p>The problem of multiple subjects can be addressed by labeling bounding boxes within images, or even labeling every pixel. Recent ImageNet challenges seem to be focusing more on this kind of object detection. But even when there's clearly one main object in view, there are problems not well addressed by ImageNet models.</p>
<p><img alt="jellyfish... statue? light?" src="img/n01910747_13396_jellyfish.jpg"></p>
<p>A garden statue of a jellyfish is labeled "jellyfish" by ImageNet. The centroid model guesses it's "mushroom." This is likely due at least in part to context clues. It's also indicative of the ResNet model's broader failure to have a flexible conception of jellyfish.</p>
<p><img alt="man, not dumbbell" src="img/n00007846_98724_person.jpg"></p>
<p>Overreliance on context is a known problem with image classifiers. For example, Google has <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">shown</a> that some of its high-performing recognizers didn't distinguish dumbbells from the arms that held them. I saw something like this when the ResNet's top three guesses for a picture of a man were "dumbbell," "water bottle," and "hammer."</p>
<p>Focusing on the parts of images that contain only a specific object can treat symptoms like the dumbbell/arm problem, but they don't address the deeper issue that current neural nets don't really have a mechanism for understanding objects in visual scenes abstractly. Caption-generators have similar failure modes. I only know of <a href="https://arxiv.org/abs/1609.05518">one explicit approach</a> to the problem, and it's not particularly elegant.</p>
<p><a href="https://arxiv.org/abs/1604.00289"><img alt="bad captions" src="img/bad_captions.png"></a></p>
<p>Neural net research seems to mostly hope that systems will learn an appropriate internal representation for a given task. For image classification, the representation is effective, but it seems different from human perception. If we knew better how thoughts are represented in the brain, we could emulate those representations directly. For the moment, we can only evaluate artificial representations indirectly.</p>
<p>I started this investigation hoping to find visual analogies with neural net activations, similar to <a href="/20170705-word_vectors_and_sat_analogies/">word analogies</a> with word vectors. I didn't expect it to work great, but I was disappointed by just how poorly it worked.</p>
<p>Word vectors are trained based on how words appear in text. One common method puts words near each other in space if they could be substituted in a sentence. Syntax and semantics dictate word placement, so word vectors pick up these characteristics. For example, phrases like "She pet the ___" will tend to put "cat" and "dog" close in word vector representation. Image activations in a neural net are based on how things look, so it's less clear how much "meaning" they should pick up.</p>
<p>I was also using a very small collection of just 200 classes, so the space is super sparse. But I still have both "bicycle" and "motorcycle" (among others) so I was hoping to find some interesting analogies.</p>
<p>Unfortunately, for my 200 categories, there are no analogies by closest item with four distinct items. That is, when you find the closest thing to x + y - z, it's always one of x, y, or z. This is <a href="http://anthology.aclweb.org/W16-2503">a known problem</a> with word vector analogies as well. The standard cheat is to just find the nearest new thing, which tends to mean you're just finding nearby items.</p>
<p>Even cheating to get uniqueness, the analogies I can come up with are not super compelling. Here are the best ones I found:</p>
<ul>
<li>sheep : camel :: seal : hippopotamus</li>
<li>flute : oboe :: domestic cat : lion</li>
<li>brassiere : maillot :: guacamole : burrito</li>
</ul>
<p>You could try to convince yourself that there's some meaning there, but it's really just that some things are close to each other in space; the relations are not super meaningful. Here are some examples with similar goodness in terms of distance:</p>
<ul>
<li>flute : oboe :: snake : centipede</li>
<li>lemon : orange :: lizard : frog</li>
<li>lemon : orange :: pitcher : bowl</li>
</ul>
<p>What about the bicycle and motorcycle?</p>
<ul>
<li>bicycle : motorcycle :: flute : oboe</li>
<li>bicycle : motorcycle :: guacamole : burrito</li>
</ul>
<p>What we see is that some things are close together; bicycle is close to motorcycle and flute is close to oboe. So when we ask for something close to bicycle + flute - oboe, for example, flute - oboe is basically zero and we find the closest thing to bicycle. The representations are successfully putting things that look similar close together.</p>
<p><img alt="burrito? guacamole?" src="img/n07880968_4922_burrito.jpg"></p>
<p>In the case of "guacamole" and "burrito" images can frequently be used for either class. At best, the representation learns about co-occurrence. So the best case is the dumbbell/arm problem again.</p>
<p>It isn't really fair to ask activations trained for visual classification to also learn interesting semantics and give fun analogies, just as it isn't reasonable to anthropomorphize neural nets and imagine that they're close to emulating human perception or approaching general artificial intelligence.</p>
<p>One direction that aims to address the limitations of simple classification tasks, for example, is sometimes described as grounded perception or embodied cognition. The idea is that an agent needs to interact in an environment to learn things like "this is used for that" and so on. This might be part of an advance.</p>
<p>In my estimation, however, the core problem of neural-symbolic integration seems unlikely to be solved by some kind of accidental emergent property. We struggle to represent input to an intelligent system, and we have very little idea how to represent the internals. Perhaps further inspiration will come from neuroscience.</p>
<hr>
<p>I presented a version of this material at the <a href="https://www.meetup.com/DC-Hack-and-Tell/events/241002526/">2017-09-14 Hack and Tell</a> using some slides (<a href="hack_and_tell.pptx">PPTX</a>, <a href="hack_and_tell.pdf">PDF</a>).</p>    
    ]]></description>
<link>http://planspace.org/20170911-problems_with_imagenet_and_its_solutions/</link>
<guid>http://planspace.org/20170911-problems_with_imagenet_and_its_solutions/</guid>
<pubDate>Mon, 11 Sep 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Berkeley Deep RL Bootcamp</title>
<description><![CDATA[

<p>At its conclusion, <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> said a major goal of his 2017 <a href="https://www.deeprlbootcamp.berkeley.edu/">Deep Reinforcement Learning Bootcamp</a> was to broaden the application of RL techniques. Around 250 representatives from research and industry had just emerged from 22 scheduled hours over a Saturday and Sunday in Berkeley. Abbeel asked the attendees to report back with tales of applying algorithms they may not have known existed previously.</p>
<p>Instruction came from leaders of modern reinforcement learning research, all delivering their expertise within a framework that highlighted the large-scale structure of the field. I found their organizing diagram to be particularly helpful.</p>
<p><img alt="landscape" src="img/landscape.png"></p>
<p>The reinforcement learning problem statement is simple: at every time step, an agent gets some observation of state \( s \), some reward \( r \), and chooses an action \( a \). So an agent is \( s,r \rightarrow a \), and reinforcement learning is largely about rearranging these three letters.</p>
<p>For example, the Q function is \( s,a \rightarrow \sum{r} \). You <a href="https://en.wikipedia.org/wiki/Bellman_equation">can</a> learn that function from experience, and it nicely induces a policy \( s \rightarrow a \). If you use a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">ConvNet</a> to learn the Q function, you can then <a href="https://deepmind.com/research/dqn/">publish</a> in <a href="https://www.nature.com/">Nature</a>.</p>
<p>Of the many good things about the bootcamp, I most valued getting a better conceptual feel for RL. It was also great to hear from experts about practical details, intuitions, and future directions. For all of these I particularly appreciated <a href="https://www.cs.toronto.edu/~vmnih/">Vlad Mnih</a>'s sessions.</p>
<p>The concerns of RL illuminate deep architectures from unique angles. The big networks that win classification challenges don't win in RL, for example, possibly getting at something about the nature of neural net training and generalization. Vlad described how fixing their target Q-network was more important with the smaller nets they used in development than on their final networks. Wins with <a href="https://arxiv.org/abs/1707.06887">distributional RL</a> may connect to a fundamental affinity of neural nets for categorical problems over regression problems.</p>
<p>Approaches like <a href="https://arxiv.org/abs/1611.05397">unsupervised auxiliary tasks</a> prompt comparisons with how the brain might work. But even cutting-edge techniques with models (\( s,a \rightarrow s \)) and planning, like <a href="https://arxiv.org/abs/1707.06203">imagination-augmented agents</a> and the <a href="https://arxiv.org/abs/1612.08810">predictron</a>, do as much to highlight differences as similarities with how we think. RL is at least as close to <a href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a> as it is to <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">AGI</a>.</p>
<p>Reinforcement learning is home to a profusion of acronyms and initialisms that can be intimidating. As best I can, I've extended the Deep RL Bootcamp's diagram to include everything that was covered, together with a few terms common elsewhere and a set of expansions and links. For didactic resources, start from <a href="https://twitter.com/karpathy">Karpathy</a>'s <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a>.</p>
<p><img alt="annotated" src="img/annotated.jpg"></p>
<ul>
<li>(<a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">PO</a>)<a href="https://en.wikipedia.org/wiki/Markov_decision_process">MDP</a>: (Partially Observable) Markov Decision Process</li>
<li><a href="https://en.wikipedia.org/wiki/Derivative-free_optimization">DFO</a>: Derivative-Free Optimization</li>
<li><a href="https://en.wikipedia.org/wiki/Cross-entropy_method">cross-entropy method</a><ul>
<li><a href="http://iew3.technion.ac.il/CE/files/papers/Learning%20Tetris%20Using%20the%20Noisy%20Cross-Entropy%20Method.pdf">Learning Tetris using the noisy cross-entropy method</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/CMA-ES">CMA</a>: Covariance Matrix Adaptation</li>
<li><a href="https://en.wikipedia.org/wiki/Natural_evolution_strategy">NES</a>: Natural Evolution Strategy<ul>
<li><a href="https://blog.openai.com/evolution-strategies/">Evolution strategies as a scalable alternative to reinforcement learning</a></li>
</ul>
</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.2545&amp;rep=rep1&amp;type=pdf">REINFORCE</a>: REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility</li>
<li><a href="https://arxiv.org/abs/1502.05477">TRPO</a>: Trust Region Policy Optimization</li>
<li><a href="https://blog.openai.com/openai-baselines-ppo/">PPO</a>: Proximal Policy Optimization</li>
<li><a href="https://arxiv.org/abs/1510.09142">SVG</a>: Stochastic Value Gradients</li>
<li><a href="https://arxiv.org/abs/1602.01783">A3C</a>: Asynchronous Advantage Actor Critic</li>
<li><a href="https://blog.openai.com/baselines-acktr-a2c/">A2C</a>: (Synchronous) Advantage Actor Critic</li>
<li><a href="https://arxiv.org/abs/1708.05144">ACKTR</a>: Actor Critic using Kronecker-Factored Trust Region</li>
<li><a href="https://arxiv.org/abs/1611.05397">UNREAL</a>: UNsupervised REinforcement and Auxiliary Learning</li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">TD</a>: Temporal Difference (also \( TD(\lambda) \))</li>
<li>FQI: Fitted Q Iteration</li>
<li><a href="https://deepmind.com/research/dqn/">DQN</a>: Deep Q-Network</li>
<li><a href="https://arxiv.org/abs/1509.02971">DDPG</a>: Deep Deterministic Policy Gradient</li>
<li><a href="https://arxiv.org/abs/1603.00748">NAF</a>: Normalized Advantage Functions</li>
<li><a href="http://realai.org/imitation-learning/">Imitation</a> Learning</li>
<li><a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf">IRL</a>: Inverse Reinforcement Learning</li>
<li><a href="https://arxiv.org/abs/1603.00448">GCL</a>: Guided Cost Learning</li>
<li><a href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_363">HRL</a>: Hierarchical Reinforcement Learning</li>
<li>FuN: FeUdal Networks<ul>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/dh93.pdf">Feudal reinforcement learning</a></li>
<li><a href="https://arxiv.org/abs/1703.01161">FeUdal Networks for hierarchical reinforcement learning</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Model_predictive_control">MPC</a>: Model Predictive Control</li>
<li><a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">PILCO</a>: Probabilistic Inference
for Learning COntrol</li>
<li>local models specifically as in <a href="https://arxiv.org/abs/1501.05611">Learning contact-rich manipulation skills with guided policy search</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">LQR</a>: Linear-Quadratic Regulator</li>
<li><a href="https://arxiv.org/abs/1612.08810">The Predictron: End-to-end learning and planning</a></li>
<li><a href="https://arxiv.org/abs/1707.06203">I2A</a>: Imagination-Augmented Agent</li>
<li>(<a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo</a>) (<a href="https://en.wikipedia.org/wiki/Tree_traversal">tree</a>) <a href="https://en.wikipedia.org/wiki/Artificial_intelligence#Search_and_optimization">search</a>, <a href="https://en.wikipedia.org/wiki/Minimax">minimax</a>, etc.</li>
<li><a href="https://en.wikipedia.org/wiki/Gaussian_process">GP</a>: Gaussian Process</li>
<li><a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">GMM</a>: Gaussian Mixture Model</li>
<li>(<a href="https://en.wikipedia.org/wiki/Deep_learning">D</a>/<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">C</a>/<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">R</a>)<a href="https://en.wikipedia.org/wiki/Artificial_neural_network">NN</a>: (Deep/Convolutional/Recurrent) Neural Network</li>
<li><a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a>: Long Short-Term Memory</li>
</ul><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20170830-berkeley_deep_rl_bootcamp/</link>
<guid>http://planspace.org/20170830-berkeley_deep_rl_bootcamp/</guid>
<pubDate>Wed, 30 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Characteristics of good theories</title>
<description><![CDATA[

<p>In <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a> gives a partial list of characteristics that good theories share:</p>
<ol>
<li><strong>Accurate</strong>: within its domain, consequences deducible from a theory should be in demonstrated agreement with the results of existing experiments and observations</li>
<li><strong>Consistent</strong>: internally (with itself) and with other currently accepted theories applicable to related aspects of nature</li>
<li><strong>Broad scope</strong>: consequences should extend far beyond the particular observations, laws, or subtheories it was originally designed to explain</li>
<li><strong>Simple</strong>: bringing order to phenomena that in its absence would be individually isolated and, as a set, confused</li>
<li><strong>Fruitful</strong>: leading to new research findings; disclosing new phenomena or previously unnoted relationships among those already known</li>
</ol>
<p>In <a href="https://en.wikipedia.org/wiki/The_Beginning_of_Infinity">The Beginning of Infinity</a>, <a href="https://en.wikipedia.org/wiki/David_Deutsch">David Deutsch</a> objects to positivism or anti-realism, but has not so different criteria to Kuhn. Deutsch focuses on these two:</p>
<ol>
<li><strong>Hard to vary</strong>: having no extraneous or non-explanatory characteristics; possibly related to Kuhn's "consistent" and "simple"</li>
<li><strong>Reach</strong>: similar to Kuhn's "broad scope," "fruitful," and possibly even "simple"</li>
</ol>
<p>Deutsch mostly takes "accurate" as a given or obvious requirement. Even this seemingly simple goal becomes complicated when people <a href="/20170823-transfer_of_allegiance_from_theory_to_theory/">disagree</a> about "the results of existing experiments and observations."</p>
<p>The focus of both authors is on scientific theories, as in (for example) physics. When explaining human affairs, the same criteria may not always be perfect guides. For example, many non-simple conspiracy theories are not true, but there can really be conspiracies, and things may not be obvious.</p>    
    ]]></description>
<link>http://planspace.org/20170825-characteristics_of_good_theories/</link>
<guid>http://planspace.org/20170825-characteristics_of_good_theories/</guid>
<pubDate>Fri, 25 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Unique IDs and Nielsen abuse</title>
<description><![CDATA[

<p>The Wall Street Journal ran an article online with the headline <a href="https://www.wsj.com/articles/in-tv-ratings-game-networks-try-to-dissguys-bad-newz-from-nielsen-1499350955">In TV Ratings Game, Networks Try to Dissguys Bad Newz from Nielsen</a>. It got picked up by <a href="https://www.theverge.com/2017/7/6/15923722/tv-network-ratings-nielsen-viewership">other outlets</a>, and ran in the print WSJ the next day, on Friday July 7, 2017, on the front page, below the fold.</p>
<p>The implication is that networks identify their programs to Nielsen via a free text field with no validation; programs don't have unique IDs. Not having unique IDs is a huge pain even when everyone is trying to be consistent. Inevitably you have to do some kind of deduplication and merging. In this case, it leads to abuse (possibly sanctioned unofficially).</p>
<p><img alt="page 1" src="img/page1.png"></p>
<p><img alt="page 9" src="img/page9.png"></p>    
    ]]></description>
<link>http://planspace.org/20170724-unique_ids_and_nielsen_abuse/</link>
<guid>http://planspace.org/20170724-unique_ids_and_nielsen_abuse/</guid>
<pubDate>Mon, 24 Jul 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer of allegiance from theory to theory</title>
<description><![CDATA[

<p>Toward the end of <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a>'s <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, he says something that feels relevant to modern popular debate:</p>
<blockquote>
<p>...communication between proponents of different theories is
inevitably partial, that what each takes to be facts depends in part
on the theory he espouses, and that an individual's transfer of
allegiance from theory to theory is often better described as
conversion than as choice.</p>
</blockquote>
<p>Kuhn describes the investigation and development of non-mainstream
ideas as a benefit resulting from diversity of opinion. At the level
of conspiracy theory, and when choice of what to believe is based on
problematic desiderata like greed and racism, it seems to become a
pathology.</p>    
    ]]></description>
<link>http://planspace.org/20170823-transfer_of_allegiance_from_theory_to_theory/</link>
<guid>http://planspace.org/20170823-transfer_of_allegiance_from_theory_to_theory/</guid>
<pubDate>Wed, 23 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Scientific and historical levels of explanation</title>
<description><![CDATA[

<p>From page 1 of <a href="https://en.wikipedia.org/wiki/James_Flynn_(academic)">Flynn</a>'s <a href="https://en.wikipedia.org/wiki/What_Is_Intelligence%3F">What is intelligence?</a>:</p>
<blockquote>
<p>A warning for everyone: there are problems that can simply be
settled by evidence, for example, whether some swans are black. But
there are deeper problems that pose paradoxes. Sometimes the
evidence that would solve them lies in an inaccessible past. That
means we have to retreat from the scientific level of explanation to
the historical level where we demand only plausibility that conforms
to the known facts. I believe that my efforts to resolve the
historical paradoxes we will discuss should be judged by whether
someone has a more satisfactory resolution to offer. The reader
should be wary throughout to distinguish the contentions I evidence
from the contentions to which I lend only plausibility.</p>
</blockquote>
<p>I'm not sure whether an existence claim (about black swans) is the
best example, or whether it matters much if other problems are deeper
or pose paradoxes, but I do think this is an interesting way to phrase
the epistemological difference between a claim supported
experimentally, as by a randomized controlled trial, and a claim based
on observational evidence that is not direct. It's the kind of thing
that I think is worth considering, especially as there seems to be so
much disagreement about how to settle problems.</p>    
    ]]></description>
<link>http://planspace.org/20170822-scientific_and_historical_levels_of_explanation/</link>
<guid>http://planspace.org/20170822-scientific_and_historical_levels_of_explanation/</guid>
<pubDate>Tue, 22 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>A downward spiral destructive of civic virtue</title>
<description><![CDATA[

<p>From page 166 of <a href="https://en.wikipedia.org/wiki/James_Flynn_(academic)">Flynn</a>'s <a href="https://en.wikipedia.org/wiki/What_Is_Intelligence%3F">What is intelligence?</a>:</p>
<blockquote>
<p>Competition for possessions without a rationally imposed limit
engenders pessimism about acceptance of the restraints necessary to
avoid ecological disaster.</p>
<p>Competition for possessions also creates a downward spiral
destructive of civic virtue. Those who wish to maximize their
economic status are reluctant to pay taxes and this diminishes state
provision of health, education, and security against misfortune. As
the quality of state provision declines, it becomes imperative to
maximize private wealth for reasons of security even if status
seeking is set aside. Even principled socialists will pay fees to
jump the queue for medical care and to get education for their
children in schools that are not a test of physical survival. The
more that is true, the more you resent any dollar leaving your
pocket in tax, so public provision drops further, so willingness to
be taxed drops further, and so forth. Indeed, since only a few can
amass the fortune needed to provide self-security, no amount of
money you can realistically hope to acquire is enough.</p>
</blockquote>
<p>I think this passage touches a lot of modern debates.</p>    
    ]]></description>
<link>http://planspace.org/20170821-downward_spiral_destructive_of_civic_virtue/</link>
<guid>http://planspace.org/20170821-downward_spiral_destructive_of_civic_virtue/</guid>
<pubDate>Mon, 21 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Word Vectors and SAT Analogies</title>
<description><![CDATA[

<p>In 2013, <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> popularized <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">word vectors</a> and <em>king - man + woman = queen</em>.</p>
<p><a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"><img alt="king queen etc." src="img/king_queen.png"></a></p>
<p>This reminded me of <a href="https://en.wikipedia.org/wiki/SAT">SAT</a> analogy questions, which <a href="http://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them">disappeared from the SAT in 2005</a>, but looked like this:</p>
<pre><code>PALTRY : SIGNIFICANCE ::

A. redundant : discussion
B. austere : landscape
C. opulent : wealth
D. oblique : familiarity
E. banal : originality</code></pre>

<p>The king/queen example is not difficult, and I don't know whether it was tested or discovered. A better evaluation would use a set of challenging pre-determined questions.</p>
<p>There is a Google set of analogy questions, but all the relationships are grammatical, geographical, or by gender. Typical: "fast : fastest :: old : oldest." (<a href="http://download.tensorflow.org/data/questions-words.txt">dataset</a>, <a href="https://arxiv.org/abs/1301.3781">paper</a>, <a href="https://aclweb.org/aclwiki/Google_analogy_test_set_(State_of_the_art)">context</a>)</p>
<p>SAT questions are more interesting. Selecting from fixed answer choices provides a nice guessing baseline (1/5 is 20%) and using a human test means it's easier to get human performance levels (average US college applicant is 57%; human voting is 81.5%).</p>
<p>Michael Littman and Peter Turney have made available <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">a set of 374 SAT analogy questions</a> since 2003. You have to email Turney to get them, and I appreciate that he helped me out.</p>
<p>Littman and Turney <a href="http://cogprints.org/4518/1/NRC-48273.pdf">used</a> a vector-based approach on their dataset back in 2005. They achieved 47% accuracy (state of the art at the time) which is a nice benchmark.</p>
<p>They made vectors for each word pair using web data. To get one value for "banal : originality" they would search <a href="https://en.wikipedia.org/wiki/AltaVista">AltaVista</a> for "banal and not originality" and take the log of the number of hits. With a list of 64 connectives they made vectors with 128 components.</p>
<p>I'm using <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> and word2vec word vectors that are per-word and based on various text corpora directly. Since the vectors are not specific to particular pairings, they may at a relative disadvantage for the SAT task. To get a vector for a word pair, I just subtract.</p>
<p>Stanford provides a variety of GloVe vectors pre-trained on three different corpora:</p>
<ul>
<li>Twitter (<code>glove.twitter</code>)<ul>
<li>2B tweets, 27B tokens, 1.2M vocab, uncased</li>
<li>as each of 25d, 50d, 100d, &amp; 200d vectors</li>
</ul>
</li>
<li>Wikipedia 2014 + Gigaword 5 (<code>glove.6B</code>)<ul>
<li>6B tokens, 400K vocab, uncased</li>
<li>as each of 50d, 100d, 200d, &amp; 300d vectors</li>
</ul>
</li>
<li>Common Crawl<ul>
<li>uncased (<code>glove.42B</code>) 42B tokens, 1.9M vocab, 300d vectors</li>
<li>cased (<code>glove.840B</code>) 840B tokens, 2.2M vocab, 300d vectors</li>
</ul>
</li>
</ul>
<p>I have one word2vec model: <code>GoogleNews-vectors</code> trained on a Google News dataset, providing 300d vectors.</p>
<p><img src="img/accuracy.png" width="480"></p>
<ul>
<li>The best word vectors for the SAT analogies task (<code>glove.840B</code>) achieve 49% accuracy. This outperforms the 2005 47% result, but is still within the confidence bounds of 42.2% to 52.5% that they report.</li>
<li>Holding the training set constant and varying the size of the vectors affects performance on the SAT analogies task: bigger vectors work better, though performance may be plateauing around 300d.</li>
<li>Twitter data does markedly worse on the SAT analogies task. This is consistent with Twitter's limitations and reputation for being less than erudite.</li>
</ul>
<p>Trained word vectors provide a fixed vocabulary. When a word was missing, I used a vector of zeros. Most embeddings only missed one to eight words from the 374 questions, but <code>glove.twitter</code> missed 105. I also checked accuracies when excluding questions that had unsupported words for a set of embeddings, and the results were remarkably close, even with the Twitter embeddings. So the accuracies are due to the embeddings and not just gaps in the vocabularies.</p>
<p>It is pretty impressive that word vectors work as well as they do on this task, but nobody should consider word vectors a solution to natural language understanding. Problems with word vectors have been pointed out (<a href="http://www.aclweb.org/anthology/N15-1098">source</a>, <a href="http://www.aclweb.org/anthology/C/C16/C16-1262.pdf">source</a>, <a href="http://anthology.aclweb.org/W16-2503">source</a>, <a href="https://arxiv.org/abs/1705.11168">source</a>).</p>
<p>Still, the idea of word vectors (translating sparse data to a learned dense representation) is super useful, and not just for words. See implementations <a href="https://keras.io/layers/embeddings/">in Keras</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embedding_column">in TensorFlow</a>.</p>
<p>These methods are not the best-performing non-human technique for these SAT analogy questions. Littman and Turney <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">report</a> several. Latent Relational Analysis comes in at 56% accuracy, against the average US college applicant at 57%.</p>
<p>This is a case in which "the average human" is not a good bar for measuring AI success. The average human has a remarkably small vocabulary relative to what a computer should easily handle, not to mention that the average human would not be admitted to most colleges you could name.</p>
<hr>
<p>I'm grateful to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for helpful discussions, and particularly Shabnam Tafreshi and Dmitrijs Milajevs for sharing references at <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/events/237114317/">the January 26, 2017 meeting</a>. Thanks also to <a href="https://twitter.com/metasemantic">Travis Hoppe</a> of <a href="http://dc.hackandtell.org/">DC Hack and Tell</a> for always doing cool things with NLP. Thanks to Peter Turney for providing the dataset and commenting on distances.</p>
<p>Notes:</p>
<ul>
<li>An <a href="https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/">example</a> of word2vec popularization.</li>
<li>My code is available at <a href="https://github.com/ajschumacher/sat_analogies">ajschumacher/sat_analogies</a>.</li>
<li>All but 20 of the 374 questions have five designed answer choices; those 20 have four and "no : choice" as the last option. To maintain comparability I kept those questions in, though it means a human guessing baseline should be slightly over 20%.</li>
<li>For efficiency, I used GloVe values as 2-byte floats, which is lossy. I also tested with 4-byte floats, and the results varied by at most one question, while being much slower to generate.</li>
<li>Cosine distance is more effective than Euclidean distance for this task, and the advantage increases with dimensionality.</li>
</ul>
<p><img src="img/cosine_advantage.png" width="480"></p>    
    ]]></description>
<link>http://planspace.org/20170705-word_vectors_and_sat_analogies/</link>
<guid>http://planspace.org/20170705-word_vectors_and_sat_analogies/</guid>
<pubDate>Wed, 05 Jul 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Questions from TensorFlow APIs Webinar</title>
<description><![CDATA[

<p>I did <a href="/20170524-more_and_better_new_tensorflow_apis/">More and Better: The New TensorFlow APIs</a> as a <a href="https://www.altoros.com/blog/event/more-and-better-the-new-tensorflow-apis/">webinar</a>. <a href="https://www.altoros.com/">Altoros</a> ran it using <a href="https://www.gotomeeting.com/webinar">GoToWebinar</a>, which reported all questions from attendees. Questions and answers:</p>
<hr>
<h3>What should I learn?</h3>
<blockquote>
<p>"New to DL and TF... several people have said to not waste time w/Keras, etc, but go deep into TF to really learn it.  From what you said, it seems like TF changes a lot, though.  Recommendations in terms of sequence/path for learning DL concepts and the tools?"</p>
</blockquote>
<p>First, regarding the rate of change of TensorFlow: The low-level API should really be quite stable now. It's the high-level APIs that are still settling down a little bit. And even there, it looks like things are getting pretty set. I wouldn't expect Keras to change a lot, in particular.</p>
<p>The question of whether to use a high-level API or a low-level API depends on your goals. I don't think it's a waste of time to use Keras. In fact, I think it would be easier to argue that it's "a waste of time" to dig into low-level TensorFlow implementation details, because there's a lot you can get done much more quickly by using Keras.</p>
<p>It depends on your goals. Do you want to understand every step in the internals of your system? I've heard people like Matt Zeiler suggest that you should write your own deep learning framework from scratch in order to understand all the details of backprop, etc. How far do you want to go?</p>
<p>For most people, I think higher-level interfaces are a good choice. Sometimes they can actually make it easier to learn the concepts that are really important, because you don't have to deal with all the details that can complicate things.</p>
<p>That's not to say that it isn't valuable to get into the low-level details as well. Sometimes you really do need to make changes or understand phenomena that are down at that level. But you don't necessarily need to start at the low level; you can also start at higher levels of abstraction and then learn lower levels later.</p>
<p>As for learning DL concepts and tools, I'm currently a big advocate of <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a> by Fran&#231;ois Chollet, the author of Keras. It's still in pre-release, but the chapters that are already available are quite good. I think it's a great book especially for people who are newer to deep learning because it explains a lot of fundamental things in approachable ways.</p>
<hr>
<h3>Keras outside or inside TensorFlow?</h3>
<blockquote>
<p>"What is the link between keras and tf that is being shown? I have used Keras and I simply indicate that it should use tensorflow in the backend and my calls do not include tf.xxx.keras.xxx... One of the goals of Keras is to be framework independent. To call from Tensorflow doesn&#8217;t really maintain that spirit."</p>
</blockquote>
<p>You can still use Keras directly with TensorFlow as the backend, maintaining framework independence. As far as I know, Keras will continue to be developed separately from TensorFlow as well as within the TensorFlow codebase. And for many things, using Keras as it appears inside TensorFlow will work just the same way as using independent Keras with the TensorFlow backend.</p>
<p>Having Keras inside TensorFlow is a convenience, for one, but it should also enable some tighter integrations. I'm expecting the layers API will become even more unified and allow for mixing and matching even more design possibilities. More importantly, I think, Keras models in TensorFlow will be easily convertible to TensorFlow Estimators, which should be pretty neat.</p>
<hr>
<h3>C? C++?</h3>
<blockquote>
<p>"how come [TensorFlow is] written in C++ but recommended interfaces should talk to the C part?"</p>
</blockquote>
<p>I suppose there are at least two reasons. One is that it standardizes the interface, making it very clear what is stable and safe to build an API to. Another is that as a practical matter, lots of languages have established standard ways to connect to a C API. To read more about TensorFlow's take, check out the <a href="https://www.tensorflow.org/extend/language_bindings">language support documentation</a>.</p>
<hr>
<h3>On-graph? Off-graph?</h3>
<blockquote>
<p>"Can you remind folks about what 'on-graph' is?"</p>
</blockquote>
<p>TensorFlow's core way of working is by defining a graph of operations and then running them; anything that runs that way is "on-graph."</p>
<p>Normal Python code doesn't work that way; you don't define a graph and then run it, you just run your Python code right away.</p>
<p>Even within the TensorFlow Python API, not everything works by putting things on the graph. For example, <code>tf.python_io.tf_record_iterator</code> doesn't use the TensorFlow graph.</p>
<p>I wrote some more about this distinction in <a href="/20170428-everything_in_the_graph_even_glob/">Everything in the Graph? Even Glob?</a>. An example of TensorFlow functionality that exists in both off-graph and on-graph versions is TFRecords processing. I have a post that uses the <a href="/20170323-tfrecords_for_humans/">off-graph API</a> and two posts that use the on-graph API: <a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">reading from disk</a> and <a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">parsing TFRecords</a>.</p>
<hr>
<h3>Python 2? Python 3?</h3>
<blockquote>
<p>"Are you using Python 2.x?  If so why?"</p>
</blockquote>
<p>I use Python 2.7 a lot because projects I work on use it. I usually recommend that new projects start up with Python 3.</p>
<p>The inertia of Python 2.7 is really incredible, but maybe the tide is turning at last. Under a year ago, this quote <a href="https://blog.openai.com/infrastructure-for-deep-learning/">appeared</a> on OpenAI's blog:</p>
<blockquote>
<p>"Like much of the deep learning community, we use Python 2.7."</p>
</blockquote>
<p>More recently though, another <a href="https://blog.openai.com/openai-baselines-dqn/">post</a> from OpenAI indicated at least some work is now on Python 3:</p>
<blockquote>
<p>"We use Python 3 and TensorFlow."</p>
</blockquote>
<hr>
<h3>Hyperparameter tuning?</h3>
<blockquote>
<p>"I would like to know if there is a recommended way to make hyperparameter tuning (number of layers, number of nodes in each layer, learning rate, number of epochs)"</p>
</blockquote>
<p>For number of epochs, you can evaluate your model multiple times during training to get train and eval performance curves which should help in making that choice.</p>
<p>For architectural choices like number of layers and number of nodes in each layer, and optimization parameters like learning rate, you can parameterize your code so that choices become command-line arguments, which makes it easier to run experiments and determine what works best.</p>
<p>Making choices based on your previous experience and what's worked for others can save you a lot of time; usually you don't really want to search over <em>all</em> the things you <em>could</em> vary.</p>
<p>When you're really not sure, you can search over all your possible parameters. You may be able to do better than grid search: even <a href="http://scikit-learn.org/stable/modules/grid_search.html#randomized-parameter-optimization">random search</a> can be better, and <a href="https://en.wikipedia.org/wiki/Bayesian_optimization">Bayesian optimization</a> could be better yet. Google has <a href="https://cloud.google.com/ml-engine/docs/how-tos/using-hyperparameter-tuning">productized hyperparameter tuning</a>, which may be worth checking out.</p>
<hr>
<h3>Visualization for very large models?</h3>
<blockquote>
<p>"I have a question, can we also visualize distributed tensorflow on very large models with billion parameters with tensorboard?"</p>
</blockquote>
<p>The answer is probably "yes" but there are multiple aspects to this question.</p>
<p>First, when running  distributed TensorFlow training following the standard pattern, probably you have your chief worker writing out TensorBoard summaries. So that centralizes the "where" of TensorBoard summary writing.</p>
<p>Second, even with small models, it's possible to generate unwieldy amounts of TensorBoard data. You probably want to think carefully about what you want to save during training. For example, you probably don't need to log things every iteration, and you probably don't need to log every parameter value if a histogram will do the job.</p>
<p>Third, some of the things you likely want to visualize for a large model are the same as things you want to visualize for a small model, like loss progression through training. There might not be any changes necessary when model size is different.</p>
<p>If you just want to visualize the model structure (the model graph) then you can take advantage of TensorBoard's grouping functionality so that you can see and reason about components across multiple scales.</p>
<p>In the end, it will depend on exactly what you want to visualize. TensorBoard has a lot of neat features, but it won't be the right choice for every possible problem.</p>
<hr>
<h3>What about serving?</h3>
<blockquote>
<p>How does TensorFlow Serving fit in currently?</p>
</blockquote>
<p><a href="https://tensorflow.github.io/serving/">TensorFlow Serving</a> is largely distinct from the APIs that I showed, except that Estimators can easily <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator#export_savedmodel">export</a> the SavedModel format that you want to use for serving.</p>    
    ]]></description>
<link>http://planspace.org/20170526-questions_from_tensorflow_apis_webinar/</link>
<guid>http://planspace.org/20170526-questions_from_tensorflow_apis_webinar/</guid>
<pubDate>Fri, 26 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>More and Better: The New TensorFlow APIs</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://www.altoros.com/blog/event/more-and-better-the-new-tensorflow-apis/">webinar</a> given Wednesday May 24, 2017. (<a href="big.html">slides</a>) (<a href="https://altoros.wistia.com/medias/e5su4b1vtz">video</a>) The Jupyter notebook and supporting files for code demos are available in a <a href="https://github.com/ajschumacher/tf_api_20170524">repository</a> on GitHub.</em></p>
<hr>
<p>Thank you!</p>
<hr>
<p>Before starting, I want to take a moment to notice how great it is to have a webinar. Thank you for checking it out! We are all really lucky. If this isn't nice, I don't know what is.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work for a company called Deep Learning Analytics. You can find out more about DLA at <a href="https://www.deeplearninganalytics.com/">deeplearninganalytics.com</a>. We work with a number of frameworks, and I've been keeping an eye on TensorFlow.</p>
<p>I do <em>not</em> work for Google, and I'm not a core TensorFlow developer. I'm just talking about my view of things, which is not authoritative or official.</p>
<hr>
<p>motivation</p>
<hr>
<p>Why this talk? What is it about, and who should care?</p>
<hr>
<p>"a low-level library, meaning you&#8217;ll be multiplying matrices and vectors." (2015)</p>
<hr>
<p>This is a description of TensorFlow from an <a href="http://fastml.com/what-you-wanted-to-know-about-tensorflow/">article</a> that came out the same month TensorFlow was released, in November of 2015. It was pretty true then.</p>
<p>Some people thought this was fine. It was like <a href="http://deeplearning.net/software/theano/">Theano</a>.</p>
<p>Some people thought it was not so fine, and so they may have decided TensorFlow was not for them.</p>
<p>But while Theano hasn't radically expanded its API to include tons of higher-level ways of using it, TensorFlow has. TensorFlow is no longer only a low-level library.</p>
<hr>
<pre><code class="language-python">...TensorFlow 1.2.0rc0</code></pre>

<hr>
<p>TensorFlow wasn't "finished" when it was released as open source. It only hit version 1.0 in February of this year. And that means the low-level API is largely stabilized. But high-level APIs are being added at a rapid pace.</p>
<p>I gave a <a href="/20170509-building_tensorflow_systems_from_components/">workshop</a> on TensorFlow at <a href="https://conferences.oreilly.com/oscon/">OSCON</a> two weeks ago, and one of the APIs I'll talk about today is new since then. I'll also mention things that aren't going to be in TensorFlow for at least a few weeks yet.</p>
<p>(I'm referring to the <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data"><code>Dataset</code> API</a>, which entered <code>contrib</code> in 1.2, and the Keras model <code>get_estimator</code> method, respectively.)</p>
<hr>
<p>"I don't know which API to use." (2017)</p>
<hr>
<p>This is a sentiment I encountered in talking to people at the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a>.</p>
<p>There are now so many ways of using TensorFlow, it can be overwhelming, and you might not know where to start. If you do jump in, you might only see one part of the API and not know what else you're missing.</p>
<hr>
<p>a tour of TensorFlow APIs</p>
<hr>
<p>This talk is not for people who are expert TensorFlow developers or people who have been following TensorFlow development super closely.</p>
<p>This talk is for people who are new to TensorFlow, or people who may have tried TensorFlow in the past but haven't been following more recent developments.</p>
<p>The hope is that you'll understand better the options you have for using TensorFlow and be able to choose what to explore.</p>
<hr>
<ul>
<li>language survey</li>
<li>Python survey</li>
<li>Python focus</li>
</ul>
<hr>
<p>We'll start very broad and then narrow in on a few high-level APIs for TensorFlow.</p>
<p>First, considering the way TensorFlow works with multiple programming languages can illuminate the core of TensorFlow's low-level design. This introduces the low-level Python API, and the key distinction between implementations on and off the TensorFlow graph.</p>
<p>We're not going to cover them all equally, but we'll survey a lot of Python API options and how they relate.</p>
<p>Finally, we'll focus on aspects of the core TensorFlow Python APIs, especially Keras and the Estimator API, with a little bit more detail and code examples.</p>
<p>(The following section is based on <a href="/20170320-tensorflow_apis_for_various_languages/">TensorFlow APIs for Various Languages</a>.)</p>
<hr>
<p>What even is TensorFlow?</p>
<hr>
<p>Here's one way to think about TensorFlow.</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>The beating heart of TensorFlow is the Distributed Execution Engine, or runtime.</p>
<p>One way to think of it is as a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">virtual machine</a> whose language is the TensorFlow graph.</p>
<p>That core is written in C++, but lots of TensorFlow functionality lives in the frontends. In particular, there's a lot in the Python frontend.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<p>Python is the richest frontend for TensorFlow. It's what we'll use today.</p>
<p>You should remember that not everything in the Python TensorFlow API touches the graph. Some parts are just Python.</p>
<hr>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>R has an unofficial <a href="https://rstudio.github.io/tensorflow/">TensorFlow API</a> which is kind of interesting in that it just wraps the Python API. It's really more like an R API to the Python API to TensorFlow. So when you use it, you write R, but Python also runs. This is not how TensorFlow encourages languages to implement TensorFlow APIs, but it's out there.</p>
<hr>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<p>The way TensorFlow encourages API development is via TensorFlow's C bindings.</p>
<hr>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<p>You could use C++, of course.</p>
<hr>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<p>Also there's Java.</p>
<hr>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<p>And there's Go.</p>
<p>Python, C++, Java, and Go are the only languages with official Google-blessed TensorFlow APIs. But there are projects offering APIs that interface with TensorFlow via the C bindings.</p>
<hr>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<p>There's Rust.</p>
<hr>
<p><img alt="C#" src="img/c_sharp.png"></p>
<hr>
<p>And there's C#.</p>
<hr>
<p><img alt="Julia" src="img/julia.png"></p>
<hr>
<p>And there's Julia.</p>
<hr>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<p>And there's even Haskell!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Currently, a rough summary is that languages other than Python have TensorFlow support that is close to the runtime. Basically make your ops and run them.</p>
<p>This is likely to be enough support to deploy a TensorFlow system doing inference in whatever language you like, but if you're developing and training systems you probably still want to use Python.</p>
<hr>
<p>graph or not graph?</p>
<hr>
<p>So this is a distinction to think about: Am I using the TensorFlow graph, or not?</p>
<p>The distinction will be pretty clear when using the low-level API. It becomes less obvious when using higher-level APIs, but the situation there is that you can safely assume they're using the graph.</p>
<hr>
<p>Python APIs</p>
<hr>
<p>So, what's this about multiple Python APIs?</p>
<p>TensorFlow started with a low-level API, and it's always been possible and even a good idea to build higher-level APIs on top of it. Plenty of people have done so.</p>
<p>Let's start by checking out some APIs that are entirely outside of TensorFlow.</p>
<p>(The following section is based on <a href="/20170321-various_tensorflow_apis_for_python/">Various TensorFlow APIs for Python</a>.)</p>
<hr>
<p><img alt="Bucksstar Coffee" src="img/bucksstar.png" height="100%"></p>
<hr>
<p>It seems like lots of projects got names involving one or the other of "tensor" or "flow". It doesn't mean they're part of TensorFlow, and it doesn't mean they're good.</p>
<p>Of course, maybe they are; some of these could be good for you, for all I know.</p>
<hr>
<p><img alt="TFLearn logo" src="img/tflearn.png" height="100%"></p>
<hr>
<p>The confusingly named <a href="https://github.com/tflearn/tflearn">TFLearn</a> (no space; perhaps more clearly identified as <a href="(http://tflearn.org/)">TFLearn.org</a>) is not at all the same thing as the TF Learn (with a space) that appears in TensorFlow at <code>tf.contrib.learn</code>. TFLearn is a <a href="http://stackoverflow.com/questions/38859354/what-is-the-difference-between-tf-learn-aka-scikit-flow-and-tflearn-aka-tflea">separate</a> Python package that uses TensorFlow. It seems like it aspires to be like Keras. Even the TFLearn logo looks like a knock-off.</p>
<hr>
<p><img alt="TensorLayer logo" src="img/tensorlayer.png"></p>
<hr>
<p>Another one with a confusing name is <a href="https://github.com/zsdonghao/tensorlayer/">TensorLayer</a>. This is a separate package from TensorFlow and it's different from TensorFlow's layers API.</p>
<hr>
<p>ImageFlow</p>
<hr>
<p><a href="https://github.com/HamedMP/ImageFlow">ImageFlow</a> was supposed to make it easier to do image work with TensorFlow. It looks like it's abandoned.</p>
<hr>
<p>Pretty Tensor</p>
<hr>
<p><a href="https://github.com/google/prettytensor">Pretty Tensor</a> is still a Google project, so it might be worth checking out if you really like <a href="https://en.wikipedia.org/wiki/Fluent_interface">fluent interfaces</a> with lots of chaining, like <a href="https://d3js.org/">d3</a>.</p>
<hr>
<p><img alt="Sonnet" src="img/sonnet.png"></p>
<hr>
<p>Google's <a href="https://deepmind.com/">DeepMind</a> group <a href="https://deepmind.com/blog/open-sourcing-sonnet/">released</a> their <a href="https://github.com/deepmind/sonnet">Sonnet</a> library, which is used in the code for their <a href="https://github.com/deepmind/learning-to-learn">learning to learn</a> and <a href="https://github.com/deepmind/dnc">Differentiable Neural Computer</a> projects. Sonnet isn't part of TensorFlow (it builds on top) but it is a Google project. Sonnet has a modular approach compared to that of <a href="https://github.com/torch/nn">Torch/NN</a>. Also they have a cool logo.</p>
<hr>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<hr>
<p>The previous APIs exist outside of TensorFlow. Let's now talk about APIs <em>inside</em> TensorFlow.</p>
<p>We'll trace the development of the stack of available APIs before looking at some code examples.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p>Python Frontend (op level)</p>
<hr>
<p>You can continue to use TensorFlow's low-level APIs.</p>
<p>There are ops, like <code>tf.matmul</code> and <code>tf.nn.relu</code>, which you might use to build a neural network architecture in full detail. To do really novel things, you may want this level of control. But you may also prefer to work with larger building blocks. The other other APIs below will mostly specialize in this kind of application.</p>
<p>There are also ops like <code>tf.image.decode_jpeg</code> (and many others) which may be necessary but don't necessarily relate to what is usually considered the architecture of neural networks. Some higher-level APIs wrap some of this functionality, but they usually stay close to the building of network architectures and the training of such networks once defined.</p>
<hr>
<p>Layers</p>
<hr>
<p>The "layer" abstraction is a natural way to think about deep neural network design.</p>
<p>This <code>layers</code> API provides a first higher level of abstraction over writing things out by individual ops. For example, <code>tf.layers.conv2d</code> implements a convolution layer that involves multiple individual ops.</p>
<hr>
<p><img alt="slim... shady?" src="img/slim_shady.png"></p>
<hr>
<p>You may have heard of <a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html">TF-Slim</a>. TF-Slim has a number of <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">components</a> but it looks like essentially we're seeing the following: <code>tf.contrib.slim.layers</code> became <code>tf.contrib.layers</code> becomes <code>tf.layers</code>.</p>
<p>Sergio Guadarrama, one of the TF-Slim authors, generously confirmed that TensorFlow is converging on a layers API and implementation along these lines, but warns that there can still be some differences. He points out that <code>tf.contrib.layers</code> have <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/arg_scope"><code>arg_scope</code></a>, while <code>tf.layers</code> don't.</p>
<p>Other parts of TF-Slim are likely also worth using, and there is a collection of <a href="https://github.com/tensorflow/models/tree/master/slim">models that use TF-Slim</a> in the <a href="https://github.com/tensorflow/models">TensorFlow models repository</a>.</p>
<p>Historical note: It looks like before calling them layers, TF-Slim overloaded the word "op" for their layer concept (see <a href="https://github.com/tensorflow/models/tree/master/inception/inception/slim">earlier documentation</a>).</p>
<p>TF-Slim is in the TensorFlow codebase as <code>tf.contrib.slim</code>.</p>
<hr>
<p><img alt="Keras" src="img/keras.jpg" height="100%"></p>
<hr>
<p><a href="https://keras.io/">Keras</a> was around before TensorFlow. It was always a high-level API for neural nets, originally running with <a href="http://www.deeplearning.net/software/theano/">Theano</a> as its backend.</p>
<p>After the release of TensorFlow, Keras moved to also work with TensorFlow as a backend. And now TensorFlow is absorbing at least some aspects of the Keras project into the TensorFlow codebase, though <a href="https://twitter.com/fchollet">Fran&#231;ois Chollet</a> seems likely to continue championing Keras as a very fine project in its own right. (See also his response to a <a href="https://www.quora.com/What-will-Keras-do-with-TensorFlow-Slim">Quora question</a> about any possible relationship between TF-Slim and Keras.)</p>
<p>Keras is appearing in the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/keras">tf.contrib.keras</a> and should move to <code>tf.keras</code> at TensorFlow 1.3, at which point it will also be possible to turn Keras models directly into Estimators, which we'll see next.</p>
<hr>
<p><img alt="scikit-learn logo" src="img/sklearn.png" height="100%"></p>
<hr>
<p>Distinct from TensorFlow, the <a href="http://scikit-learn.org/">scikit-learn</a> project makes a lot of machine learning models conveniently available in Python.</p>
<p>A key design element of scikit is that many different models offer the same simple API: they are all implemented as <em>estimators</em>. So regardless of whether the model is linear regression or a GBM (Gradient Boosting Machine), you get the same simple interface.</p>
<p>The estimators API started as <a href="https://github.com/tensorflow/skflow">skflow</a> ("scikit-flow") before moving into the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">tf.contrib.learn</a> ("TF Learn") and now the base estimator code is getting situated in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator">tf.estimator</a> and <a href="/20170521-navigating_tensorflow_estimator_documentation/">documentation</a> is accumulating.</p>
<hr>
<p>Estimators</p>
<hr>
<p>The Estimators API really just defines an interface. You can design models that fit into the Estimator system, or you can use Estimator-based models that are already "set up" to varying degrees.</p>
<hr>
<p>Canned Estimators</p>
<hr>
<p>Canned estimators are concrete pre-defined models that follow the estimator conventions. Currently there are a bunch right in <code>tf.contrib.learn</code>, such as <code>LinearRegressor</code> and <code>DNNClassifier</code>. There are some elsewhere. For example, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/g3doc/kmeans.md">kmeans</a> is in <code>tf.contrib.factorization</code>. It isn't clear to me exactly where all the canned estimators will eventually settle down in the API; some may eventually move out of <code>contrib</code>.</p>
<p>"Canned Estimators" are a lot like scikit-learn in the level at which they make functionality available to programmers.</p>
<hr>
<ul>
<li>automatic checkpoints</li>
<li>automatic logging</li>
<li>separate train/eval/pred</li>
<li>easily train distributed</li>
</ul>
<hr>
<p>What do you get with Estimators? A bunch of things!</p>
<hr>
<p><img alt="data pipeline" src="img/tf_data_pipeline.gif"></p>
<hr>
<p>One more thing: loading data. A lot of people have gotten hung up on the complicated multi-thread, multi-queue, queue-runner design that TensorFlow has espoused for loading data.</p>
<hr>
<p>Datasets</p>
<hr>
<p>TensorFlow developers <a href="https://github.com/tensorflow/tensorflow/issues/7951">listened</a>, and they came up with <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data">Datasets</a>, which is a new API that wraps all the complexity in a pretty nice interface.</p>
<p>This is new as of about a week ago.</p>
<hr>
<p><img alt="latest diagram" src="img/io17_tiers.png"></p>
<hr>
<p>So this is the latest diagram from Google I/O. It adds XLA, which I won't talk about.</p>
<p>(Image from <a href="https://www.youtube.com/watch?v=OzAdKMPgUt4">TensorFlow Frontiers</a> at Google I/O '17.)</p>
<hr>
<p>code!</p>
<hr>
<p>Let's look at some code, shall we?</p>
<p>Go to a notebook (<a href="https://github.com/ajschumacher/tf_api_20170524/blob/master/APIs_start.ipynb">start</a>/<a href="https://github.com/ajschumacher/tf_api_20170524/blob/master/APIs_finish.ipynb">finish</a>) in the <a href="https://github.com/ajschumacher/tf_api_20170524">tf_api_20170524</a> repo.</p>
<hr>
<p>more</p>
<hr>
<p>For lots more, there are plenty of places you can go.</p>
<p>My <a href="/">blog</a> is one place.</p>
<p><a href="https://www.tensorflow.org/">TensorFlow</a>'s documentation is another.</p>
<p>The latest <a href="https://www.youtube.com/watch?v=OzAdKMPgUt4">video</a>s from Google is another. The I/O 2017 videos include a few TensorFlow ones, and there's also still the February TensorFlow DevFest video collection.</p>
<hr>
<p><img alt="Deep Learning with Python book cover" src="img/chollet_dlp.png"></p>
<hr>
<p>I want to also mention this book by the author of Keras: <a href="https://www.manning.com/books/deep-learning-with-python">Deep Learning with Python</a>. It's pretty good. If you're looking to understand deep learning, this is now the book I recommend. It's currently in pre-release, and the available chapters are pretty great already. It focuses on Keras rather than TensorFlow per se, but especially since Keras is now <em>in</em> TensorFlow, any Keras thing is also a TensorFlow thing.</p>
<p>The first chapter is free.</p>
<hr>
<p><img alt="necessary qualities" src="img/qualities.png"></p>
<hr>
<p>I want to close with a thought that is not so much about programming. This is a question I got on LinkedIn, and it made me think.</p>
<p>It is eventually necessary to have some proficiency with particular tools, but the qualities I find most important are not really particular tool skills.</p>
<p>What problem do you need to solve? Can you identify it, articulate it, frame it in a way that makes it tractable? Can you work with what's available to come up with a solution?</p>
<p>I hope knowing more about TensorFlow is helpful to you as you go forward with your work, but I also think it's important to remember that TensorFlow is just a tool. Your success comes from what you do.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170524-more_and_better_new_tensorflow_apis/</link>
<guid>http://planspace.org/20170524-more_and_better_new_tensorflow_apis/</guid>
<pubDate>Wed, 24 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Navigating TensorFlow Estimator Documentation</title>
<description><![CDATA[

<p>The TensorFlow documentation keeps improving, but it can still be hard to find what you're looking for. Here's a way of organizing <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">Estimator</a>s or <a href="https://www.tensorflow.org/api_guides/python/contrib.learn"><code>tf.contrib.learn</code></a> documentation, from high-level to low-level:</p>
<ul>
<li>Using Estimators (already made or "canned")<ul>
<li><a href="https://www.tensorflow.org/get_started/tflearn"><code>DNNClassifier</code> on iris dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide"><code>LinearClassifier</code> (logistic) on census income dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide_and_deep"><code>DNNLinearCombinedClassifier</code> (extends census income example)</a></li>
</ul>
</li>
<li>Aspects of using Estimators<ul>
<li><a href="https://www.tensorflow.org/tutorials/linear">Feature columns (in context of linear models)</a></li>
<li><a href="https://www.tensorflow.org/get_started/input_fn">Using <code>input_fn</code>; <code>DNNRegressor</code> on Boston housing dataset</a></li>
<li><a href="https://www.tensorflow.org/get_started/monitors">Logging etc. with <code>ValidationMonitor</code> (extends iris example)</a></li>
</ul>
</li>
<li>Making your own Estimator<ul>
<li><a href="https://www.tensorflow.org/extend/estimators">With <code>layers</code> in a <code>model_fn</code> on the abalones dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/layers">With <code>layers</code> in a <code>model_fn</code> on MNIST</a></li>
</ul>
</li>
<li>Lower-level API than Estimators<ul>
<li><a href="https://www.tensorflow.org/get_started/mnist/pros#build_a_multilayer_convolutional_network">Use the lower-level API to make a system for MNIST</a></li>
</ul>
</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</link>
<guid>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</guid>
<pubDate>Sun, 21 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Code Reading Questions at OSCON</title>
<description><![CDATA[

<p><a href="https://www.oreilly.com/">O'Reilly</a>'s 2017 <a href="https://conferences.oreilly.com/oscon/">OSCON</a> had a "<a href="https://conferences.oreilly.com/oscon/oscon-tx/public/content/game">code game</a>" with ten questions covering nine different languages. It was supposed to get people engaged in the expo hall, but the quiz-like gamification content reminded me of my old <a href="/20150616-code_reading_question/">code reading question</a> idea. The questions are available in a <a href="https://cdn.oreillystatic.com/en/assets/1/event/214/oscon2017_code_game.pdf">PDF</a> (<a href="oscon2017_code_game.pdf">mirror</a>). I'll put the questions here as well.</p>
<hr>
<h3>1. Rust</h3>
<pre><code class="language-rust">use std::collections::HashSet;
use std::io::{BufRead, Result};

fn f&lt;I: BufRead&gt;(input: &amp;mut I) -&gt; Result&lt;usize&gt; {
    Ok(input.lines()
       .map(|r| r.expect(&#8220;ara ara&#8221;))
       .flat_map(|l| l.split_whitespace()
                     .map(str::to_owned)
                     .collect::&lt;Vec&lt;_&gt;&gt;())
       .collect::&lt;HashSet&lt;_&gt;&gt;()
       .len())
}</code></pre>

<p>This function reads input. What else does it do?</p>
<ul>
<li>A. Counts the number of white-space-separated &#8220;words&#8221;</li>
<li>B. Counts the number of distinct &#8220;words&#8221;</li>
<li>C. Finds the &#8220;word&#8221; that appears most frequently</li>
<li>D. Finds the longest line</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>2. JavaScript</h3>
<pre><code class="language-javascript">function search(values, target) {
  for(var i = 0; i &lt; values.length; ++i){
    if (values[i] == target) { return i; }
  }
  return -1;
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Depth-first search</li>
<li>B. Binary search</li>
<li>C. Merge search</li>
<li>D. Linear search</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>3. Go</h3>
<pre><code class="language-go">func function(s []float64) float64 {
        var sum float64 = 0.0
        for _, n := range s {
                sum += n
        }
        return sum / float64(len(s))
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Sums the contents of a slice</li>
<li>B. Finds the maximum value in a slice</li>
<li>C. Averages the contents of a slice</li>
<li>D. Appends values to a slice</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>4. Perl 5</h3>
<pre><code class="language-perl">sub mystery {
    return @_ if @_ &lt; 2;
    my $p = pop;
    mystery(grep $_ &lt; $p, @_), $p,
    mystery(grep $_ &gt;= $p, @_);
}</code></pre>

<p>What does the mystery subroutine do?</p>
<ul>
<li>A. Binary search</li>
<li>B. Merge sort</li>
<li>C. Removes items that are too large or too small</li>
<li>D. Quick sort</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>5. Java 8</h3>
<pre><code class="language-java">static void function(int[] ar)
 {
   Random rnd = ThreadLocalRandom.current();
   for (int i = ar.length - 1; i &gt; 0; i--)
   {
     int index = rnd.nextInt(i + 1);
     int a = ar[index];
     ar[index] = ar[i];
     ar[i] = a;
   }
 }</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Merge sort</li>
<li>B. Shuffle</li>
<li>C. Increases size of array</li>
<li>D. Decreases size of array</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>6. Ruby</h3>
<pre><code class="language-ruby">def f(hash)
  prs = hash.inject({}) do |hsh, pr|
    k, v = yield pr
    hsh.merge(k =&gt; v)
  end
  Hash[prs]
end</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Reverses an array</li>
<li>B. Administers a booster shot</li>
<li>C. Enters a freeway safely</li>
<li>D. Transforms a hash</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>7. Python</h3>
<pre><code class="language-python">def function(list):
     return [x for x in list if x == x[::-1]]</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Finds and returns all palindromes within the given list</li>
<li>B. Reverses all strings in the given list</li>
<li>C. Swaps the first and last letter in each word in the given list</li>
<li>D. Returns a list of anagrams for each word in the given list</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>8. Scala</h3>
<pre><code class="language-scala">object Op {
  val r1: Regex = &#8220;&#8221;&#8221;([^aeiouAEIOU\d\s]+)([^\d\s]*)$&#8221;&#8221;&#8221;.r
  val r2: Regex = &#8220;&#8221;&#8221;[aeiouAEIOU][^\d\s]*$&#8221;&#8221;&#8221;.r
  val s1:String = &#8220;\u0061\u0079&#8221;
  val s2:String = &#8220;\u0077&#8221; + s1
  def apply(s: String): String = {
    s.toList match {
      case Nil =&gt; &#8220;&#8221;
      case _ =&gt; s match {
        case r1(c, r) =&gt; r ++ c ++ s1 case r2(_*) =&gt; s ++ s2
        case _ =&gt; throw new
RuntimeException(&#8220;Sorry&#8221;)
      }
    }
  }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Converts a given String to a JavaScript-based String</li>
<li>B. Converts a Unix/MacOSX String format into a Windows String format</li>
<li>C. Converts a word into the children&#8217;s language equivalent called &#8220;Pig Latin&#8221;</li>
<li>D. Converts a given String to IPV6 format since IP numbers are running out</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>9. Swift</h3>
<pre><code class="language-swift">import Foundation

let i = &#8220;Hell&#248;, Swift&#8221;

let t = i.precomposedStringWithCanonicalMapping

let c = t.utf8.map({UnicodeScalar($0+2)})
let j = i.utf8.map({UnicodeScalar($0+1)}).count / 2

var d = String(repeating: String(describing: c[j]), count: j)

d.append(Character(&#8220;&#127462;&#127482;&#127482;&#127480;&#8221;))

let result = &#8220;\(d): \(d.characters.count)&#8221;</code></pre>

<p>What is the value of &#8220;result&#8221;?</p>
<ul>
<li>A. ......&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>B. ......&#127462;&#127482;&#127482;&#127480;: 8</li>
<li>C. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>D. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 8</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>10. JavaScript</h3>
<pre><code class="language-javascript">function thing (n) {
    for (var i = 0; i &lt; n; i++) {
        setTimeout(function () {console.log(i);}, 0);
    }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Prints numbers 0 through n</li>
<li>B. Prints n n time</li>
<li>C. Prints 0 n times</li>
<li>D. Prints nothing</li>
</ul>
<!-- Correct: B -->

<hr>
<p>View HTML source to see the answers.</p>
<p>I tried to keep the above close to what appeared on the physical
cards. I haven't tried to correct the little mistakes and bizarre
indentation throughout.</p>
<p>It's fun!</p>    
    ]]></description>
<link>http://planspace.org/20170517-code_reading_questions_at_oscon/</link>
<guid>http://planspace.org/20170517-code_reading_questions_at_oscon/</guid>
<pubDate>Wed, 17 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Caffe and TensorFlow at Deep Learning Analytics</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">presentation</a> given Wednesday May 10, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a> as part of <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/full/tensorflow-day">TensorFlow Day</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<p>Thank you!</p>
<hr>
<p>Thanks to all the OSCON organizers and participants, and especially to the Google folks organizing TensorFlow Day, and everyone coming out for TensorFlow events! What fun!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png" width="100%"></p>
<hr>
<p>I work at a company called <a href="https://www.deeplearninganalytics.com/">Deep Learning Analytics</a>, or DLA.</p>
<p>The advantage of a name like Deep Learning Analytics is that, assuming you've heard of deep learning, you immediately know something about what we do.</p>
<hr>
<p>deeplearninganalytics.com</p>
<hr>
<p>The disadvantage of a name like Deep Learning Analytics is that our URL is really long.</p>
<hr>
<p><img alt="some of the Deep Learning Analytics team" src="img/dla_team.jpg" width="100%"></p>
<hr>
<p>This is most of us at our Arlington location, just outside DC. We do a mix of government contracting and commercial work.</p>
<hr>
<p><img alt="DarLA" src="img/darla.png" height="100%"></p>
<hr>
<p>We have a cute robot mascot called DarLA.</p>
<p>DarLA the robot could be a metaphor for any machine learning system. You have to build the arms and the legs, and that's a good deal of work. There's also generally a lot of work in getting data to train the system with. But you also need a brain, and that's inevitably sort of the interesting part.</p>
<p>So what do you use for the brain?</p>
<hr>
<p>warning: opinions</p>
<hr>
<p>I'm going to comment on a few aspects of a few systems, based on what I've seen. Everybody involved in this space is incredibly smart, and probably correct in some ways even when I disagree.</p>
<hr>
<ul>
<li>Caffe?</li>
<li>TensorFlow?</li>
</ul>
<hr>
<p>The two systems I'll talk about are Caffe and TensorFlow.</p>
<p>I'm aware of Caffe 2 but I don't have anything to say about it today, aside from that it seems a lot like Caffe.</p>
<hr>
<p><img src="img/nvidia_comparison.jpg" alt="DL framework comparison" height="100%"></p>
<hr>
<p>This is a picture of a slide from a talk some NVIDIA folks gave a couple weeks ago. I will now agree and disagree with it.</p>
<p>Caffe easy to start? I disagree, and I'll talk about why.</p>
<p>Caffe easy to develop? I disagree, and I'll talk about why.</p>
<p>Caffe limited capability? I agree, and I'll talk about why.</p>
<p>TensorFlow portable and nice documentation? I mostly agree.</p>
<p>TensorFlow slow? I think to the extent this is true, it doesn't matter, and I'll talk about some related considerations.</p>
<p>My colleagues have done some work in Torch as well, but I'm not going to talk about these others.</p>
<hr>
<p>Easy to start?</p>
<hr>
<p>So which framework makes it easier to get started?</p>
<hr>
<p><code>pip install:</code><br>
&#10008; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>This is a convenience, but it's also really nice. There are advantages to compiling things yourself too. But for quickly getting started, this is great.</p>
<p>Even if you're happy to compile an optimized build for your system, having easy installation options available can be nice for setting up test environments, for example.</p>
<hr>
<p>"Easy" models:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>By "easy" models, I mean models that are either completely pre-specified and pre-trained, or very easily specified by a very high-level API.</p>
<p>The Caffe model zoo might have been the first recognized collection of pre-trained models and model architecture specifications. One or two years ago, it might have been the best way to start working with an interesting deep net without going through the whole training process yourself.</p>
<p>Now, though, I think TensorFlow is really passing Caffe on this. You can get a number of pre-trained models already with just one line of Python, using <code>tf.contrib.keras</code>. Hopefully others will be able to match that ease of redistribution.</p>
<p>Further, TensorFlow's "canned models" in <code>tf.contrib.learn</code> provide a scikit-learn-like interface that Caffe never attempts.</p>
<hr>
<p>Easy to develop?</p>
<hr>
<p>What about ease of development?</p>
<hr>
<p>API level:</p>
<ul>
<li>high? middle? low?</li>
</ul>
<hr>
<p>For development, it matters what level you want to work at. You can do a lot quickly if you can stay at a high level. But sometimes you need to get low-level to control details or do something slightly non-standard.</p>
<p>I think TensorFlow now is doing a better job than Caffe of providing API surfaces across a range of levels.</p>
<p>Caffe's API, at the protocol buffer text format level you have to eventually get to, is sort of a middle-low level. The vocabulary is more limited than what you get with TensorFlow ops.</p>
<p>You can build higher-level APIs with Caffe, and DLA has an in-house library that we use to make Caffe easier to work with.</p>
<p>TensorFlow has Keras.</p>
<hr>
<p>Python integration:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>Both Caffe and TensorFlow are written with C++, but interfacing with Caffe can feel like interfacing with the separate free-standing program, whereas the TensorFlow interface is seamless.</p>
<p>If there's a way to use Caffe without at some point writing protobuf text files to disk and then having Caffe read them, I don't know it.</p>
<hr>
<p>modular design</p>
<hr>
<p>I'll show an example of a Caffe layer in order to talk about some of the design issues.</p>
<hr>
<pre><code>layer {name: "data"
       type: "Data"
       top: "data"
       top: "label"
       transform_param {crop_size: 227}
       data_param {source: "train_lmdb_path"
                   batch_size: 256
                   backend: LMDB}}</code></pre>

<hr>
<p>I condensed a Caffe data layer spec a bit.</p>
<p>Caffe likes reading data from LMDB databases. So this layer includes a path to a location on disk, where an LMDB database of a particular form needs to be. So this is a layer that reads from disk.</p>
<p>But wait - this layer spec also specifies the training batch size.</p>
<p>Does it do anything else? What's that "<code>crop_size</code>"?</p>
<p>This layer also implicitly takes random crops from the images it reads, 227 pixels by 227 pixels. So the images in the database should be at least that big.</p>
<p>That's a lot happening in one layer!</p>
<p>On the one hand, this is pretty neat. Caffe does a lot for you. And once you've made a lot of choices, you can optimize the implementation, which is part of how Caffe gets pretty fast.</p>
<p>On the other hand, Caffe is making a lot of decisions for us, and it isn't particularly happy if we want to change those decisions. Caffe can feel more like final application code than framework code.</p>
<p>What if we want non-square crops? What if we want to introduce random distortions? Or if we want to read data from JPG files instead of LMDB? None of these are supported by this LMDB layer, and these changes aren't easily composable. We can switch to a memory data layer to get training batches in, but then we have to independently implement cropping if we want it, and so on.</p>
<p>In contrast, TensorFlow doesn't have such tight integrations. The closest analog in TensorFlow might be reading from TFRecords files, but that functionality is isolated, and doesn't bundle in choices about batch size or cropping.</p>
<p>You get a more modular system with TensorFlow, so you shouldn't find yourself having to pick apart tightly-coupled functionality later on.</p>
<p>At the same time, TensorFlow does include the higher-level APIs mentioned earlier, so it isn't completely a choice between low-level and high-level either.</p>
<hr>
<p>multi-GPU</p>
<hr>
<p>How do we get multi-GPU systems?</p>
<hr>
<p><img src="img/alexnet.png" alt="AlexNet diagram" width="100%"></p>
<hr>
<p>This is the diagram from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>, the 2012 paper that popularized deep learning. It has over 11,000 citations now.</p>
<p>This was a multi-GPU implementation!</p>
<p>I want to point out how crazy it is. Alex Krizhevsky split individual convolutional layers between two GPUs. Whoa.</p>
<hr>
<p>Caffe</p>
<hr>
<p>So how do we do multi-GPU with Caffe?</p>
<hr>
<p><img alt="multi-GPU Caffe docs" src="img/caffe_multi.png"></p>
<hr>
<p>For Caffe, there's this page of documentation. As far as I know there is no link to this page of documentation. I found it <a href="https://github.com/BVLC/caffe/blob/master/docs/multigpu.md">via</a> the GitHub repo that backs the website.</p>
<p>There are some limitations in Caffe's multi-GPU support.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>Well, how do we do multi-GPU with TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same way we do single-GPU! The API stays the same, and you just say where you want things to run. This is also more flexible in terms of the ways you can parallelize things. You could have two copies of a model on two GPUs, or one really big model across two GPUs, for example.</p>
<hr>
<p><img alt="Jenn" src="img/jenn.jpg"></p>
<hr>
<p>My colleague Jenn parallelized some GAN (Generative Adversarial Network) code to use multiple GPUs. I was super curious to see what she had to do. She was really puzzled about why I was so curious, because TensorFlow made it so easy.</p>
<p>Also, if you're curious about GANs, you should really check out Jenn's <a href="https://github.com/jennsleeman/introtogans_dcdatascience_2017">talk on the subject</a>.</p>
<hr>
<p>multi-machine</p>
<hr>
<p>What if we want to train across multiple machines now?</p>
<hr>
<p>Caffe</p>
<hr>
<p>How about Caffe?</p>
<hr>
<p><img src="img/trivial_mpi.png" alt="trivial MPI" width="100%"></p>
<hr>
<p>Caffe doesn't do it, but Yangqing helpfully <a href="https://github.com/BVLC/caffe/issues/876">points out</a> that you could make your own distributed Caffe system by adding MPI. Some people have actually <a href="https://software.intel.com/en-us/articles/caffe-training-on-multi-node-distributed-memory-systems-based-on-intel-xeon-processor-e5">done</a> this. I'm not sure it's trivial.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>And TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same API! You just put computation on various machines!</p>
<p>There is a little bit more to it than that, but I think it's easier than MPI.</p>
<hr>
<p><img alt="infrastructure" src="img/infr.png"></p>
<hr>
<p>Another thing we're excited about is possibilities for scaling out our work with TensorFlow. OpenAI <a href="https://blog.openai.com/infrastructure-for-deep-learning/">published</a> information and code on their system, and it seems like a pretty great model.</p>
<hr>
<p>responsiveness</p>
<hr>
<p>I'm going to talk about one anecdotal measures of developer responsiveness. It isn't totally fair, because Caffe development is basically over as Caffe 2 emerges, but it may still be interesting.</p>
<p>I occasionally submit very minor pull requests, for things like ensuring a variable is the correct type, or fixing typos in documentation. These are simple PRs that don't generally require discussion or alteration; they just get merged in.</p>
<p>So how quickly does that happen with Caffe and with TensorFlow?</p>
<hr>
<p>merge time:</p>
<ul>
<li>TensorFlow: 12 hours</li>
<li>Caffe: 329 days</li>
</ul>
<hr>
<p>TensorFlow developers seem more responsive.</p>
<p>(<a href="https://github.com/tensorflow/tensorflow/pull/9451">TensorFlow PR</a>)</p>
<p>(<a href="https://github.com/BVLC/caffe/pull/4182">Caffe PR</a>)</p>
<hr>
<p><img alt="TensorFlow logo" src="img/tf.jpg" height="100%"></p>
<hr>
<p>And I think that, along with many other things, bodes well for the future of TensorFlow.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>    
    ]]></description>
<link>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</link>
<guid>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</guid>
<pubDate>Wed, 10 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Building TensorFlow Systems from Components</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">workshop</a> given Tuesday May 9, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<h3>Workshop participant materials</h3>
<p>These are things you can use during the workshop. Use them when you need them.</p>
<ul>
<li>Focus one: <em>getting data in</em><ul>
<li>Download notebook: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
<li><a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a></li>
</ul>
</li>
<li>Focus two: <em>distributed programs</em><ul>
<li><a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/"><code>mapreduce_with_tensorflow</code> code and data</a></li>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
</li>
<li>Focus three: <em>high-level ML APIs</em><ul>
<li>Download notebook: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
</li>
</ul>
<p>The whole slide deck and everything follows, and it's long.</p>
<hr>
<p>Thank you!</p>
<hr>
<p>Before starting, I want to take a moment to notice how great it is to have a conference. Thank you for being here! We are all really lucky. If this isn't nice, I don't know what is.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<p>If you want to participate in the workshop, <em>really go</em> to <code>planspace.org</code> and pull up these materials!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work for a company called Deep Learning Analytics.</p>
<p>I'll talk more about DLA tomorrow <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">as part of TensorFlow Day</a>. Hope to see you there as well!</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>You'll want to have TensorFlow version 1.1 installed. TensorFlow 1.1 was only officially released on April 20, and the API really has changed.</p>
<p>The <em>core documented</em> API, mostly the low-level API, is frozen as of TensorFlow 1.0, but a lot of higher-level stuff is still changing. TensorFlow 1.1 brings in some really neat stuff, like Keras, which we'll use later.</p>
<hr>
<p><img src="img/hello_tensorflow.png" alt="Hello, TensorFlow! on O'Reilly" height="100%"></p>
<hr>
<p>As an example: I wrote <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> about a year ago. It used TensorFlow 0.8.</p>
<p>After TensorFlow 1.0 came out, I went and looked at the summary code block at the end of "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>". It had 15 lines of TensorFlow code. Of that, 5 lines no longer worked. I had to update 33% of the lines of my simple TensorFlow example code because of API changes.</p>
<hr>
<p><img alt="one third" src="img/one_third.jpg"></p>
<hr>
<p>I was surprised that it was one third.</p>
<p>I have an <a href="/20160620-hello_tensorflow_just_the_code/">updated code snippet</a>, but we're not doing "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>" today.</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>So please have TensorFlow 1.1 installed, is the point of that whole story.</p>
<p>The stable bits of TensorFlow really are stable, but there's a lot of exciting new stuff, and I don't want you to miss out today!</p>
<hr>
<p>THE BIG IDEA</p>
<hr>
<p>Okay! What is this workshop about?</p>
<hr>
<p>use what you need</p>
<hr>
<p>TensorFlow works for you! Use it where it does things that help you.</p>
<p>TensorFlow is a tool. It's a very general tool, on the one hand. It's also a tool with lots of pieces. You can use <a href="/20170312-use_only_what_you_need_from_tensorflow/">some</a> of the pieces. Or you can decide not to use TensorFlow altogether!</p>
<p>We'll look at several specific aspects of TensorFlow. Maybe you'll want to use them. Maybe you won't. The hope is that you'll be more comfortable with what's available and able to decide what to apply when.</p>
<p>If you like <a href="https://www.infoq.com/presentations/Simple-Made-Easy">Rich Hickey words</a>, maybe I'm trying to <em>decomplect</em> the strands within TensorFlow so they can be understood individually.</p>
<hr>
<p>THE PLAN</p>
<hr>
<p>Here's the plan for this workshop.</p>
<hr>
<ul>
<li>one short work</li>
<li>three longer works</li>
</ul>
<hr>
<p>I'll talk about some things, but the hope of the workshop is that you do some good work.</p>
<p>I hope that you don't finish all the things you think of, but want to keep working after the workshop is over.</p>
<p>Also, there's a break from 10:30 to 11:00 on the official schedule, but I don't really like that. If we happen to be working during the break, fine. Feel free to take a break whenever you need a break.</p>
<hr>
<p>Do what you want!</p>
<hr>
<p>TensorFlow is really big, and not every part will be interesting or important to you.</p>
<p>I'll have very specific things for you to work on for each of the works, but if you think of something better to do, you better do it!</p>
<hr>
<p>Intro by Logo</p>
<hr>
<p>To get creative juices flowing a little, let's explore some logo history.</p>
<hr>
<p><img alt="LOGO turle" src="img/logo_turtle.jpg" height="100%"></p>
<hr>
<p>Not that sort of <a href="https://en.wikipedia.org/wiki/Logo_(programming_language)">Logo</a>!</p>
<p>Fine to think about, though.</p>
<hr>
<p><img alt="G&#246;del, Escher, Bach" src="img/geb.jpg" height="100%"></p>
<hr>
<p>In the beginning, there was <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">G&#246;del, Escher, Bach: An Eternal Golden Braid</a> by Douglas Hofstadter.</p>
<p>This "metaphorical fugue on minds and machines in the spirit of Lewis Carroll" includes the <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> idea and related discussion of consciousness and formal systems. Hofstadter's influence can be seen, for example, <a href="https://cs.illinois.edu/news/strange-loop-conference">in the name</a> of the <a href="http://www.thestrangeloop.com/">Strange Loop</a> tech conference.</p>
<p>It seems likely that many people working in artificial intelligence and machine learning have encountered G&#246;del, Escher, Bach.</p>
<hr>
<p><img src="img/chernin.jpg" alt="Chernin Entertainment" height="100%"></p>
<hr>
<p>Of course, there's also <a href="https://en.wikipedia.org/wiki/Chernin_Entertainment">Chernin Entertainment</a>, the production company.</p>
<p>So we shouldn't rule out the possibility that Google engineers are fans of <a href="http://www.imdb.com/company/co0286257/">Chernin's work</a>. <a href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden Figures</a> is quite good. And I guess a lot of people like <a href="https://en.wikipedia.org/wiki/New_Girl">New Girl</a>?</p>
<hr>
<p><img alt="TensorFlow logo - old?" src="img/tf-old-big.png" height="100%"></p>
<hr>
<p>In any event, somehow we get to this TensorFlow logo:</p>
<p>If you look carefully, does it seem like the right side of the "T" view is too short?</p>
<hr>
<p><img alt="Issue 1922" src="img/issue_1922.png"></p>
<hr>
<p>This very serious concern appears as <a href="https://github.com/tensorflow/tensorflow/issues/1922">issue #1922</a> on the TensorFlow github, and <a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XhO1sqp4l4g">on the TensorFlow mailing list</a> complete with ASCII art illustration.</p>
<p>The consensus response seemed to be some variant of "won't fix" (it wouldn't look as cool, anyway) until...</p>
<hr>
<p><img alt="TensorFlow logo - new?" src="img/tf-new.jpg" height="100%"></p>
<hr>
<p>As of around the 1.0 release of TensorFlow, which was around the first <a href="https://events.withgoogle.com/tensorflow-dev-summit/">TensorFlow Dev Summit</a>, this logo variant seems to be in vogue. It removes the (possibly contentious) shadows, and adds additional imagery in the background.</p>
<p>I want to suggest that the image in the background can be a kind of Rorschach test for at least three ways you might be thinking about TensorFlow.</p>
<ul>
<li>Is it a diagram of connected neurons? Maybe you're interested in TensorFlow because you want to make neural networks.</li>
<li>Is it a diagram of a computation graph? Maybe you're interested in TensorFlow for general calculation, possibly using GPUs.</li>
<li>Is it a diagram of multiple computers in a distributed system? Maybe you're interested in TensorFlow for its distributed capabilities.</li>
</ul>
<p>There can be overlap among those interpretations as well, but I hope the point is not lost that TensorFlow can be different things to different people.</p>
<hr>
<p>short work</p>
<hr>
<p>I want to get you thinking about systems without further restriction. The idea is to imagine and start fleshing out a system that might involve TensorFlow.</p>
<p>Later on we'll follow the usual workshop pattern of showing you something you can do and having you do it. But it's much more realistic and interesting to start from something you want to do and then try to figure out how to do it. So let's start from the big picture.</p>
<hr>
<p>draw a system!</p>
<ul>
<li>block diagram</li>
<li>add detail</li>
<li>pseudocode?</li>
</ul>
<hr>
<p>You can draw a system you've already made, or something you're making, or something you'd like to make. It could be something you've heard about, or something totally unique.</p>
<p>You don't have to know how to build everything in the system. You don't need to know how TensorFlow fits in. Feel free to draw what you <em>wish</em> TensorFlow might let you do.</p>
<p>Keep adding more detail. If you get everything laid out, start to think about what functions or objects you might need to have, and start putting pseudocode together.</p>
<hr>
<p>short work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p>Let's do hard AI!</p>
<hr>
<p>I'm going to walk through an example and talk about how TensorFlow can fit in.</p>
<p>Doing hard AI, or Artificial General Intelligence (AGI), is intended as a joke, but it's increasingly less so, with Google DeepMind and OpenAI both explicitly working on getting to AGI.</p>
<hr>
<p>(build)</p>
<hr>
<p>I've got 35 slides of sketches, so they're in <a href="build.pdf">a separate PDF</a> to go through.</p>
<hr>
<p><img alt="system" src="img/system.jpg"></p>
<hr>
<p>Here's the final system that we built to.</p>
<p>We're not going to do everything in there today.</p>
<hr>
<ul>
<li>getting data in</li>
<li>distributed programs</li>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>These are the three focuses for today.</p>
<p>The <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">original description</a> for this workshop also mentioned serving, but I'm not covering it today. Sorry. Not enough time.</p>
<hr>
<p>What even is TensorFlow?</p>
<hr>
<p>Here's one way to think about TensorFlow.</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>The beating heart of TensorFlow is the Distributed Execution Engine, or runtime.</p>
<p>One way to think of it is as a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">virtual machine</a> whose language is the TensorFlow graph.</p>
<p>That core is written in C++, but lots of TensorFlow functionality lives in the frontends. In particular, there's a lot in the Python frontend.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<p>Python is the richest frontend for TensorFlow. It's what we'll use today.</p>
<p>You should remember that not everything in the Python TensorFlow API touches the graph. Some parts are just Python.</p>
<hr>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>R has an unofficial <a href="https://rstudio.github.io/tensorflow/">TensorFlow API</a> which is kind of interesting in that it just wraps the Python API. It's really more like an R API to the Python API to TensorFlow. So when you use it, you write R, but Python runs. This is not how TensorFlow encourages languages to implement TensorFlow APIs, but it's out there.</p>
<hr>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<p>The way TensorFlow encourages API development is via TensorFlow's C bindings.</p>
<hr>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<p>You could use C++, of course.</p>
<hr>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<p>Also there's Java.</p>
<hr>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<p>And there's Go.</p>
<hr>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<p>And there's Rust.</p>
<hr>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<p>And there's even Haskell!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Currently, languages other than Python have TensorFlow support that is very close to the runtime. Basically make your ops and run them.</p>
<p>This is likely to be enough support to deploy a TensorFlow system in whatever language you like, but if you're developing and training systems you probably still want to use Python.</p>
<hr>
<p>graph or not graph?</p>
<hr>
<p>So this is a distinction to think about: Am I using the TensorFlow graph, or not?</p>
<hr>
<ul>
<li>getting data in</li>
</ul>
<hr>
<p>So here we are at the first focus area.</p>
<p>We'll do two sub-parts.</p>
<hr>
<ul>
<li>getting data in<ul>
<li>to the graph</li>
<li>TFRecords</li>
</ul>
</li>
</ul>
<hr>
<p>First, a quick review of the the graph and putting data into it.</p>
<p>Second, a bit about TensorFlow's TFRecords format.</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
</ul>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>work with data!</p>
<ul>
<li>your own system?</li>
<li>planspace.org: <code>mystery.tfrecords</code></li>
<li>move to graph</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Almost certainly there's some data input that needs to happen. How are you going to read that data, and possibly get it into TensorFlow?</p>
<p>If you want to stay really close to the TensorFlow stuff just demonstrated, here's a fun little puzzle for you: What's in <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a>?</p>
<p>That could be hard, or it could be easy. If you want to extend it, migrate the reading and parsing of the TFRecords file into the TensorFlow graph. The demonstration in the notebook worked with TFRecords/Examples without using the TensorFlow graph. The links on the <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a> page can help with this.</p>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="data" src="img/data_data.jpg" width="100%"></p>
<hr>
<p>As a wrap-up: Working directly with data, trying to get it into the right shape, cleaning it, etc., may not be the most fun, but it's got to be done. Here's a horrible pie chart <a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/">from some guy on Forbes</a>, the point of which is that people spend a good deal of time fighting with data.</p>
<hr>
<ul>
<li>distributed programs</li>
</ul>
<hr>
<p>We arrive at the second focus area. TensorFlow has some pretty wicked distributed computing capabilities.</p>
<hr>
<ul>
<li>distributed programs<ul>
<li>command-line arguments</li>
<li>MapReduce example</li>
</ul>
</li>
</ul>
<hr>
<p>I'm putting a bit about command-line arguments in here because I think it's interesting. It doesn't necessarily fit in with distributed computing, although you might well have a distributed program that takes command-line arguments. Google Cloud ML can use command-line arguments for hyperparameters, for example.</p>
<p>Then we'll get to a real distributed example, in which we implement a distributed MapReduce word count in 50 lines of Python TensorFlow code.</p>
<hr>
<p>command-line arguments</p>
<hr>
<p>So let's take a look at some ways to do command-line arguments.</p>
<p>This section comes from the post <a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a>.</p>
<hr>
<pre><code class="language-bash">$ python script.py --color red
a red flower</code></pre>

<hr>
<p>I'll show eight variants that all do the same thing. You provide a <code>--color</code> argument, and it outputs (in text) a flower of that color.</p>
<hr>
<pre><code class="language-python">import sys

def main():
    assert sys.argv[1] == '--color'
    print('a {} flower'.format(sys.argv[2]))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This is a bare-bones <code>sys.argv</code> method. Arguments become elements of the <code>sys.argv</code> list, and can be accessed as such. This has limitations when arguments get more complicated.</p>
<hr>
<pre><code class="language-python">import sys
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main():
    flags.FLAGS._parse_flags(args=sys.argv[1:])
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This <code>FLAGS</code> API is <a href="/20170313-tensorflow_use_of_google_technologies/">familiar to Googlers</a>, I think. It's interesting to me where some Google-internal things peak out from the corners of TensorFlow.</p>
<hr>
<pre><code class="language-python">import sys
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main():
    gflags.FLAGS(sys.argv)
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>You could also install the <code>gflags</code> module, which works much the same way.</p>
<hr>
<pre><code class="language-python">import sys
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args(sys.argv[1:])
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>Here's Python's own standard <a href="https://docs.python.org/3/library/argparse.html"><code>argparse</code></a>, set up to mimic the <code>gflags</code> example, still using <code>sys.argv</code> explicitly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

def main(args):
    assert args[1] == '--color'
    print('a {} flower'.format(args[2]))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>Using <code>tf.app.run()</code>, another Google-ism, frees us from accessing <code>sys.argv</code> directly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main(args):
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>We can combine <code>tf.app.run()</code> with <code>tf.app.flags</code>.</p>
<hr>
<pre><code class="language-python">import google.apputils.app
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main(args):
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    google.apputils.app.run()</code></pre>

<hr>
<p>To see the equivalent outside the TensorFlow package, we can combine <code>gflags</code> and <code>google.apputils.app</code>.</p>
<hr>
<pre><code class="language-python">import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args()
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This has all been fun, but here's what you should really do. Just use <code>argparse</code>.</p>
<hr>
<p>Whew!</p>
<hr>
<p>That was a lot of arguing.</p>
<p>It may be worth showing all these because you'll encounter various combinations as you read code out there in the world. More recent examples are tending to move to <code>argparse</code>, but there are some of the other variants out there as well.</p>
<hr>
<p>MapReduce example</p>
<hr>
<p>Now to the MapReduce example!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Recall that the core of TensorFlow is a <em>distributed</em> runtime. What does that mean?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>Behold, the power of <code>tf.device()</code>!</p>
<hr>
<p></p><pre><code>with tf.device('/cpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local CPU.</p>
<hr>
<p></p><pre><code>with tf.device('/gpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local GPU.</p>
<hr>
<p></p><pre><code>with tf.device('/job:ps/task:0'):
    # Do something.</code></pre>
<hr>
<p>In exactly the same way, you can specify that work happen on <em>a different computer</em>!</p>
<p>This is pretty amazing. I think of it as sort of <a href="/20170423-tensorflow_as_automatic_mpi/">declarative MPI</a>.</p>
<hr>
<p><img alt="distributing a TensorFlow graph" src="img/distributed_graph.png"></p>
<hr>
<p>TensorFlow automatically figures out when it needs to send information between devices, whether they're on the same machine or on different machines. So cool!</p>
<hr>
<p><img alt="oh map reduce..." src="img/oh_map_reduce.png"></p>
<hr>
<p>MapReduce is often associated with Hadoop. It's just divide and conquer.</p>
<p>So let's do it with TensorFlow!</p>
<hr>
<p>(demo)</p>
<hr>
<p>It's time to see how this looks in practice! (Well, or at least to see how it looks in a cute little demo.)</p>
<p>The demo uses the contents of the <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">mapreduce_with_tensorflow</a> repo on GitHub. For more explanation, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a> and <a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a>.</p>
<hr>
<p>(code)</p>
<hr>
<p>Walking through some details inside <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/blob/master/count.py"><code>count.py</code></a>.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>different distributed functionality?</li>
<li>add command-line?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Maybe command-line args are relevant to you. Maybe running a system across multiple machines is relevant for you. Or maybe not.</p>
<p>If you want to exercise the things just demonstrated, you could add a command-line argument to the distributed word-count program. For example, you could make it count only a particular word, or optionally count characters, or something else. Or you could change the distributed functionality without any command-line fiddling. (You could make it a distributed neural net training program, for example.)</p>
<p>Here are some links that might be helpful:</p>
<ul>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="oh kubernetes..." src="img/oh_kubernetes.jpg"></p>
<hr>
<p>I should probably say that you don't really want to start all your distributed TensorFlow programs by hand. <a href="https://www.docker.com/">Containers</a> and <a href="https://kubernetes.io/">Kubernetes</a> and all that.</p>
<hr>
<ul>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>Let's to some machine learning!</p>
<hr>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<hr>
<p>New and exciting things are being added in TensorFlow Python land, building up the ladder of abstraction.</p>
<p>This material comes from <a href="/20170321-various_tensorflow_apis_for_python/">Various TensorFlow APIs for Python</a>.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="slim... shady?" src="img/slim_shady.png" height="100%"></p>
<hr>
<p>The "layer" abstractions largely from TF-Slim are now appearing at <code>tf.layers</code>.</p>
<hr>
<p><img alt="scikit-learn logo" src="img/sklearn.png" height="100%"></p>
<hr>
<p>The Estimators API now at <code>tf.estimator</code> is drawn from <code>tf.contrib.learn</code> work, which is itself heavily inspired by scikit-learn.</p>
<hr>
<p><img alt="Keras" src="img/keras.jpg" height="100%"></p>
<hr>
<p>And Keras is entering TensorFlow first as <code>tf.contrib.keras</code> and soon just <code>tf.keras</code> with version 1.2.</p>
<hr>
<ul>
<li>high-level ML APIs<ul>
<li>training an Estimator</li>
<li>pre-trained Keras</li>
</ul>
</li>
</ul>
<hr>
<p>So let's try this out!</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
</ul>
<p>There's also a TensorBoard demo baked in there. A couple backup slides follow.</p>
<hr>
<p><img alt="graph" src="img/tensorboard_graph.png"></p>
<hr>
<p>This is what the graph should look like in TensorBoard.</p>
<hr>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<hr>
<p>Steps per second should look something like this.</p>
<hr>
<p><img alt="loss" src="img/loss.png"></p>
<hr>
<p>Loss should look like this.</p>
<hr>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg" height="100%"></p>
<hr>
<p>I just enjoy this image too much not to share it.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>flip regression to classification?</li>
<li>classify some images?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Do you need a model of some kind in your system? Maybe you can use TensorFlow's high-level machine learning APIs.</p>
<p>If you want to work with the stuff just shown some more, that's also totally cool! Instead of simple regression, maybe you want to flip the presidential GDP problem around to be logistic regression. TensorFlow has <code>tf.contrib.learn.LinearClassifier</code> for that. And many more variants!</p>
<p>Or maybe you want to classify your own images, or start to poke around the model some more. Also good! If you want more example images, there's <a href="https://github.com/ajschumacher/imagen">this set</a>.</p>
<p>Here are some links that could be helpful:</p>
<ul>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<hr>
<p>Oh my! Are we out of time already?</p>
<hr>
<p>What else?</p>
<hr>
<p>There are a lot of things we haven't covered.</p>
<hr>
<p>debugging, optimizing (XLA, low-precision, etc.), serving, building custom network architectures, embeddings, recurrent, generative, bazel, protobuf, gRPC, queues, threading...</p>
<hr>
<p>Here's a list of the first things that came to mind.</p>
<p>I hope you continue to explore!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170509-building_tensorflow_systems_from_components/</link>
<guid>http://planspace.org/20170509-building_tensorflow_systems_from_components/</guid>
<pubDate>Tue, 09 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Regression with a TensorFlow Estimator</title>
<description><![CDATA[

<p>With TensorFlow 1.1, the <a href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators">Estimator</a> API is now at <code>tf.estimator</code>. A number of "canned estimators" are <a href="https://www.tensorflow.org/extend/estimators">at</a> <code>tf.contrib.learn</code>. This higher-level API bakes in some best practices and makes it much easier to do a lot quickly with TensorFlow, similar to using APIs available in other languages.</p>
<hr>
<h2>Data</h2>
<p>This example will use the very simple <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/">US Presidential Party and GDP Growth dataset</a>: <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/president_gdp.csv">president_gdp.csv</a>.</p>
<p>The regression problem will be to predict annualized percentage GDP growth from presidential party.</p>
<hr>
<h2>R</h2>
<p><a href="https://www.r-project.org/">R</a> is made for problems such as this, with an API that makes it quite easy:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; model = lm(growth ~ party, data)
&gt; predict(model, data.frame(party=c('R', 'D')))
##        1        2
## 2.544444 4.332857</code></pre>

<p>The dataset is very small, and we won't introduce a train/test split. Linear regression is just a way of calculating means: we expect our model to predict the mean GDP growth conditional on party. Annual GDP growth during Republican presidents has been about 2.5%, and during Democratic presidents about 4.3%.</p>
<hr>
<h2>sklearn</h2>
<p>Moving into Python, let's first read in the data and get it ready, using <a href="http://www.numpy.org/">NumPy</a> and <a href="http://pandas.pydata.org/">Pandas</a>.</p>
<pre><code class="language-python">import numpy as np
import pandas as pd

data = pd.read_csv('president_gdp.csv')
party = data.party == 'D'
party = np.expand_dims(party, axis=1)
growth = data.growth</code></pre>

<p>With R, we relied on automatic handling of categorical variables. Here we explicitly change the strings 'R' and 'D' to be usable in a model: Boolean values will become zeros and ones. We also adjust the <code>party</code> data shape to be one row per observation.</p>
<p>Tracking <a href="/20170321-various_tensorflow_apis_for_python/">TensorFlow Python APIs</a>, the Estimator API comes from TF Learn, which is inspired by <a href="http://scikit-learn.org/">scikit-learn</a>. Here's the regression with scikit:</p>
<pre><code class="language-python">import sklearn.linear_model

model = sklearn.linear_model.LinearRegression()
model.fit(X=party, y=growth)
model.predict([[0], [1]])
## array([ 2.54444444,  4.33285714])</code></pre>

<hr>
<h2>TensorFlow</h2>
<p>This will abuse the API a little to maximize comparability to the examples above; you'll see warnings when you run the code, which will be addressed in the next section.</p>
<pre><code class="language-python">import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col])</code></pre>

<p>Unlike with scikit, we need to specify the structure of our regressors when we instantiate the model object. This is done with FeatureColumns. There are several <a href="https://www.tensorflow.org/tutorials/wide#selecting_and_engineering_features_for_the_model">options</a>; <code>real_valued_column</code> is probably the simplest but others are useful for general categorical data, etc.</p>
<p>We're providing that data as a simple matrix, so it's important that we use the empty string <code>''</code> for <code>column_name</code>. If there is a substantial <code>column_name</code>, we'll have to provide data in dictionaries with column names as keys.</p>
<pre><code class="language-python">model.fit(x=party, y=growth, steps=1000)
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>TensorFlow needs to be told how many steps of gradient descent to run, or it will keep going indefinitely, without additional configuration. A thousand iterations gets very close to the results achieved with R and with scikit.</p>
<p>There are a lot of things that <code>LinearRegressor</code> takes care of. In this code, we did not have to explicitly:</p>
<ul>
<li>Create any TensorFlow variables.</li>
<li>Create any Tensorflow ops.</li>
<li>Choose an optimizer or learning rate.</li>
<li>Create a TensorFlow session.</li>
<li>Run ops in a session.</li>
</ul>
<p>This API also does a lot more than the R or scikit examples above, and allows for even more extensions.</p>
<hr>
<h2>TensorFlow Extensions</h2>
<p>The Estimator API does a lot by default, and allows for a lot more optionally.</p>
<p>First, there is a <code>model_dir</code>. Above, TensorFlow automatically used a temporary directory. It's nicer to explicitly choose a <code>model_dir</code>.</p>
<pre><code class="language-python">model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')</code></pre>

<p>The <code>model_dir</code> is used for two main purposes:</p>
<ul>
<li>Saving TensorBoard summaries (log info)</li>
<li>Saving model checkpoints</li>
</ul>
<h3>Automatic TensorBoard</h3>
<p><a href="/20170430-tensorflows_queuerunner/">Like</a> an <code>input_producer</code>, an Estimator automatically writes information for TensorBoard. To check them out, point TensorBoard at the <code>model_dir</code> and browse to <code>localhost:6006</code>.</p>
<pre><code class="language-bash">$ tensorboard --logdir tflinreg</code></pre>

<p>For the example above, we get the model graph and two scalar summaries.</p>
<p>Here's what was was constructed in the TensorFlow graph for our <code>LinearRegressor</code>:</p>
<p><img alt="graph" src="img/graph.png"></p>
<p>In the scalar summaries, we get a measure of how fast the training process was running, in global steps per second:</p>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<p>The variation in speed shown here is not particularly meaningful.</p>
<p>And we get the training loss:</p>
<p><img alt="loss" src="img/loss.png"></p>
<p>We didn't really need to train for a full thousand steps.</p>
<p>By default, summaries are generated every 100 steps, but this can be set via <code>save_summary_steps</code> in a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>, along with several other settings.</p>
<p>Further customization, with support for additional metrics, validation on separate data, and even automatic early stopping, is available with <a href="https://www.tensorflow.org/get_started/monitors">ValidationMonitor</a>.</p>
<h3>Automatic Model Save/Restore</h3>
<p>After training for 1,000 steps above, TensorFlow saved the model to the <code>model_dir</code>. If we point to the same <code>model_dir</code> again in a new Python session, the model will be automatically restored from that checkpoint.</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>For more control over how often and when checkpoints are saved, see <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>.</p>
<h3>Using input functions</h3>
<p>Above, training data was provided via <code>x</code> and <code>y</code> arguments, which is like how scikit works, but not really what TensorFlow Estimators should use.</p>
<p>The appropriate mechanism is to make an <a href="https://www.tensorflow.org/get_started/input_fn">input function</a> that returns the equivalents to <code>x</code> and <code>y</code> when called. The function is passed as the <code>input_fn</code> argument to <code>model.fit()</code>, for example.</p>
<p>This approach is flexible and makes it easy to avoid, for example, keeping track of separate data structures for data and labels.</p>
<h3>Distributed Training</h3>
<p>Among the <code>tf.contrib.learn</code> <a href="https://www.tensorflow.org/api_guides/python/contrib.learn">goodies</a> is <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"><code>tf.contrib.learn.Experiment</code></a>, which works with an Estimator to help do distributed training. It looks like this one is still settling down, with a lot of deprecated bits at the moment. I'm interested to see more about this. For now, you could check out a Google Cloud ML <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/iris/trainer/task.py">example</a> that works with <code>learn_runner</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</link>
<guid>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</guid>
<pubDate>Sat, 06 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Dataset: US Presidential Party and GDP Growth</title>
<description><![CDATA[

<p>Here's a simple <a href="president_gdp.csv">dataset</a> for use in examples. It's taken from the <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">online appendix</a> to <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20140913">Presidents and the US Economy: An Econometric Exploration</a> by Blinder and Watson.</p>
<p><a href="president_gdp.csv"><code>president_gdp.csv</code></a></p>
<p>The fields are:</p>
<ul>
<li><code>term</code>: A short text description of the presidential term, like "Reagan 2".</li>
<li><code>party</code>: The political party of the presidency, either "D" for the <a href="https://en.wikipedia.org/wiki/Democratic_Party_(United_States)">Democratic Party</a> or "R" for the <a href="https://en.wikipedia.org/wiki/Republican_Party_(United_States)">Republican Party</a>.</li>
<li><code>growth</code>: The average annualized growth in US Gross Domestic Product (<a href="https://en.wikipedia.org/wiki/Gross_domestic_product">GDP</a>) for that presidential term, expressed as percentage points.</li>
</ul>
<p>For more details, please see the <a href="http://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.20140913">paper</a> and <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">appendix</a>.</p>
<p>The only changes I've made are to order the data chronologically and put it in the convenient CSV format.</p>
<p>For example, in <a href="https://www.r-project.org/">R</a>, you can do the following:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; lm(growth ~ party, data)
&gt; t.test(growth ~ party, data)</code></pre>

<p>Because the <a href="president_gdp.csv">dataset</a> is so small, I'll also display it as spaced-out text here:</p>
<pre><code>term,             party,  growth
Truman,               D,    6.57
Eisenhower 1,         R,    2.72
Eisenhower 2,         R,    2.26
Kennedy-Johnson,      D,    5.74
Johnson 2,            D,    4.95
Nixon 1,              R,    3.57
Nixon-Ford,           R,    1.97
Carter,               D,    3.56
Reagan 1,             R,    3.12
Reagan 2,             R,    3.89
G.H.W. Bush,          R,    2.05
Clinton 1,            D,    3.53
Clinton 2,            D,    4.00
G.W. Bush 1,          R,    2.78
G.W. Bush 2,          R,    0.54
Obama 1,              D,    1.98</code></pre>    
    ]]></description>
<link>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</link>
<guid>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</guid>
<pubDate>Fri, 05 May 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
