<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan ➔ space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>TensorFlow Logging</title>
<description><![CDATA[

<p>TensorFlow wraps the standard Python <a href="https://docs.python.org/3/library/logging.html">logging library</a>, exposing functionality at <code>tf.logging</code>.</p>
<p>This means that you can start logging things immediately after importing TensorFlow:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.logging.info('Things are good!')
INFO:tensorflow:Things are good!</code></pre>

<p>TensorFlow also automatically logs things using this functionality. For example, estimators will log out information about the many things they're up to.</p>
<p>The TensorFlow <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py">logging code</a> also adds some non-standard functionality. A comment notes that some of the code "is taken from pyglib/logging". I assume pyglib is a Google-internal Python library. I think it's interesting to see these Google tools <a href="/20170315-tensorflow_and_a_googleverse_like_the_hadleyverse/">appearing here and there</a>.</p>
<p>Here's an example of TensorFlow logging's added functionality: <code>log_every_n</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
INFO:tensorflow:Common error!
&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
INFO:tensorflow:Common error!</code></pre>

<p>(This example won't work in an <a href="https://ipython.org/">IPython</a> shell because every new REPL "read" there looks like a new file to Python.)</p>
<hr>
<p>If you want to use regular Python logging techniques in addition to TensorFlow's mechanism, everything should generally work, but it is possible to get into a situation that seems surprising in a REPL.</p>
<pre><code class="language-python">&gt;&gt;&gt; import logging
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; logging.warn('Something interesting!')
WARNING:root:Something interesting!
&gt;&gt;&gt; tf.logging.info('1+1=2')
INFO:tensorflow:1+1=2
INFO:tensorflow:1+1=2</code></pre>

<p>This doubling is because TensorFlow's log messages are bubbling up to the root logger as intended, but both the root logger and TensorFlow's logger are getting handled in the same place, so we see the message twice.</p>
<p>Shortly after the initial release of TensorFlow this kind of thing <a href="http://stackoverflow.com/questions/33662648/tensorflow-causes-logging-messages-to-double">was happening more than it should</a>, but the current behavior is correct.</p>
<p>The situation shown above happens because both the base logging package and TensorFlow's logging are trying to automatically be helpful by writing things out to the interactive session.</p>
<p>In practice you should set up log handlers yourself, <a href="http://docs.python-guide.org/en/latest/writing/logging/">as appropriate</a>. And of course if you only use one or the other of <code>logging</code> or <code>tf.logging</code> you'll be particularly unlikely to have any conflicts.</p>
<p>A poor solution to the doubling above would be to turn off log message propagation from the TensorFlow logger.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.logging._logger.propagate = False
&gt;&gt;&gt; tf.logging.info('1+1=2')
INFO:tensorflow:1+1=2</code></pre>

<p>But I can't recommend using that as anything but a quick fix in a REPL if double messages happen to be bothering you.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170322-tensorflow_logging/</link>
<guid>http://planspace.org/20170322-tensorflow_logging/</guid>
<pubDate>Wed, 22 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Various TensorFlow APIs for Python</title>
<description><![CDATA[

<p>An early <a href="http://fastml.com/what-you-wanted-to-know-about-tensorflow/">perception</a> of TensorFlow was that the API was quite low-level.</p>
<blockquote>
<p>"meaning you&#8217;ll be multiplying matrices and vectors."</p>
</blockquote>
<p>TensorFlow offers ops for low-level operations, and from the beginning programmers used those low-level ops to build higher-level APIs.</p>
<p>To paraphrase <a href="http://www.paulgraham.com/avg.html">Paul Graham</a>:</p>
<blockquote>
<p>"if you have a choice of several [APIs], it is, all other things being equal, a mistake to program in anything but the [highest-level] one."</p>
</blockquote>
<p>You don't need to be a Lisp hacker to make it easier to use TensorFlow the way you want by writing new functions and classes, and you probably should.</p>
<p>But because TensorFlow had already done the obviously hard work of making low-level things work, it was relatively easy for lots of people to attempt the subtly hard work of designing higher-level APIs.</p>
<p>This led to a proliferation of Python APIs, which can sometimes be confusing to track.</p>
<p>Things are still moving around and settling a bit, but Google is communicating a pretty clear plan for official Python APIs for TensorFlow.</p>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<p>This plan changes some names and rearranges things slightly, but it's an incremental development of things that have existed for a while. We can look at how things have developed over time both for what's becoming official TensorFlow API and some of the projects that still exist outside official TensorFlow.</p>
<hr>
<h3>Python Frontend (op level)</h3>
<p>You can continue to use TensorFlow's low-level APIs.</p>
<p>There are ops, like <code>tf.matmul</code> and <code>tf.nn.relu</code>, which you might use to build a neural network architecture in full detail. To do really novel things, you may want this level of control. But you may also prefer to work with larger building blocks. The other other APIs below will mostly specialize in this kind of application.</p>
<p>There are also ops like <code>tf.image.decode_jpeg</code> (and many others) which may be necessary but don't necessarily relate to what is usually considered the architecture of neural networks. Some higher-level APIs wrap some of this functionality, but they usually stay close to the building of network architectures and the training of such networks once defined.</p>
<hr>
<h3>Layers (as from TF-Slim)</h3>
<p><a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html">TF-Slim</a> has a number of <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">components</a> but it looks like essentially we're seeing the following: <code>tf.contrib.slim.layers</code> became <code>tf.contrib.layers</code> becomes <code>tf.layers</code>.</p>
<p>This <code>layers</code> API provides a first higher level of abstraction over writing things out by individual ops. For example, <code>tf.layers.conv2d</code> implements a convolution layer that involves multiple individual ops.</p>
<p>Other parts of TF-Slim are likely still worth using, and there is a collection of <a href="https://github.com/tensorflow/models/tree/master/slim">models that use TF-Slim</a> in the <a href="https://github.com/tensorflow/models">TensorFlow models repository</a>.</p>
<p>Historical note: It looks like before calling them layers, TF-Slim overloaded the word "op" for their layer concept (see <a href="https://github.com/tensorflow/models/tree/master/inception/inception/slim">earlier documentation</a>).</p>
<p>TF-Slim is in the TensorFlow codebase as <code>tf.contrib.slim</code>.</p>
<p><img alt="slim... shady?" src="img/slim_shady.png"></p>
<hr>
<h3>Estimator (as from TF Learn)</h3>
<p>Distinct from TensorFlow, the <a href="http://scikit-learn.org/">scikit-learn</a> project makes a lot of machine learning models conveniently available in Python.</p>
<p><img alt="scikit-learn logo" src="img/sklearn.png"></p>
<p>A key design element of scikit is that many different models offer the same simple API: they are all implemented as <em>estimators</em>. So regardless of whether the model is linear regression or a GBM, if your instantiated scikit-learn model object is called <code>estimator</code>, you can do all of these:</p>
<pre><code class="language-python">estimator.fit(data, labels)       # train
estimator.score(data, labels)     # test
estimator.predict(data)           # run on new data</code></pre>

<p>The estimator API for TensorFlow is directly inspired by scikit-learn, though implemented quite independently. Here's nearly equivalent TensorFlow code, assuming you have a TensorFlow estimator model instantiated as <code>estimator</code>:</p>
<pre><code class="language-python">estimator.fit(data, labels)       # train
estimator.evaluate(data, labels)  # test
estimator.predict(data)           # run on new data</code></pre>

<p>The only thing that changed in the code is scikit-learn's <code>score</code> method becomes <code>evaluate</code>, which I think is going to do a lot of good for the self esteem of TensorFlow models.</p>
<p>There are other differences too. In scikit-learn, usually models have an idea of training "until done"&#8212;but in TensorFlow, if you don't specify a number of steps to train for, the model will go on training forever. TensorFlow estimators also do a lot of things by default, like adding TensorBoard summaries and saving model checkpoints.</p>
<p>The estimators API started as <a href="https://github.com/tensorflow/skflow">skflow</a> ("scikit-flow") before moving into the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">tf.contrib.learn</a> ("TF Learn") and now the base estimator code is getting situated in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator">tf.estimator</a> and <a href="https://www.tensorflow.org/extend/estimators">documentation</a> is accumulating.</p>
<hr>
<h3>Keras Model (absorbing Keras)</h3>
<p><a href="https://keras.io/">Keras</a> was around before TensorFlow. It was always a high-level API for neural nets, originally running with <a href="http://www.deeplearning.net/software/theano/">Theano</a> as its backend.</p>
<p>After the release of TensorFlow, Keras moved to also work with TensorFlow as a backend. And now TensorFlow is absorbing at least some aspects of the Keras project into the TensorFlow codebase, though <a href="https://twitter.com/fchollet">Fran&#231;ois Chollet</a> seems likely to continue championing Keras as a very fine project in its own right. (See also his response to a <a href="https://www.quora.com/What-will-Keras-do-with-TensorFlow-Slim">Quora question</a> about any possible relationship between TF-Slim and Keras.)</p>
<p>Once specified, Keras models offer basically the same API (<code>fit</code>/<code>evaluate</code>/<code>predict</code>) as estimators (above).</p>
<p>Keras is appearing in the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/keras">tf.contrib.keras</a> and should move to <code>tf.keras</code> at TensorFlow 1.2.</p>
<p><img alt="Keras" src="img/keras.jpg"></p>
<hr>
<h3>Canned Estimators</h3>
<p>Canned estimators are concrete pre-defined models that follow the estimator conventions. Currently there are a bunch right in <code>tf.contrib.learn</code>, such as <code>LinearRegressor</code> and <code>DNNClassifier</code>. There are some elsewhere. For example, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/g3doc/kmeans.md">kmeans</a> is in <code>tf.contrib.factorization</code>. It isn't clear to me exactly where all the canned estimators will eventually settle down in the API.</p>
<hr>
<h3>Other APIs</h3>
<p>All the APIs that follow are not part of the official TensorFlow project. I guess you could use them.</p>
<p><img alt="Bucksstar Coffee" src="img/bucksstar.png"></p>
<hr>
<h3>Pretty Tensor</h3>
<p><a href="https://github.com/google/prettytensor">Pretty Tensor</a> is still a Google project, so it might be worth checking out if you really like <a href="https://en.wikipedia.org/wiki/Fluent_interface">fluent interfaces</a> with lots of chaining, like <a href="https://d3js.org/">d3</a>.</p>
<hr>
<h3>TFLearn</h3>
<p>The confusingly named <a href="https://github.com/tflearn/tflearn">TFLearn</a> (no space; perhaps more clearly identified as <a href="(http://tflearn.org/)">TFLearn.org</a>) is not at all the same thing as the TF Learn (with a space) that appears in TensorFlow at <code>tf.contrib.learn</code>. TFLearn is a <a href="http://stackoverflow.com/questions/38859354/what-is-the-difference-between-tf-learn-aka-scikit-flow-and-tflearn-aka-tflea">separate</a> Python package that uses TensorFlow. It seems like it aspires to be like Keras. Here is the TFLearn logo:</p>
<p><img alt="TFLearn logo" src="img/tflearn.png"></p>
<hr>
<h3>TensorLayer</h3>
<p>Another one with a confusing name is <a href="https://github.com/zsdonghao/tensorlayer/">TensorLayer</a>. This is a separate package from TensorFlow and it's different from TensorFlow's layers API.</p>
<p><img alt="TensorLayer logo" src="img/tensorlayer.png"></p>
<hr>
<p>My recommendation is to use the rich APIs available inside TensorFlow. You shouldn't have to import anything else, with the possible exception of Keras if you like it and are working before the Keras integration into TensorFlow is complete.</p>
<hr>
<p>Thanks to <a href="https://twitter.com/amuellerml">Andreas Mueller</a> of scikit-learn for his <a href="https://twitter.com/amuellerml/status/844300337666240514">summary</a> of the relation between scikit-learn and TensorFlow.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170321-various_tensorflow_apis_for_python/</link>
<guid>http://planspace.org/20170321-various_tensorflow_apis_for_python/</guid>
<pubDate>Tue, 21 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow APIs for Various Languages</title>
<description><![CDATA[

<p>You couldn't be blamed for thinking that <a href="https://www.tensorflow.org/">TensorFlow</a> is a Python package. And a lot of TensorFlow functionality is unique to Python. But the core of TensorFlow&#8212;the part of TensorFlow that most truly <em>is</em> TensorFlow&#8212;is the distributed runtime ("TensorFlow Distributed Execution Engine") and Python is just one way to talk to it.</p>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<p>The runtime itself is written in C++, but it has APIs ("frontends") in many languages.</p>
<hr>
<h3>C</h3>
<p>The only TensorFlow APIs with any <a href="https://www.tensorflow.org/programmers_guide/version_semantics">official stability</a> are the <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h">C API</a> and (parts of) the Python API.</p>
<p>TensorFlow <a href="https://www.tensorflow.org/extend/language_bindings">suggests</a> that you use the C API if you are making a TensorFlow API for some other language. Lots of programming languages have mechanisms for connecting with <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a>. In a very real sense the TensorFlow C API exists <em>so that</em> other APIs can be built.</p>
<p>There are a number of non-Python <a href="https://www.tensorflow.org/api_docs/">languages with TensorFlow APIs</a> but for the most part I will consider them "deploy only" ("inference only") and largely ignore them.</p>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<h3>C++</h3>
<p>The <a href="https://www.tensorflow.org/api_docs/cc/">C++ API</a> is "exposed through header files in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc">tensorflow/cc</a>" and <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a> is the language that the TensorFlow runtime is written in, but the C++ API is still marked as experimental and has fewer features than are accessible via the Python API. It may be where you want to be for some deploy scenarios.</p>
<p>As a possibly interesting historical note, <a href="https://research.google.com/archive/mapreduce.html">the MapReduce paper</a> was another place where the world saw Google (and Jeff Dean) making something cool with C++. Yahoo! (and others) eventually implemented those concepts in Java, as <a href="http://hadoop.apache.org/">Hadoop</a>. Google seems to continue to do a lot of work in C++.</p>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<h3>Java</h3>
<p>Possibly recognizing <a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Java's</a> presence in industry, TensorFlow now has a <a href="https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/package-summary">Java API</a>.</p>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<h3>Go</h3>
<p><a href="https://en.wikipedia.org/wiki/Go_(programming_language)">Go</a> is a Google programming language, and so it seems appropriate that there is a <a href="https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go">Go API</a> for TensorFlow.</p>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<h3>Rust</h3>
<p><a href="https://en.wikipedia.org/wiki/Rust_(programming_language)">Rust</a> is a neat language, and it has a TensorFlow <a href="https://github.com/tensorflow/rust">api</a> too (<a href="https://tensorflow.github.io/rust/tensorflow/">docs</a>).</p>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<h3>Haskell</h3>
<p><a href="https://en.wikipedia.org/wiki/Haskell_(programming_language)">Haskell</a> is not a language I would have guessed would have a TensorFlow API, but <a href="https://github.com/tensorflow/haskell">it does</a> (<a href="https://tensorflow.github.io/haskell/haddock/">docs</a>).</p>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<h3>Python</h3>
<p><a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> is where the action is. TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/">Python API documentation</a> is headed simply "All symbols in TensorFlow" and it really does <a href="https://www.tensorflow.org/extend/language_bindings">seem to be</a> that TensorFlow functionality is prototyped in Python and then moved into the C++ core:</p>
<blockquote>
<p>Python was the first client language supported by TensorFlow and currently supports the most features. More and more of that functionality is being moved into the core of TensorFlow (implemented in C++) and exposed via a C API.</p>
</blockquote>
<p>The Python API is so rich, you'll have to choose which levels of Python API you'll want to use.</p>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<h3>R</h3>
<p>The <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R</a> TensorFlow API made by <a href="https://www.rstudio.com/">RStudio</a> takes a different approach than the APIs that use TensorFlow's C API to connect with TensorFlow. <a href="https://rstudio.github.io/tensorflow/">The R API</a> (<a href="https://github.com/rstudio/tensorflow">on github</a>) wraps the Python API, with all the good and bad that comes with that approach. But whether or not Google approves of the approach, there is a way to access TensorFlow from R!</p>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170320-tensorflow_apis_for_various_languages/</link>
<guid>http://planspace.org/20170320-tensorflow_apis_for_various_languages/</guid>
<pubDate>Mon, 20 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Thank You for Reaching Out</title>
<description><![CDATA[

<p>I make myself pretty <a href="/aaron/">easy to contact</a>, and sometimes people contact me.</p>
<h3>1. Will I hire you?</h3>
<p>I'm not in a position to hire you (or give you an internship). Please don't send me your r&#233;sum&#233;. If you want to work where I work, or anywhere I've worked before, please go through official channels.</p>
<h3>2. Will I recommend you for a job?</h3>
<p>If I already know your work and I know it to be good, then yes.</p>
<h3>3. How can you find a job?</h3>
<p>I've found jobs mostly through people I know, and once through a recruiter.</p>
<p>Don't meet people for the sake of finding a job. Meet people by being active in your community and doing good work. Go to a <a href="https://www.meetup.com/">meetup</a>. Talk to people. Help where you can. Give a talk about something you know or are learning about. Help out on a <a href="http://brigade.codeforamerica.org/brigade/">community project</a>. Keep developing new skills.</p>
<p>The majority of the cold contacts I get are from recruiters. I don't have an answer for them on this page because they don't <em>care</em>; it is their <em>job</em> to contact people all the time&#8212;even people who don't seem to be looking for work&#8212;on the off chance that they make a contact at exactly the right time. Can they find you? They will send you jobs.</p>
<h3>4. How can you develop skills?</h3>
<p>I have a presentation about my career path and things I think are very generally important. It's called <a href="/20151206-how_to_eat_computers/">How to Eat Computers</a> and it was made for children, but even such a very mature person as you might enjoy it.</p>
<p>I have two short lists of recommended books:</p>
<ul>
<li><a href="/20160322-books_for_programmers/">Books for Programmers</a></li>
<li><a href="/20160320-books_for_professionals/">Books for Professionals</a></li>
</ul>
<p>You should read and write a lot. Develop interests and pursue them.</p>
<h3>5. What educational program should you do?</h3>
<p>You should go for the best program(s) you can, but remember that individual (student) variability is much larger than variability between programs.</p>
<h3>6. Should you do online courses?</h3>
<p>This is like asking "Should I read books?" Put them in the same category. Choose what will best help you learn what you want to learn.</p>
<h3>7. Should you do a coding bootcamp?</h3>
<p>Don't think of a bootcamp as a credential. They don't play in the same world as graduate degrees. If you go for a bootcamp, you are saying to the world that you are <em>scrappy</em>; you are going to <em>work hard</em> and you are going to <em>make something happen</em>. The structures of the program may help you to do more than you would on your own, but no program can do everything for you.</p>
<p>At the end of the program you are going to have your work to show and talk about. Could you have done that work without the program? Yes. Would you have?</p>
<h3>8. What's the best coding bootcamp?</h3>
<p>I've taught data science evening classes for <a href="https://generalassemb.ly/">General Assembly</a> and the full-time program for <a href="https://www.thisismetis.com/">Metis</a>. I developed some materials and I hope I helped both programs, but things vary a lot with locations, staff, and time: I don't know which programs are better than others.</p>
<h3>9. Will I collaborate with you? Will I be your mentor?</h3>
<p>If you have a project or question that you think I'm uniquely positioned to help with, let's talk about that project or question.</p>
<p>If you're looking for people in general but don't yet have a project or question to talk about, thank you for thinking of me, and please contact me if I am still the right person to talk to after you do. I keep some project ideas in my blog's <a href="https://github.com/ajschumacher/ajschumacher.github.io/issues">github issues</a>; you could look there to see if anything interests you.</p>    
    ]]></description>
<link>http://planspace.org/20170319-thank_you_for_reaching_out/</link>
<guid>http://planspace.org/20170319-thank_you_for_reaching_out/</guid>
<pubDate>Sun, 19 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Much Ado about the TensorFlow Logo</title>
<description><![CDATA[

<p>In the beginning, there was <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">G&#246;del, Escher, Bach: An Eternal Golden Braid</a> by Douglas Hofstadter.</p>
<p><img alt="G&#246;del, Escher, Bach" src="img/geb.jpg"></p>
<p>This "metaphorical fugue on minds and machines in the spirit of Lewis Carroll" includes the <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> idea and related discussion of consciousness and formal systems. Hofstadter's influence can be seen, for example, <a href="https://cs.illinois.edu/news/strange-loop-conference">in the name</a> of the <a href="http://www.thestrangeloop.com/">Strange Loop</a> tech conference.</p>
<p>It seems likely that many people working in artificial intelligence and machine learning have encountered G&#246;del, Escher, Bach.</p>
<p>Of course, there's also <a href="https://en.wikipedia.org/wiki/Chernin_Entertainment">Chernin Entertainment</a>, the production company.</p>
<p><img alt="Chernin Entertainment" src="img/chernin.jpg"></p>
<p>So we shouldn't rule out the possibility that Google engineers are fans of <a href="http://www.imdb.com/company/co0286257/">Chernin's work</a>. I hear <a href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden Figures</a> is quite good. And I guess a lot of people like <a href="https://en.wikipedia.org/wiki/New_Girl">New Girl</a>?</p>
<p>In any event, somehow we get to this TensorFlow logo:</p>
<p><img alt="TensorFlow logo - old?" src="img/tf-old.png"></p>
<p>If you look carefully, does it seem like the right side of the "T" view is too short?</p>
<p>This very serious concern appears as <a href="https://github.com/tensorflow/tensorflow/issues/1922">issue #1922</a> on the TensorFlow github, and <a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XhO1sqp4l4g">on the TensorFlow mailing list</a> complete with ASCII art illustration.</p>
<p>The consensus response seemed to be some variant of "won't fix" (it wouldn't look as cool, anyway) until...</p>
<p><img alt="TensorFlow logo - new?" src="img/tf-new.jpg"></p>
<p>As of around the 1.0 release of TensorFlow, which was around the first <a href="https://events.withgoogle.com/tensorflow-dev-summit/">TensorFlow Dev Summit</a>, this logo variant seems to be in vogue. It removes the (possibly contentious) shadows, and adds a picture of a computation graph, in case you were about to forget that TensorFlow is about computation graphs. (If you want to think of it as a neural network, you're free to do so.)</p>
<p>Logos are fun!</p>
<hr>
<p>Thanks to <a href="https://twitter.com/divergentdave">David Cook</a> and <a href="https://twitter.com/philipashlock">Philip Ashlock</a> for helping me find Chernin again based on a very murky memory.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170318-much_ado_about_the_tensorflow_logo/</link>
<guid>http://planspace.org/20170318-much_ado_about_the_tensorflow_logo/</guid>
<pubDate>Sat, 18 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow and a Googleverse like the Hadleyverse</title>
<description><![CDATA[

<p>I've been having a good time looking at <a href="/20170313-tensorflow_use_of_google_technologies/">how TensorFlow incorporates Google technologies</a>, and how some things (like the <a href="/20170314-command_line_apps_and_tensorflow/">command-line helpers</a>) seem to reflect Google's internal conventions for development. Taken together, the view of Google software practices via TensorFlow reminds me of the Hadleyverse.</p>
<p><img alt="Hadley Wickham" src="img/hadley.png"></p>
<p>"The Hadleyverse" refers to the many packages that <a href="http://blog.revolutionanalytics.com/2015/03/hadleyverse.html">Hadley Wickham</a> has created and contributed to, which largely replace, for proficient users, the standard functionality available in the <a href="https://www.r-project.org/">R</a> programming environment. Wickham more humbly allows the name "<a href="http://tidyverse.org/">tidyverse</a>" and indeed, tidiness is part of the philosophy of his packages.</p>
<p>In addition to just providing useful packages with better functionality and better APIs than the ones that come standard with R, the Hadleyverse includes a lot of "how-to" as well. Among Wickham's freely available books are <a href="http://adv-r.had.co.nz/">Advanced R</a>, which contains a <a href="http://adv-r.had.co.nz/Style.html">style guide</a>, and the <a href="http://r-pkgs.had.co.nz/">R Packages</a> book for package developers.</p>
<p>Wickham is exceptionally productive and has had a large influence in the R community. He's also very open, sharing and communicating freely with tons of people.</p>
<p>Google might be even more productive than Hadley, but Google doesn't always seem to share as much. So the TensorFlow project can seem like a window into that Googleverse. And like the Hadleyverse, there is code, but there is also philosophy and practices.</p>
<p><img alt="google" src="img/google.png"></p>
<p>For one thing, the TensorFlow <a href="https://github.com/tensorflow/tensorflow">code</a> is an example of a Python code-base that follows the <a href="https://google.github.io/styleguide/pyguide.html">Google Python style guide</a>. (I'm primarily interested in the Python parts.)</p>
<p>Of course Python has the venerable <a href="https://www.python.org/dev/peps/pep-0008/">PEP 8</a> style guide, and I tend to like the <a href="https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt">NumPy/SciPy documentation guide</a> for docstrings. But looking at the TensorFlow code has nearly convinced me that their more concise argument documentation could be the way to go, for example.</p>
<p>I'm also impressed with the religious use of <a href="https://www.pylint.org/">pylint</a>, and even just the organization of unit tests.</p>
<p>In places you can see TensorFlow moving from Google-internal things to more standard Python things: <a href="https://github.com/google/python-gflags">gflags</a> and <a href="https://github.com/google/google-apputils">apputils</a> to <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/googletest.py">testing.pybase.googletest</a> to <a href="https://docs.python.org/3/library/unittest.html">unittest</a>.</p>
<p>Google also doesn't just use <a href="https://github.com/">GitHub</a>; they've got a bunch of cool extra automation and review processes set up. When a pull request is submitted, <a href="https://github.com/googlebot">googlebot</a> and <a href="https://github.com/tensorflow-jenkins">tensorflow-jenkins</a> spring into action. Contributors have to fill out a Contributor License Agreement (CLA) online, and an admin is pulled in for review and to have tests run for the pull request. After all approvals are in, the new code can go in to TensorFlow.</p>
<p>Oh and there's also <a href="https://github.com/tensorflower-gardener">tensorflower-gardener</a>, which I speculate is some connection between GitHub and work that happens inside Google proper.</p>
<p>It's interesting to think about what aspects of the Googleverse should (and/or could) be adopted by developers outside Google.</p>    
    ]]></description>
<link>http://planspace.org/20170315-tensorflow_and_a_googleverse_like_the_hadleyverse/</link>
<guid>http://planspace.org/20170315-tensorflow_and_a_googleverse_like_the_hadleyverse/</guid>
<pubDate>Wed, 15 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Command-Line Apps and TensorFlow</title>
<description><![CDATA[

<p>You can do pretty much with TensorFlow interactively in a Python <a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">REPL</a> or <a href="http://jupyter.org/">notebook</a>, but there are a lot of reasons why you might want to turn your code into something you can run at the command line.</p>
<p>Running model training may take a lot of time, so you may want to run it "by itself" as an independent process. You may prefer to share <code>.py</code> files rather than notebooks, for the universality of text editors over notebook viewers. You may want to make it easier to re-run or eventually productionize your code, and especially to easily run your code on other computers.</p>
<p>You may also want to test the behavior of a system with a variety of values for certain parameters. It becomes easier and more repeatable to keep your code the same and run it with different command-line arguments, rather than editing values in the code directly and re-running.</p>
<p>If you're using a cloud service like Google's <a href="https://cloud.google.com/products/machine-learning/">Cloud ML</a>, command-line arguments may be particularly important. As they <a href="https://cloud.google.com/ml-engine/docs/concepts/trainer-considerations">say</a>, "Command-line arguments are the primary mechanism for communicating with your [TensorFlow program] at the time of execution." Writing your program to support command-line arguments like <code>--argument_name argument_value</code> is required, if you're using <a href="https://cloud.google.com/ml-engine/docs/concepts/hyperparameter-tuning-overview">Cloud ML's hyperparameter tuning features</a>.</p>
<p>There are multiple options for how to set up your command-line app. I'll show eight of them below. (Not all of them are good options!) All the scripts below have the same behavior when called like this:</p>
<pre><code class="language-bash">$ python script.py --color red
a red flower</code></pre>

<hr>
<h3>Script <a href="code/script1_argv.py">1</a>: <code>sys.argv</code></h3>
<pre><code class="language-python">import sys

def main():
    assert sys.argv[1] == '--color'
    print('a {} flower'.format(sys.argv[2]))

if __name__ == '__main__':
    main()</code></pre>

<p>You can access <code>sys.argv</code> to get a list of arguments, split on spaces in the command line, starting with the script name.</p>
<p>This direct approach is concise, for this simple example, but brittle (what if there are multiple arguments, in different order?) and opaque (what was argument 7 again?).</p>
<p>The approaches below will improve on this. All of them will make it clearer what the arguments are, both in the code and by automatically providing interactive help information on the command line, with <code>python script.py --help</code>, for example. And they'll all handle the parsing of command lines, taking them from lists of strings to data structures that are more easily used in programs.</p>
<hr>
<h3>Script <a href="code/script2_argv_tfappflags.py">2</a>: <code>tf.app.flags</code></h3>
<pre><code class="language-python">import sys
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main():
    flags.FLAGS._parse_flags(args=sys.argv[1:])
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<p>You shouldn't do this, for multiple reasons, but it does give our first example of using something that manages your command-line interface more than manipulating <code>sys.argv</code> directly.</p>
<p>Like all the following examples, we set here a default value and some documentation for the command-line argument. We won't demonstrate other possible features, like specifying constraints on arguments and the like.</p>
<p>The first <a href="http://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/33938519#33938519">reason</a> not to use this particular technique is that while TensorFlow currently includes some argument-parsing functionality, it's there mostly so that it's easy to make cute demos, and it could change at any time.</p>
<p>The second reason not to do this is that we're still providing <code>sys.argv</code> ourselves, which is awkward, especially with TensorFlow's mechanism here. The next two examples will continue specifying <code>sys.argv</code> like this, before turning to nicer methods.</p>
<hr>
<h3>Script <a href="code/script3_argv_gflags.py">3</a>: gflags</h3>
<pre><code class="language-python">import sys
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main():
    gflags.FLAGS(sys.argv)
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<p>The TensorFlow <code>tf.app.flags</code> mechanism is a sort of partial re-implementation of Google's <a href="https://github.com/google/python-gflags">gflags</a> system for command-line arguments. It's an interesting case of <a href="/20170313-tensorflow_use_of_google_technologies/">Google conventions influencing TensorFlow</a>, and the comparison between the TensorFlow code in Script 2 and the gflags code in Script 3 illustrates this.</p>
<p>The Google gflags for Python <a href="https://pypi.python.org/pypi/python-gflags">package</a> can be installed with <code>pip install python-gflags</code>.</p>
<hr>
<h3>Script <a href="code/script4_argv_argparse.py">4</a>: argparse with <code>sys.argv</code></h3>
<pre><code class="language-python">import sys
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args(sys.argv[1:])
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<p>The usual way to write command-line programs in Python is with the Python standard library's <a href="https://docs.python.org/3/library/argparse.html">argparse</a>. Interestingly, while the API of TensorFlow's <code>tf.app.flags</code> is very close to gflags, it is itself implemented with argparse.</p>
<hr>
<h3>Script <a href="code/script5_tfapprun.py">5</a>: <code>tf.app.run()</code></h3>
<pre><code class="language-python">import tensorflow as tf

def main(args):
    assert args[1] == '--color'
    print('a {} flower'.format(args[2]))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<p>Rather than access <code>sys.argv</code> directly, it's nice to use something that will automatically process it.</p>
<p>The <code>tf.app.run()</code> functionality does a minimal transformation in this case, passing <code>sys.argv</code> as an argument to <code>main()</code>.</p>
<p>The choice of <code>main()</code> as the function to run is by convention, and can be overridden, for example like <code>tf.app.run(main=my_cool_function)</code>.</p>
<p>Just as TensorFlow has built-in functionality that is not gflags but mimics part of gflags, <code>tf.app.run()</code> looks a lot like functionality from Google's <a href="https://github.com/google/google-apputils">apputils</a>. That functionality can't be shown quite as independently as in the example here, because it would automatically involve gflags too. That kind of combination between an argument parser and an app runner will be shown in the next examples.</p>
<hr>
<h3>Script <a href="code/script6_tfapprun_tfappflags.py">6</a>: <code>tf.app.run()</code> with <code>tf.app.flags</code></h3>
<pre><code class="language-python">import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main(args):
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<p>Now only un-parsed arguments are passed to <code>main()</code>, and this is starting to look like something we could use.</p>
<p>Reasons not to use this built-in TensorFlow functionality still include that it is not officially supported and could change. It's worth noting also though that the TensorFlow argument-parsing and app-running functionality are stripped-down versions, which is enough for simple demos like what I'm showing here, but may not be enough for a real project. And of course you may just feel that some separation of concerns is in order, and TensorFlow shouldn't be doing your machine learning and also managing your command-line arguments.</p>
<hr>
<h3>Script <a href="code/script7_apputils_gflags.py">7</a>: apputils and gflags</h3>
<pre><code class="language-python">import google.apputils.app
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main(args):
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    google.apputils.app.run()</code></pre>

<p>The combination of Google's <a href="https://github.com/google/google-apputils">apputils</a> with gflags looks just like the TensorFlow code in Script 6, but is independent of TensorFlow and offers a lot more features. For example, using <code>google.apputils.app.run()</code> automatically gives your app an extra debugging mode.</p>
<p>The Google apputils <a href="https://pypi.python.org/pypi/google-apputils">package</a> can be installed with <code>pip install google-apputils</code>. Possible reasons not to use it include that it has not been updated (in the <a href="https://github.com/google/google-apputils">open source</a> version) since March 2015.</p>
<hr>
<h3>Script <a href="code/script8_argparse.py">8</a>: argparse</h3>
<pre><code class="language-python">import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args()
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<p>This is a pretty conventional way to use argparse; just omitting any argument to <code>parse_args()</code> brings in <code>sys.argv</code> automatically.</p>
<p>A method like this is what you should probably use. For examples of using it in full TensorFlow systems, one place to look is in some of the <a href="https://github.com/GoogleCloudPlatform/cloudml-samples">Google Cloud ML examples</a>.</p>
<hr>
<p>Thanks to <a href="https://twitter.com/mrry">Derek Murray</a> at <a href="https://research.google.com/teams/brain/">Google Brain</a> for pointing me in the right direction in understanding the relationship between TensorFlow and gflags.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170314-command_line_apps_and_tensorflow/</link>
<guid>http://planspace.org/20170314-command_line_apps_and_tensorflow/</guid>
<pubDate>Tue, 14 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow use of Google Technologies</title>
<description><![CDATA[

<p>TensorFlow is a large project. It has a lot of unique features, and as a Google project, it also connects with other Google projects: gflags, apputils, Bazel, protobuf, gRPC, gemmlowp, StreamExecutor, GFile, and even XLA. To better understand and use TensorFlow, it helps to know what these pieces are and how they fit together.</p>
<hr>
<h3>gflags</h3>
<p>In TensorFlow code, you may see, for <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/mnist/trainable/trainer/task.py">example</a>, <code>tf.app.flags.FLAGS</code>. This is part of a TensorFlow <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/flags.py">implementation</a> for <a href="https://en.wikipedia.org/wiki/Command-line_interface">command-line</a> argument parsing. It happens automatically when using <code>tf.app.run()</code>. Internally it uses Python's standard <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, but the API is inspired by Google's gflags.</p>
<p>Formerly Google Commandline Flags, gflags is a Google system for building standard command-line interfaces. There's a <a href="https://github.com/gflags/gflags">C++ implementation</a>, which has <a href="https://gflags.github.io/gflags/">a documentation page</a> reminiscent of a command-line interface. The <a href="https://github.com/google/python-gflags">Python gflags</a> is documented primarily via a collection of <a href="https://github.com/google/python-gflags/tree/master/examples">examples</a>.</p>
<p>Python users may be more familiar with the standard <a href="https://docs.python.org/3/library/argparse.html">argparse</a> API, or perhaps even the older <a href="https://docs.python.org/2/library/optparse.html">optparse</a>, which both achieve functionality similar to that of gflags.</p>
<p>While TensorFlow doesn't include a full gflags implementation, the TensorFlow version appears in pretty many code examples, where it imparts a little bit of extra Google flavor to argument handling for TensorFlow scripts. You can use it (though there are <a href="http://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/33938519#33938519">reasons not to</a>) or opt for a separate and possibly more full-featured package, whether it be the full <a href="https://github.com/google/python-gflags">gflags</a>, <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, or <a href="https://pythonhosted.org/horetu/">something stranger</a>.</p>
<hr>
<h3>apputils</h3>
<p>The TensorFlow <code>tf.app.run()</code> invocation doesn't actually use Google's Python apputils, but like with gflags, TensorFlow is mimicking some of the behavior of Google tooling.</p>
<p>The <a href="https://github.com/google/google-apputils">open source google-apputils for Python</a> hasn't been updated since March 2015, but it's still pretty interesting, with a lot of functionality that could still be relevant.</p>
<hr>
<h3>Bazel</h3>
<p><a href="https://bazel.build/">Bazel</a> is a build tool like <a href="https://www.gnu.org/software/make/">make</a>. Bazel is <a href="https://github.com/bazelbuild/bazel">open source</a>; <a href="https://en.wikipedia.org/wiki/Bazel_(software)">originally</a> there was Google's internal "Blaze" tool.</p>
<p>You might encounter Bazel if you're <a href="https://www.tensorflow.org/install/install_sources">building TensorFlow from source</a>, for example.</p>
<p><img alt="bazel" src="img/bazel.png"></p>
<hr>
<h3>protobuf</h3>
<p><a href="https://developers.google.com/protocol-buffers/">Protocol buffers</a> (often protobuf, or just proto) "are Google's [<a href="https://github.com/google/protobuf">open source</a>] language-neutral, platform-neutral, extensible mechanism for serializing structured data &#8211; think XML, but smaller, faster, and simpler."</p>
<p>When you look at protocol buffers in their text representations, they look something like <a href="http://www.json.org/">JSON</a>.</p>
<p>You likely won't encounter protocol buffers directly when using TensorFlow. They're used under the hood all over the place: for example, as the format for <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a> summaries, and to help make data transfer efficient in distributed settings.</p>
<hr>
<h3>gRPC</h3>
<p><a href="http://www.grpc.io/">gRPC</a> is Google's <a href="https://github.com/grpc/grpc">open source</a> framework for <a href="https://en.wikipedia.org/wiki/Remote_procedure_call">remote procedure call (RPC)</a> systems. It uses protocol buffers.</p>
<p>If RPC is unfamiliar, there is some comparison to <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">RESTful</a> communications for web services.</p>
<p>You might encounter gRPC if you build a client for a <a href="https://tensorflow.github.io/serving/">TensorFlow serving</a> server, for example.</p>
<p><img alt="gRPC" src="img/grpc.svg"></p>
<hr>
<h3>gemmlowp</h3>
<p><a href="https://github.com/google/gemmlowp">gemmlowp</a> is Google's open source low-precision matrix multiplication library.</p>
<p>You won't likely encounter gemmlowp directly, but you'll benefit from it if you're using TensorFlow for <a href="https://github.com/google/gemmlowp/blob/master/doc/low-precision.md">low-precision</a> implementations.</p>
<hr>
<h3>StreamExecutor</h3>
<p><a href="http://www.nvidia.com/">NVIDIA</a> currently dominates the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> market, and so their <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a> language for programming GPUs is the dominant GPU language. But other vendors would like to sell GPUs too, and they would like everybody to standardize on <a href="https://en.wikipedia.org/wiki/OpenCL">OpenCL</a> for programming them. Programmers would rather just program once for whatever platform; they would like to have <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">general-purpose computing on graphics processing units (GPGPU)</a>.</p>
<p>StreamExecutor is Google's GPGPU system. They've <a href="http://lists.llvm.org/pipermail/llvm-dev/2016-March/096576.html">talked about</a> open-sourcing it directly as part of the <a href="http://llvm.org/">LLVM project</a>, but as far as I know it still exists in open source only <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/stream_executor">inside TensorFlow</a>.</p>
<p>OpenCL support is not necessarily perfect, but you may be able to try it out if you <a href="https://www.tensorflow.org/install/install_sources">build from source</a>. It's <a href="https://github.com/tensorflow/tensorflow/issues/22">issue #22</a> for the <a href="https://github.com/tensorflow/tensorflow/issues/22">TensorFlow GitHub project</a>.</p>
<p>StreamExecutor is another implementation technology that you aren't likely to encounter directly as a user of TensorFlow.</p>
<hr>
<h3>GFile</h3>
<p>Google has C++ file I/O code that avoids thread locking, and <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/lib/io">this</a> is <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py">included</a> as <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/gfile.py">part</a> TensorFlow.</p>
<p>This is another implementation technology that should benefit TensorFlow users silently within the system.</p>
<hr>
<h3>XLA</h3>
<p>The <a href="https://www.tensorflow.org/versions/master/experimental/xla/">Accelerated Linear Algebra (XLA)</a> system is described as <a href="https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html">the TensorFlow compiler framework</a>, but there's more to it than that.</p>
<p>For one thing, the aspects of XLA that appear in open source TensorFlow aren't all the XLA that exists; Google uses their own hardware, <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">TPUs</a>, and both that hardware and the XLA components to target it are internal to Google.</p>
<p>For another, XLA has been described as <a href="https://autodiff-workshop.github.io/slides/JeffDean.pdf">"designed for reuse"</a>, and other projects could incorporate the technology as well.</p>
<p>XLA is still experimental, and largely deep inside TensorFlow's implementation. Users might <a href="https://gist.github.com/yaroslavvb/53052184e50cdfec35f0a127dd6df843">try</a> turning on XLA to see whether it helps speed up their computations.</p>
<hr>
<p>Thanks to Googlers <a href="https://twitter.com/juliaferraioli">Julia Ferraioli</a>, <a href="https://twitter.com/petewarden">Pete Warden</a>, and <a href="https://twitter.com/martin_wicke">Martin Wicke</a>, for a brief <a href="https://twitter.com/petewarden/status/841062427202527232">twitter exchange</a> that informed this list, and to <a href="https://twitter.com/mrry">Derek Murray</a> for <a href="https://twitter.com/mrry/status/841315298221342720">correcting</a> my misconception about gflags in TensorFlow.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170313-tensorflow_use_of_google_technologies/</link>
<guid>http://planspace.org/20170313-tensorflow_use_of_google_technologies/</guid>
<pubDate>Mon, 13 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Use only what you need from TensorFlow</title>
<description><![CDATA[

<p>There isn't just one decision to use <a href="https://www.tensorflow.org/">TensorFlow</a> or not use TensorFlow; you have to make decisions about which pieces of TensorFlow you're going to use.</p>
<p>I've thought about <a href="/20170311-does_tensorflow_suffer_from_the_second_system_effect/">whether Tensorflow suffers from the second-system effect</a>, and my conclusion is that while TensorFlow has a huge abundance of features, it can't really be said to "suffer" from this. The engineering is solid and it supports very interesting applications.</p>
<p>For the individual user of TensorFlow, however, <a href="https://en.wikipedia.org/wiki/Overengineering">over-engineering</a> is a relative term, and it's relative to the problem at hand.</p>
<p>One relevant dimension to consider is where you'll be running your code, which is related to the size of your data.</p>
<p><img alt="mac mini" src="img/mac_mini.jpg"></p>
<p>If your data fits in memory and you're running on one local machine, a lot of simple approaches will work for you.</p>
<p><img alt="data center" src="img/data_center.jpg"></p>
<p>If you're using massive quantities of data on multitudes of machines in a data center, you have a lot of concerns that you wouldn't have on one machine.</p>
<p>TensorFlow can run well in large distributed settings. But if you're not going to run that way, you may not need to use all that functionality.</p>
<p><img alt="TensorFlow data pipeline" src="img/tf_data_pipeline.gif"></p>
<p>For example, the bulk of the TensorFlow <a href="https://www.tensorflow.org/programmers_guide/reading_data">documentation on reading data</a> focuses on a data pipeline that involves multiple processes and multiple coordinating queues. For reading large datasets from network storage (in <a href="https://wiki.apache.org/hadoop/HDFS">HDFS</a>, say) this might make a lot of sense.</p>
<p><img alt="the Titanic" src="img/titanic.jpg"></p>
<p>On the other hand, you may be trying out the <a href="https://www.kaggle.com/c/titanic">Titanic machine learning challenge</a> on <a href="https://www.kaggle.com/">Kaggle</a>, where the entire training set is a single CSV file with 892 lines.</p>
<p><img alt="the Titanic sinking" src="img/titanic_sinking.jpg"></p>
<p>If you decide you need to use the full TensorFlow data pipeline instead of something like <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html">pandas.read_csv</a>, you're doing a lot of unnecessary work, before you've even started working on the substance of your problem.</p>
<p>There's something to be said for trying out functionality you don't really need, just for the sake of learning it for possible future application. And if you're working on a toy problem like <a href="https://www.kaggle.com/c/titanic">Titanic</a>, maybe that is exactly what you want to do.</p>
<p>But for most work, if <a href="https://en.wikipedia.org/wiki/You_aren't_gonna_need_it">you aren't gonna need it</a>, don't use it&#8212;even if it's available in TensorFlow.</p>    
    ]]></description>
<link>http://planspace.org/20170312-use_only_what_you_need_from_tensorflow/</link>
<guid>http://planspace.org/20170312-use_only_what_you_need_from_tensorflow/</guid>
<pubDate>Sun, 12 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Does TensorFlow Suffer from the Second-System Effect?</title>
<description><![CDATA[

<p>In <a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month">The Mythical Man Month</a>, Fred Brooks includes an essay called <a href="http://wiki.c2.com/?SecondSystemEffect">The Second-System Effect</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month"><img alt="The Mythical Man Month (book cover)" src="img/mythical_man_month.jpg"></a></p>
<p>The second-system effect describes two problems likely when building a new project like one you've done before:</p>
<ul>
<li>too many features: "frill after frill and embellishment after embellishment"</li>
<li>focus on the wrong features: "a tendency to refine [obsolete] techniques"</li>
</ul>
<p>The "too many features" problem is the one most commonly associated with the second-system effect.</p>
<hr>
<p><a href="https://www.tensorflow.org/">TensorFlow</a> is a second system, in that Google had a system called <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf">DistBelief</a>, and TensorFlow is <a href="http://download.tensorflow.org/paper/whitepaper2015.pdf">explicitly</a> the second-generation system based on experience with DistBelief.</p>
<p><a href="https://www.tensorflow.org/"><img alt="TensorFlow" src="img/tensorflow.jpg"></a></p>
<p>TensorFlow has a <em>lot</em> of features. It aims to support research as well as production, running on multiple processing targets (CPU, GPU, etc.) on single devices and distributed, on mobile devices, workstations, and data centers. It includes a visualization system. It includes a serving system. It includes a compiler. It includes multiple high-level APIs. The tagline it uses is <a href="https://www.youtube.com/watch?v=mWl45NkFBOc">Machine Learning for Everyone</a>.</p>
<p>TensorFlow may not focus on obsolete features, but it also doesn't necessarily focus on features that matter to every user. The fundamental design priority of the computation graph, which then supports device-specific optimization and distributed execution, also introduces conceptual and computational overhead that may do more harm than good for users who won't use these features.</p>
<hr>
<p>While TensorFlow could be seen as a second system, and has some possible symptoms of the second-system effect, criticism of TensorFlow is rare.</p>
<p>Do people love TensorFlow as much as it seems they do? Or are some people quietly griping about (or just ignoring) TensorFlow, with issues like <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> being supported on GPU but not CPU?</p>
<p>Is the second-system effect no longer relevant? The Mythical Man-Month was published in 1975. Using 26 bytes to handle leap years was an example of second-system excess for Brooks. And this was a <a href="https://en.wikipedia.org/wiki/Waterfall_model">waterfall</a> closed-source world.</p>
<p>Brooks also focused on individuals: the second-system effect was specifically about second systems <em>for individual developers</em> (or architects). TensorFlow is certainly not the second system <a href="https://www.quora.com/What-are-all-the-Jeff-Dean-facts">Jeff Dean</a> has been involved with. Maybe Google's engineering culture is so good, the second-system effect is no match for it.</p>
<p>Or was the second-system effect never really a useful idea? Second systems have always had the benefit of experience, as much or more than the temptation of feature creep.</p>    
    ]]></description>
<link>http://planspace.org/20170311-does_tensorflow_suffer_from_the_second_system_effect/</link>
<guid>http://planspace.org/20170311-does_tensorflow_suffer_from_the_second_system_effect/</guid>
<pubDate>Sat, 11 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>OmegaGo</title>
<description><![CDATA[

<p><em>This is a little fun presentation for <a href="http://dc.hackandtell.org/">DC Hack and Tell</a> on <a href="https://www.meetup.com/DC-Hack-and-Tell/events/236404104/">Thursday, February 16, 2017</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p></p><center>
&#175;\_&#937;_/&#175;
</center>
<hr>
<p>Here is a very serious logo.</p>
<p>(It's based on <a href="http://emojipedia.org/shrug/">&#175;\_(&#12484;)_/&#175;</a>, if that wasn't clear.)</p>
<hr>
<p><img alt="pi go wave" src="img/pi_wave.jpg"></p>
<hr>
<p>Here's what Go sometimes looked like in <a href="https://en.wikipedia.org/wiki/Pi_(film)">Pi</a>.</p>
<hr>
<p><img alt="pi go game" src="img/pi_game.png"></p>
<hr>
<p>Go really looks more like this. It's a game, see?</p>
<hr>
<p><img alt="game AIs on xkcd" src="img/game_ais.png"></p>
<hr>
<p><a href="https://xkcd.com/">xkcd</a> had this <a href="https://xkcd.com/1002/">cartoon</a> about game AIs and Go was on it.</p>
<p>Go was supposed to be hard for computers to be good at.</p>
<hr>
<p><img alt="alphago game" src="img/alphago_game.jpg"></p>
<hr>
<p>But... <a href="https://deepmind.com/research/alphago/">AlphaGo</a>.</p>
<hr>
<p>TWO THINGS</p>
<hr>
<p>First technically interesting things, and then a personally interesting thing.</p>
<hr>
<p><img alt="nets" src="img/nets.png"></p>
<hr>
<p>Neural nets! Policy networks! Value networks!</p>
<p>Interesting, but/and I do that kind of thing for work.</p>
<p>(Figure from the <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Nature article</a>.)</p>
<hr>
<p><img alt="youtube course" src="img/silver.png"></p>
<hr>
<p>David Silver's <a href="https://www.youtube.com/watch?v=2pWv7GOvuf0">reinforcement learning course</a>.</p>
<hr>
<p><img alt="tree search" src="img/tree_search.png"></p>
<hr>
<p>Okay tree search is the part I'm interested in.</p>
<p>(Figure from the <a href="http://www.nature.com/nature/journal/v529/n7587/full/nature16961.html">Nature article</a>.)</p>
<hr>
<p>me: 0</p>
<p>computer: &#8734;</p>
<hr>
<p>The other thing is that I suck at Go, so I don't need/want a Go computer to be so spanking good.</p>
<hr>
<p></p><center>
&#175;\_&#937;_/&#175;
</center>
<hr>
<p>Here is a very serious logo.</p>
<hr>
<p>code!</p>
<hr>
<p>Talk about code here. It's <a href="https://github.com/ajschumacher/omegago">on github</a>.</p>
<hr>
<p></p><center>
&#175;\_&#937;_/&#175;
</center>
<hr>
<p>Here is a very serious logo.</p>
<hr>
<p>demo!</p>
<hr>
<p>Demo that code from up there!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170216-omegago/</link>
<guid>http://planspace.org/20170216-omegago/</guid>
<pubDate>Thu, 16 Feb 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Presenting “Hello, TensorFlow!” Again</title>
<description><![CDATA[

<p><em>This is an updated presentation version of my &#8220;Hello, TensorFlow!&#8221; <a href="https://www.oreilly.com/learning/hello-tensorflow">O'Reilly post</a>. The <a href="/20160629-presenting_hello_tensorflow/">original</a> was given at the <a href="http://www.meetup.com/TensorFlow-Washington-DC/">TensorFlow Washington DC</a> meetup on Wednesday June 29, 2016, &#8220;<a href="http://www.meetup.com/TensorFlow-Washington-DC/events/231860618/">Deep Dive into TensorFlow</a>.&#8221; This version was created for a Wednesday October 5 2016 guest presentation at the DC <a href="https://generalassemb.ly/education/data-science-immersive">General Assembly Data Science Immersive</a> and for a Wednesday October 26 2016 presentation at the <a href="http://dc.gputechconf.com/">NVIDIA GPU Technology Conference (GTC) in DC</a>. (<a href="big.html">slides</a>) (<a href="https://mygtc.gputechconf.com/events/35/schedules/3388">NVIDIA page with video link</a>) (<a href="https://www.youtube.com/watch?v=n350wsivoQk">video on youtube</a>) <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> is now <a href="https://www.safaribooksonline.com/oriole/hello-tensorflow-oriole">an interactive code and video Oriole</a> as well.</em></p>
<p>I've given this talk a couple times, which means this is the improved version.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Hello, TensorFlow! on O'Reilly" src="img/screenshot.png"></p>
<hr>
<p>This presentation started as a blog post I <a href="/20160619-writing_with_oreilly/">wrote</a> which was <a href="https://www.oreilly.com/learning/hello-tensorflow">published</a> on O'Reilly.</p>
<p>If you want you can follow along <a href="https://www.oreilly.com/learning/hello-tensorflow">there</a> and we'll stay pretty close to <a href="https://www.oreilly.com/learning/hello-tensorflow">that content</a>.</p>
<p>I also recorded an O'Reilly <a href="http://www.oreilly.com/oriole/">Oriole</a> video and interactive version which should come out some time soon on O'Reilly's <a href="http://safari.oreilly.com">Safari</a> service.</p>
<hr>
<p><img alt="Hello, TensorFlow! on O'Reilly China" src="img/screenshot_chinese.png"></p>
<hr>
<p>O'Reilly also translated and published a <a href="https://www.oreilly.com.cn/ideas/?p=533">Chinese version</a>, so if you prefer to read Chinese you can do that too.</p>
<hr>
<p>UNCLASSIFIED</p>
<hr>
<p>That reminds me, the classification level for this talk is UNCLASSIFIED.</p>
<p><em>This is a little joke for the government-heavy <a href="http://dc.gputechconf.com/">GTC DC</a> audience, which I hope nobody has reason to take offense at.</em></p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>A lot of people have already contributed to this material. I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA) and have to especially thank John for his support, and all my colleagues there for their feedback. I'm also particularly indebted to the <a href="http://www.meetup.com/DC-Machine-Learning-Journal-Club/">DC Machine Learning Journal Club</a>, and of course the folks at O'Reilly.</p>
<hr>
<p><img alt="caffe and tensorflow contributions on github" src="img/caffe_tf_contrib.png"></p>
<hr>
<p>A brief note on popularity as measured by development activity on <a href="https://github.com/">GitHub</a>. <a href="http://caffe.berkeleyvision.org/">Caffe</a> is another framework you might consider for deep learning. This is one way to compare <a href="https://github.com/BVLC/caffe/graphs/contributors">Caffe</a> and <a href="https://github.com/tensorflow/tensorflow/graphs/contributors">TensorFlow</a>. TensorFlow is pretty active &#8212; so much so that it can be difficult to keep up with all the developments.</p>
<p><em>Data and details for producing this graph are in <a href="contrib_plot.ipynb">contrib_plot.ipynb</a>.</em></p>
<hr>
<p><img alt="braided river" src="img/braided_river.jpg"></p>
<hr>
<p>I'm going to try to show in a very hands-on way some of the details of how TensorFlow works. The idea is not to exercise all the features of TensorFlow but to really understand some of the important underlying concepts.</p>
<p><em>Image from <a href="https://www.flickr.com/photos/alaskanps/8029733984">National Park Service, Alaska Region on Flickr</a>.</em></p>
<hr>
<ul>
<li>tensors</li>
<li>flows</li>
<li>GPUs</li>
<li>pictures</li>
<li>servers</li>
</ul>
<hr>
<p>There are a couple interesting things about TensorFlow.</p>
<p>I'm really only going to demonstrate the middle two, but let's mention them all.</p>
<hr>
<p><img alt="not neurons, tensors" src="img/not_neurons.png"></p>
<hr>
<p>First thing about TensorFlow: tensors.</p>
<p>TensorFlow hates neurons and it loves linear algebra.</p>
<p>When you design software you make choices that affect what you can do and how you can do it.</p>
<p>For neural nets, there has in the past been a fixation on the biological metaphor and the "neurons" in particular. If you've used software like R's <a href="https://cran.r-project.org/web/packages/neuralnet/">neuralnet</a> package or Python's <a href="http://pybrain.org/">PyBrain</a>, you've seen interfaces where you typically define the number of neurons or "hidden units" that you want in each layer, and the connections between them (and perhaps some other components) are set up automatically one way or another.</p>
<p>TensorFlow does also provide a higher-level interface in <a href="https://www.tensorflow.org/versions/master/tutorials/tflearn/index.html">tf.contrib.learn</a> that talks about hidden units and so on. But if you're looking at the math you have to evaluate inside these neural net models, the "neurons" hardly exist at all. The weights on the connections certainly exist, and there are operations that connect values as they flow through the network, but in the end you don't really need "neurons" to be a dominant abstraction in the system.</p>
<p>If you go this route, everything is just matrix math - or, for arbitrary dimensions, tensor math. This is the choice TensorFlow makes. It makes TensorFlow really flexible and lets it achieve efficient computation that would be more difficult otherwise.</p>
<p>So TensorFlow can do more than "just" neural nets. As demonstrated in the <a href="https://www.udacity.com/course/deep-learning--ud730">Google/Udacity Deep Learning MOOC</a>, you can implement logistic regression easily with TensorFlow, for example. There's even now a section of non-machine-learning applications in the <a href="https://www.tensorflow.org/versions/r0.11/tutorials/index.html">official tutorials</a>.</p>
<p>Focusing on (typically fixed-dimension) tensors does arguably make it more awkward to attempt esoteric techniques like dynamically changing the architecture of your neural net by adding neurons, for example. (This comment was inspired by <a href="https://drive.google.com/file/d/0Bxf-khCt_eknWEF6aFJ1ZU4yQ00/view">Let your Networks Grow</a>, <a href="https://sites.google.com/site/nnb2tf/">Neural Nets Back to the Future</a> at <a href="http://icml.cc/2016/">ICML 2016</a>.)</p>
<p>The linear algebra you actually have to think about is not too tricky, but for our purposes I'm not going to use anything more than individual numbers, so there's no chance of being distracted by linear algebra.</p>
<p>The relevant point is that while some other software focuses on neurons and mostly ignores weights, in the case of TensorFlow we focus on weights and mostly ignore neurons as a top-level abstraction. In any event, it's all just numbers.</p>
<hr>
<p><img alt="simple computation graph" src="img/math_graph.png"></p>
<hr>
<p>Second thing about TensorFlow: flows.</p>
<p>TensorFlow loves graphs.</p>
<p>It's important to distinguish between graphs and pictures. Here we mean <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graphs</a> as in the abstract data type. Data flow graphs.</p>
<p>This is a picture of a data flow graph. You might prefer to think of it as \(y=mx+b\). This is what TensorFlow's graphs are like.</p>
<p>The advantage of graphs is that the software can manipulate them and "reason about" how to answer questions you're asking before it commits to particular execution paths.</p>
<p>Lots of software is moving toward working via graphs in this way. Often they're <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> but I don't care too much about whether that's always strictly the case. <a href="https://twitter.com/planarrowspace/status/569963882900561921">&#129370;</a></p>
<p>For example, <a href="http://deeplearning.net/software/theano/">Theano</a> works similarly enough to TensorFlow that <a href="http://keras.io/">Keras</a> can use either as its backend. <a href="http://torch.ch/">Torch</a> is also similar.</p>
<p>Outside of software mostly for deep learning, <a href="http://spark.apache.org/">Spark</a> works with a kind of computation graph to manage distributed processing. Tools like <a href="https://github.com/spotify/luigi">Luigi</a> and <a href="https://github.com/Factual/drake">Drake</a> manage graph pipelines of data. And many <a href="http://drivendata.github.io/cookiecutter-data-science/">recommend</a> that all data work be thought of as graph-like pipelines.</p>
<p>The graph concept is interesting and worth exploring in order to understand TensorFlow, so I'll spend some time with it today.</p>
<p><em>Image made with <a href="https://draw.io/">draw.io</a>.</em></p>
<hr>
<p><img alt="GPU" src="img/gpu.jpg"></p>
<hr>
<p>In part because it structures computations with tensors in a computation graph that it can manage, TensorFlow is able to move computation onto GPUs in ways that can improve performance. This can happen automatically or be specified in code; see the <a href="https://www.tensorflow.org/versions/r0.11/how_tos/using_gpu/">Using GPUs tutorial</a>.</p>
<p>This GPU utilization is not only for "neural" computations; TensorFlow can be used as an easy way to move many computations onto GPUs, without having to write any CUDA (etc.) yourself.</p>
<p>You can also distribute computation across multiple GPUs (and multiple machines) but it isn't always automatic. <a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html">TF-Slim</a> provides some helpful functionality, and more seems to be becoming available all the time.</p>
<hr>
<p><img alt="Visualization of a TensorFlow graph" src="img/graph_vis_animation.gif"></p>
<hr>
<p>Third thing about TensorFlow: pictures.</p>
<p>If you use Python's <a href="http://scikit-learn.org/">scikit-learn</a>, you likely also use a separate visualization tool, likely including <a href="http://matplotlib.org/">matplotlib</a>. If you've tried to <a href="/20151129-see_sklearn_trees_with_d3/">visualize decision tree rules</a>, you may have had a frustrating time. You may or may not use any explicit logging system at all.</p>
<p>The <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md">TensorBoard</a> tooling that comes with TensorFlow is a great logging and visualizing system that gives you access to a lot of internals that could otherwise be a lot of work to expose.</p>
<p>I think TensorBoard <em>by itself</em> is a compelling reason to use TensorFlow over other similar software.</p>
<p>TensorBoard is super great and I will be showing several sides of it today.</p>
<p><em>Image from <a href="https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/index.html">TensorBoard: Graph Visualization</a>.</em></p>
<hr>
<p><img alt="Life of a Servable" src="img/serving_architecture.svg"></p>
<hr>
<p>Fourth thing about TensorFlow: servers.</p>
<p>TensorFlow loves moving directly into production.</p>
<p>In addition to everything else, TensorFlow has a highly-engineered serving architecture. So if you develop a TensorFlow model on your laptop, you can (relatively) easily build a deployment system to serve that model in a serious way. It seems pretty neat, but I haven't used it and I'm not going to talk more about it.</p>
<p><em>Image from the <a href="https://tensorflow.github.io/serving/architecture_overview">TensorFlow Serving Architecture Overview</a>.</em></p>
<hr>
<p>demo</p>
<hr>
<p>From here to the end is just demo! The starting point is <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20160629-presenting_hello_tensorflow/demo_begin.ipynb">demo_start.ipynb</a> and the ending point (with output) is <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20160629-presenting_hello_tensorflow/demo_end.ipynb">demo_end.ipynb</a>.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20161005-presenting_hello_tensorflow_again/</link>
<guid>http://planspace.org/20161005-presenting_hello_tensorflow_again/</guid>
<pubDate>Wed, 05 Oct 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Highlights from ICML 2016</title>
<description><![CDATA[

<p>I was fortunate to attend the <a href="http://icml.cc/">International Conference on Machine Learning</a> in New York from June 19-24 this year. There were walls and walls of multi-arm bandit posters (etc.) but I mostly checked out neural net, deep learning things. Here are my highlights.</p>
<p>In general: People are doing so many interesting things! Wow. Too many things to try. Architectures are more complex and more flexibly experimented with, it seems. And everybody loves <a href="https://arxiv.org/abs/1502.03167">batch norm</a>. I didn't see anything that seemed like a huge conceptual breakthrough that would change the direction of the field, but maybe I'm wishing for too much, maybe the field doesn't need such a thing, or maybe it was there and I missed it!</p>
<p>For my breakdown of the conference <em>structure</em> itself, see <a href="/20160703-conferences_what_is_the_deal/">Conferences: What is the Deal?</a>.</p>
<hr>
<p>Flashiest paper award: <a href="https://arxiv.org/abs/1605.05396">Generative Adversarial Text to Image Synthesis</a> is a system where you put in text and the computer makes a picture.</p>
<p><a href="https://arxiv.org/abs/1605.05396"><img alt="birds" src="img/birds.png"></a></p>
<p>Good job, computer!</p>
<hr>
<p>Best talk award: <a href="http://www.slideshare.net/ajschumacher/machine-learning-applications-at-bell-labs-holmdel">Machine Learning Applications at Bell Labs, Holmdel</a> was Larry Jackel's history of neural nets, starting back in 1976. Great perspective and personal stories, presented as part of <a href="https://sites.google.com/site/nnb2tf/">Neural Nets Back to the Future</a>.</p>
<p><a href="http://www.slideshare.net/ajschumacher/machine-learning-applications-at-bell-labs-holmdel"><img alt="bets" src="img/bets.png"></a></p>
<p>Yes, Jackel's bets involve the <a href="https://en.wikipedia.org/wiki/Vladimir_Vapnik">V</a> of <a href="https://en.wikipedia.org/wiki/VC_dimension">VC Dimension</a>, the <a href="https://en.wikipedia.org/wiki/Yann_LeCun">Le</a> of <a href="http://yann.lecun.com/exdb/lenet/">LeNet</a>, and SGD-master <a href="https://en.wikipedia.org/wiki/L%C3%A9on_Bottou">Bottou</a>.</p>
<hr>
<p>The first <a href="http://icml.cc/2016/?page_id=97">tutorial</a> I heard was from <a href="http://kaiminghe.com/">Kaiming He</a> on <a href="http://icml.cc/2016/tutorials/icml2016_tutorial_deep_residual_networks_kaiminghe.pdf">Deep Residual Networks</a>. ResNets and their cousins (like <a href="http://people.idsia.ch/~rupesh/very_deep_learning/">highway networks</a>) succeed essentially because wiggling a little tends to be better than going somewhere entirely different.</p>
<p>The intuition: It's hard to train deeper networks. But they ought to be better! Well, if we take a net and add another layer that encodes the identity, that's deeper, and exactly as good! What if all the layers were like the identity plus a little bit that's learned?</p>
<p>I was also impressed that, for example, a 152-layer ResNet can have fewer parameters than a 16-layer VGG net. Models aren't always getting more complex in terms of number of parameters!</p>
<p>Slightly more: If you're careful where you put nonlinearities, it's like you're always <a href="https://twitter.com/planarrowspace/status/744525688829534208">adding</a> instead of multiplying! Stacking two convolutional layers before "reconnecting" works better in practice than doing one or three!</p>
<p>Boom: you can train a model with a thousand layers.</p>
<hr>
<p>The second tutorial I <a href="https://twitter.com/planarrowspace/status/744545675896033280">heard</a> was from <a href="http://www.thespermwhale.com/jaseweston/">Jason Weston</a> on <a href="http://www.thespermwhale.com/jaseweston/icml2016/">Memory Networks for Language Understanding</a>. Interesting references:</p>
<ul>
<li><a href="http://arxiv.org/pdf/1511.08130v2.pdf">A Roadmap towards Machine Intelligence</a></li>
<li><a href="http://arxiv.org/pdf/1506.03340v3.pdf">Teaching Machines to Read and Comprehend</a></li>
<li><a href="https://arxiv.org/abs/1502.05698">Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks</a></li>
<li><a href="https://research.facebook.com/research/babi/">bAbI natural language tasks</a></li>
</ul>
<p>It was neat seeing what can and can't be done currently for the "toy tasks." And Weston was <a href="https://twitter.com/planarrowspace/status/744549249019437062">stylish</a>.</p>
<hr>
<p>The third tutorial I heard was <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>'s on <a href="http://hunch.net/~beygel/deep_rl_tutorial.pdf">Deep Reinforcement Learning</a> (with <a href="http://icml.cc/2016/tutorials/AlphaGo-tutorial-slides.pdf">bonus deck on AlphaGo</a>). Reinforcement learning is a big deal and getting bigger, it seems. I should really get into it more. Silver has <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course materials</a> on his site. Also, <a href="https://openai.com/">OpenAI</a> is into it, and they have a whole <a href="https://gym.openai.com/">OpenAI Gym</a> where you can play with things. Neat!</p>
<hr>
<p>I liked the problem statement and solution of CReLUs. Too briefly: With ReLUs you tend to get paired positive/negative filters because one filter can't represent both. So build pairing into your system, and things work better. Very cogent content. Paper: <a href="https://arxiv.org/abs/1603.05201">Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units</a></p>
<hr>
<p>On sizing layers, an <a href="https://twitter.com/planarrowspace/status/745602200366354432">SVD approach</a> based on statistical structure in the weights was interesting. Paper: <a href="https://arxiv.org/abs/1602.02285">A Deep Learning Approach to Unsupervised Ensemble Learning</a></p>
<hr>
<p><a href="https://arxiv.org/abs/1605.08283">Discrete Deep Feature Extraction: A Theory and New Architectures</a> is a little math-heavy but close to interesting ideas about understanding and using the internals of neural networks.</p>
<hr>
<p><a href="https://arxiv.org/abs/1512.09300">Autoencoding beyond pixels using a learned similarity metric</a> does neat things using the internals of a network for more network work - it might be metaphorically "reflective" - and comes up with some flashy visual results, too.</p>
<hr>
<p><a href="https://arxiv.org/abs/1606.01583">Semi-Supervised Learning with Generative Adversarial Networks</a> shows that a fairly natural idea for GANs does in fact work: you can use labels like "real_a, real_b, real_c, fake" instead of just "real, fake".</p>
<blockquote>
<p>We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN.</p>
</blockquote>
<hr>
<p><a href="https://arxiv.org/abs/1511.06393">Fixed Point Quantization of Deep Convolutional Networks</a> was pretty neat, if you're into that kind of thing.</p>
<hr>
<p>I really liked the <a href="https://sites.google.com/site/nnb2tf/">Neural Nets Back to the Future</a> workshop.</p>
<p><img alt="Back to the Future" src="img/bttf.jpg"></p>
<p>I was so interested in the historical perspective from the experienced people presenting, for a while I was <a href="https://twitter.com/planarrowspace/status/745958455865933824">tweeting</a> every slide.</p>
<hr>
<p>From pictures I took (and <a href="https://twitter.com/planarrowspace/status/745959355724468224">tweeted</a>) I reconstructed <a href="http://www.slideshare.net/ajschumacher/machine-learning-applications-at-bell-labs-holmdel">slides</a> for "Machine Learning Applications at Bell Labs, Holmdel" from Larry Jackel, as mentioned above.</p>
<hr>
<p>I also <a href="https://twitter.com/planarrowspace/status/745972920040689664">tweeted</a> all through Gary Tesauro's talk, but thought it was of less general interest.</p>
<hr>
<p>Patrice Simard's <a href="http://papers.nips.cc/paper/833-backpropagation-without-multiplication">Backpropagation without Multiplication</a> was fascinating, and much better for his commentary. For example:</p>
<blockquote>
<p>Discretizing the gradient is potentially very dangerous. Convergence may no longer be guaranteed, learning may hecome prohibitively slow, and final performance after learning may be be too poor to be interesting,</p>
</blockquote>
<p>Simard said that quote from the paper isn't really true; you can just use his method without worrying about that. This is a quote I <a href="https://twitter.com/planarrowspace/status/746035221318012928">took down</a> as he was speaking:</p>
<blockquote>
<p>Gradient descent is so robust to errors in the gradient, your code probably has bugs, but you don't need to fix them.</p>
</blockquote>
<hr>
<p>During a panel discussion, Yann LeCun said max pooling makes adversarial training unstable, and so it's better to use strided convolution if you want to downsample.</p>
<p>Later I noticed related sentiment in Karpathy's <a href="http://cs231n.github.io/convolutional-networks/">notes</a>:</p>
<blockquote>
<p><em>Getting rid of pooling.</em>&#8203; Many people dislike the pooling operation and think that we can get away without it. For example, <a href="http://arxiv.org/abs/1412.6806">Striving for Simplicity: The All Convolutional Net</a> proposes to discard the pooling layer in favor of architecture that only consists of repeated CONV layers. To reduce the size of the representation they suggest using larger stride in CONV layer once in a while. Discarding pooling layers has also been found to be important in training good generative models, such as variational autoencoders (VAEs) or generative adversarial networks (GANs). It seems likely that future architectures will feature very few to no pooling layers.</p>
</blockquote>
<hr>
<p><a href="http://www.bcl.hamilton.ie/~barak/">Barak Pearlmutter</a> gave a <a href="https://sites.google.com/site/nnb2tf/abstracts#pearlmutter">talk</a> about automatic differentiation and a really fast system called vlad that I think runs on <a href="https://github.com/barak/stalin">stalin</a>. Pretty impressive speed! Following up later he did add:</p>
<blockquote>
<p>keep in mind that if all your computations are actually array
operations on the GPU, which takes all the time, then the overhead
of the AD system might not really matter since it's just scheduling
the appropriate derivative-computing array operations on the GPU.</p>
</blockquote>
<p>The next day <a href="https://twitter.com/ryan_p_adams">Ryan Adams</a> gave a talk on the same topic at the <a href="https://sites.google.com/site/automl2016/scientific-program">AutoML workshop</a> featured a Python <a href="https://github.com/HIPS/autograd">autograd</a> package that seemed to have some of the features (closure, for example) as vlad.</p>
<p>Everybody loves automatic differentiation!</p>
<hr>
<p>I was briefly at the <a href="https://sites.google.com/site/ondeviceintelligence/icml2016">Workshop on On-Device Intelligence</a> but ended up wandering.</p>
<hr>
<p>I visited the <a href="https://sites.google.com/site/dataefficientml/">Data-Efficient Machine Learning</a> workshop and saw <a href="http://andrewgelman.com/">Gelman</a>'s <a href="https://twitter.com/planarrowspace/status/746357503009689601">amusing</a> talk called Toward Routine Use of Informative Priors.</p>
<hr>
<p>I spent some time at the <a href="https://sites.google.com/site/mlsys2016/schedule">Machine Learning Systems</a> workshop.</p>
<p>Unfortunately I missed <a href="http://daggerfs.com/">Yangqing Jia</a>'s <a href="https://docs.google.com/viewer?a=v&amp;pid=sites&amp;srcid=ZGVmYXVsdGRvbWFpbnxtbHN5czIwMTZ8Z3g6NGM3ZTgxYmYyZDBiM2VjNA">Towards a better DL framework: Lessons from Brewing Caffe</a>.</p>
<p>I did see <a href="http://www.slideshare.net/justinbasilico/is-that-a-time-machine-some-design-patterns-for-real-world-machine-learning-systems">Design Patterns for Real-World Machine Learning Systems</a>, at which I was impressed at the <a href="https://twitter.com/planarrowspace/status/746394115240239105">number</a> of named machine learning design patterns at Netflix.</p>
<hr>
<p>I closed out the conference at the <a href="https://sites.google.com/site/automl2016/scientific-program">AutoML workshop</a>. Of possible interest in connection with this topic is a recent <a href="http://www.darpa.mil/news-events/2016-06-17">DARPA effort</a>.</p>    
    ]]></description>
<link>http://planspace.org/20160707-icml_2016/</link>
<guid>http://planspace.org/20160707-icml_2016/</guid>
<pubDate>Thu, 07 Jul 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Conferences: What is the Deal?</title>
<description><![CDATA[

<p>I was fortunate to attend all six days of the
<a href="http://icml.cc/">International Conference on Machine Learning</a> in New
York this year. I hadn't attended such a conference before. Here's
what I learned about how academic conferences like ICML go down.</p>
<h3>Conferences: The Deal</h3>
<p><em>Tutorials</em>: I usually think of tutorials as being mostly how-to
affairs, possibly with a hands-on component. But ICML tutorials are
just long talks. They run for two hours each, and provide fairly
general overviews of a given sub-field or problem type. They're good
for getting a feel for a broad area, with less focus on new work. The
first day of the conference (Sunday) was given over to these.</p>
<p><em>Receptions/Parties/etc.</em>: There's a ton of food and drink and people!
There are official conference events, and then there are also
company-sponsored affairs. There's a "job opportunities" email list
you can get on when you register for the conference, but the real
reason to subscribe is to get the invites for all the social events.
It's exhausting.</p>
<p><em>Main conference</em>: People give fifteen-minute talks about their new
papers in giant rooms with barely any electrical outlets. This goes on
for three days. Monday. Tuesday. Wednesday.</p>
<p><em>Poster sessions</em>: These are what you really want to go to during the
main conference days. You can quickly check out way more papers than
you could sit through the talks for, and you can usually start talking
to one of the authors immediately if you're interested. I found more
cool things this way. I suppose if you were really proactive you could
check out all the papers in advance online, but then why are you at
the conference anyway? (That's a good question in general, by the
way; see below.)</p>
<p><em>Workshops</em>: The workshops are like whole mini-conferences, each a day
long. These ran on Thursday and Friday. They succeed or fail with
their organizers. I went to a great one on Thursday, and I visited
four different ones on Friday. Many have their own poster sessions,
which are great if you can hit them.</p>
<h3>Conferences: A Good Deal?</h3>
<p>Maybe you should stay home! With papers on <a href="https://arxiv.org/">arXiv</a>
and a lot of video turning up online, it may not be worth the trip and
days of time, if all you want to do is learn about work in the field.
Of course if you want to meet collaborators or look for a job, your
incentives are quite different.</p>
<p>I'm sure other conferences are different. Even the conference that
might be a "competitor" with ICML, <a href="https://nips.cc/">NIPS</a>, has quite
a different feel, I'm led to understand.</p>    
    ]]></description>
<link>http://planspace.org/20160703-conferences_what_is_the_deal/</link>
<guid>http://planspace.org/20160703-conferences_what_is_the_deal/</guid>
<pubDate>Sun, 03 Jul 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Presenting “Hello, TensorFlow!”</title>
<description><![CDATA[

<p><em>A presentation version of my &#8220;Hello, TensorFlow!&#8221; <a href="https://www.oreilly.com/learning/hello-tensorflow">O'Reilly post</a>. Given at the <a href="http://www.meetup.com/TensorFlow-Washington-DC/">TensorFlow Washington DC</a> meetup on Wednesday June 29, 2016, &#8220;<a href="http://www.meetup.com/TensorFlow-Washington-DC/events/231860618/">Deep Dive into TensorFlow</a>&#8221; (<a href="https://www.eventbrite.com/e/washington-dc-meetup-deep-dive-into-tensorflow-tickets-26035651334">registration</a>). (<a href="big.html">slides</a>) (<a href="http://blog.altoros.com/how-tensorflow-can-be-applied-to-wildfire-detection-and-prediction.html">summary with some video</a>) <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> is now <a href="https://www.safaribooksonline.com/oriole/hello-tensorflow-oriole">an interactive code and video Oriole</a> as well.</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Hello, TensorFlow! on O'Reilly" src="img/screenshot.png"></p>
<hr>
<p>This presentation started as a blog post I <a href="/20160619-writing_with_oreilly/">wrote</a> which was <a href="https://www.oreilly.com/learning/hello-tensorflow">published</a> on O'Reilly about a week and a half ago.</p>
<p>If you want you can follow along <a href="https://www.oreilly.com/learning/hello-tensorflow">there</a> and we'll stay pretty close to <a href="https://www.oreilly.com/learning/hello-tensorflow">that content</a>.</p>
<p>Also, please interrupt with comments and questions! We're still working on the <a href="http://www.oreilly.com/oriole/">Oriole</a> version, so your feedback can help others who eventually see that.</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>A lot of people have already contributed to this material. I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA) and have to especially thank John for his support, and all my colleagues there for their feedback. I'm also particularly indebted to the <a href="http://www.meetup.com/DC-Machine-Learning-Journal-Club/">DC Machine Learning Journal Club</a>, and of course the folks at O'Reilly.</p>
<hr>
<p><img alt="braided river" src="img/braided_river.jpg"></p>
<hr>
<p>This evening is billed as a &#8220;Deep Dive into TensorFlow&#8221;. For my part, you can think of it as a deep dive into the shallow end.</p>
<p>I'm going to try to show in a very hands-on way some of the details of how TensorFlow works. The idea is not to exercise all the features of TensorFlow but to really understand some of the important underlying concepts.</p>
<p>It should be a slightly more hands-on thorough exposition of topics Saba touched on <a href="http://blog.altoros.com/videos-from-washington-dc-tensorflow-meetup-march-9-2016.html">in March</a>.</p>
<p><em>Image from <a href="https://www.flickr.com/photos/alaskanps/8029733984">National Park Service, Alaska Region on Flickr</a>.</em></p>
<hr>
<ul>
<li>tensors</li>
<li>flows</li>
<li>pictures</li>
<li>servers</li>
</ul>
<hr>
<p>But first, a couple interesting things about TensorFlow.</p>
<p>I'm really only going to demonstrate the middle two, but let's mention them all.</p>
<hr>
<p><img alt="not neurons, tensors" src="img/not_neurons.png"></p>
<hr>
<p>First thing about TensorFlow: tensors.</p>
<p>TensorFlow hates neurons and it loves linear algebra.</p>
<p>When you design software you make choices that affect what you can do and how you can do it.</p>
<p>For neural nets, there has in the past been a fixation on the biological metaphor and the "neurons" in particular. If you've used software like R's <a href="https://cran.r-project.org/web/packages/neuralnet/">neuralnet</a> package or Python's <a href="http://pybrain.org/">PyBrain</a>, you've seen interfaces where you typically define the number of neurons or "hidden units" that you want in each layer, and the connections between them (and perhaps some other components) are set up automatically one way or another.</p>
<p>But if you're looking at the math you have to evaluate inside these neural net models, the "neurons" hardly exist at all. The weights on the connections certainly exist, and there are operations that connect values as they flow through the network, but in the end you don't really need "neurons" to be a dominant abstraction in the system.</p>
<p>If you go this route, everything is just matrix math - or, for arbitrary dimensions, tensor math. This is the choice TensorFlow makes. It makes TensorFlow really flexible and lets it achieve efficient computation that would be more difficult otherwise.</p>
<p>So TensorFlow can do more than "just" neural nets. As demonstrated in the <a href="https://www.udacity.com/course/deep-learning--ud730">Google/Udacity Deep Learning MOOC</a>, you can implement logistic regression easily with TensorFlow, for example.</p>
<p>Focusing on (typically fixed-dimension) tensors does arguably make it more awkward to attempt esoteric techniques like dynamically changing the architecture of your neural net by adding neurons, for example. (This comment was inspired by <a href="https://drive.google.com/file/d/0Bxf-khCt_eknWEF6aFJ1ZU4yQ00/view">Let your Networks Grow</a>, <a href="https://sites.google.com/site/nnb2tf/">Neural Nets Back to the Future</a> at <a href="http://icml.cc/2016/">ICML 2016</a>.)</p>
<p>The linear algebra you actually have to think about is not too tricky, but for our purposes I'm not going to use anything more than individual numbers, so there's no chance of being distracted by linear algebra.</p>
<p>The relevant point is that while some other software focuses on neurons and mostly ignores weights, in the case of TensorFlow we focus on weights and mostly ignore neurons as a top-level abstraction. In any event, it's all just numbers.</p>
<hr>
<p><img alt="simple computation graph" src="img/math_graph.png"></p>
<hr>
<p>Second thing about TensorFlow: flows.</p>
<p>TensorFlow loves graphs.</p>
<p>It's important to distinguish between graphs and pictures. Here we mean <a href="https://en.wikipedia.org/wiki/Graph_(abstract_data_type)">graphs</a> as in the abstract data type. Data flow graphs.</p>
<p>This is a picture of a data flow graph. You might prefer to think of it as \(y=mx+b\). This is what TensorFlow's graphs are like.</p>
<p>The advantage of graphs is that the software can manipulate them and "reason about" how to answer questions you're asking before it commits to particular execution paths.</p>
<p>Lots of software is moving toward working via graphs in this way. Often they're <a href="https://en.wikipedia.org/wiki/Directed_acyclic_graph">directed acyclic graphs</a> but I don't care too much about whether that's always strictly the case. <a href="https://twitter.com/planarrowspace/status/569963882900561921">&#129370;</a></p>
<p>For example, <a href="http://deeplearning.net/software/theano/">Theano</a> works similarly enough to TensorFlow that <a href="http://keras.io/">Keras</a> can use either as its backend. <a href="http://torch.ch/">Torch</a> is also similar.</p>
<p>Outside of software mostly for deep learning, <a href="http://spark.apache.org/">Spark</a> works with a kind of computation graph to manage distributed processing. Tools like <a href="https://github.com/spotify/luigi">Luigi</a> and <a href="https://github.com/Factual/drake">Drake</a> manage graph pipelines of data. And many <a href="http://drivendata.github.io/cookiecutter-data-science/">recommend</a> that all data work be thought of as graph-like pipelines.</p>
<p>The graph concept is interesting and worth exploring in order to understand TensorFlow, so I'll spend some time with it today.</p>
<p><em>Image made with <a href="https://draw.io/">draw.io</a>.</em></p>
<hr>
<p><img alt="Visualization of a TensorFlow graph" src="img/graph_vis_animation.gif"></p>
<hr>
<p>Third thing about TensorFlow: pictures.</p>
<p>If you use Python's <a href="http://scikit-learn.org/">scikit-learn</a>, you likely also use a separate visualization tool, likely including <a href="http://matplotlib.org/">matplotlib</a>. If you've tried to <a href="/20151129-see_sklearn_trees_with_d3/">visualize decision tree rules</a>, you may have had a frustrating time. You may or may not use any explicit logging system at all.</p>
<p>The <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/tensorboard/README.md">TensorBoard</a> tooling that comes with TensorFlow is a great logging and visualizing system that gives you access to a lot of internals that could otherwise be a lot of work to expose.</p>
<p>I think TensorBoard <em>by itself</em> is a compelling reason to use TensorFlow over other similar software.</p>
<p>TensorBoard is super great and I will be showing several sides of it today.</p>
<p><em>Image from <a href="https://www.tensorflow.org/versions/r0.8/how_tos/graph_viz/index.html">TensorBoard: Graph Visualization</a>.</em></p>
<hr>
<p><img alt="Life of a Servable" src="img/serving_architecture.svg"></p>
<hr>
<p>Fourth thing about TensorFlow: servers.</p>
<p>TensorFlow loves moving directly into production.</p>
<p>In addition to everything else, TensorFlow has a highly-engineered serving architecture. So if you develop a TensorFlow model on your laptop, you can (relatively) easily build a deployment system to serve that model in a serious way. It seems pretty neat, but I haven't used it and I'm not going to talk more about it.</p>
<p><em>Image from the <a href="https://tensorflow.github.io/serving/architecture_overview">TensorFlow Serving Architecture Overview</a>.</em></p>
<hr>
<p>demo</p>
<hr>
<p>From here to the end is just demo! The starting point is <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20160629-presenting_hello_tensorflow/demo_begin.ipynb">demo_start.ipynb</a> and the ending point (with output) is <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20160629-presenting_hello_tensorflow/demo_end.ipynb">demo_end.ipynb</a>.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20160629-presenting_hello_tensorflow/</link>
<guid>http://planspace.org/20160629-presenting_hello_tensorflow/</guid>
<pubDate>Wed, 29 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Hello, TensorFlow! (Just the Code)</title>
<description><![CDATA[

<p>This is the final code block from <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> I like it as
a cheat sheet for TensorFlow basics.</p>
<pre><code class="language-python">import tensorflow as tf

x = tf.constant(1.0, name='input')
w = tf.Variable(0.8, name='weight')
y = tf.mul(w, x, name='output')
y_ = tf.constant(0.0, name='correct_value')
loss = tf.pow(y - y_, 2, name='loss')
train_step = tf.train.GradientDescentOptimizer(0.025).minimize(loss)

for value in [x, w, y, y_, loss]:
    tf.scalar_summary(value.op.name, value)

summaries = tf.merge_all_summaries()

sess = tf.Session()
summary_writer = tf.train.SummaryWriter('log_simple_stats', sess.graph)

sess.run(tf.initialize_all_variables())
for i in range(100):
    summary_writer.add_summary(sess.run(summaries), i)
    sess.run(train_step)

summary_writer.close()</code></pre>

<p><a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> is now <a href="https://www.safaribooksonline.com/oriole/hello-tensorflow-oriole">an interactive code and video Oriole</a> as well.</p>    
    ]]></description>
<link>http://planspace.org/20160620-hello_tensorflow_just_the_code/</link>
<guid>http://planspace.org/20160620-hello_tensorflow_just_the_code/</guid>
<pubDate>Mon, 20 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Writing with O'Reilly</title>
<description><![CDATA[

<p><a href="http://www.oreilly.com/">O'Reilly Media</a> is tech publishing brand best known for its iconic <a href="http://shop.oreilly.com/">books</a>. They're also the force behind a number of <a href="http://www.oreilly.com/conferences/">conferences</a> and they've been publishing more web content in their <a href="https://www.oreilly.com/ideas">Ideas</a> and <a href="https://www.oreilly.com/learning">Learning</a> sections. I'm very excited to now be able to contribute a small article which will join their site. "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>" should be live at 7am EDT on Monday June 20, 2016. It was a fun experience getting it there, and I thought I'd describe the process.</p>
<h3>Summary</h3>
<ul>
<li>Inception: Connections, Initiative, Luck</li>
<li>Editing: Markdown, Google Doc, Web Publishing</li>
<li>Publishing: Adventure!</li>
</ul>
<h3>Inception</h3>
<p>The post was born through a combination of connections, some initiative, and good fortune.</p>
<p>I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA). O'Reilly got in touch and I think it may have been Paco who originally proposed a discussion about DLA and O'Reilly doing something together. This led to a call between <a href="https://twitter.com/bigdata">Ben</a> and <a href="https://twitter.com/cmariebeau">Marie</a> from O'Reilly and John and I from DLA. I joined because I've done some blogging and teaching and am interested in such things. (It was surprisingly weird to be on a phone call with Ben after hearing his voice in my headphones during so many episodes of the <a href="http://radar.oreilly.com/tag/oreilly-data-show-podcast">O'Reilly Data Show</a>.) Nothing came directly of our initial call aside from establishing that there was  interest that could eventually take some real form.</p>
<p>Months later, I was starting to play with TensorFlow on my own time, collecting some notes as I went along, and starting to think about shaping them into a post for my personal blog. I guess I mentioned this to John, who suggested that we cultivate the content a little more and see if it would work as a post for O'Reilly. I was very fortunate in this connection to have the support of DLA to spend some more time writing the post, and to get initial feedback from a number of DLA colleagues.</p>
<p>After a draft was together, we sent it off to Marie to see if O'Reilly would be interested. Sure enough there was interest, and we moved on to editing. Sometimes O'Reilly republishes existing blog posts, but in this case it wasn't yet published anywhere and Marie wanted to do a round of editing, which I think has turned out to be a very good idea.</p>
<p>It is interesting how various publishers find things to publish. I've talked to "acquisitions editors" who are looking for people to write books, and I occasionally hear from people who want to republish a blog post or solicit new posts. Everybody wants content. The ease of publishing means anybody can write a book, and a lot of publishers seem to be going for long tail content, or even just <em>anything</em> new.</p>
<p>I think O'Reilly is the most discerning of the tech publishers. Aside from liking the people who work there, it's their good reputation that made putting an article on their site attractive to me. I mean, this is the house of <a href="http://shop.oreilly.com/product/9780596004927.do">the camel book</a>! That reputation also motivates linking DLA and O'Reilly, however indirectly. The apparent editorial policy of O'Reilly to not do sponsored content or other branding tie-ins preserves their status and makes them the more attractive as an ally.</p>
<h3>Editing</h3>
<p>I write generally pretty raw unedited posts (such as this one) for my blog, and I had heard that O'Reilly had cool processes in place for the editing and publishing process, so I was curious to see what it would be like working with them. The article went through three stages, first as markdown, then as a Google Doc, and then in the O'Reilly web publishing platform.</p>
<p>I almost always write text files with just a little bit of <a href="https://daringfireball.net/projects/markdown/">markdown</a> syntax (<a href="http://planspace.org/20160209-how_i_blog/">more details</a>). For the TensorFlow post, I got feedback on this form first from colleagues at <a href="http://www.deeplearninganalytics.com/">DLA</a>, which included <a href="https://slack.com/">slack</a> comments, email comments, and a pull request. Also, the local <a href="http://www.meetup.com/DC-Machine-Learning-Journal-Club/">machine learning journal club</a> has been working with TensorFlow, and they generously allowed me to present a version at one of their meetups. That experience led to a number of changes, such as clarifying the distinction between graphs and sessions.</p>
<p>Once I had the draft ready I sent it off as a zip file (including images) to Marie, O'Reilly's lead data editor. Marie put my content into a <a href="https://www.google.com/docs/about/">Google Doc</a>. I never saw Google Docs used so well. It helped that her edits were good, but she also made expert use of change tracking and commenting so that everybody knew what was going on, and I could confirm changes or discuss alternatives. I still prefer the peace of mind of git, but the ease of use of Google Docs combined with Marie's thoroughness was remarkably satisfying.</p>
<p>In terms of the actual content of the edits, I was incredibly pleased. It has been my experience in the past that collaborative document editing can be among the most painful of endeavors. One manifestation of this is when a collaborator makes changes that decrease the quality of the document. Fortunately my experience here was quite the reverse. Thanks Marie! As one example, my original title was "TensorFlow from the Plumbing Up." John had suggested "Hello TensorFlow Graph" or something like that. When Marie changed it to "Hello, TensorFlow!" it was clear it should never have been anything else.</p>
<p>We spent some time working with the Google Doc version, and then passed it off to Jenn, O'Reilly's online managing editor. (You may know Jenn's voice from <a href="http://radar.oreilly.com/tag/oreilly-radar-podcast">O'Reilly Radar</a>!) Jenn put the content into the O'Reilly web system, where it appeared to us in read-only preview form behind a login. After a couple rounds of emails for final adjustments, it was ready to go and the publish date was set. I think it looks really nice now - thanks Jenn!</p>
<p>There was a little bit of friction around the transitions from system to system (markdown to google doc to web publishing system). If I was the dictator of systems I guess I would keep everything markdown, but that probably wouldn't have all the features that the O'Reilly publishing system needs, and I don't know of any markdown applications that have all the collaboration features and ease of use that you get with Google Docs. It's really interesting to me the systems and processes that turn out existing in the world; often they seem to grow more than be built, and even for things that seem like they should be easy it often turns out that there are a lot of details in the way. The system we used worked for us, but it definitely required the expert handling of its masters.</p>
<h3>Publishing</h3>
<p>Everything was ready on Friday afternoon, but we agreed that it would be best to run the post on Monday morning. This is presumably because people are much more likely to read it when they're supposed to be working than when they have free time. (I understand this, but I still think it's funny.)</p>
<p>The last email we exchanged looped in three more O'Reilly folks who do marketing things, as I understand it. I'm curious to see what this will mean for a simple blog post like this.</p>
<p>The <a href="https://www.oreilly.com/learning">O'Reilly learning</a> site is really beautiful, with great fonts and all those things publishers care about. I'm super excited to see our post up there!</p>
<p>Dropping Monday June 20: <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> I hope it's not so awful that it irritates Google or other TensorFlow developers and users!</p>    
    ]]></description>
<link>http://planspace.org/20160619-writing_with_oreilly/</link>
<guid>http://planspace.org/20160619-writing_with_oreilly/</guid>
<pubDate>Sun, 19 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Your Favorite Class</title>
<description><![CDATA[

<p>I read a fairly bad <a href="/20160207-talk_like_ted/">book</a> which included this:</p>
<blockquote>
<p>When you think back to your favorite college class, there's a good
chance the professor you most enjoyed injected a fair amount of
humor into his or her presentations.</p>
</blockquote>
<p>That section was about humor in presentations, but the reference got
me thinking about my own favorite college class, which wasn't
particularly funny.</p>
<p>I couldn't name or even recognize the professor of the class I
remember most strongly. The class was memorable because the professor
wasn't doing the same things other professors did. The professor was
orchestrating the class so that <em>students</em> did things.</p>
<p>This was the first class I recall in which the professor would pose a
question and tell us to turn and talk about it with the people near us
for a solid five minutes, multiple times through a "lecture."</p>
<p>This class had a long reading list from which we chose what we wanted
to read. I read <a href="https://www.amazon.com/Agile-Gene-Nature-Turns-Nurture/dp/006000679X">The Agile Gene</a>. I probably thought more about that
book than anything else I read as an undergrad.</p>
<p>In this class we did a group project. My group decided to transcribe
<a href="https://en.wikipedia.org/wiki/Family_Guy">Family Guy</a> in the <a href="https://en.wikipedia.org/wiki/International_Phonetic_Alphabet">International Phonetic Alphabet</a>. It was so
much fun, I'm sure we learned more about the IPA and thought more
about regional dialects than we would have done with a more typical
assignment.</p>
<p>I learned more about teaching techniques like these, which I might
summarize as <a href="https://en.wikipedia.org/wiki/Student-centred_learning">student-centered</a>, when I studied
<a href="http://www.bard.edu/mat/">teaching at Bard</a>. They were so rare at <a href="http://www.wisc.edu/">my university</a> that it
was shocking when I encountered a class that didn't reduce perfectly
well to a series of videos.</p>
<p>If the most memorable thing about a class is the person teaching it,
that's an indictment of the class, not a recommendation. It certainly
doesn't hurt for a teacher to be charismatic, but it's sad for
students to go through school hoping to encounter super-teachers who
are expert entertainers, as if that's the best or only way to learn.</p>    
    ]]></description>
<link>http://planspace.org/20160609-your_favorite_class/</link>
<guid>http://planspace.org/20160609-your_favorite_class/</guid>
<pubDate>Thu, 09 Jun 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>A Machine to Foretell the Future?</title>
<description><![CDATA[

<p>In 1973 a collection of essays by <a href="https://en.wikipedia.org/wiki/E._F._Schumacher">E. F. Schumacher</a> was published as <a href="https://en.wikipedia.org/wiki/Small_Is_Beautiful">Small Is Beautiful: A Study of Economics As If People Mattered</a>. I don't always agree with him but I think there's a lot to consider in there, and it's an interesting view from the past&#8212;in turns prescient and archaic. The selection excerpted below includes an underrepresented viewpoint for people doing modern data work.</p>
<p><a href="http://www.amazon.com/dp/0061997765/"><img alt="Small is Beautiful (cover)" src="small_is_beautiful.jpg"></a></p>
<hr>
<h3>A Machine to Foretell the Future?</h3>
<p>The reason for including a discussion on predictability in this volume
is that it represents one of the most important metaphysical&#8212;and
therefore practical&#8212;problems with which we are faced. There have never
been so many futurologists, planners, forecasters, and model-builders
as there are today, and the most intriguing product of technological
progress, the computer, seems to offer untold new possibilities.
People talk freely about &#8220;machines to foretell the future.&#8221; Are not
such machines just what we have been waiting for? All men at all times
have been wanting to know the future.</p>
<p>The ancient Chinese used to consult the <em>I Ching</em>, also called <em>The
Book of Changes</em> and reputed to be the oldest book of mankind. Some of
our contemporaries do so even today. The <em>I Ching</em> is based on the
conviction that, while everything changes all the time, change itself
is unchanging and conforms to certain ascertainable metaphysical laws.
&#8220;To everything there is a season,&#8221; said Ecclesiastes, &#8220;and a time to
every purpose under heaven ... a time to break down and a time to
build up ... a time to cast away stones and a time to gather stones
together,&#8221; or, as we might say, a time for expansion and a time for
consolidation. And the task of the wise man is to understand the great
rhythms of the Universe and to gear in with them. While the Greeks&#8212;and
I suppose most other nations&#8212;went to living oracles, to their Pythias,
Casandras, prophets and seers, the Chinese, remarkably, went to a book
setting out the universal and necessary pattern of changes, the very
Laws of Heaven to which all nature conforms inevitably and to which
man will conform freely as a result of insight gained either from
wisdom or from suffering. Modern man goes to the computer.</p>
<p>Tempting as it may be to compare the ancient oracles and the modern
computer, only a comparison by contrast is possible. The former deal
exclusively with qualities; the latter, with quantities. The
inscription over the Delphic temple was &#8220;Know Thyself,&#8221; while the
inscription on an electronic computer is more likely to be: &#8220;Know Me,&#8221;
that is, &#8220;Study the Operating Instructions before Plugging in.&#8221; It
might be thought that the <em>I Ching</em> and the oracles are metaphysical
while the computer model is &#8220;real&#8221;; but the fact remains that a
machine to foretell the future is based on metaphysical assumptions of
a very definite kind. It is based on the implicit assumption that &#8220;the
future is already here,&#8221; that it exists already in a determinate form,
so that it requires merely good instruments and good techniques to get
it into focus and make it visible. The reader will agree that this is
a very far-reaching metaphysical assumption, in fact, a most
extraordinary assumption which seems to go against all direct personal
experience. It implies that human freedom does not exist or, in any
case, that it cannot alter the predetermined course of events. We
cannot shut our eyes to the fact, on which I have been insisting
throughout this book, that such an assumption, like all metaphysical
theses, whether explicit or implicit, has decisive practical
consequences. The question is simply: is it true or is it untrue?</p>
<p>When the Lord created the world and people to live in it&#8212;an enterprise
which, according to modern science, took a very long time&#8212;I could well
imagine that He reasoned with Himself as follows: &#8220;If I make
everything predictable, these human beings, whom I have endowed with
pretty good brains, will undoubtedly learn to predict everything, and
they will thereupon have no motive to do anything at all, because they
will recognise that the future is totally determined and cannot be
influenced by any human action. On the other hand, if I make
everything unpredictable: they will gradually discover that there is
no rational basis for any decision whatsoever and, as in the first
case, they will thereupon have no motive to do anything at all.
Neither scheme would make sense. I must therefore create a mixture of
the two. Let some things be predictable and let others be
unpredictable. They will then, amongst many other things, have the
very important task of finding out which is which.&#8221;</p>
<p>And this, indeed, is a very important task, particularly today, when
people try to devise machines to foretell the future. Before anyone
makes a prediction, he should be able to give a convincing reason why
the factor to which his prediction refers is inherently predictable.</p>
<p>Planners, of course, proceed on the assumption that the future is not
&#8220;already here,&#8221; that they are not dealing with a predetermined&#8212;and
therefore predictable&#8212;system, that they can determine things by their
own free will, and that their plans will make the future different
from what it would have been had there been no plan. And yet it is the
planners, more than perhaps anyone else, who would like nothing better
than to have a machine to foretell the future. Do they ever wonder
whether the machine might incidentally also foretell their own plans
before they have been conceived?</p>
<h5>Need for Semantics</h5>
<p>However this may be, it is clear that the question of predictability
is not only important but also somewhat involved. We talk happily
about estimating, planning, forecasting, budgeting, about surveys,
programmes, targets, and so forth, and we tend to use these terms as
if they were freely interchangeable and as if everybody would
automatically know what was meant. The result is a great deal of
confusion, because it is in fact necessary to make a number of
fundamental distinctions. The terms we use may refer to the past or to
the future; they may refer to acts or to events; and they may signify
certainty or uncertainty. The number of combinations possible where
there are three pairs of this kind is \(2^3\), or 8, and we really
ought to have eight different terms to be quite certain of what we are
talking about. Our language, however, is not as perfect as that. The
most important distinction is generally that between acts and events.
The eight possible cases may there fore be ordered as follows:</p>
<pre><code>1. Act           5. Event
   Past             Past
   Certain          Certain
2. Act           6. Event
   Future           Future
   Certain          Certain
3. Act           7. Event
   Past             Past
   Uncertain        Uncertain
4. Act           8. Event
   Future           Future
   Uncertain        Uncertain</code></pre>

<p>The distinction between acts and events is as basic as that between
active and passive or between &#8220;within my control&#8221; or &#8220;outside my
control.&#8221; To apply the word &#8220;planning&#8221; to matters outside the
planner's control is absurd. Events, as far as the planner is
concerned, simply happen. He may be able to forecast them and this may
well influence his plan; but they cannot possibly be part of the plan.</p>
<p>The distinction between the past and the future proved to be necessary
for our purpose, because, in fact, words like &#8220;plan&#8221; or &#8220;estimate&#8221; are
being used to refer to either. If I say: &#8220;I shall not visit Paris
without a plan,&#8221; this can mean: &#8220;I shall arm myself with a street plan
for orientation&#8221; and would therefore refer to case 5. Or it can mean:
&#8220;I shall arm myself with a plan which outlines in advance where I am
going to go and how I am going to spend my time and money&#8221;&#8212;case 2
or 4. If someone claims that &#8220;to have a plan is indispensable,&#8221; it is
not without interest to find out whether he means the former or the
latter. The two are <em>essentially</em> different.</p>
<p>Similarly, the word &#8220;estimate,&#8221; which denotes uncertainty, may apply
to the past or to the future. In an ideal world, it would not be
necessary to make estimates about things that had already happened.
But in the actual world, there is much uncertainty even about matters
which, in principle, could be fully ascertained. Cases 3, 4, 7, and 8
represent four different types of estimates. Case 3 relates to
something I have done in the past; case 7, to something that has
happened in the past. Case 4 relates to something I plan to do in the
future, while case 8 relates to something I expect to happen in the
future. Case 8, in fact, is a forecast in the proper sense of the term
and has nothing whatever to do with &#8220;planning.&#8221; How often, however,
are forecasts presented as if they were plans&#8212;and <em>vice versa!</em> The
British &#8220;National Plan&#8221; of 1965 provides an outstanding example and,
not surprisingly, came to nothing.</p>
<p>Can we ever speak of future acts or events as certain (cases 2 and 6)?
If I have made a plan with full knowledge of all the relevant facts,
being inflexibly resolve to carry it through&#8212;case 2&#8212;I may, in this
respect, consider my future actions as certain. Similarly, in
laboratory science, dealing with carefully isolated deterministic
systems, future events may he described as certain. The real world,
however, is not a deterministic system; we may be able to talk with
certainty about acts or events of the past&#8212;cases 1 or 5&#8212;but we can do
so <em>about future events only on the basis of assumptions.</em> In other
words, we can formulate conditional statements about the future, such
as: &#8220;<em>If</em> such and such a trend of events continued for another x
years, this is where it would take us.&#8221; This is not a forecast or
prediction, which must always be uncertain in the real world, but an
exploratory calculation, which, being conditional, has the virtue of
mathematical certainty.</p>
<p>Endless confusion results from the semantic muddle in which we find
ourselves today. As mentioned before, &#8220;plans&#8221; are put forward which
upon inspection turn out to relate to events totally outside the
control of the planner. &#8220;Forecasts&#8221; are offered which upon inspection
turn out to be conditional sentences, in other words, exploratory
calculations. The latter are misinterpreted as if they were forecasts
or predictions. &#8220;Estimates&#8221; are put forward which upon inspection turn
out to be plans. And so on and so forth. Our academic teachers would
perform a most necessary and really helpful task if they taught their
students to make the distinctions discussed above and developed a
terminology which fixed them in words.</p>
<h5>Predictability</h5>
<p>Let us now return to our main subject&#8212;predictability. Is prediction or
forecasting&#8212;the two terms would seem to be interchangeable&#8212;possible at
all? The future does not exist; how could there be knowledge about
something nonexistent? This question is only too well justified. In
the strict sense of the word, knowledge can only be about the past.
The future is always in the making, but it is being made <em>largely</em> out
of existing material, about which a great deal can be known. The
future, there, is <em>largely</em> predictable, if we have solid and
extensive knowledge of the past. <em>Largely</em>, but by no means wholly;
for into the making of the future there enters that mysterious and
irrepressible factor called human freedom. It is the freedom of a
being of which it has been said that it was made in the image of God
the Creator: the freedom of creativity.</p>
<p>Strange to say, under the influence of laboratory science many people
today seem to use their freedom only for the purpose of denying its
existence. Men and women of great gifts find their purest delight in
magnifying every &#8220;mechanism,&#8221; every &#8220;inevitability,&#8221; everything where
human freedom does not enter or does not appear to enter. A great
shout of triumph goes up whenever anybody has found some further
evidence&#8212;in physiology or psychology or sociology or economics or
politics&#8212;of unfreedom, some further indication that people cannot help
being what they are and doing what they are doing, no matter how
inhuman their actions might be. The denial of freedom, of course, is a
denial of responsibility: there are no acts, but only events;
everything simply happens; no one is responsible. And this is no doubt
the main cause of the semantic confusion to which I have referred
above. It is also the cause for the belief that we shall soon have a
machine to foretell the future.</p>
<p>To be sure, if everything simply happened, if there were no element of
freedom, choice, human creativity and responsibility, everything would
be perfectly predictable, subject only to accidental and temporary
limitations of knowledge. The absence of freedom would make human
affairs suitable for study by the natural sciences or at least by
their methods, and reliable results would no doubt quickly follow the
systematic observation of facts. Professor Phelps Brown, in his
presidential address to the Royal Economic Society, appears to adopt
precisely this point of view when talking about &#8220;The Underdevelopment
of Economics.&#8221; &#8220;Our own science,&#8221; he says, &#8220;has hardly yet reached its
seventeenth century.&#8221; Believing that economics is <em>metaphysically</em> the
same as physics, he quotes another economist, Professor Morgenstern,
approvingly, as follows:</p>
<blockquote>
<p>The decisive break which came in physics in the seventeenth century,
specifically in the field of mechanics, was possible only because of
previous developments in astronomy. It was backed by several
millennia of systematic, scientific, astronomical observation....
Nothing of this sort has occurred in economic science. It would have
been absurd in physics to have expected Kepler and Newton without
Tycho&#8212;and there is no reason to hope for an easier development in
economics.</p>
</blockquote>
<p>Professor Phelps Brown concludes therefore that we need many, many
more years of observations of behaviour. &#8220;<em>Until then, our
mathematisation is premature.</em>&#8221;</p>
<p>It is the intrusion of human freedom and responsibility that makes
economics metaphysically different from physics and makes human
affairs largely unpredictable. We obtain predictability, of course,
when we or others are acting according to a plan. But this is so
precisely because a plan is the result of an exercise in the freedom
of choice: the choice has been made; all alternatives have been
eliminated. If people stick to their plan, their behaviour is
predictable simply because they have chosen to surrender their freedom
to act otherwise than prescribed in the plan.</p>
<p>In principle, everything which is immune to the intrusion of human
freedom, like the movements of the stars, is predictable, and
everything subject to this intrusion is unpredictable. Does that mean
that all human actions are unpredictable? No, because most people,
most of the time, make no use of their freedom and act purely
mechanically. Experience shows that when we are dealing with large
numbers of people many aspects of their behaviour are indeed
predictable; for out of a large number, at any one time, only a tiny
minority are using their power of freedom, and they often do not
significantly affect the total outcome. Yet all really important
innovations and changes normally start from tiny minorities of people
who <em>do</em> use their creative freedom.</p>
<p>It is true that social phenomena acquire a certain steadiness and
predictability from the non-use of freedom, which means that the great
majority of people responds to a given situation in a way that does
not alter greatly in time, unless there are really overpowering new
causes.</p>
<p>We can therefore distinguish as follows:</p>
<ol>
<li>Full predictability (in principle) exists only in the absence of
    human freedom, <em>i.e.</em> in &#8220;sub-human&#8221; nature. The limitations of
    predictability are purely limitations of knowledge and technique.</li>
<li>Relative predictability exists with regard to the behaviour
    pattern of very large numbers of people doing &#8220;normal&#8221; things
    (routine).</li>
<li>Relatively full predictability exists with regard to human'
    actions controlled by a plan which eliminates freedom, <em>e.g.</em>
    railway timetable.</li>
<li>Individual decisions by individuals are in principle
    unpredictable.</li>
</ol>
<h5>Short-Term Forecasts</h5>
<p>In practice all prediction is simply extrapolation, modified by known
&#8220;plans.&#8221; But how do you extrapolate? How many years do you go back?
Assuming there is a record of growth, what precisely do you
extrapolate&#8212;the average rate of growth, or the increase in the rate of
growth, or the annual increment in absolute terms? As a matter of
fact. there are no rules: it is just a matter of &#8220;feel&#8221; or judgment.
(When there are seasonal or cyclical patterns, it is, of course,
necessary to go back by at least a year or a cycle; but it is a matter
of judgement to decide how many years or cycles.)</p>
<p>It is good to know of all the different possibilities of using the
same time series for extrapolations with very different results. Such
knowledge will prevent us from putting undue faith in any
extrapolation. At the same time, and by the same token, the
development of (what purport to be) better forecasting techniques can
become a vice. In short-term forecasting, say, for next year, a
refined technique rarely produces significantly different results from
those of a crude technique. After a year of growth&#8212;what can you
predict?</p>
<ol>
<li>that we have reached a (temporary) ceiling;</li>
<li>that growth will continue at the same, or a slower, or a faster
    rate;</li>
<li>that there will be a decline.</li>
</ol>
<p>Now, it seems clear that the choice between these three basic
alternative predictions cannot be made by &#8220;forecasting technique&#8221; but
only by informed judgment. It depends, of course, on what you are
dealing with. When you have something that is normally growing very
fast, like the consumption of electricity, your threefold choice is
between the same rate of growth, a faster rate, or a slower rate.</p>
<p>It is not so much forecasting technique, as a full understanding of
the current situation shat can help in the formation of a sound
judgment for the future. If the present level of performance (or rate
of growth) is known to be influenced by quite abnormal factors which
are unlikely to apply in the coming year, it is, of course, necessary
to take these into account. The forecast, &#8220;same as last year,&#8221; may
imply a &#8220;real&#8221; growth or a &#8220;real&#8221; decline on account of exceptional
factors being present this year, and this, of course, must be made
explicit by the forecaster.</p>
<p>I believe, therefore, that all effort needs to be put into
understanding the current situation, to identify and, if need be,
eliminate &#8220;abnormal&#8221; and non-recurrent factors from the current
picture. This having been done, the method of forecasting can hardly
be crude enough. No amount of refinement will help one come to the
fundamental judgment&#8212;is next year going to be the same as last year,
or better, or worse?</p>
<p>At this point, it may be objected that there ought to be great
possibilities of short-term forecasting with the help of electronic
computers, because they can very easily and quickly handle a great
mass of data and fit to them some kind of mathematical expression. By
means of &#8220;feedback&#8221; the mathematical expression can be kept up to date
almost instantaneously, and once you have a really good mathematical
fit, the the machine can predict the future.</p>
<p>Once again, we need to have a look at the metaphysical basis of such
claims. What is the meaning of a &#8220;good mathematical fit?&#8221; Simply that
a sequence of quantitative changes in the past has been elegantly
described in precise mathematical language. But the fact that I&#8212;or the
machine&#8212;have been able to describe this sequence so exactly by no
means establishes a presumption that the pattern will continue. It
could continue only if <em>(a)</em> there were no human freedom and <em>(b)</em>
there was no possibility of any change in the causes that have given
rise to the observed pattern.</p>
<p>I should accept the claim that a very clear and very strongly
established pattern (of stability, growth, or decline) can be expected
to continue for a little longer, unless there is definite knowledge of
the arrival of new factors likely to change it. But I suggest that for
the detection of such clear, strong and persistent patterns the
non-electronic human brain is normally cheaper, faster, and more
reliable than its electronic rival. Or to put it the other way round:
if it is really necessary to apply such highly refined methods of
mathematical analysis for the detection of a pattern that one needs an
electronic computer, the pattern is too weak and too obscure to be a
suitable basis for extrapolation in real life.</p>
<p>Crude methods of forecasting&#8212;after the current picture has been
corrected for abnormalities&#8212;are not likely to lead into the errors of
spurious verisimilitude and spurious detailing&#8212;the two greatest vices
of the statistician. Once you have a formula and an electronic
computer, there is an awful temptation to squeeze the lemon until it
is dry and to present a picture of the future which through its very
precision and verisimilitude carries conviction. Yet a man who uses an
imaginary map, thinking it a true one, is likely to be worse off than
someone with no map at all; for he will fail to inquire wherever he
can, to observe every detail on his way, and to search continuously
with all his senses and all his intelligence for indications of where
he should go.</p>
<p>The person who makes the forecasts may still have a precise
appreciation of the assumptions on which they are based. But the
person who uses the forecasts may have no idea at all that the whole
edifice, as is often the case, stands and falls with one single,
unverifiable assumption. He is impressed by the thoroughness of the
job done, by the fact that everything seems to &#8220;add up,&#8221; and so forth.
If the forecasts were presented quite artlessly, as it were, on the
back of an envelope, he would have a much better chance of
appreciating their tenuous character and the fact that, forecasts or
no forecasts, someone has to take an entrepreneurial decision about
the unknown future.</p>
<h5>Planning</h5>
<p>I have already insisted that a plan is something essentially different
from a forecast. It is a statement of intention, of what the
planners&#8212;or their masters&#8212;intend to do. Planning (as I suggest the
term should be used) is inseparable from power. It is natural and
indeed desirable that everybody wielding any kind of power should have
some sort of a plan, that is to say, that he should use power
deliberately and consciously, looking some distance ahead in time. In
doing so he must consider what other people are likely to do; in other
words, he cannot plan sensibly without doing a certain amount of
forecasting. This is quite straightforward as long as that which has
to be forecast is, in fact, &#8220;forecastable,&#8221; if it relates either to
matters into which human freedom does not enter, or to the routine
actions of a very large number of individuals, or to the established
plans of other people wielding power. Unfortunately, the matters to be
forecast very often belong to none of these categories but are
dependent on the individual decisions of single persons or small
groups of persons. In such cases forecasts are little more than
&#8220;inspired guesses,&#8221; and no degree of improvement in forecasting
technique can help. Of course, some people may turn out to make better
guesses than others, but this will not be due to their possessing a
better forecasting technique or better mechanical equipment to help
them in their computations.</p>
<p>What, then, could be the meaning of a &#8220;national plan&#8221; in a free I
society? It cannot mean the concentration of all power at one point,
because that would imply the end of freedom: genuine planning is
coextensive with power. It seems to me that the only intelligible
meaning of the words &#8220;a national plan&#8221; in a free society would be the
fullest possible statement of intentions by all people wielding
substantial economic power, such statements being collected and
collated by some central agency. The very inconsistencies of such a
composite &#8220;plan&#8221; might give valuable pointers.</p>
<h5>Long-term Forecasts and Feasibility Studies</h5>
<p>Let us now turn to long-term forecasting, by which I mean producing
estimates dive or more years ahead. It must be clear that, change
being a function of time, the longer-term future is even less
predictable than the short-term. In fact, all long-term forecasting is
somewhat presumptuous and absurd, unless it is of so general a kind
that it merely states the obvious. All the same, there is often a
practical necessity for &#8220;taking a view&#8221; on the future, as decisions
have to be taken and long-term commitments entered. Is there nothing
that could help?</p>
<p>Here I should like to emphasise again the distinction between
forecasts on the one hand and &#8220;exploratory calculations&#8221; or
&#8220;feasibility studies&#8221; on the other. In the one case I assert that this
or that will be the position in, say, twenty years' time. In the other
case I merely explore the long-term effect of certain assumed
tendencies. It is unfortunately true that in macro-economics
feasibility studies are very rarely carried beyond the most
rudimentary beginnings. People are content to rely on general
forecasts which are rarely worth the paper they are written on.</p>
<p>It may be helpful if I give a few examples. It is very topical these
days to talk about the development of underdeveloped countries and
countless &#8220;plans&#8221; (so-called) are being produced to this end. If we go
by the expectations that are being aroused all over the world, it
appears to be assumed that within a few decades most people the world
over are going to be able to live more or less as the western
Europeans are living today. Now, it seems to me, it would be very
instructive if someone undertook to make a proper, detailed
feasibility study of this project. He might choose the year 2000 as
the terminal date and work backwards from there. What would be the
required output of foodstuffs, fuels, metals, textile fibres, and so
forth? What would be the stock of industrial capital? Naturally, he
would have to introduce many new assumptions as he went along. Each
assumption could then become the object of a further feasibility
study. He might then find that he could not solve his equations unless
he introduced assumptions which transcended all bounds of reasonable
probability. This might prove highly instructive. It might conceivably
lead to the conclusion that, while most certainly there ought to be
substantial economic development throughout the countries where great
masses of people live in abject misery, there are certain choices
between alternative <em>patterns</em> of development that could be made, and
that some types of development would appear more feasible than others.</p>
<p>Long-term thinking, supported by conscientious feasibility studies,
would seem to be particularly desirable with regard to all
non-renewable raw materials of limited availability, that is to say,
mainly fossil fuels and metals. At present, for instance, there is a
replacement of coal or oil. Some people seem to assume that coal is on
the way out. A careful feasibility study, making use of all available
evidence of coal, oil, and natural gas reserves, proved as well as
merely assumed to exist, would be exceedingly instructive.</p>
<p>On the subject of population increase and food supplies, we have had
the nearest thing to feasibility studies so far, coming mainly from
United Nations organisations. They might be carried much further,
giving not only the totals of food production to be attained by 1980
or 2000, but also showing in much greater detail than has so far been
done the timetable of specific steps that would have to be taken in
the near future if these totals are to be attained.</p>
<p>In all this, the most essential need is a purely intellectual one: a
clear appreciation of the difference between a forecast and a
feasibility study. It is surely a sign of statistical illiteracy to
confuse the two. A long-term forecast, as I said, is presumptuous; but
a long- term feasibility study is a piece of humble and unpretentious
work which we shall neglect at our peril.</p>
<p>Again the question arises whether this work could be facilitated by
more mechanical aids such as electronic computers. Personally, I am
inclined to doubt it. It seems to me that the endless multiplication
of mechanical aids in fields which require judgment more than anything
else is one of the chief dynamic forces behind Parkinson's Law. Of
course, an electronic computer can work out a vast number of
permutations, employing varying assumptions, within a few seconds or
minutes, while it might take the non-electronic brain as many months
to do the same job. But the point is that the non-electronic brain
need never attempt to do that job. By the power of judgment it can
concentrate on a few decisive parameters which are quite sufficient to
outline the ranges of reasonable probability. Some people imagine that
it would be possible and helpful to set up a machine for long-range
forecasting into which current &#8220;news&#8221; could be fed continuously and
which, in response, would produce continual revisions of some
long-term forecasts. No doubt, this would be possible; but would it be
helpful? Each item of &#8220;news&#8221; has to be judged for its long-term
relevance, and a sound judgment is generally not possible immediately.
Nor can I see any value in the continual revision of long-term
forecasts, as a matter of mechanical routine. A forecast is required
only when a long-term decision has to be taken or reviewed, which is a
comparatively rare event even in the largest of businesses, and then
it is worth while deliberately and conscientiously to assemble the
best evidence, to judge each item in the light of accumulated
experience, and finally to come to a view which appears reasonable to
the best brains available. It is a matter of self-deception that this
laborious and uncertain process could be short-circuited by a piece of
mechanical apparatus.</p>
<p>When it comes to feasibility studies, as distinct from forecasts, it
may occasionally seem useful to have apparatus which can quickly test
the effect of variations in one's assumptions. But I have yet to be
convinced that a slide rule and a set of compound interest tables are
not quite sufficient for the purpose.</p>
<h5>Unpredictability and Freedom</h5>
<p>If I hold a rather negative opinion about the usefulness of
&#8220;automation&#8221; in matters of economic forecasting and the like, I do not
underestimate the value of electronic computers and similar apparatus
for other tasks, like solving mathematical problems or programming
production runs. These latter tasks belong to the exact sciences or
their applications. Their subject matter is non-human, or perhaps I
should say, sub-human. Their very exactitude is a sign of the absence
of human freedom, the absence of choice, responsibility and dignity.
As soon as human freedom enters, we are in an entirely different world
where there is great danger in any proliferation of mechanical
devices. The tendencies which attempt to obliterate the distinction
should be resisted with the utmost determination. Great damage to
human dignity has resulted from the misguided attempt of the social
sciences to adopt and imitate the methods of the natural sciences.
Economics, and even more so, applied economics, is not an exact
science; it is in fact, or ought to be, something much greater: a
branch of wisdom. Mr. Colin Clark once claimed &#8220;that long-period world
economic equilibrium develop themselves in their own peculiar manner,
entirely independently of political and social changes.&#8221; On the
strength of this metaphysical heresy he wrote a book, in 1941,
entitled <em>The Economics of 1960</em>. It would be unjust to say that the
picture he drew bears no resemblance to what actually came to pass;
there is, indeed, the kind of resemblance which simply stems from the
fact that man uses his freedom within an unchanged setting of physical
laws of nature. But the lesson from Mr. Clark's book is that his
metaphysical assumption is untrue; that, in fact, world economic
equilibria, even in the longer run, are highly dependent on political
and social changes; and that the sophisticated and ingenious methods
of forecasting employed by Mr. Clark merely served to produce a work
of spurious verisimilitude.</p>
<h5>Conclusion</h5>
<p>I thus come to the cheerful conclusion that life, including economic
life, is still worth living because it is sufficiently unpredictable
to be interesting. Neither the economist nor the statistician will get
it &#8220;taped.&#8221; Within the limits of the physical laws of nature, we are
still masters of our individual and collective destiny, for good or
ill.</p>
<p>But the know-how of the economist, the statistician, the natural
scientist and engineer, and even of the genuine philosopher can help
to clarify the limits within which our destiny is confined. The future
cannot be forecast, but it can be explored. Feasibility studies can
show us where we appear to be going, and this is more important today
than ever before, since &#8220;growth&#8221; has become a keynote of economics all
over the world.</p>
<p>In his urgent attempt to obtain reliable knowledge about his
essentially indeterminate future, the modern man of action may
surround himself by ever-growing armies of forecasters, by
ever-growing mountains of factual data to be digested by ever more
wonderful mechanical contrivances: I fear that the result is little
more than a huge game of make-believe and an ever more marvellous
vindication of Parkinson's Law. The best decisions will still be based
on the judgments of mature non-electronic brains possessed by men who
have looked steadily and calmly at the situation and seen it whole.
&#8220;Stop, look, and listen&#8221; is a better motto than &#8220;Look it up in the
forecasts.&#8221;</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20160531-a_machine_to_foretell_the_future/</link>
<guid>http://planspace.org/20160531-a_machine_to_foretell_the_future/</guid>
<pubDate>Tue, 31 May 2016 12:00:00 -0500</pubDate>
</item>
<item>
<title>Old Article for the Cardinal: "UW computer models weather patterns"</title>
<description><![CDATA[

<p>Years ago, the better student newspaper of my alma mater, <a href="http://www.dailycardinal.com/">The Daily Cardinal</a>, decided to get science articles written by people who were in the <a href="https://nerdnite.com/">Nerd Nite</a> Madison <a href="https://www.facebook.com/groups/nerdnite.madison/">Facebook group</a>. I eventually agreed to write an article about a new computer the university was standing up, which ran on November 29, 2011. I happened to find an old copy of that paper recently, and I thought I'd include that old article here, not because it is good for the sake of completism.</p>
<p>I managed to find <a href="http://www.dailycardinal.com/article/2011/11/uw-computer-models-weather-patterns">the article still on the paper's web site</a>. I also found <a href="11292011.page04.pdf">a PDF of the page from the paper that the article ran on</a> thanks to Google's completist storage of all my old email. I'll put the text of the article below, because why not.</p>
<hr>
<h3>UW computer models weather patterns</h3>
<p>Tuesday, November 29, 2011 - Aaron Schumacher - The Daily Cardinal</p>
<p>If a butterfly flaps its wings in Brazil, will a hurricane destroy Florida? If a satellite sees clouds in Nevada, will it mean rain for crops or mud slides down mountains? Questions like these need a computer with real muscle, run by the best people, and that&#8217;s what UW-Madison now has.</p>
<p>The S4 supercomputer built on campus this year is now fully operational and the most powerful resource of its kind at the university&#8217;s disposal. It&#8217;s working to fully incorporate data from satellites in models of the Earth&#8217;s oceans and atmosphere&#8212;data that could help improve our understanding and forecasting of these complex systems.</p>
<p>Just how powerful is the new supercomputer? Well, it&#8217;s gotten harder to make computer processors faster, so engineers are placing more than one computer brain or &#8220;core&#8221; on each chip. Even your iPhone has two cores that work together to send text messages as quickly as possible. A new iMac has four cores on its single chip, for blazing fast Facebook rendering.</p>
<p>The processors in the new supercomputer at UW-Madison&#8217;s Space Science and Engineering Center (SSEC) each have 12 cores per chip, while a typical desktop has only one. There are four of these chips in each unit.</p>
<p>Oh, and there are a whopping 64 of these units working together in the S4, keeping track of a multitude of data and grid points over the Earth&#8217;s surface. In terms of RAM, the working memory of any computer, the S4&#8217;s processing units alone have over 2,000 times what you&#8217;d find in an iMac.</p>
<p>This speedy behemoth fills up five ceiling-high racks in the 1225 Dayton St. building. There are 26 more computers serving as the collective hard disk, providing a total of 456 terabytes in storage space.</p>
<p>To fund the construction of the S4 supercomputer, the SSEC received a grant of $1 million from the National Oceanic and Atmospheric Administration (NOAA) earlier this year. This decision was made in favor of UW-Madison largely because computer engineers on campus have the special skills to create such extreme computing systems.</p>
<p>The S4 supercomputer project extends a collaboration between the NOAA and UW-Madison&#8217;s SSEC. NOAA researchers, including many at UW-Madison, use the supercomputer for important computationally complex tasks.</p>
<p>The S4 supercomputer is a powerful new resource on campus. Its availability to researchers will help advance atmospheric and oceanic modeling techniques for national applications and will expand the capabilities of UW-Madison researchers working on many other projects.</p>    
    ]]></description>
<link>http://planspace.org/20160528-old_article_for_the_cardinal/</link>
<guid>http://planspace.org/20160528-old_article_for_the_cardinal/</guid>
<pubDate>Sat, 28 May 2016 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
