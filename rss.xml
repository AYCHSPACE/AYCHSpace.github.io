<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>See sklearn trees with D3</title>
<description><![CDATA[

<p>The <a href="http://scikit-learn.org/stable/modules/tree.html">decision trees</a> from <a href="http://scikit-learn.org/">scikit-learn</a> are very easy to train and predict with, but it's not easy to see the rules they learn. The code below makes it easier to see inside <code>sklearn</code> classification trees, enabling visualizations that look like this:</p>
<p><a href="http://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f"><img alt="partial tree view" src="partial_tree_view.png"></a></p>
<p>This shows, for example, that all the <a href="https://en.wikipedia.org/wiki/Iris_flower_data_set">irises</a> with <code>petal length (cm)</code> less than 2.45 were <code>setosa</code>.</p>
<p>The ability to interpret the rules of a decision tree is often considered a strength of the algorithm, and in <a href="https://www.r-project.org/">R</a> you can usually <code>summary()</code> and <code>plot()</code> a tree fit to see the rules. In <a href="https://www.python.org/">Python</a> with <code>sklearn</code>, there is <a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.export_graphviz.html"><code>export_graphviz</code></a>, but it isn't terribly convenient. It shouldn't be so hard to see what's going on inside a tree.</p>
<p>The following <a href="http://www.json.org/">JSON</a> format is simple and works with common <a href="http://d3js.org/">D3</a> tree graphing code, so let's target this format:</p>
<pre><code class="language-json">{name: "container thing",
 children: [{name: "leaf thing one"},
            {name: "leaf thing two"}]}</code></pre>

<p>Each <code>name</code> will describe a true/false decision rule for an inner node or the distribution of training example labels for a leaf node. The first of a pair of <code>children</code> is where the rule is true, and the second is where the rule is false. (These are binary trees.)</p>
<p>The way <code>sklearn</code> trees store their rules internally is described <a href="https://github.com/scikit-learn/scikit-learn/blob/0.16.1/sklearn/tree/_tree.pyx#L2956-L3008">in <code>_tree.pyc</code></a>. The <code>rules</code> function here examines a fit <code>sklearn</code> decision tree to generate a Python dictionary (with structure like the above) representing the decision tree's rules:</p>
<pre><code class="language-python">def rules(clf, features, labels, node_index=0):
    """Structure of rules in a fit decision tree classifier

    Parameters
    ----------
    clf : DecisionTreeClassifier
        A tree that has already been fit.

    features, labels : lists of str
        The names of the features and labels, respectively.

    """
    node = {}
    if clf.tree_.children_left[node_index] == -1:  # indicates leaf
        count_labels = zip(clf.tree_.value[node_index, 0], labels)
        node['name'] = ', '.join(('{} of {}'.format(int(count), label)
                                  for count, label in count_labels))
    else:
        feature = features[clf.tree_.feature[node_index]]
        threshold = clf.tree_.threshold[node_index]
        node['name'] = '{} &gt; {}'.format(feature, threshold)
        left_index = clf.tree_.children_left[node_index]
        right_index = clf.tree_.children_right[node_index]
        node['children'] = [rules(clf, features, labels, right_index),
                            rules(clf, features, labels, left_index)]
    return node</code></pre>

<p>How is this used? Let's get a quick example decision tree and take a look:</p>
<pre><code class="language-python">from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier

data = load_iris()

clf = DecisionTreeClassifier(max_depth=3)
clf.fit(data.data, data.target)

rules(clf, data.feature_names, data.target_names)</code></pre>

<p>The <code>rules</code> function returns the following Python dictionary, formatted for readability here:</p>
<pre><code class="language-python">{'name': 'petal length (cm) &gt; 2.45000004768',
 'children': [
     {'name': 'petal width (cm) &gt; 1.75',
      'children': [
         {'name': 'petal length (cm) &gt; 4.85000038147',
          'children': [
              {'name': '0 of setosa, 0 of versicolor, 43 of virginica'},
              {'name': '0 of setosa, 1 of versicolor, 2 of virginica'}]},
         {'name': 'petal length (cm) &gt; 4.94999980927',
          'children': [
              {'name': '0 of setosa, 2 of versicolor, 4 of virginica'},
              {'name': '0 of setosa, 47 of versicolor, 1 of virginica'}]}]},
     {'name': '50 of setosa, 0 of versicolor, 0 of virginica'}]}</code></pre>

<p>This is pretty readable, but now we can also write the result out to a file and visualize it with D3:</p>
<pre><code class="language-python">import json

r = rules(clf, data.feature_names, data.target_names)
with open(rules.json', 'w') as f:
    f.write(json.dumps(r))</code></pre>

<p>Check out the <a href="http://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f">interactive view</a>! Once again, a partially expanded view looks like this:</p>
<p><a href="http://bl.ocks.org/ajschumacher/65eda1df2b0dd2cf616f"><img alt="partial tree view" src="partial_tree_view.png"></a></p>    
    ]]></description>
<link>http://planspace.org/20151129-see_sklearn_trees_with_d3/</link>
<guid>http://planspace.org/20151129-see_sklearn_trees_with_d3/</guid>
<pubDate>Sun, 29 Nov 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>A New Culture of Learning</title>
<description><![CDATA[

<p>I read this <a href="http://www.newcultureoflearning.com/">book</a> and it introduced me to the word aporia, which I appreciate. Here is a definition that comes up on Google:</p>
<blockquote>
<p>aporia: an irresolvable internal contradiction or logical disjunction in a text, argument, or theory</p>
</blockquote>
<p>Here is a summary of this book, in one quote from its page 107:</p>
<blockquote>
<p>In our view, MMOs are almost perfect illustrations of a new learning environment.</p>
</blockquote>
<p>I have minor gripes and major gripes about this book. A minor gripe is they say "petri dish" a lot, and I think they shouldn't. A major gripe is that the whole book is essentially a false dichotomy between a straw man version of schooling and impoverished, unworkable gesticulations labeled "new learning." It isn't that there aren't any good ideas here, but they are old ideas misunderstood and presented as if new.</p>
<p>Skip this book.</p>
<hr>
<p>A New Culture of Learning: Cultivating the Imagination for a World of Constant Change</p>
<p>by Douglas Thomas and John Seely Brown</p>
<p><a href="http://www.amazon.com/New-Culture-Learning-Cultivating-Imagination/dp/1456458884"><img alt="cover" src="cover.jpg"></a></p>    
    ]]></description>
<link>http://planspace.org/20151122-a_new_culture_of_learning/</link>
<guid>http://planspace.org/20151122-a_new_culture_of_learning/</guid>
<pubDate>Sun, 22 Nov 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Lauren Ipsum</title>
<description><![CDATA[

<p>I finally read a copy of <a href="http://www.laurenipsum.org/">Lauren Ipsum</a> that I had lying around, wanting to read it before giving it away when I go talk at a school as part of their upcoming <a href="https://hourofcode.com/">Hour of Code</a> festivities.</p>
<p>I liked it so much, I gave my initial copy to a friend who teaches computer science, and I'm getting three more copies to give away at that school and as gifts to others.</p>
<p>I agree with the Phantom Tollbooth <a href="http://boingboing.net/2014/12/29/lauren-ipsum-the-phantom-toll.html">comparison</a>. The writing is dense at multiple scales with fun computer science concepts, references, and wordplay, from the value of elegant design, to forests of <a href="https://en.wikipedia.org/wiki/Red%E2%80%93black_tree">red-black trees</a>, to optimizing away literal <a href="https://en.wikipedia.org/wiki/Tail_call">tails</a>. I laughed out loud.</p>
<p><a href="http://www.amazon.com/Lauren-Ipsum-Computer-Science-Improbable/dp/1593275749/"><img alt="cover" src="cover.png"></a></p>    
    ]]></description>
<link>http://planspace.org/20151122-lauren_ipsum/</link>
<guid>http://planspace.org/20151122-lauren_ipsum/</guid>
<pubDate>Sun, 22 Nov 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Debugging Teams</title>
<description><![CDATA[

<p><a href="http://shop.oreilly.com/product/0636920042372.do"><img alt="cover of debugging teams" src="debugging_teams.jpg"></a></p>
<p>I read this book after listening to <a href="http://radar.oreilly.com/2015/10/the-first-rule-of-management-resist-the-urge-to-manage.html">a podcast</a>. It's been described as &#8220;<a href="https://en.wikipedia.org/wiki/How_to_Win_Friends_and_Influence_People">How to Win Friends and Influence People</a> for programmers&#8221;.</p>
<p>Debugging Teams is supposed to be a new, less software-developer-specific version of <a href="http://shop.oreilly.com/product/0636920018025.do">Team Geek</a>. I haven't read Team Geek, but Debugging Teams still had pretty much that was focused on software-related things. I thought this was a good thing. Sure it could generalize.</p>
<p>The main thesis of the book is their &#8220;HRT&#8221;: humility, respect, and trust. These are all good things to have. As they say:</p>
<ul>
<li><em>Humility</em> You are not the center of the universe. You&#8217;re neither omniscient nor infallible. You&#8217;re open to self-improvement.</li>
<li><em>Respect</em> You genuinely care about others you work with. You treat them as human beings, and appreciate their abilities and accomplishments.</li>
<li><em>Trust</em> You believe others are competent and will do the right thing, and you&#8217;re OK with letting them drive when appropriate.</li>
</ul>
<p>Here are some more quotes I pulled out as I was reading; some of them are themselves quotes.</p>
<blockquote>
<p>&#8220;People are basically a giant pile of intermittent bugs.&#8221;</p>
<p>Shouldn&#8217;t people be allowed to work however they want? Actually, no. In this case we assert that you&#8217;re doing it wrong, and it is a big deal. Here&#8217;s why. Hiding Is Considered Harmful</p>
<p>Bus factor (noun): the number of people that need to get hit by a bus before your project is completely doomed.</p>
<p>we work best in tight feedback loops.</p>
<p>Software development is a team sport.</p>
<p>you are not your code. Say that over and over.</p>
<p>&#8220;Failure is an option.&#8221;</p>
<p>Write up &#8220;postmortems,&#8221; as they&#8217;re often called in our business. Take extra care to make sure the postmortem document isn&#8217;t just a useless list of apologies or excuses &#8212; that&#8217;s not its purpose. A proper postmortem should always contain an explanation of what was learned and what is going to change as a result of the learning experience. Then make sure you put it in an easy-to-find place and really follow through on the proposed changes. Remember that properly documenting failures also makes it easier for other people (present and future) to know what happened and avoid repeating history. Don&#8217;t erase your tracks &#8212; light them up like a runway for those who follow you!</p>
<p>The problem is that once you reach a local maximum on your team, you stop learning.</p>
<p>The more you are open to influence, the more you are able to influence; the more vulnerable you are, the stronger you appear.</p>
<p>A &#8220;strong culture&#8221; is one that is open to change that improves it, yet is resistant to radical change that harms it.</p>
<p>A good general rule around communication is to include as few people as necessary in synchronous communication (like meetings and phone calls), and to go for a broader audience in asynchronous communication (like email, issue trackers, and document comments).</p>
<p>We like our meetings like we like our sewage treatment plants: few, far between, and downwind.</p>
<p>We&#8217;ve seen some cultures where meeting attendance is equated with status, so nobody wanted to be left out. Not to put too fine a point on it, but that is patently insane.</p>
<p>If you&#8217;re trying to design something new, try to include no more than five people in your meeting</p>
<p>flat out ignore invitations to a meeting that has no agenda.</p>
<p>many engineers rush right into coding before designing the software they intend to write, and this usually ends very badly.</p>
<p>When many people first hear about IRC these days, they scoff at its primitive text-based environment because even the most modern of IRC clients tend to be less whizzy than outdated versions of iChat or Google Talk. Don&#8217;t be fooled by the outdated look and feel of IRC &#8212; its killer features are that it was designed for multiperson chat and it&#8217;s asynchronous; most clients keep an unlimited scroll-back record so that you can read back to see conversations among others that you missed. Slack is basically the modern-day version of IRC, and despite its whizzy integration of graphics, avatars, and emoji, at its heart it&#8217;s still a text-based messaging system like IRC. It may be tempting to try out fancy videoconferencing packages, shared whiteboard systems, and more, but these systems often tend to be ineffective and can eliminate the asynchronous advantage of text-based group chat. If you&#8217;re going to use something other than Slack or IRC, find something that is actually designed for group chat and isn&#8217;t just an instant messaging system with group chat bolted on.</p>
<p>Comments should be focused on why the code is doing what it&#8217;s doing, not what the code is doing.</p>
<p>infer what various symbols are and what invariants are true about them.</p>
<p>attempting to add and remove names from a source file is a never-ending exercise in insanity.</p>
<p>the overwhelming majority of effort that goes into a culture turns out to be communication.</p>
<p>saying no to all of the distractions is what keeps you focused.</p>
<p>the best leaders work to serve their team using the principles of humility, respect, and trust.</p>
<p>A boat without a captain is nothing more than a floating waiting room</p>
<p>Managers wind up acting like parents, and consequently employees react like children.</p>
<p>Traditional managers worry about how to get things done, while leaders worry about what things get done&#8230;(and trust their team to figure out how to do it).</p>
<p>A TL is typically responsible for the technical direction for all (or part) of a product, while a TLM is responsible for the technical direction for all (or part) of a product in addition to the careers and happiness of the people on the team.</p>
<p>&#8220;Above all, resist the urge to manage.&#8221;</p>
<p>you should strive to hire people who are smarter than you and can replace you.</p>
<p>&#8220;Hope is not a strategy.&#8221;</p>
<p>&#8220;A people hire other A people; B people hire C people.&#8221;</p>
<p>As an engineer, you likely developed an excellent sense of skepticism and cynicism, but this can be a liability when you&#8217;re trying to lead a team.</p>
<p>As a leader, your job is to inspire, but inspiration is a 24/7 job.</p>
<p>In many cases, knowing the right person is more valuable than knowing the right answer.</p>
<p>We strongly advise against using the compliment sandwich,</p>
<p>It&#8217;s the behaviors you want to filter out, not particular individuals.</p>
<p>Document all history: not just code history, but also design decisions, important bug fixes, and prior mistakes.</p>
<p>Streamline the barrier to entry for newcomers.</p>
<p>Attention and focus are the scarcest resources you have.</p>
<p>A strong culture based on HRT is irreplaceable, while technical contributions are definitely replaceable.</p>
<p>genius is such a commodity these days that it&#8217;s not acceptable to be an eccentric anymore.</p>
<p>it&#8217;s not worth compromising your culture for the short-term gains</p>
<p>there are only a few crazy people out there; the Internet just makes it seems like they all live next door.&#8221;</p>
<p>Most people work in dysfunctional corporate bureaucracies and need to employ certain manipulative techniques to get things done effectively.</p>
<p>It boils down to this: is your manager serving you? Or are you serving your manager? It should always be the former.</p>
<p>&#8220;It&#8217;s impossible to simply stop a bad habit; you need to replace it with a good habit.&#8221;</p>
<p>categorize all work as either &#8220;offensive&#8221; or &#8220;defensive.&#8221;</p>
<p>Every company has a gray-market favor economy that lives outside the org chart,</p>
<p>Every company has a &#8220;shadow&#8221; org chart that is unwritten but through which power and influence flow.</p>
<p>A good Three Bullets and a Call to Action email contains (at most) three bullet points detailing the issue at hand, and one &#8212; and only one &#8212; call to action.</p>
<p>Most industries are a lot smaller than you think, and people talk more than you think,</p>
<p>Friends come and go&#8230;enemies accumulate.</p>
<p>product laziness.</p>
<p>It&#8217;s really obvious (and infuriating) when a programmer could have made something friendly and easy for the end user but didn&#8217;t bother.</p>
<p>Most programmers vastly underestimate the importance of application speed (or latency, which sounds more scientific).</p>
<p>&#8220;Speed is a feature.&#8221;</p>
<p>An elegant design makes easy things easy and hard things possible.</p>
<p>the user&#8217;s data needs to be accessible</p>
<p>learn great patience.</p>
<p>there is no such thing as a temporary lapse of integrity.</p>
<p>Trust is your most sacred resource.</p>
<p>Humans are unpredictable and tricky to deal with no matter what the context.</p>
</blockquote>
<p>And finally, <a href="http://www.cs.virginia.edu/~robins/YouAndYourResearch.html">&#8220;You and Your Research&#8221; by Richard Hamming</a> seems like good reading they recommended.</p>    
    ]]></description>
<link>http://planspace.org/20151118-debugging_teams/</link>
<guid>http://planspace.org/20151118-debugging_teams/</guid>
<pubDate>Wed, 18 Nov 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Creative Data Science</title>
<description><![CDATA[

<p><em>A &#8220;Brown Bag Lunch&#8221; &#8220;Chalk Talk&#8221; for the <a href="http://www.worldbank.org/">World Bank Group</a> IFC (<a href="http://www.ifc.org/">International Finance Corporation</a>) RMES (<a href="http://red-sphere.com/clients/WBG/video/index.html">Results, Measurement and Evaluation Stream</a>) on Thursday November 12, 2015. (<a href="big.html">slides</a>)</em></p>
<p>This is the flier the World Bank folks made:</p>
<p><img alt="flier" src="flier.png"></p>
<p>Here are some questions the audience was asked to answer and discuss before things got rolling:</p>
<ul>
<li>What are you reading?</li>
<li>What is your favorite tool, process, or technique for your work?</li>
<li>What is your <em>least</em> favorite tool, process, or technique for your work?</li>
<li>What was the last new tool, process, or technique you learned (or are currently learning)?</li>
<li>What would you like to learn next?</li>
</ul>
<p>The text below starts out fairly complete, and then toward the end it's more demo and telegraphic notes to myself.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img height="1000%" src="tool_bench.jpg" title="tool bench"></p>
<hr>
<p>I was invited to give a talk about &#8220;the data scientist's toolbox&#8221;.</p>
<p>A lot of people, when they hear &#8220;the data scientist's toolbox&#8221;, think of programming languages, or particular suites of implementations like Hadoop or Spark.</p>
<hr>
<p><img width="1000%" src="alexnet.png" title="AlexNet"></p>
<hr>
<p>So here's a tool!</p>
<p>It's the AlexNet deep convolutional neural network, which was state of the art for identifying things in images a couple years ago. We could use a Caffe Model Zoo implementation, which would still have the 11-by-11 convolutions on the first layer, but likely wouldn't split the computation across two GPUs that way, yadda yadda yadda, this kind of thing is relevant to some of the work my current company is doing, yadda yadda yadda.</p>
<p>This might be interesting for some people, but for a lot of people I suspect it would not be directly useful.</p>
<hr>
<p>&#8220;Be regular and orderly in your life so that you may be violent and original in your work.&#8221;</p>
<hr>
<p>I came across this <a href="https://en.wikipedia.org/wiki/Gustave_Flaubert">Flaubert</a> quote <a href="http://planspace.org/2014/01/19/daily-rituals-is-sort-of-inspiring/">somewhere</a>, and I like it quite a lot.</p>
<p>The tools I want to focus on help me to be regular and orderly in my everyday work.</p>
<p>Being regular and orderly in everyday work means that everyday work becomes easier, leaving capacity for more sophisticated work.</p>
<p>From the other side, even if your work is very sophisticated, you still have to attend to the everyday, and life will be better if it is well attended.</p>
<hr>
<p><img src="canvas.jpg" title="blank canvas" width="1000%"></p>
<hr>
<p>Another way to think about tools is to weigh what's more important to the painting, the painter or the brush?</p>
<p>Certainly you need both, and tools are more or less a requirement everywhere. But the mental tools, the skills and experience of the painter, these make the lion's share of the difference in the end results.</p>
<p>I haven't taught painting, but I've taught &#8220;data science tools&#8221; for a number of years, and my overwhelming impression has been that mental tools matter more than the particular software people happen to use.</p>
<p>The good news is that while you may not always be able to choose your tools, you can always choose (and improve) the self that you bring to them!</p>
<hr>
<p><img height="1000%" src="debugging_teams.jpg" title="Debugging Teams cover"></p>
<hr>
<p>The first question I asked earlier (&#8220;What are you reading?&#8221;) may have seemed not to be about tools, but it is. Reading builds you new tools.</p>
<p>Here's what I'm currently reading: a book called &#8220;Debugging Teams&#8221;. Good book!</p>
<p>You have a great team here at the World Bank, and I want to point out that you're doing a great thing today, taking your lunch to become even greater. That's exactly the kind of thing that strong teams do.</p>
<hr>
<p>create</p>
<hr>
<p>A central point of &#8220;Debugging Teams&#8221; is the importance of humility, respect, and trust.</p>
<p>In that spirit, let me describe a best-case scenario for our time together:</p>
<p>I will mostly tell you about other people's ideas. (And I may not always cite sources.)</p>
<p>Worse, I probably won't even have quite the right ideas for you.</p>
<p>I'm going to talk about some things that I think help create good work with data.</p>
<p>But I sure don't have your expertise, experience, and other knowledge about everything that you're working on.</p>
<p>This kind of event doesn't make sense if it's just me talking and then everybody goes back to working exactly the same as they always do.</p>
<p>You've come together in this safe collaborative space. The people who should get something out of it are you. What will you go on to do that you wouldn't have done if you weren't in this room today? This is a chance for you to think up something great! It could be the opposite of what I suggest, or totally unrelated. Today could be the day you have that great idea that saves the world!</p>
<p>Create!</p>
<hr>
<ul>
<li>immutability</li>
<li>separation of concerns</li>
<li>interoperability</li>
<li>message-passing</li>
</ul>
<hr>
<p>These are the key ideas that I want to explore today. It's okay if they don't make sense now.</p>
<hr>
<p>[demo]</p>
<hr>
<p>This bit is a kind of game.</p>
<p>I'm going to do a demonstration with Excel, which I understand some of you have seen before, and I want you to evaluate what I do.</p>
<p>It's as simple as "Aaron did X, and it was good (or bad) because Y."</p>
<p>After I finish, we'll discuss in pairs and try to get a list of twelve such evaluations.</p>
<hr>
<p><img src="important_stats1.png" title="screenshot"></p>
<hr>
<p>Here's what <code>important_stats.xlsx</code> looks like originally.</p>
<hr>
<p><img src="important_stats2.png" title="screenshot"></p>
<hr>
<p>Here's what <code>important_stats.xlsx</code> looks like after we're done with it.</p>
<hr>
<p>Discussion!</p>
<ul>
<li>good things?</li>
<li>bad things?</li>
<li>WWYD?</li>
</ul>
<hr>
<p>That's "What Would You Do".</p>
<p>Let's get twelve things!</p>
<hr>
<p>immutability</p>
<hr>
<p>What is this?</p>
<hr>
<p><img height="1000%" src="rick1.png" title="healthy Rick"></p>
<hr>
<p>Here is normal Rick.</p>
<p>Until he does some weird chemical things.</p>
<hr>
<p><img width="1000%" src="rick2.png" title="Cronenberg Rick"></p>
<hr>
<p>Rick is mutated! Old Rick is gone! He's all Cronenberg'ed up!</p>
<hr>
<p>destroy &#8800; create</p>
<hr>
<p>Old Rick has been <em>destroyed</em>, which is the opposite of <em>creative</em>. We want to make things, not destroy them!</p>
<p>When I saved over the original version of the file, I destroyed it! It is gone forever!</p>
<hr>
<p>reproducible research</p>
<hr>
<p>What is research that isn't reproducible? Wrong!</p>
<hr>
<p><img width="1000%" src="tree.png" title="commit graph"></p>
<hr>
<p>Make new versions! It's a DAG now!</p>
<hr>
<p>[demo]</p>
<hr>
<p>Using <code>important_stats_20151111.xlsx</code>.</p>
<hr>
<p><img src="git.png" width="1000%" title="git"></p>
<hr>
<p><a href="http://git-scm.com/">git</a> is the real answer.</p>
<hr>
<p>separation of concerns</p>
<hr>
<p>This is concept two.</p>
<hr>
<ul>
<li>entry/editing</li>
<li>storage</li>
<li>calculation</li>
<li>presentation</li>
</ul>
<hr>
<p>These are all things that we were doing in Excel.</p>
<hr>
<p><img src="swiss_army_knife.png" title="Swiss Army knife" height="1000%"></p>
<hr>
<p>Some people use "Swiss Army knife" as a kind of compliment, but I don't think it should be. You've got a knife and a screwdriver, but you almost certainly have your very <em>worst</em> knife and your very <em>worst</em> screwdriver.</p>
<p>At least for tools, usually a jack of all trades is a master of none.</p>
<p>It can be convenient to have a Swiss Army knife around, but it isn't the tool of choice when a professional goes to work.</p>
<hr>
<p>[demo]</p>
<hr>
<p>Use Excel to change values, save as CSV; now we have data entry and storage separated.</p>
<hr>
<p>interoperability</p>
<hr>
<p>Choose tools that work together! Multiply possibilities!</p>
<hr>
<p>&#8220;The idea is not to replace one tool with another but rather build a better ecosystem of complementary tools.&#8221;</p>
<hr>
<p>This is a quote from a <a href="https://www.youtube.com/watch?v=GdoDLuPe-Wg">presentation</a> given by <a href="https://twitter.com/jeffrey_heer">Jeffrey Heer</a>.</p>
<hr>
<p><img src="person.png" title="person" height="1000%"></p>
<hr>
<p>Interview person who only works with other club members.</p>
<hr>
<p>open standards</p>
<hr>
<p>Some standardization is necessary, but it should be open and Free!</p>
<hr>
<p><code>text text text text text text</code></p>
<hr>
<p>Text is great!</p>
<hr>
<p><img src="git.png" width="1000%" title="git"></p>
<hr>
<p><code>git</code> example: lots of vendors, etc. Not locked in!</p>
<hr>
<p>languages: tools for making tools</p>
<hr>
<p>R and Python!</p>
<hr>
<p>[demo]</p>
<hr>
<p>Write it in R! In RStudio! Yeah! (See <code>important_mean.R</code>.)</p>
<p>Using <code>R</code> just for calculation!</p>
<p>Add <code>cowsay</code> for presentation!</p>
<pre><code class="language-bash">Rscript important_mean.R | cowsay</code></pre>

<p>Repeat with Python! In Emacs! Yeah! (See <code>important_mean.py</code>.)</p>
<p>Using <code>python</code> just for calculation!</p>
<p>Add <code>cowsay</code> for presentation!</p>
<pre><code class="language-bash">python important_mean.py | cowsay</code></pre>

<pre><code> __________________
&lt; 0.00331428571429 &gt;
 ------------------
        \   ^__^
         \  (oo)\_______
            (__)\       )\/\
                ||----w |
                ||     ||</code></pre>

<hr>
<p>message-passing</p>
<hr>
<p>Saw some with the connections between R, Python, and <code>cowsay</code>.</p>
<p>How does anyone know what I'm working on? The changes I've made? etc.</p>
<p>Publish widely! Making available is different than notifying!</p>
<p>Give your paper to someone else!</p>
<hr>
<ul>
<li>immutability</li>
<li>separation of concerns</li>
<li>interoperability</li>
<li>message-passing</li>
</ul>
<hr>
<p>Wrapping up!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20151112-creative_data_science/</link>
<guid>http://planspace.org/20151112-creative_data_science/</guid>
<pubDate>Thu, 12 Nov 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Interactive Perceptron Training Toy</title>
<description><![CDATA[

<p>A little while ago I <a href="../20150610-a_javascript_perceptron/">contributed</a> a simple <a href="https://en.wikipedia.org/wiki/Perceptron">perceptron</a> model for the <a href="http://simplestatistics.org/">Simple Statistics</a> JavaScript library. This makes possible things like this interactive perceptron model training environment, in which you can get a &#8220;hands-on&#8221; feel for how the model works in two dimensions.</p>
<p>The space below starts all red, which means the model starts predicting any given point is &#8220;negative.&#8221; As you update things, the color will update to show where the model would predict positive (blue) and negative (red).</p>
<p>The space below is clickable! One normal (left) click will add a blue (positive; label &#8220;1&#8221;) point. Clicking on a blue point will turn it red (negative; label &#8220;0&#8221;). Clicking on a red point will remove it. So you can cycle between no point, blue point, red point, no point.</p>
<p>To train the model, choose a point and use your other click (control click or right click). The perceptron model updates when it makes an incorrect prediction. You'll see details about this process below the box, as it happens!</p>
<p></p><center><div id="space"></div></center>
<h3>Diagnostics appear with model training:</h3>
<div id="status"></div>

<p>You can follow the model fitting step by step! (To reset everything, just reload the page.)</p>
<p>Since we're in two dimensions, each data point has an <em>x</em> and <em>y</em> coordinate, written <em>[x, y]</em>. The perceptron model has two weights that correspond to the <em>x</em> and <em>y</em> directions (let's just call them <em>a</em> and <em>b</em>), and a bias term which we can call <em>c</em>.  The model predicts <em>positive</em> (blue) if <em>a<em>x + b</em>y + c</em> is greater than zero, and <em>negative</em> (red) otherwise. (You can think of the bias term as a weight that is always multiplied by one.)</p>
<p>One thing that became particularly clear as I put this together is how really essential centering and scaling data is. With points within three of the origin in either direction, the model can usually do well in a reasonable number of training steps, especially if the points are spread around the origin. But try putting two points of opposing color on a diagonal near the same corner of the box. It takes <em>forever</em> to fit! The perceptron can have a hard time moving away from intercepting the origin.</p>
<p>You can also see quite plainly that the (single layer) perceptron classifies by linear separation, so it can't handle the <a href="http://www.ece.utep.edu/research/webfuzzy/docs/kk-thesis/kk-thesis-html/node19.html">XOR problem</a>.</p>
<p>Fun!</p>
<p><link rel="stylesheet" href="style.css">
<script src="script.js"></script></p>    
    ]]></description>
<link>http://planspace.org/20150907-interactive_perceptron_training_toy/</link>
<guid>http://planspace.org/20150907-interactive_perceptron_training_toy/</guid>
<pubDate>Mon, 07 Sep 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Don't Shoot the Dog!</title>
<description><![CDATA[

<p>On the <a href="https://www.youtube.com/watch?v=hY14Er6JX2s">recommendation</a> of <a href="http://blog.sashalaundy.com/">Sasha Laundy</a>, I read <a href="http://www.amazon.com/Dont-Shoot-Dog-Teaching-Training/dp/0553380397">Don't Shoot the Dog!</a> by Karen Pryor.</p>
<ol>
<li>The advice on memorizing by starting with the end of a passage, so that rehearsal always starts with the difficult and then progresses to familiar material, strikes me as good advice that I may have heard before but forgotten.</li>
<li>I would like to play the Training Game, which is described in the selection below.</li>
</ol>
<p>It's a fun book, and it makes me want to train a cat or dog to do fun things.</p>
<blockquote>
<p>Don't Shoot the Dog! pages 52 to 54, The Training Game</p>
<p>Even if you know and understand the principles of shaping, you can't apply them unless you practice them. Shaping is not a verbal process, it is a nonverbal skill&#8212;a flow of interactive behavior through time, like dancing, or making love, or surfing. As such, it can't really be learned by reading or thinking or talking about it. You have to do it.</p>
<p>One easy and fascinating way to develop shaping skills is by playing the Training Game. I use the Training Game in teaching the techniques of training. Many trainers play it for sport; it makes an interesting party game.</p>
<p>You need two people at least: the subject and the trainer. Six is ideal because then every person can experience being both subject and trainer at least once before the group gets tired; but larger groups&#8212;a classroom or lecture audience, for example&#8212;are feasible, because observing is almost as much fun as participating.</p>
<p>You send the subject out of the room. The rest of the people select a trainer and choose  a behavior to be shaped: for example, to write one's name on the blackboard, jump up and down, or stand on a chair. The subject is invited back in and told to move about the room and be active; the trainer reinforces, by blowing on a whistle, movements in the general direction of the desired behavior. I like to make a rule at least for the first few reinforcements that the &#8220;animal&#8221; has to go back to the doorway after each reinforcer and start anew; it seems to help prevent a tendency of some subjects to just stand still wherever reinforcement was last received. And no talking. Laughter, groans, and other signs of emotion are permitted, but instructions and discussion are out until after the behavior is achieved.</p>
<p>Ordinarily the Training Game goes quite fast. Here's an example: Six of us are playing the game in a friend's living room. Ruth volunteers to be the animal, and it's Anne's turn to be the trainer. Ruth goes our of the room. We all decide that the behavior should be to turn on the lamp on the end table beside the couch.</p>
<p>Ruth is called back in and begins wandering around the room. When she heads in the direction of the lamp, Anne blows the whistle. Ruth goes back to &#8220;Start&#8221; (the doorway), then moves purposefully to the spot where she was reinforced and stops. No whistle. She waves her hands about. No whistle. She moves off the spot, tentatively, away from the lamp as it happens. Still hearing no whistle, Ruth begins walking around again. When once again she walks toward the lamp, Anne blows the whistle.</p>
<p>Ruth returns to the door and then returns to the new spot where she just heard the whistle, but this time she keeps walking forward. Bingo: whistle. Without going back to the door, she walks forward some more and hears the whistle just as she is coming up against the end table. She stops. She bumps the end table. No whistle. She waves her hands around; no whistle. One hand brushes the lampshade, and Anne whistles. Ruth begins touching the lampshade all over&#8212;moving it, turning it, rocking it: no whistle. Ruth reaches up underneath the lampshade. Whistle. Ruth reaches underneath the shade again, and, the gesture being very familiar and having a purpose, she executes the purpose and turns on the lamp. Anne whistles and the rest of us applaud.</p>
<p>Things don't always go that smoothly, even with simple, familiar behaviors. Anne, as it turned out, made a good training decision when she withheld reinforcement as Ruth moved from the spot where she'd first been reinforced, but in the wrong direction. If, however, Ruth had then moved back to the spot and just stood there, Anne might have been in trouble.</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20150705-dont_shoot_the_dog/</link>
<guid>http://planspace.org/20150705-dont_shoot_the_dog/</guid>
<pubDate>Sun, 05 Jul 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Code Reading Question</title>
<description><![CDATA[

<p>Here's an idea for a type of question to help assess a person's skill in programming. Give the person some code to read:</p>
<pre><code class="language-python">def norf(bar, foo):
    for baz in bar:
        if baz in foo or norf(baz, foo+[baz]):
            return True
    return False</code></pre>

<p>This is four lines of Python.</p>
<p>Some of the questions you could ask:</p>
<ul>
<li>What does <code>norf</code> do?</li>
<li>What are the types of the arguments <code>bar</code> and <code>foo</code>?</li>
<li>What are better names for <code>norf</code>, <code>bar</code>, <code>foo</code>, and <code>baz</code>?</li>
<li>What data structure are we working with?</li>
<li>What are alternatives for the design of this data structure?</li>
<li>What are limitations of this function?</li>
</ul>
<p>I suspect that this could be interesting. I haven't seen anyone use interview questions of this type. I wonder how well this one would work.</p>    
    ]]></description>
<link>http://planspace.org/20150616-code_reading_question/</link>
<guid>http://planspace.org/20150616-code_reading_question/</guid>
<pubDate>Tue, 16 Jun 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>The NVIDIA Jetson TK1 with Caffe on MNIST</title>
<description><![CDATA[

<p>Let's do <a href="https://en.wikipedia.org/wiki/Deep_learning">deep learning</a> for image classification on a <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a>!</p>
<p>Go!</p>
<p><img alt="Jetson TK1" src="jetson.png"></p>
<p><em><a href="https://www.flickr.com/photos/120586634@N05/14488628209/in/album-72157645736592714/">Image from Gareth Halfacree</a>.</em></p>
<p>The <a href="https://developer.nvidia.com/jetson-tk1">Jetson</a> is a pretty remarkable little machine. And NVIDIA prices it at just $192: one dollar per core on the GPU. But there is a little work involved in getting a Jetson set up. Steps described here:</p>
<ol>
<li>Flash the Jetson to most recent OS (L4T 21.3)</li>
<li>Install the <a href="http://caffe.berkeleyvision.org/">Caffe</a> deep learning framework</li>
<li>Train a classic <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">convolutional</a> net on <a href="http://yann.lecun.com/exdb/mnist/">MNIST</a></li>
</ol>
<p>Without extra hardware, the easiest way to start working with the Jetson is to plug it in directly to your router with an ethernet cable. Then you can <code>ssh</code> in to <code>tegra-ubuntu</code> on the local network, using the default username and password (both <code>ubuntu</code>):</p>
<pre><code class="language-bash">ssh ubuntu@tegra-ubuntu.local</code></pre>

<h3>1. Flash the Jetson to the most recent OS (L4T 21.3)</h3>
<p>The GPU on the Jetson is a Tegra K1. The Jetson I received came preinstalled with an <a href="https://developer.nvidia.com/linux-tegra-rel-19">early version</a> of the "<a href="https://developer.nvidia.com/linux-tegra">Linux for Tegra</a>" (L4T) <a href="http://www.ubuntu.com/">Ubuntu</a>-based operating system. Step one is updating to the most recent version (currently <a href="https://developer.nvidia.com/linux-tegra-r213">21.3</a>).</p>
<p>I flashed with the <a href="https://developer.nvidia.com/jetson-tk1-development-pack">Jetson TK1 Development Pack</a>. Two things about the &#8220;JetPack&#8221;:</p>
<ul>
<li>You can only run the JetPack on 64-bit Ubuntu 12.04 and 14.04. I used 14.04.</li>
<li>The JetPack is about 12 gigabytes in size, so make sure you have enough space on your machine or any virtual machine you plan to use.</li>
</ul>
<p>I'm running a Mac over here, so I used <a href="https://www.virtualbox.org/">VirtualBox</a> (4.3.28, plus the extension pack) to install a virtual <a href="http://www.ubuntu.com/download/desktop">Ubuntu Desktop</a> 14.04.2 LTS from ISO. I hadn't created a VirtualBox image this way before; it's pretty cool! You can tell VirtualBox that the ISO is in a virtual DVD drive, and boot from it. An NVIDIA Developer Zone <a href="https://devtalk.nvidia.com/default/topic/718220/tegra-tools/jetson-tegra-k1-development-kit/">post</a> was very helpful in figuring all this out. Takes me back to the days when you had to burn an ISO to a physical CD...</p>
<p>It took me a couple tries to get everything right; to get it all done in one pass you would want to connect the Jetson via USB and start it in recovery mode now, then adjust the VirtualBox settings so that there's a &#8220;USB Device Filter&#8221; for the &#8220;NVIDIA Corp. APX [0101]&#8221; device accessible, before even installing the VM.</p>
<p>There was one headache in getting the Ubuntu desktop to display at reasonable resolutions. The display resolution started at 640 by 480, with no apparent way to change it. Thanks to <a href="http://askubuntu.com/users/394518/johnathan-davis">Jonathan Davis</a> for <a href="http://askubuntu.com/questions/588943/experiencing-small-resolution-issue-in-ubuntu-14-04-2-with-virtualbox-getting-s">suggesting</a> a solution that worked for me:</p>
<pre><code class="language-bash">sudo apt-get remove libcheese-gtk23
sudo apt-get install xserver-xorg-core
sudo apt-get install -f virtualbox-guest-x11</code></pre>

<p>Reboot and get full-screen majesty!</p>
<p>In Ubuntu then I was able to install the JetPack. This whole process takes at least enough time to eat some Thai food.</p>
<p>After flashing, I cleaned out the old record from <code>~/.ssh/known_hosts</code> and was able to log in to a shiny new &#8220;Linux for Tegra&#8221; 21.3 install!</p>
<pre><code class="language-bash">ssh ubuntu@tegra-ubuntu.local</code></pre>

<p>At the end of the JetPack process, you need to enter the IP address of your Jetson. You can use <code>ifconfig</code> (as directed) on the Jetson to get the right IP address.</p>
<p>The JetPack process continues then and includes installing more things on the Jetson, including <code>cuda-toolkit-6-5</code>. Nice!</p>
<p>The JetPack also includes <a href="http://opencv.org/">OpenCV</a>, but doesn't seem to install it on the Jetson automatically. So move this file onto the Jetson yourself:</p>
<pre><code class="language-bash">JetPackTK1-1.1/jetpack_download/libopencv4tegra-repo_l4t-r21_2.4.10.1_armhf.deb</code></pre>

<p>On the Jetson then, install it something like the following. The package itself contains three other packages which have to be installed, and which have a couple dependencies that will need to be resolved first:</p>
<pre><code class="language-bash">sudo dpkg -i libopencv4tegra-repo_l4t-r21_2.4.10.1_armhf.deb
sudo apt-get install libavcodec54 libavformat54 libavutil52 libswscale2
sudo dpkg -i /var/opencv4tegra-repo/libopencv4tegra_2.4.10.1_armhf.deb
sudo dpkg -i /var/opencv4tegra-repo/libopencv4tegra-dev_2.4.10.1_armhf.deb
sudo dpkg -i /var/opencv4tegra-repo/libopencv4tegra-python_2.4.10.1_armhf.deb</code></pre>

<p>If you're a &#8220;CUDA Registered Developer&#8221; (they have to manually approve applications?) now might be a good time to install <a href="https://developer.nvidia.com/cuDNN">cuDNN</a> R1.</p>
<h3>2. Install the Caffe deep learning framework.</h3>
<p>Pete Warden's <a href="http://petewarden.com/2014/10/25/how-to-run-the-caffe-deep-learning-vision-library-on-nvidias-jetson-mobile-gpu-board/">post</a> on setting up Caffe on the Jetson isn't all perfectly applicable after you've flashed to L4T 21.3, but I very much appreciated his translation of the Caffe <a href="http://caffe.berkeleyvision.org/installation.html">install</a> dependencies into Ubuntu package names:</p>
<pre><code class="language-bash">sudo apt-get install \
    libprotobuf-dev protobuf-compiler gfortran \
    libboost-dev cmake libleveldb-dev libsnappy-dev \
    libboost-thread-dev libboost-system-dev \
    libatlas-base-dev libhdf5-serial-dev libgflags-dev \
    libgoogle-glog-dev liblmdb-dev gcc-4.7 g++-4.7</code></pre>

<p>The GCC 4.7 vs. 4.8 issues are resolved now I think, so the last two items there could probably be skipped.</p>
<p>Warden also advised some manual tweaks to <code>$PATH</code> and <code>$LD_LIBRARY_PATH</code>, but these are taken care of already by the install process. You may still need to <code>source ~/.bashrc</code>, depending on when you last logged in.</p>
<p>To get Python integration compiled you need <code>numpy</code>:</p>
<pre><code class="language-bash">sudo apt-get install python-numpy</code></pre>

<p>It looks like it's best to install Caffe from the <code>master</code> branch of the GitHub repository. (The <code>dev</code> branch hasn't been updated in three months.)</p>
<pre><code class="language-bash">sudo apt-get install git
git clone https://github.com/BVLC/caffe.git
cd caffe
cp Makefile.config.example Makefile.config</code></pre>

<p>Unfortunately <code>master</code> has a really large value for <code>LMDB_MAP_SIZE</code> in <code>src/caffe/util/db.cpp</code>, which confuses our little 32-bit ARM processor on the Jetson, eventually leading to Caffe tests failing with errors like <code>MDB_MAP_FULL: Environment mapsize limit reached</code>. Caffe GitHub issue <a href="https://github.com/BVLC/caffe/issues/1861">#1861</a> has some discussion about this and maybe it will be fixed eventually, but for the moment if you manually adjust the value from <code>1099511627776</code> to <code>536870912</code>, you'll be able to run all the Caffe tests successfully. I'm still having problems actually applying the relevant code paths, but at least the tests pass.</p>
<pre><code class="language-bash">make -j 4 all
make -j 4 test
make -j 4 runtest</code></pre>

<p>Then you can run the following command with a pre-built model and see that you get pretty nice prediction speed!</p>
<pre><code class="language-bash">run build/tools/caffe time --model=models/bvlc_alexnet/deploy.prototxt --gpu=0</code></pre>

<h3>3. Train a classic convolutional net on MNIST</h3>
<p>At this point you can use Caffe for whatever, including all the great tutorials! For example, there's the <a href="http://caffe.berkeleyvision.org/gathered/examples/mnist.html">MNIST tutorial</a>.</p>
<p>Since Caffe's <a href="http://symas.com/mdb/">LMDB</a> support still isn't working right on my Jetson install, I had to switch the tutorial example to use <a href="http://leveldb.org/">LevelDB</a>. This isn't too hard; just change <code>BACKEND="lmdb"</code> to <code>BACKEND="leveldb"</code> in <code>examples/mnist/create_mnist.sh</code> and then in <code>examples/mnist/lenet_train_test.prototxt</code> change both directories ending in <code>lmdb</code> to end in <code>leveldb</code> and both <code>LMDB</code>s to <code>LEVELDB</code>s.</p>
<p>Aside from that the <a href="http://caffe.berkeleyvision.org/gathered/examples/mnist.html">tutorial</a> is perfect! (Well, test accuracy gets up to 99.03%, anyway.)</p>
<p>There's a ton more to work with and think about, even just within the Caffe examples. And Caffe is just one of the available frameworks! What about <a href="http://torch.ch/">Torch</a>? What about <a href="https://code.google.com/p/cuda-convnet2/">CUDA ConvNet</a>? But for now...</p>
<p>Deep learning: Done!</p>    
    ]]></description>
<link>http://planspace.org/20150614-the_nvidia_jetson_tk1_with_caffe_on_mnist/</link>
<guid>http://planspace.org/20150614-the_nvidia_jetson_tk1_with_caffe_on_mnist/</guid>
<pubDate>Sun, 14 Jun 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>A JavaScript Perceptron</title>
<description><![CDATA[

<p>Watching <a href="https://www.coursera.org/course/neuralnets">Neural Networks for Machine Learning</a> at lunch with my class at <a href="http://www.thisismetis.com/">Metis</a>, I thought it would be fun to implement a simple <a href="http://en.wikipedia.org/wiki/Perceptron">perceptron</a> for the <a href="https://github.com/tmcw/simple-statistics">simple-statistics</a> JavaScript module.</p>
<p>And it <em>was</em> fun!</p>
<p>Now there's an open <a href="https://github.com/tmcw/simple-statistics/pull/103">pull request</a> for adding the functionality to <code>simple-statistics</code>. Here I'm pulling out just the perceptron code, as of commit <a href="https://github.com/ajschumacher/simple-statistics/commit/a5a092a7c6b5d3276acc783c712814748cd53c3f">a5a092a</a>. It's here in <a href="perceptron.js">perceptron.js</a> and also visible at the bottom of this post. It's used like this:</p>
<pre><code class="language-javascript">// Create a perceptron model:
var p = perceptron()

// Train with a feature vector [0] that has label 1,
//        and a feature vector [1] that has label 0.
p.train([0], 1)
p.train([1], 0)
p.train([0], 1)

// The perceptron has learned enough to classify correctly:
p.predict([0])
// 1
p.predict([1])
// 0</code></pre>

<p>There are just slightly fancier examples in the <a href="https://github.com/ajschumacher/simple-statistics/blob/a5a092a7c6b5d3276acc783c712814748cd53c3f/test/perceptron.test.js">tests</a>. You can use feature vectors of whatever length you like, so long as you're consistent.</p>
<p>To really use the model you have to feed it labeled examples until it (hopefully) converges to a solution. You could keep training until <code>p.weights()</code> and <code>p.bias()</code> are no longer changing, for example.</p>
<p>I recommend <a href="https://www.coursera.org/course/neuralnets">Neural Networks for Machine Learning</a>, and I recommend <a href="https://github.com/tmcw/simple-statistics">simple-statistics</a>!</p>
<p>Here's the <a href="perceptron.js">perceptron.js</a> code:</p>
<pre><code class="language-javascript">// # [Perceptron Classifier](http://en.wikipedia.org/wiki/Perceptron)
//
// This is a single-layer perceptron classifier that takes
// arrays of numbers and predicts whether they should be classified
// as either 0 or 1 (negative or positive examples).
function perceptron() {
    var perceptron_model = {},
        // The weights, or coefficients of the model;
        // weights are only populated when training with data.
        weights = [],
        // The bias term, or intercept; it is also a weight but
        // it's stored separately for convenience as it is always
        // multiplied by one.
        bias = 0;

    // ## Predict
    // Use an array of features with the weight array and bias
    // to predict whether an example is labeled 0 or 1.
    perceptron_model.predict = function(features) {
        // Only predict if previously trained
        // on the same size feature array(s).
        if (features.length !== weights.length) return null;
        // Calculate the sum of features times weights,
        // with the bias added (implicitly times one).
        var score = 0;
        for (var i = 0; i &lt; weights.length; i++) {
            score += weights[i] * features[i];
        }
        score += bias;
        // Classify as 1 if the score is over 0, otherwise 0.
        return score &gt; 0 ? 1 : 0;
    };

    // ## Train
    // Train the classifier with a new example, which is
    // a numeric array of features and a 0 or 1 label.
    perceptron_model.train = function(features, label) {
        // Require that only labels of 0 or 1 are considered.
        if (label !== 0 &amp;&amp; label !== 1) return null;
        // The length of the feature array determines
        // the length of the weight array.
        // The perceptron will continue learning as long as
        // it keeps seeing feature arrays of the same length.
        // When it sees a new data shape, it initializes.
        if (features.length !== weights.length) {
            weights = features;
            bias = 1;
        }
        // Make a prediction based on current weights.
        var prediction = perceptron_model.predict(features);
        // Update the weights if the prediction is wrong.
        if (prediction !== label) {
            var gradient = label - prediction;
            for (var i = 0; i &lt; weights.length; i++) {
                weights[i] += gradient * features[i];
            }
            bias += gradient;
        }
        return perceptron_model;
    };

    // Conveniently access the weights array.
    perceptron_model.weights = function() {
        return weights;
    };

    // Conveniently access the bias.
    perceptron_model.bias = function() {
        return bias;
    };

    // Return the completed model.
    return perceptron_model;
}</code></pre>    
    ]]></description>
<link>http://planspace.org/20150610-a_javascript_perceptron/</link>
<guid>http://planspace.org/20150610-a_javascript_perceptron/</guid>
<pubDate>Wed, 10 Jun 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>TextBlob Sentiment: Calculating Polarity and Subjectivity</title>
<description><![CDATA[

<p>The <a href="http://textblob.readthedocs.org/">TextBlob</a> package for Python is a convenient way to do a lot of Natural Language Processing (NLP) tasks. For example:</p>
<pre><code class="language-python">from textblob import TextBlob

TextBlob("not a very great calculation").sentiment
## Sentiment(polarity=-0.3076923076923077, subjectivity=0.5769230769230769)</code></pre>

<p>This tells us that the English phrase &#8220;not a very great calculation&#8221; has a <em>polarity</em> of about -0.3, meaning it is slightly negative, and a <em>subjectivity</em> of about 0.6, meaning it is fairly subjective.</p>
<p>But where do these numbers come from?</p>
<p>Let's find out by going to the source. (This will refer to <a href="https://github.com/sloria/TextBlob">sloria/TextBlob</a> on GitHub at commit <a href="https://github.com/sloria/TextBlob/tree/eb08c120d364e908646731d60b4e4c6c1712ff63">eb08c12</a>.)</p>
<p>After digging a bit, you can find that the main default sentiment calculation is defined in <a href="https://github.com/sloria/TextBlob/blob/eb08c120d364e908646731d60b4e4c6c1712ff63/textblob/_text.py">_text.py</a>, which gives credit to <a href="http://www.clips.ua.ac.be/pages/pattern-web">the pattern library</a>. (I'm not sure how much is original and how much is from <code>pattern</code>.)</p>
<p>There are helpful comments like this one, which gives us more information about the numbers we're interested in:</p>
<pre><code class="language-python"># Each word in the lexicon has scores for:
# 1)     polarity: negative vs. positive    (-1.0 =&gt; +1.0)
# 2) subjectivity: objective vs. subjective (+0.0 =&gt; +1.0)
# 3)    intensity: modifies next word?      (x0.5 =&gt; x2.0)</code></pre>

<p>The lexicon it refers to is in <a href="https://github.com/sloria/TextBlob/blob/eb08c120d364e908646731d60b4e4c6c1712ff63/textblob/en/en-sentiment.xml">en-sentiment.xml</a>, an XML document that includes the following four entries for the word &#8220;great&#8221;.</p>
<pre><code class="language-xml">&lt;word form="great" cornetto_synset_id="n_a-525317" wordnet_id="a-01123879" pos="JJ" sense="very good" polarity="1.0" subjectivity="1.0" intensity="1.0" confidence="0.9" /&gt;
&lt;word form="great" wordnet_id="a-01278818" pos="JJ" sense="of major significance or importance" polarity="1.0" subjectivity="1.0" intensity="1.0" confidence="0.9" /&gt;
&lt;word form="great" wordnet_id="a-01386883" pos="JJ" sense="relatively large in size or number or extent" polarity="0.4" subjectivity="0.2" intensity="1.0" confidence="0.9" /&gt;
&lt;word form="great" wordnet_id="a-01677433" pos="JJ" sense="remarkable or out of the ordinary in degree or magnitude or effect" polarity="0.8" subjectivity="0.8" intensity="1.0" confidence="0.9" /&gt;</code></pre>

<p>In addition to the polarity, subjectivity, and intensity mentioned in the comment above, there's also &#8220;confidence&#8221;, but I don't see this being used anywhere. In the case of &#8220;great&#8221; here it's all the same part of speech (<code>JJ</code>, adjective), and the senses are themselves natural language and not used. To simplify for readability:</p>
<pre><code class="language-text">word   polarity  subjectivity  intensity
great       1.0           1.0        1.0
great       1.0           1.0        1.0
great       0.4           0.2        1.0
great       0.8           0.8        1.0</code></pre>

<p>When calculating sentiment for a single word, TextBlob uses a sophisticated technique known to mathematicians as &#8220;averaging&#8221;.</p>
<pre><code class="language-python">TextBlob("great").sentiment
## Sentiment(polarity=0.8, subjectivity=0.75)</code></pre>

<p>At this point we might feel as if we're touring a sausage factory. That feeling isn't going to go away, but remember how delicious sausage is! Even if there isn't a lot of magic here, the results can be useful&#8212;and you certainly can't beat it for convenience.</p>
<p>TextBlob doesn't not handle negation, and that ain't nothing!</p>
<pre><code class="language-python">TextBlob("not great").sentiment
## Sentiment(polarity=-0.4, subjectivity=0.75)</code></pre>

<p>Negation multiplies the polarity by -0.5, and doesn't affect subjectivity.</p>
<p>TextBlob also handles modifier words! Here's the summarized record for &#8220;very&#8221; from the lexicon:</p>
<pre><code class="language-text">word   polarity  subjectivity  intensity
very        0.2           0.3        1.3</code></pre>

<p>Recognizing &#8220;very&#8221; as a modifier word, TextBlob will ignore polarity and subjectivity and just use intensity to modify the following word:</p>
<pre><code class="language-python">TextBlob("very great").sentiment
## Sentiment(polarity=1.0, subjectivity=0.9750000000000001)</code></pre>

<p>The polarity gets maxed out at 1.0, but you can see that subjectivity is also modified by &#8220;very&#8221; to become \( 0.75 \cdot 1.3 = 0.975 \).</p>
<p>Negation combines with modifiers in an interesting way: in addition to multiplying by -0.5 for the polarity, the inverse intensity of the modifier enters for both polarity and subjectivity.</p>
<pre><code class="language-python">TextBlob("not very great").sentiment
## Sentiment(polarity=-0.3076923076923077, subjectivity=0.5769230769230769)</code></pre>

<p>How's that?</p>
<p>\[ \text{polarity} = -0.5 \cdot \frac{1}{1.3} \cdot 0.8 \approx -0.31 \]</p>
<p>\[ \text{subjectivity} = \frac{1}{1.3} \cdot 0.75 \approx 0.58 \]</p>
<p>TextBlob will ignore one-letter words in its sentiment phrases, which means things like this will work just the same way:</p>
<pre><code class="language-python">TextBlob("not a very great").sentiment
## Sentiment(polarity=-0.3076923076923077, subjectivity=0.5769230769230769)</code></pre>

<p>And TextBlob will ignore words it doesn't know anything about:</p>
<pre><code class="language-python">TextBlob("not a very great calculation").sentiment
## Sentiment(polarity=-0.3076923076923077, subjectivity=0.5769230769230769)</code></pre>

<p>TextBlob goes along finding words and phrases it can assign polarity and subjectivity to, and it averages them all together for longer text.</p>
<p>And while I'm being a little critical, and such a system of coded rules is in some ways the antithesis of machine learning, it is still a pretty neat system and I think I'd be hard-pressed to code up a better such solution.</p>
<p>Check out <a href="https://github.com/sloria/TextBlob/blob/eb08c120d364e908646731d60b4e4c6c1712ff63/textblob/_text.py">the source</a> yourself to see all the details!</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20150607-textblob_sentiment/</link>
<guid>http://planspace.org/20150607-textblob_sentiment/</guid>
<pubDate>Sun, 07 Jun 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Mergic</title>
<description><![CDATA[

<p><em>A <a href="http://opendatascicon.com/schedule/practical-mergic-how-to-join-anything/">talk</a> for the <a href="http://opendatascicon.com/">Open Data Science Conference</a> in Boston on Saturday May 30, 2015. Including some material originally given in a <a href="/20150520-practical_mergic/">talk</a> for the <a href="http://www.meetup.com/nyhackr/">New York Open Statistical Programming Meetup</a> on <a href="http://www.meetup.com/nyhackr/events/222328498/">Wednesday May 20, 2015</a> and a <a href="/20150514-mergic/">lightning talk</a> at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>.</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<p>I've also done a <a href="/20150520-practical_mergic/">longer version</a> and a <a href="/20150514-mergic/">shorter version</a> of this talk, and those are also on the blog. So if you find yourself wanting more you can get more, and if you want less I can also give you less.</p>
<p>This is probably the cheekiest of the three versions.</p>
<hr>
<p><img width="1000%" alt="Open Data Science Conference title slide for Practical Mergic with Aaron Schumacher" src="img/title_slide.png"></p>
<hr>
<p>Here's an official cover slide.</p>
<hr>
<p><img height="1000%" alt="Metis" src="img/metis.png"></p>
<hr>
<p>I teach at <a href="http://www.thisismetis.com/">Metis</a>, and I'm really happy that we're helping sponsor the Open Data Science Conference. Metis is a data science bootcamp, currently just in New York (sorry Boston). We get the best people, we have a lot of fun, and at the end out come data scientists that you could hire.</p>
<p>I could just talk about Metis, but instead I thought it'd be a good idea to talk about un-sexy everyday problems.</p>
<hr>
<p>problem</p>
<hr>
<p>In general it's just one problem, and if you work with data you have this problem.</p>
<hr>
<p>your data sucks</p>
<hr>
<p>Data is awful! You do not want to swim in a data lake. It's a mess out there!</p>
<p>Of course these days we have open data, which is a huge improvement.</p>
<hr>
<p>our data sucks</p>
<hr>
<p>So maybe we release some data sets, put them on a nice Socrata open data portal...</p>
<hr>
<pre><code class="language-text">   dbn grade year category num_tested
01M019     3 2010   Female         16
01M019     3 2010     Male         20
01M019     3 2010     Male          2</code></pre>

<hr>
<p>Here's an example of open data <a href="http://schools.nyc.gov/NR/exeres/05289E74-2D81-4CC0-81F6-E1143E28F4C4,frameless.htm">released</a> by the New York City Department of Education.</p>
<p>The <code>dbn</code> is a school identifier, and <code>grade</code> and <code>year</code> make sense. This file has the number of students tested in three categories: &#8220;Female&#8221;, &#8220;Male&#8221;, and &#8220;Male&#8221;.</p>
<p>What does that mean?</p>
<p>There are fourteen hundred extra &#8220;Male&#8221; rows like this!</p>
<p>You can look more deeply into this and conclude with some confidence that you can probably drop these extra &#8220;Male&#8221; rows. It's clearly a mistake of some kind.</p>
<hr>
<p><img width="1000%" title="This happens." src="img/this_happens.png"></p>
<hr>
<p><em>Original image from <a href="http://en.wikipedia.org/wiki/Magnolia_%28film%29">Magnolia</a> via <a href="http://indie-outlook.com/2012/09/19/jeremy-blackman-on-magnolia-pta-0s-1s-and-pink-drink/">Indie Outlook</a>.</em></p>
<p>This is a scene from a movie called Magnolia where it's raining frogs. One of the things they say in that movie is that strange things happen, and if you've worked with any variety of data sets, you've probably encountered very strange things indeed. You need to check everything&#8212;including things that you shouldn&#8217;t have to check.</p>
<p>I would also like to offer, before I get to what I'm really going to talk about, that it is a serious problem that I can't easily contribute a fix to that DOE data set. We should expect collaborative checking and editing of data sets to be part of the benefit of open data.</p>
<p>I don't know of any system currently in existence that does this well. We need something in between Wikipedia and GitHub. The <a href="http://dat-data.com/">dat project</a> isn't doing it, at least not yet. I like the architecture of <a href="http://www.datomic.com/">Datomic</a>, but it's not quite right either. <a href="https://www.wikidata.org/">Wikidata</a> might be moving in the right direction, but I'm not sure yet.</p>
<hr>
<p>problems when names are the same</p>
<hr>
<p>How is this relevant to merging? Well, lots of things can go wrong when you have exact duplicates that you aren't expecting.</p>
<hr>
<p>merge</p>
<hr>
<p>One big reason to want nice unique IDs is that you would like to merge two data sets, and you need something to match records by. Let's do a quick refresher on merging, or joining.</p>
<hr>
<p><img height="1000%" title="dplyr cheat sheet joins" src="img/dplyr_joins.png"></p>
<hr>
<p>This is a section from <a href="http://www.rstudio.com/">RStudio</a>'s <a href="http://www.rstudio.com/resources/cheatsheets/">cheatsheet</a> for <a href="https://github.com/hadley/dplyr">dplyr</a>. These cheatsheets are fantastic.</p>
<p>We'll do a merge between two data sets. For simplicity say that they have one column which is an identifier for each row, and some data in other columns. There are a couple ways we can join the data.</p>
<p>Some people like to think about these in terms of Venn diagrams.</p>
<hr>
<p><img width="1000%" title="left join" src="img/left_join.png"></p>
<hr>
<p>This picture comes from <a href="https://twitter.com/codinghorror">Jeff Atwood</a>'s post called <a href="http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/">visual explanation of SQL joins</a>.</p>
<p>A former co-worker told me that seeing these pictures changed his life. I hope you like them.</p>
<p>This is a left join: you get all the keys from the left data set, regardless of whether they're in the right data set.</p>
<hr>
<p><img width="1000%" title="right join" src="img/right_join.png"></p>
<hr>
<p>Jeff Atwood doesn't have a picture of a right join on his blog, but you can make one with a little <a href="http://www.imagemagick.org/">ImageMagick</a>.</p>
<pre><code class="language-bash">$ convert -rotate 180 left_join.png right_join.png</code></pre>

<p>You're welcome.</p>
<hr>
<p><img width="1000%" title="inner join" src="img/inner_join.png"></p>
<hr>
<p>An inner join, or natural join, only gives you results for keys that appear in both the left and right data sets.</p>
<hr>
<p><img width="1000%" title="outer join" src="img/outer_join.png"></p>
<hr>
<p>And an outer join gives you everything. Great!</p>
<p>There are a few other terms we could add, but let's not.</p>
<hr>
<p><img height="1000%" title="dplyr cheat sheet joins" src="img/dplyr_joins.png"></p>
<hr>
<p>Here's the <code>dplyr</code> summary again. You can see how you can introduce missing values when doing left, right, and outer joins.</p>
<p>Ready? Here's a test.</p>
<hr>
<p>How many rows do you get when you outer join two tables?</p>
<hr>
<p>Think about this question, discuss it with somebody near you, come up with everything you can say about the number of rows you might expect when you join two tables. Introduce any quantities you think you'd like to know.</p>
<p>Take about three minutes and then come back.</p>
<p><em>three minutes pass</em></p>
<p>Say there are \( N \) rows in the first table and \( M \) rows in the second table. Then the smallest number of rows we can get from the outer join is the greater of \( N \) and \( M \). But we might get as many as \( N \cdot M \) rows, if all the keys are the same!</p>
<p>If you said the maximum was \( N + M \), you were probably assuming, implicitly or explicitly, that all they keys were unique. This is a common assumption that you should really check.</p>
<hr>
<pre><code class="language-r">&gt; nrow(first)
## [1] 3
&gt; nrow(second)
## [1] 3
&gt; result &lt;- merge(first, second)
&gt; nrow(result)
## [1] 3</code></pre>

<hr>
<p><em>This code is runnable in <a href="count_trouble.R">count_trouble.R</a>.</em></p>
<p>Is it enough to check the numbers of rows when we do joins?</p>
<p>This does an inner join, which is the default for <code>merge</code> in R. (I'm using R here as a personal favor to <a href="https://twitter.com/jaredlander">Jared Lander</a>.)</p>
<p>Think about it.</p>
<hr>
<pre><code class="language-text"> x    y1        x   y2        x    y1   y2
 1 looks        1 good        1 looks good
 2    oh        2  boy        2    oh  boy
 3  well        2   no        2    oh   no</code></pre>

<hr>
<p>There is no peace while you don't have unique IDs.</p>
<p>There are times when you don't want every ID to be unique in a table, but really really often you do. You probably want to check that uniqueness explicitly.</p>
<hr>
<p>problems when names are the same</p>
<hr>
<p>This has been a discussion of problems arising from names being the same.</p>
<hr>
<p>problems when names aren't the same</p>
<hr>
<p>And now, you probably also want to check that the intersection you get is what you expect.</p>
<p>You don't want to silently drop a ton of rows when you merge!</p>
<p>This is the beginning of our problems with names that aren't the same.</p>
<hr>
<p>let's play tennis</p>
<hr>
<p>I downloaded the <a href="https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics">Tennis Major Tournament Match Statistics Data Set</a> from the <a href="https://archive.ics.uci.edu/ml/">University of California, Irvine Machine Learning Repository</a>.</p>
<p>Do you know the process for getting a data set up up on the UCI ML repository?</p>
<p>The process is this: You fill in a form on their web site, and then maybe they put up your data set.</p>
<hr>
<pre><code class="language-bash">$ head -2 AusOpen-women-2013.csv | cut -c 1-40
## Player1,Player2,Round,Result,FNL1,FNL2,F
## Serena Williams,Ashleigh Barty,1,1,2,0,5

$ head -2 USOpen-women-2013.csv | cut -c 1-40
## Player 1,Player 2,ROUND,Result,FNL.1,FNL
## S Williams,V Azarenka,7,1,2,1,57,44,43,2</code></pre>

<hr>
<p>There are eight files like this. You can see that the player names are not perfectly consistent.</p>
<p>Some good people up-state New York contributed this data set, and I got in touch with them. I was curious about where the data came from; it was scraped from some web sites. Did they use it in any of their classes? Yes, it was used for some activities. Did they use it for teaching any particular data cleaning techniques? I haven't heard back since then.</p>
<hr>
<pre><code class="language-bash">$ wc -l names.txt
## 1886

$ sort names.txt | uniq | wc -l
## 669</code></pre>

<hr>
<p>So we'll take the tennis player names as our example data set. We have a file with 1,886 names, 669 of which are unique.</p>
<p>There are too many unique strings&#8212;sometimes more than one string for the same player. As a result, a count of the most common names will not accurately tell us who played the most in these 2013 tennis competitions.</p>
<hr>
<pre><code class="language-bash">$ sort names.txt | uniq -c | sort -nr | head -5
##  21 Rafael Nadal
##  17 Stanislas Wawrinka
##  17 Novak Djokovic
##  17 David Ferrer
##  15 Roger Federer</code></pre>

<hr>
<p>The list above is not the answer we&#8217;re looking for. We want to be correct.</p>
<hr>
<p>single field deduplication</p>
<hr>
<p>We're going to think about this problem, which is pretty common, of de-duplicating (or making a merge table for) a single text field.</p>
<hr>
<pre><code>Lukas Lacko             F Pennetta
Leonardo Mayer          S Williams
Marcos Baghdatis        C Wozniacki
Santiago Giraldo        E Bouchard
Juan Monaco             N.Djokovic
Dmitry Tursunov         S.Giraldo
Dudi Sela               Y-H.Lu
Fabio Fognini           T.Robredo
...                     ...</code></pre>

<hr>
<p>To be clear, the problem looks like this. And the problem often looks like this: You have either two columns with slightly different versions of identifiers, or one long list of things that you need to resolve to common names. These problems are fundamentally the same.</p>
<p>Do you see the match here? (It's Santiago!)</p>
<p>So we need to find the strings that refer to the same person.</p>
<hr>
<p><img width="1000%" title="Open Refine" src="img/open_refine.png"></p>
<hr>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good.</p>
<p>An interesting side story is that Open Refine was formerly Google Refine, and before that <a href="http://en.wikipedia.org/wiki/Metaweb">Metaweb</a>'s &#8220;Freebase Gridworks&#8221;. Google is shutting down <a href="http://www.freebase.com/">Freebase</a>, and we have to hope that <a href="https://www.wikidata.org/">Wikidata</a> will then be the open match for Google's <a href="http://en.wikipedia.org/wiki/Knowledge_Graph">Knowledge Graph</a>.</p>
<p>Open Refine is quite good, but there are several things I wish it did differently.</p>
<p>For one, although it has an internal record of changes, the interface Open Refine exposes changes your data in place. That's not okay.</p>
<hr>
<pre><code class="language-text">Cluster ID,name
1,Lukas Lacko
2,Leonardo Mayer
3,Marcos Baghdatis
...</code></pre>

<hr>
<p>One way to keep output from a deduplication process is a merge table like this. The <code>csvdedupe</code> interface on the Python <a href="https://github.com/datamade/dedupe">dedupe</a> project from <a href="http://datamade.us/">DataMade</a> in Chicago will give you this kind of output. It's much better than no accessible record of the transformation, but it's also not a format I want to look at, as a human.</p>
<hr>
<pre><code class="language-json">{
    "Ann's group": [
        "Ann"
    ],
    "Bob's group": [
        "Bob",
        "Robert"
    ]
}</code></pre>

<hr>
<p>Here's a format that I propose could be usable for describing deduplication. It's as readable by humans as any <a href="http://json.org/">JSON</a>, and it can be easily produced and consumed by anything that speaks JSON, which is everything.</p>
<hr>
<pre><code class="language-text">new,original
Ann's group,Ann
Bob's group,Bob
Bob's group,Robert</code></pre>

<hr>
<p>You can still produce a merge table from the JSON representation, and it will have whatever new keys you specify, possibly human-readable ones.</p>
<hr>
<p><img height="1000%" title="ermahgerd mergic" src="img/ermahgerd.png"></p>
<hr>
<p><code>mergic</code> is a tool that supports doing deduplication with these conventions.</p>
<hr>
<ul>
<li>simple</li>
<li>customizable</li>
<li>reproducible</li>
</ul>
<hr>
<p>The goals of <code>mergic</code> are to be:</p>
<ul>
<li>simple, meaning largely text-based and obvious; the tool disappears</li>
<li>customizable, meaning you can easily use a custom distance function</li>
<li>reproducible, meaning everything you do can be done again automatically</li>
</ul>
<hr>
<p>the tool disappears</p>
<hr>
<p>Good tools disappear.</p>
<p>Whatever text editor you use, the your work product is a text file. You can use any text editor, or use different ones for different purposes, and so on.</p>
<p>Your merge process shouldn't rely on any particular piece of software for its replicability.</p>
<hr>
<p>any distance function</p>
<hr>
<p>Your distance function can make all the difference. You need to be able to plug in any distance function that works well for your data.</p>
<hr>
<p>really reproducible</p>
<hr>
<p>People can see what's happening, and computers can keep doing the process without clicking or re-running particular executables.</p>
<hr>
<p><img width="1000%" title="big data" src="img/big_data.png"></p>
<hr>
<p>A quick disclaimer!</p>
<p>This is John Langford's slide, about what big data is. He says that small data is data for which \( O(n^2) \) algorithms are feasible. Currently <code>mergic</code> is strictly for this kind of "artisanal" data, where we want to ensure that our matching is correct but want to reduce the amount of human work to ensure that. And we are about to get very \( O(n^2) \).</p>
<hr>
<pre><code>Santiago Giraldo,Leonardo Mayer
Santiago Giraldo,Dudi Sela
Santiago Giraldo,Juan Monaco
Santiago Giraldo,S Williams
Santiago Giraldo,C Wozniacki
Santiago Giraldo,S.Giraldo
Santiago Giraldo,Marcos Baghdatis
Santiago Giraldo,Y-H.Lu
...</code></pre>

<hr>
<p>So we make all possible pairs of identifiers!</p>
<p>One of the things that Open Refine gets right is that it doesn't show us humans all the pairs it's looking at.</p>
<p>All these pairs are annoying for a computer, and awful for humans. The computer can calculate a lot of pairwise distances, but I don't want to look at all the pairs.</p>
<p>Do you see the match here? (It's Santiago again!)</p>
<hr>
<pre><code>Karolina Pliskova,K Pliskova</code></pre>

<hr>
<p>Aside from being a drag to look at, there's a bigger problem with verifying equality on a pairwise basis.</p>
<p>Do these two records refer to the same person? (Tennis fans may see where I'm going with this.)</p>
<hr>
<pre><code>Kristyna Pliskova,K Pliskova</code></pre>

<hr>
<p>Karolina has a twin sister, and Kristyna also plays professional tennis! This may well not be obvious if you only look at pairs individually. What matters is the set of names that are transitively judged as equal.</p>
<hr>
<p>sets &gt; pairs</p>
<hr>
<p>Both perceptually and logically, it's better to think in sets than in a bunch of individual pairs.</p>
<hr>
<p>workflow support for reproducible deduplication and merging</p>
<hr>
<p>This is what <code>mergic</code> is for. <code>mergic</code> is a simple tool designed to make it less painful when you need to merge things that don't yet merge.</p>
<hr>
<p>demo: mergic tennis</p>
<hr>
<p>With all that background, let's see how <code>mergic</code> attempts to support a good workflow.</p>
<pre><code class="language-bash">$ pew new odsc</code></pre>

<p>I'll start by making a new <a href="https://virtualenv.pypa.io/">virtual environment</a> using <a href="https://github.com/berdario/pew">pew</a>.</p>
<pre><code class="language-bash">$ pip install mergic</code></pre>

<p><code>mergic</code> is very new (version 0.0.6) and it currently installs with no extra dependencies.</p>
<pre><code class="language-bash">$ mergic -h</code></pre>

<p><code>mergic</code> includes a command-line script based on <a href="https://docs.python.org/2/library/argparse.html">argparse</a> that uses a default string distance function.</p>
<pre><code>usage: mergic [-h] {calc,make,check,diff,apply,table} ...

positional arguments:
  {calc,make,check,diff,apply,table}
    calc                calculate all partitions of data
    make                make a JSON partition from data
    check               check validity of JSON partition
    diff                diff two JSON partitions
    apply               apply a patch to a JSON partition
    table               make merge table from JSON partition

optional arguments:
  -h, --help            show this help message and exit</code></pre>

<p>In the tennis data, names appear sometimes with full first names and sometimes with only first initials. To get good comparisons, we should:</p>
<ul>
<li>Transform all the data to the same format, as nearly as possible.</li>
<li>Use a good distance on the transformed data.</li>
</ul>
<p>We can do both of these things with a simple custom script, <a href="tennis/tennis_mergic.py">tennis_mergic.py</a>. It only <a href="requirements.txt">requires</a> the <code>mergic</code> and <code>python-Levenshtein</code> packages.</p>
<pre><code class="language-python">#!/usr/bin/env python

import re
import Levenshtein
import mergic


def first_initial_last(name):
    initial = re.match("^[A-Z]", name).group()
    last = re.search("(?&lt;=[ .])[A-Z].+$", name).group()
    return "{}. {}".format(initial, last)


def distance(x, y):
    x = first_initial_last(x)
    y = first_initial_last(y)
    return Levenshtein.distance(x, y)


mergic.Blender(distance).script()</code></pre>

<p>Note that there's a transformation step in there, normalizing the form of the names to have just a first initial and last name. This kind of normalization can be very important.</p>
<p>As a more extreme example, a friend of mine has used the following transform: Google it. Then you can use a distance on the result set to deduplicate.</p>
<p>Now <a href="tennis/tennis_mergic.py">tennis_mergic.py</a> can be used just like the standard <code>mergic</code> script.</p>
<pre><code class="language-bash">$ ./tennis_mergic.py calc names.txt
## num groups, max group, num pairs, cutoff
## ----------------------------------------
##        669,         1,         0, -1
##        358,         5,       384, 0
##        348,         6,       414, 1
##        332,         6,       470, 2
##        262,        85,      5117, 3
##        165,       324,     52611, 4
##         86,       496,    122899, 5
##         46,       584,    170287, 6
##         24,       624,    194407, 7
##         16,       641,    205138, 8
##         10,       650,    210940, 9
##          4,       663,    219459, 10
##          2,       668,    222778, 11
##          1,       669,    223446, 12</code></pre>

<p>There is a clear best cutoff here, as the size of the max group jumps from 6 items to 85 and the number of within-group comparisons jumps from 470 to 5,117. So we create a partition where the Levenshtein distance between names in our standard first initial and last name format is no more than two, and put the result in a file called <code>groups.json</code>:</p>
<pre><code class="language-bash">$ ./tennis_mergic.py make names.txt 2 &gt; groups.json</code></pre>

<p>This kind of JSON grouping file could be produced and edited by anything, not just <code>mergic</code>.</p>
<p>As expected, the proposed grouping has combined things over-zealously in some places:</p>
<pre><code class="language-bash">$ head -5 groups.json
## {
##     "Yen-Hsun Lu": [
##         "Di Wu",
##         "Yen-Hsun Lu",
##         "Y-H.Lu",</code></pre>

<p>Manual editing can produce a corrected version of the original grouping, which could be saved as <code>edited.json</code>:</p>
<pre><code class="language-bash">$ head -8 edited.json
## {
##     "Yen-Hsun Lu": [
##         "Yen-Hsun Lu",
##         "Y-H.Lu"
##     ],
##     "Di Wu": [
##         "Di Wu"
##     ],</code></pre>

<p>Parts of the review process would be difficult or impossible for a computer to do accurately.</p>
<p>After editing, you can check that the new grouping is still valid. At this stage we aren't using anything custom any more, so the default <code>mergic</code> is fine:</p>
<pre><code class="language-bash">$ mergic check edited.json
## 669 items in 354 groups</code></pre>

<p>The <code>mergic</code> diffing tools make it easy to make comparisons that would otherwise be difficult, letting us focus on and save only changes that are human reviewers make rather than whole files.</p>
<pre><code class="language-bash">$ mergic diff groups.json edited.json &gt; diff.json</code></pre>

<p>Now <code>diff.json</code> only has the entries that represent changes from the original <code>groups.json</code>.</p>
<p>The edited version can be reconstructed from the original and the diff with <code>mergic apply</code>:</p>
<pre><code class="language-bash">$ mergic apply groups.json diff.json &gt; rebuilt.json</code></pre>

<p>The order of <code>rebuilt.json</code> may not be identical to the original <code>edited.json</code>, but the diff will be empty, meaning the file is equivalent:</p>
<pre><code class="language-bash">$ mergic diff edited.json rebuilt.json
## {}</code></pre>

<p>Finally, to generate a CSV merge table that you'll be able to use with any other tool:</p>
<pre><code class="language-bash">$ mergic table edited.json &gt; merge.csv</code></pre>

<p>Now the file <code>merge.csv</code> has two columns, <code>original</code> and <code>mergic</code>, where <code>original</code> contains all the values that appeared in the original data and <code>mergic</code> contains the deduplicated keys. You can join this on to your original data and go to town.</p>
<p>Here's how we might do that to quickly get a list of who played the most in these 2013 tennis events:</p>
<pre><code class="language-bash">$ join -t, &lt;(sort names.txt) &lt;(sort merge.csv) | cut -d, -f2 | sort | uniq -c | sort -nr | head
##  24 Novak Djokovic
##  22 Rafael Nadal
##  21 Serena Williams
##  21 David Ferrer
##  20 Na Li
##  19 Victoria Azarenka
##  19 Agnieszka Radwanska
##  18 Stanislas Wawrinka
##  17 Tommy Robredo
##  17 Sloane Stephens</code></pre>

<p>Note that this is not the same as the result we got before resolving these name issues:</p>
<pre><code class="language-bash">$ sort names.txt | uniq -c | sort -nr | head
##  21 Rafael Nadal
##  17 Stanislas Wawrinka
##  17 Novak Djokovic
##  17 David Ferrer
##  15 Roger Federer
##  14 Tommy Robredo
##  13 Richard Gasquet
##  11 Victoria Azarenka
##  11 Tomas Berdych
##  11 Serena Williams</code></pre>

<p>As it happens, using a cutoff of 0 and doing no hand editing will still give the correct top ten. In general the desired result and desired level of certainty in its correctness will inform the level of effort that is justified.</p>
<hr>
<p>distance matters</p>
<hr>
<p>Having a good distance function might be the most important thing. It's hard to imagine a machine learning as good a distance function as you could just right based on your human intelligence.</p>
<p>There is work on learnable edit distances; notably there's a current Python project to implement hidden alignment conditional random fields for classifying string pairs: <a href="https://github.com/dirko/pyhacrf">pyhacrf</a>. Python dedupe is <a href="https://github.com/datamade/dedupe/issues/14">eager</a> to incorporate this.</p>
<p>See also: <a href="http://www.cs.utexas.edu/users/ml/papers/marlin-kdd-03.pdf">Adaptive Duplicate Detection Using Learnable String Similarity Measures</a></p>
<hr>
<p>extension to multiple fields</p>
<hr>
<p>We've been talking about single field deduplication.</p>
<hr>
<pre><code class="language-text">name
----
Bob
Rob
Robert</code></pre>

<hr>
<p>This means that we have one field, say name.</p>
<hr>
<pre><code class="language-text">name, name
----------
Bob, Bobby
Bob, Robert
Bobby, Robert</code></pre>

<hr>
<p>And we look at all the possible pairs and calculate those pairwise distances.</p>
<hr>
<p><img width="1000%" title="one dimensional" src="img/one_dimensional.png"></p>
<hr>
<p>While we described it even in Open Refine as &#8220;clustering&#8221;, we've really been doing a classification task: either a pair is in the same group or they aren't. We've had one dimension, and we hope that we can just divide true connections from different items with a simple cutoff.</p>
<hr>
<pre><code class="language-text">name,    hometown
-----------------
Bob,     New York
Rob,     NYC
Robert,  "NY, NY"</code></pre>

<hr>
<p>Often, there's more than one field involved, and it might be good to treat all the fields separately.</p>
<p>So let's calculate distances between each field entry for each pair of rows, in this case.</p>
<hr>
<p><img height="1000%" title="two dimensional" src="img/two_dimensional.png"></p>
<hr>
<p>The reason it might be good to treat the fields separately is that they might be more useful together; we might be able to classify all the true duplicates using the information from both fields.</p>
<p>Maybe you can find two clusters, one for true duplicates and one for different items.</p>
<p>Or maybe you could get some training data and use whatever classification algorithm you like.</p>
<p>The Python <a href="https://github.com/datamade/dedupe">dedupe</a> project and the R <a href="">RecordLinkage</a> package do this.</p>
<p>The <code>csvdedupe</code> interface using <code>dedupe</code> even asks us to clarify things, doing a live training phase. The hope is that it will learn what matters.</p>
<p>You can also build this kind of behavior into your own systems; there is an <a href="http://datamade.github.io/dedupe-examples/docs/csv_example.html">example</a>.</p>
<hr>
<p>real clustering?</p>
<hr>
<p>The &#8220;clustering&#8221; that we've been doing hasn't been much like usual clustering.</p>
<hr>
<p><img width="1000%" title="one dimensional" src="img/one_dimensional.png"></p>
<hr>
<p>In part, this is because we we've only had distances between strings without having a real &#8220;string space&#8221;.</p>
<p>I'm going to just sketch out this direction; I think it's interesting but I haven't seen any real results in it yet.</p>
<hr>
<p>dog, doge, kitten, kitteh</p>
<hr>
<p>Say these are the items we're working with. We can use Levenshtein edit distance to make a distance matrix.</p>
<hr>
<pre><code class="language-text">       dog doge kitten kitteh
   dog   0    1      6      6
  doge   1    0      5      5
kitten   6    5      0      1
kitteh   6    5      1      0</code></pre>

<hr>
<p>So here's a distance matrix, and it looks the way we'd expect. But we still don't have <em>coordinates</em> for our words.</p>
<p>Luckily, there is at least one technique for coming up with coordinates when you have a distance matrix. Let's use <a href="http://en.wikipedia.org/wiki/Multidimensional_scaling">multidimensional scaling</a>! There's a nice <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html">implementation in sklearn</a>.</p>
<hr>
<pre><code class="language-python">from sklearn.manifold import MDS
mds = MDS(dissimilarity='precomputed')
coords = mds.fit_transform(distances)</code></pre>

<hr>
<p>Here's all the code it takes.</p>
<hr>
<p><img width="1000%" title="MDS coordinates from distance matrix" src="img/mds_words.png"></p>
<hr>
<p>And here's the result! This is at least a fun visualization, and I wonder if doing clustering in a space like this might sometimes lead to better results.</p>
<p>The key thing here is that we're clustering on the elements themselves, rather than indirectly via the pairwise distances.</p>
<p>There are other ways of getting coordinates for words. This includes the very interesting <a href="https://code.google.com/p/word2vec/">word2vec</a> and related techniques.</p>
<p>Also, you might have items are already naturally coordinates, for example if you have medical data like a person's height or weight.</p>
<hr>
<p>open</p>
<hr>
<p>This is the Open Data Science Conference, right?</p>
<p>This mergic project is something I think could be a good direction for particular kinds of data problems. Of course it's <a href="https://github.com/ajschumacher/mergic">on GitHub</a> and of course I'd love help in making it something that might actually help people.</p>
<p>Our community is bigger than any one problem or any one solution, and I want to hear about what problems are real for you and what solutions you're excited about.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thanks for listening&#8212;let's talk!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20150530-practical_mergic_at_odsc/</link>
<guid>http://planspace.org/20150530-practical_mergic_at_odsc/</guid>
<pubDate>Sat, 30 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>TF-IDF is about what matters</title>
<description><![CDATA[

<p>TF-IDF is <em>term frequency inverse document frequency</em>. But what does that mean?</p>
<p><img alt="cat and dog" src="cat_and_dog.png"></p>
<!-- Image from Wikimedia Commons: http://commons.wikimedia.org/wiki/File:Cat_and_dog.JPG -->

<p>Consider a collection (or corpus, or set) of four sentences (or documents, or strings) made up of words (or terms, or tokens):</p>
<ol>
<li>the cat and dog sat</li>
<li>the dog and cat sat</li>
<li>the cat sat and sat</li>
<li>the cat killed the dog</li>
</ol>
<p>One thing we can do is transform this corpus to a bag of words representation, which just means we&#8217;re keeping track of what words there are but not what order they&#8217;re in. We could use binary values to represent whether a word appears or not:</p>
<pre><code class="language-text">    the    cat    and    dog    sat   killed
1     1      1      1      1      1        0
2     1      1      1      1      1        0
3     1      1      1      0      1        0
4     1      1      0      1      0        1</code></pre>

<p>In this representation we've lost word order, but we've also lost <em>term frequency</em>: we can't tell that sentence three has twice as much sitting as sentence two. Let's use the counts:</p>
<pre><code class="language-text">    the    cat    and    dog    sat   killed
1     1      1      1      1      1        0
2     1      1      1      1      1        0
3     1      1      1      0      2        0
4     2      1      0      1      0        1</code></pre>

<p>Now we can see how many times each term appears in each document.</p>
<p>Is it good that &#8220;killed&#8221; is given the same score as &#8220;cat&#8221; in sentence four? The word &#8220;killed&#8221; is much rarer, after all; it is more &#8220;special&#8221; to sentence four. Let&#8217;s count the number of sentences that have each word, and call this the <em>document frequency</em>:</p>
<pre><code class="language-text">    the    cat    and    dog    sat  killed
      4      4      3      3      3       1</code></pre>

<p>The document frequencies are high for words that are boring, so dividing by the document frequencies will give us low scores for boring words and high scores for interesting words:</p>
<pre><code class="language-text">    the    cat    and   dog     sat  killed
1  0.25   0.25   0.33   0.33   0.33       0
2  0.25   0.25   0.33   0.33   0.33       0
3  0.25   0.25   0.33      0   0.67       0
4  0.50   0.25      0   0.33      0    1.00</code></pre>

<p>These scores match the intuition that &#8220;killed&#8221; is a very important word. In addition to making some intuitive sense, scores like these tend to work well in machine learning tasks.</p>
<p>All we did was this:</p>
<p>\[ \frac{\text{number of times the word is in this document}}{\text{number of documents the word is in}} \]</p>
<p>Or, for short:</p>
<p>\[ \frac{\text{term frequency}}{\text{document frequency}} \]</p>
<p>Dividing by a number is the same as multiplying by its inverse, \( \frac{1}{\text{a number}} \), so we can call this transformation <em>term frequency inverse document frequency</em>.</p>
<p>This is not a very difficult idea. You could have invented it yourself. But it gets so buried in terminology and secondary implementation details that people sometimes talk about it as if it were a deep and mysterious modeling technique. (It's not.)</p>
<p>Here's an <a href="http://www.cs.utexas.edu/~ml/papers/marlin-dissertation-06.pdf">example</a> of how the exposition can lose people. It starts nicely:</p>
<blockquote>
<p>&#8220;Tokens that occur frequently in a given string should have higher contribution to similarity than those that occur few times, as should those tokens that are rare among the set of strings under consideration.&#8221;</p>
</blockquote>
<p>Who could disagree? Then it jumps to this:</p>
<blockquote>
<p>&#8220;The Term Frequency-Inverse Document Frequency (TF-IDF) weighting scheme achieves this by associating a weight \( w_{v_i,s} = \frac{N(v_i,s)}{\text{max}_{v_j \in s} N(v_j,s)} \cdot \text{log} \frac{N}{N(v_i)} \) with every token \( v_i \) from string \( s \), where \( N(v_i, s) \) is the number of times \( v_i \) occurs in \( s \) (term frequency), \( N \) is the number of strings in the overall corpus under consideration, and \( N(v_i) \) is the number of strings in the corpus that include \( v_i \) (document frequency).&#8221;</p>
</blockquote>
<p>There's some notation there, and we've also thrown in two normalizations and a log. None of this is a particularly big deal, but it makes TF-IDF seem complicated. There are <a href="http://en.wikipedia.org/wiki/Tf%E2%80%93idf">a lot of ways</a> you could do the particulars of TF-IDF but they all achieve essentially what we did above by just counting and dividing.</p>
<p>It can also be confusing when results don't match what you expect. Here is what you get by default (with <a href="tfidf.py">this script</a>) from <a href="http://scikit-learn.org/">sklearn</a>'s <a href="http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer</a>:</p>
<pre><code class="language-text">    the    cat    and    dog    sat   killed
1  0.39   0.39   0.48   0.48   0.48        0
2  0.39   0.39   0.48   0.48   0.48        0
3  0.32   0.32   0.40      0   0.79        0
4  0.63   0.31      0   0.38      0     0.60</code></pre>

<p>The formula <code>sklearn</code> version 0.16.1 uses to compute TF-IDF is this:</p>
<p>\[ \text{term frequency} \cdot \left ( \text{log} \left ( \frac{\text{number of documents} + 1}{\text{document frequency} +1} \right ) + 1 \right ) \]</p>
<p>And then <code>sklearn</code> also scales the rows of the results to each have unit length.</p>
<p>So you could be surprised that the results of a TF-IDF implementation don't exactly match the method you were thinking of. As above, there are a lot of small choices in exactly how to calculate TF-IDF, and <code>sklearn</code> chooses one particular implementation.</p>
<hr>
<p>I've seen better classification performance with TF-IDF than with raw counts, but I haven't seen a systematic comparison across various TF-IDF methods; it might be interesting to see such a comparison.</p>
<hr>
<p>Thanks to <a href="https://twitter.com/hallr">Rob Hall</a> for doing <a href="https://github.com/ga-students/DAT_SF_13/tree/master/lectures#session-14-natural-language-processing">work</a> that helped inspire this post!</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20150524-tfidf_is_about_what_matters/</link>
<guid>http://planspace.org/20150524-tfidf_is_about_what_matters/</guid>
<pubDate>Sun, 24 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Chains</title>
<description><![CDATA[

<blockquote>
<p>&#8220;We were arguing energetically about whether the world is actually evolving, headed in a particular direction, or whether the entire universe is just a returning rhythm's game, a renewal of eternity.&#8221;</p>
</blockquote>
<p>There&#8217;s this little <a href="https://djjr-courses.wdfiles.com/local--files/soc180%3Akarinthy-chain-links/Karinthy-Chain-Links_1929.pdf">story</a>, published in 1929, by <a href="http://en.wikipedia.org/wiki/Frigyes_Karinthy">Frigyes Karinthy</a>. <a href="http://www.qwantz.com/">Dinosaur Comics</a> put me on to this:</p>
<p><a href="http://www.qwantz.com/index.php?comic=2646"><img alt="dinosaur comics #2646" src="http://www.qwantz.com/comics/comic2-2656.png"></a></p>
<blockquote>
<p>&#8220;Everything returns and renews itself. The difference now is that the rate of these returns has increased, in both space and time, in an unheard-of fashion. Now my thoughts can circle the globe in minutes. Entire passages of world history are played out in a couple of years.&#8221;</p>
</blockquote>
<p>Published in 1929.</p>    
    ]]></description>
<link>http://planspace.org/20150522-chains/</link>
<guid>http://planspace.org/20150522-chains/</guid>
<pubDate>Fri, 22 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Mergic</title>
<description><![CDATA[

<p><em>A talk for the <a href="http://www.meetup.com/nyhackr/">New York Open Statistical Programming Meetup</a> on <a href="http://www.meetup.com/nyhackr/events/222328498/">Wednesday May 20, 2015</a>. Including some material originally given in a <a href="/20150514-mergic/">lightning talk</a> at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>.</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img width="1000%" title="Tweed Courthouse" src="img/tweed.jpg"></p>
<hr>
<p><em><a href="http://commons.wikimedia.org/wiki/File:Tweed_Courthouse_north_main_facade_118443pv.jpg">Image from Wikimedia Commons</a>.</em></p>
<p>The first time I wrote software to address this problem, I was working for the New York City Department of Education, in Tweed Courthouse.</p>
<p>The NYC DOE had unique IDs for every student, and for every teacher, but did not have unique IDs for principals. This was the case at least for some of the data system at that time. And this made sense, because there are only about sixteen hundred New York City public schools.</p>
<p>Those of you who have experience with humans will know that names are not unique IDs. People change their names, or add titles like &#8220;PhD&#8221;, or have their names entered differently at different times for no good reason. In the case of principals, sometimes they switch schools and change their names <em>at the same time</em>.</p>
<p>The DOE makes some decisions based on data, God bless them. The data associated with a principal might determine whether they get a bonus or an unpleasant phone call. In a situation like this, an approximate matching solution is not acceptable.</p>
<p>I had to get a perfect matching of all these principals' names, so I wrote some code to help speed the process. I had to verify the match by pairs, and it was awful, and I wished there was a better way to do it.</p>
<p>My primary concerns were, and are, to speed up the de-duplication process while allowing corrections by hand&#8212;still being reproducible and easily auditable by humans.</p>
<p>Since then, I've developed a process, or workflow, that I think is pretty good, and I've written a new tool to help with this process.</p>
<p>I've also learned about some of the broader ecosystem of techniques and tools available, and I'll talk about these as well.</p>
<p>I'll finish by suggesting that we probably need to do something entirely different. I'd like to encourage a discussion that could lead to more and better work with data.</p>
<hr>
<blockquote>
<p>&#8220;There are only two hard things in computer science: cache invalidation and naming things.&#8221;</p>
</blockquote>
<hr>
<p><a href="http://www.meerkat.com/karlton/">Phil Karlton</a> <a href="http://martinfowler.com/bliki/TwoHardThings.html">said</a> that &#8220;There are only two hard things in computer science: cache invalidation and naming things.&#8221;</p>
<p>When working with data (let's call it &#8220;data science&#8221; then, instead of &#8220;computer science&#8221;) you have problems not only with your own names, but also with everybody else's names.</p>
<p>It's just semantics, I suppose.</p>
<hr>
<p>What are we talking about?</p>
<hr>
<p>&#51060;&#47492;&#51060;&#46976; &#44163;&#51008; &#51221;&#47568; &#51473;&#50836;&#54633;&#45768;&#45796;. &#50696;&#47484; &#46308;&#47732;, &#48120;&#44397;&#50640;&#49436; &#54620;&#44397;&#47568;&#47196;...</p>
<p>My Korean isn't that good. What I'm trying to say here is that agreeing on names is important, and the issue is a big one.</p>
<p>Maintaining a practical focus, let's look at just two classes of problems:</p>
<hr>
<p>when names are the same</p>
<hr>
<p>Lots of things can go wrong when names are re-used when they shouldn't be.</p>
<hr>
<p>when names aren't the same</p>
<hr>
<p>It can be even worse when names for the same thing are <em>not</em> the same.</p>
<p>Both these problems are closely related to merging, and we'll think a lot about that context.</p>
<p>But first, an example illustrating the importance of checking that your names are unique (that is, not ever the same).</p>
<hr>
<p>demo: DOE data</p>
<hr>
<p>The <a href="doe/SchoolMathResults20062012Public.xlsx">Excel file here</a> was <a href="http://schools.nyc.gov/NR/rdonlyres/A77DF9C5-BD62-4171-9995-4EB41E7E4067/0/SchoolMathResults20062012Public.xlsx">downloaded</a> from the <a href="http://schools.nyc.gov/NR/exeres/05289E74-2D81-4CC0-81F6-E1143E28F4C4,frameless.htm">NYC DOE site</a>. It contains standardized test results for New York City schools for individual grades and other sub-groups. We'll use two sheets that have been saved as CSV (originally for my <a href="http://planspace.org/2014/01/07/clean-data-with-r/">Clean Data with R</a> talk), <code>gender.csv</code> and <code>all.csv</code>. The R script itself is in <a href="doe/check_unique.R">check_unique.R</a>.</p>
<p>There's a little setup to make reading in the data easier.</p>
<pre><code class="language-r">library("dplyr")

read.doe &lt;- function(filename) {
  data &lt;- read.csv(filename, as.is=TRUE,
                   skip=6, check.names=FALSE,
                   na.strings="s")
  stopifnot(names(data) == c("DBN", "Grade", "Year", "Category",
                             "Number Tested","Mean Scale Score",
                             "#","%","#","%","#","%","#","%","#","%"))
  names(data) &lt;- c("dbn", "grade", "year", "category",
                   "num_tested", "mean_score",
                   "num1", "per1", "num2", "per2", "num3", "per3",
                   "num4", "per4", "num34", "per34")
 return(tbl_df(data))
}</code></pre>

<p>Now we can start looking at the data, using <a href="https://github.com/hadley/dplyr">dplyr</a>:</p>
<pre><code class="language-r">gender &lt;- read.doe("gender.csv")

gender
## Source: local data frame [68,028 x 16]
##
##       dbn grade year category num_tested mean_score num1 per1 num2 per2
## 1  01M015     3 2006   Female         23        675    0  0.0    7 30.4
## 2  01M015     3 2006     Male         16        657    2 12.5    4 25.0
## 3  01M015     3 2007   Female         11        679    2 18.2    0  0.0
## 4  01M015     3 2007     Male         20        668    0  0.0    3 15.0
## 5  01M015     3 2008   Female         17        661    0  0.0    5 29.4
## 6  01M015     3 2008     Male         20        674    0  0.0    1  5.0
## 7  01M015     3 2009   Female         13        667    0  0.0    1  7.7
## 8  01M015     3 2009     Male         20        668    0  0.0    3 15.0
## 9  01M015     3 2010   Female         13        681    2 15.4    7 53.8
## 10 01M015     3 2010     Male         13        673    4 30.8    5 38.5
## ..    ...   ...  ...      ...        ...        ...  ...  ...  ...  ...
## Variables not shown: num3 (int), per3 (dbl), num4 (int), per4 (dbl), num34
##   (int), per34 (dbl)</code></pre>

<p>What's our unique key for this data set? It looks like it should be <code>dbn</code> (a unique identifier for a school), <code>grade</code>, <code>year</code>, and <code>category</code>. Let's check. The following should be zero if there are no duplicates.</p>
<pre><code class="language-r">gender %&gt;%
  select(dbn, grade, year, category) %&gt;%
  duplicated %&gt;%
  sum
## [1] 1421</code></pre>

<p>Shocking! Seeing that there are duplicates doesn't yet tell us how the duplicates are distributed; is it 1,422 copies of the same combination, or something else?</p>
<pre><code class="language-r">gender %&gt;%
  group_by(dbn, grade, year, category) %&gt;%
  summarize(n=n()) %&gt;%
  group_by(n) %&gt;%
  summarize(count=n())
## Source: local data frame [2 x 2]
##
##   n count
## 1 1 65186
## 2 2  1421</code></pre>

<p>Much like using <code>table</code>, now we can see that most key combinations appear just once, but 1,421 appear twice. Interesting! Let's look at them.</p>
<pre><code class="language-r">gender %&gt;%
  group_by(dbn, grade, year, category) %&gt;%
  filter(1 &lt; n())
## Source: local data frame [2,842 x 16]
## Groups: dbn, grade, year, category
##
##       dbn      grade year category num_tested mean_score num1 per1 num2
## 1  01M019          3 2010     Male         20        677    3   15    7
## 2  01M019          3 2010     Male          2         NA   NA   NA   NA
## 3  01M019          4 2010     Male         20        674    1    5    9
## 4  01M019          4 2010     Male          1         NA   NA   NA   NA
## 5  01M019          5 2010     Male          1         NA   NA   NA   NA
## 6  01M019          5 2010     Male         17        688    0    0    3
## 7  01M019 All Grades 2010     Male          4         NA   NA   NA   NA
## 8  01M019 All Grades 2010     Male         57         NA    4    7   19
## 9  01M020          3 2010     Male         50        686    8   16   15
## 10 01M020          3 2010     Male          1         NA   NA   NA   NA
## ..    ...        ...  ...      ...        ...        ...  ...  ...  ...
## Variables not shown: per2 (dbl), num3 (int), per3 (dbl), num4 (int), per4
##   (dbl), num34 (int), per34 (dbl)</code></pre>

<p>Looks like there are two different kinds of males! How strange! Can we see what's going on by looking at the rest of the file?</p>
<pre><code class="language-r">gender %&gt;%
  filter(dbn=='01M019', year==2010, grade==3)
## Source: local data frame [3 x 16]
##
##      dbn grade year category num_tested mean_score num1 per1 num2 per2
## 1 01M019     3 2010   Female         16        687    0    0    9 56.3
## 2 01M019     3 2010     Male         20        677    3   15    7 35.0
## 3 01M019     3 2010     Male          2         NA   NA   NA   NA   NA
## Variables not shown: num3 (int), per3 (dbl), num4 (int), per4 (dbl), num34
##   (int), per34 (dbl)</code></pre>

<p>Unfortunately not; we'll have to look at additional data to try to determine what's going on. (This is typical.)</p>
<pre><code class="language-r">all_students &lt;- read.doe("all.csv")
data &lt;- bind_rows(all_students, gender)

data %&gt;%
  filter(dbn=='01M019', year==2010, grade==3)
## Source: local data frame [4 x 16]
##
##      dbn grade year     category num_tested mean_score num1 per1 num2 per2
## 1 01M019     3 2010 All Students         36        682    3  8.3   16 44.4
## 2 01M019     3 2010       Female         16        687    0  0.0    9 56.3
## 3 01M019     3 2010         Male         20        677    3 15.0    7 35.0
## 4 01M019     3 2010         Male          2         NA   NA   NA   NA   NA
## Variables not shown: num3 (int), per3 (dbl), num4 (int), per4 (dbl), num34
##   (int), per34 (dbl)</code></pre>

<p>It looks like these extra males aren't being counted in the total for &#8220;All Students&#8221;, so maybe we can drop them. Or maybe the &#8220;All Students&#8221; total is wrong.</p>
<hr>
<p><img width="1000%" title="This happens." src="img/this_happens.png"></p>
<hr>
<p><em>Original image from <a href="http://en.wikipedia.org/wiki/Magnolia_%28film%29">Magnolia</a> via <a href="http://indie-outlook.com/2012/09/19/jeremy-blackman-on-magnolia-pta-0s-1s-and-pink-drink/">Indie Outlook</a>.</em></p>
<blockquote>
<p>&#8220;This happens. This is a thing that happens.&#8221;</p>
</blockquote>
<p>This is a scene from a movie called Magnolia when it's raining frogs. One of the things they say in that movie is that strange things happen, and if you've worked with any variety of data sets, you've probably encountered very strange things. You need to check everything&#8212;including things that you shouldn&#8217;t have to check.</p>
<p>(One good way to check is to use <a href="https://twitter.com/tonyfischetti">Tony</a>'s <a href="http://www.onthelambda.com/wp-content/uploads/2015/03/assertr.html">Assertive R</a> package!)</p>
<hr>
<p>merge</p>
<hr>
<p>One big reason to want nice unique IDs is that you would like to merge two data sets, and you need something to match records by. Let's do a quick refresher on merging, or joining.</p>
<hr>
<p><img height="1000%" title="dplyr cheat sheet joins" src="img/dplyr_joins.png"></p>
<hr>
<p>This is a section from <a href="http://www.rstudio.com/">RStudio</a>'s <a href="http://www.rstudio.com/resources/cheatsheets/">cheatsheet</a> for <a href="https://github.com/hadley/dplyr">dplyr</a>. These cheatsheets are fantastic.</p>
<p>We'll do a merge between two data sets. For simplicity say that they have one column which is an identifier for each row, and some data in other columns. There are a couple ways we can join the data.</p>
<p>Some people like to think about these in terms of Venn diagrams.</p>
<hr>
<p><img width="1000%" title="left join" src="img/left_join.png"></p>
<hr>
<p>This picture comes from <a href="https://twitter.com/codinghorror">Jeff Atwood</a>'s post called <a href="http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/">visual explanation of SQL joins</a>.</p>
<p>A former co-worker told me that seeing these pictures changed his life. I hope you like them.</p>
<p>This is a left join: you get all the keys from the left data set, regardless of whether they're in the right data set.</p>
<hr>
<p><img width="1000%" title="right join" src="img/right_join.png"></p>
<hr>
<p>Jeff Atwood doesn't have a picture of a right join on his blog, but you can make one with a little <a href="http://www.imagemagick.org/">ImageMagick</a>.</p>
<pre><code class="language-bash">$ convert -rotate 180 left_join.png right_join.png</code></pre>

<p>You're welcome.</p>
<hr>
<p><img width="1000%" title="inner join" src="img/inner_join.png"></p>
<hr>
<p>An inner join, or natural join, only gives you results for keys that appear in both the left and right data sets.</p>
<hr>
<p><img width="1000%" title="outer join" src="img/outer_join.png"></p>
<hr>
<p>And an outer join gives you everything. Great!</p>
<p>There are a few other terms we could add, but let's not.</p>
<hr>
<p><img height="1000%" title="dplyr cheat sheet joins" src="img/dplyr_joins.png"></p>
<hr>
<p>Here's the <code>dplyr</code> summary again. You can see how you can introduce missing values when doing left, right, and outer joins.</p>
<p>Ready? Here's a test.</p>
<hr>
<p>How many rows do you get when you outer join two tables?</p>
<hr>
<p>Think about this question, discuss it with somebody near you, come up with everything you can say about the number of rows you might expect when you join two tables. Introduce any quantities you think you'd like to know.</p>
<p>Take about three minutes and then come back.</p>
<p><em>three minutes pass</em></p>
<p>Say there are \( N \) rows in the first table and \( M \) rows in the second table. Then the smallest number of rows we can get from the outer join is the greater of \( N \) and \( M \). But we might get as many as \( N \cdot M \) rows, if all the keys are the same!</p>
<p>If you said the maximum was \( N + M \), you were probably assuming, implicitly or explicitly, that all they keys were unique. This is a common assumption that you should really check.</p>
<hr>
<pre><code class="language-r">&gt; nrow(first)
## [1] 3
&gt; nrow(second)
## [1] 3
&gt; result &lt;- merge(first, second)
&gt; nrow(result)
## [1] 3</code></pre>

<hr>
<p><em>This code is runnable in <a href="count_trouble.R">count_trouble.R</a>.</em></p>
<p>Is it enough to check the numbers of rows when we do joins?</p>
<p>This does an inner join, which is the default for <code>merge</code> in R.</p>
<p>Think about it.</p>
<hr>
<pre><code class="language-text"> x    y1        x   y2        x    y1   y2
 1 looks        1 good        1 looks good
 2    oh        2  boy        2    oh  boy
 3  well        2   no        2    oh   no</code></pre>

<hr>
<p>There is no peace while you don't have unique IDs.</p>
<p>There are times when you don't want every ID to be unique in a table, but really really often you do. You probably want to check that uniqueness explicitly.</p>
<hr>
<p>when names are the same</p>
<hr>
<p>This has been a discussion of problems arising from names being the same.</p>
<hr>
<p>when names aren't the same</p>
<hr>
<p>And now, you probably also want to check that the intersection you get is what you expect.</p>
<p>You don't want to silently drop a ton of rows when you merge!</p>
<p>This is the beginning of our problems with names that aren't the same.</p>
<hr>
<p>demo: let's play tennis</p>
<hr>
<p><em>Here begins material also present in <a href="https://github.com/ajschumacher/mergic/tree/master/tennis">ajschumacher/mergic:tennis</a>.</em></p>
<p>Download the <a href="https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics">Tennis Major Tournament Match Statistics Data Set</a> from the <a href="https://archive.ics.uci.edu/ml/">UC Irvine Machine Learning Repository</a> into an empty directory:</p>
<pre><code class="language-bash">$ wget https://archive.ics.uci.edu/ml/machine-learning-databases/00300/Tennis-Major-Tournaments-Match-Statistics.zip</code></pre>

<p>This file should be stable, but it's also included <a href="tennis/Tennis-Major-Tournaments-Match-Statistics.zip">here</a> and/or you can verify that its <code>md5</code> is <code>e9238389e4de42ecf2daf425532ce230</code>.</p>
<p>Unpack eight CSV files from the <code>Tennis-Major-Tournaments-Match-Statistics.zip</code>:</p>
<pre><code class="language-bash">$ unzip Tennis-Major-Tournaments-Match-Statistics.zip</code></pre>

<p>You should see that the first two columns of each file contain player names, though the column names are not consistent. For example:</p>
<pre><code class="language-bash">$ head -2 AusOpen-women-2013.csv | cut -c 1-40
## Player1,Player2,Round,Result,FNL1,FNL2,F
## Serena Williams,Ashleigh Barty,1,1,2,0,5

$ head -2 USOpen-women-2013.csv | cut -c 1-40
## Player 1,Player 2,ROUND,Result,FNL.1,FNL
## S Williams,V Azarenka,7,1,2,1,57,44,43,2</code></pre>

<p>Make a <code>names.txt</code> with all the names that appear:</p>
<pre><code class="language-bash">$ for filename in *2013.csv
do
    for field in 1 2
    do
        tail +2 $filename | cut -d, -f$field &gt;&gt; names.txt
    done
done</code></pre>

<p>Now you have a file with 1,886 lines, each one of 669 unique strings, as you can verify:</p>
<pre><code class="language-bash">$ wc -l names.txt
## 1886

$ sort names.txt | uniq | wc -l
## 669</code></pre>

<p>There are too many unique strings&#8212;sometimes more than one string for the same player. As a result, a count of the most common names will not accurately tell us who played the most in these 2013 tennis competitions.</p>
<pre><code class="language-bash">$ sort names.txt | uniq -c | sort -nr | head
##  21 Rafael Nadal
##  17 Stanislas Wawrinka
##  17 Novak Djokovic
##  17 David Ferrer
##  15 Roger Federer
##  14 Tommy Robredo
##  13 Richard Gasquet
##  11 Victoria Azarenka
##  11 Tomas Berdych
##  11 Serena Williams</code></pre>

<p>The list above is not the answer we&#8217;re looking for. We want to be correct.</p>
<hr>
<p>single field deduplication</p>
<hr>
<p>We're going to think about this problem, which is pretty common, of de-duplicating (or making a merge table for) a single text field.</p>
<hr>
<pre><code>Lukas Lacko             F Pennetta
Leonardo Mayer          S Williams
Marcos Baghdatis        C Wozniacki
Santiago Giraldo        E Bouchard
Juan Monaco             N.Djokovic
Dmitry Tursunov         S.Giraldo
Dudi Sela               Y-H.Lu
Fabio Fognini           T.Robredo
...                     ...</code></pre>

<hr>
<p><em>Here begins material also presented in a <a href="/20150514-mergic/">lightning talk</a> at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>.</em></p>
<p>To be clear, the problem looks like this. And the problem often looks like this: You have either two columns with slightly different versions of identifiers, or one long list of things that you need to resolve to common names. These problems are fundamentally the same.</p>
<p>Do you see the match here? (It's Santiago!)</p>
<p>So we need to find the strings that refer to the same person.</p>
<hr>
<p>demo: Open Refine</p>
<hr>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good.</p>
<p>An interesting side story is that Open Refine was formerly Google Refine, and before that <a href="http://en.wikipedia.org/wiki/Metaweb">Metaweb</a>'s &#8220;Freebase Gridworks&#8221;. Google is shutting down <a href="http://www.freebase.com/">Freebase</a>, and we have to hope that <a href="https://www.wikidata.org/">Wikidata</a> will then be the open match for Google's <a href="http://en.wikipedia.org/wiki/Knowledge_Graph">Knowledge Graph</a>.</p>
<p>Thanks to <a href="https://twitter.com/jqnatividad">Joel Natividad</a> for pointing out an <a href="https://github.com/OpenRefine/OpenRefine/issues/983">interesting algorithmic development</a> connected with ongoing work on Open Refine. He also pointed out that there is a Python module called <a href="https://github.com/PaulMakepeace/refine-client-py">refine-client</a> for using Open Refine from Python.</p>
<p>Steps of simple Open Refine demo:</p>
<ul>
<li>Start the Open Refine app</li>
<li>Browse to <a href="http://localhost:3333/">http://localhost:3333/</a></li>
<li>Click &#8220;Create Project&#8221;</li>
<li>Click &#8220;Choose Files&#8221;</li>
<li>Select <code>names.txt</code></li>
<li>Click &#8220;Next &#187;&#8220;</li>
<li>Click &#8220;Create Project &#187;&#8221;</li>
<li>Click the down arrow next to &#8220;Column 1&#8221;, then follow &#8220;Edit cells&#8221; to &#8220;Cluster and edit&#8230;&#8221;</li>
</ul>
<p><img width="1000%" title="Open Refine" src="img/open_refine.png"></p>
<p>Open Refine has <a href="https://github.com/OpenRefine/OpenRefine/wiki/Clustering">introductory</a> and <a href="https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth">in-depth</a> documentation about their &#8220;clustering&#8221; mechanisms.</p>
<p>In this interface, we can use &#8220;key collision&#8221; or &#8220;nearest neighbor&#8221; methods.</p>
<p>The &#8220;key collision&#8221; method maps every value to one &#8220;key&#8221;, and items are identical if they have the same key. This allows us to avoid calculating anything for all pairs. (This should remind you of hashing.)</p>
<p>There are four &#8220;keying functions&#8221; in Open Refine:</p>
<ul>
<li>&#8220;fingerprint&#8221; standardizes a string by case and punctuation.</li>
<li>&#8220;ngram-fingerprint&#8221; standardizes a bit further, using character ngrams.</li>
<li>&#8220;metaphone3&#8221; standardizes by the phonetic <a href="http://www.amorphics.com/">Metaphone 3</a> algorithm so that things that sound the same in English should be keyed together. (There are strange licensing issues around Metaphone 3.)</li>
<li>&#8220;cologne-phonetic&#8221; standardizes by the <a href="http://de.wikipedia.org/wiki/K%C3%B6lner_Phonetik">K&#246;lner Phonetik</a> algorithm so that things that sound the same in German should be keyed together. (There is a <a href="https://commons.apache.org/proper/commons-codec/apidocs/org/apache/commons/codec/language/ColognePhonetic.html">real open source version</a>.)</li>
</ul>
<p>The &#8220;nearest neighbor&#8221; method calculates pairwise distances, which is slow. Open Refine uses blocking to break things up into blocks that it won&#8217;t compare across, which reduces the number of comparisons to improve speed of calculation.</p>
<p>There are two &#8220;distance functions&#8221; in Open Refine:</p>
<ul>
<li>&#8220;levenshtein&#8221; is the well-known <a href="http://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein edit distance</a></li>
<li>&#8220;PPM&#8221; estimates how different strings are by how well they compress separately versus together, using <a href="http://en.wikipedia.org/wiki/Prediction_by_partial_matching">Prediction by Partial Matching</a>.</li>
</ul>
<p>The &#8220;radius&#8221; is the distance below which two items will be clustered together. With a higher value for &#8220;radius&#8221;, groups will tend to be larger.</p>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good, but there are several things I would like:</p>
<ul>
<li>See all the items rather than just the ones being grouped.</li>
<li>Customize / break up groupings that are incorrect, while preserving others.</li>
<li>Easily use a custom distance function.</li>
<li>Easily use a custom function for choosing the &#8220;New Cell Value&#8221;.</li>
<li>See what would happen with different &#8220;radii&#8221; without trying them all.</li>
<li><strong>Have a record of the whole transformation that&#8217;s easy to review, edit, and reapply.</strong></li>
</ul>
<hr>
<p><img height="1000%" title="ermahgerd mergic" src="img/ermahgerd.png"></p>
<hr>
<p>So I made <code>mergic</code>.</p>
<hr>
<ul>
<li>simple</li>
<li>customizable</li>
<li>reproducible</li>
</ul>
<hr>
<p>The goals of <code>mergic</code> are to be:</p>
<ul>
<li>simple, meaning largely text-based and obvious; the tool disappears</li>
<li>customizable, meaning you can easily use a custom distance function</li>
<li>reproducible, meaning everything you do can be done again automatically</li>
</ul>
<hr>
<p>the tool disappears</p>
<hr>
<p>Good tools disappear.</p>
<p>Whatever text editor you use, the your work product is a text file. You can use any text editor, or use different ones for different purposes, and so on.</p>
<p>Your merge process shouldn't rely on any particular piece of software for its replicability.</p>
<hr>
<p>any distance function</p>
<hr>
<p>Your distance function can make all the difference. You need to be able to plug in any distance function that works well for your data.</p>
<hr>
<p>really reproducible</p>
<hr>
<p>People can see what's happening, and computers can keep doing the process without clicking or re-running particular executables.</p>
<hr>
<p><img width="1000%" title="big data" src="img/big_data.png"></p>
<hr>
<p>A quick disclaimer!</p>
<p>This is John Langford's slide, about what big data is. He says that small data is data for which O(n<sup>2</sup>) algorithms are feasible. Currently <code>mergic</code> is strictly for this kind of "artisanal" data, where we want to ensure that our matching is correct but want to reduce the amount of human work to ensure that. And we are about to get very O(n<sup>2</sup>).</p>
<hr>
<pre><code>Santiago Giraldo,Leonardo Mayer
Santiago Giraldo,Dudi Sela
Santiago Giraldo,Juan Monaco
Santiago Giraldo,S Williams
Santiago Giraldo,C Wozniacki
Santiago Giraldo,S.Giraldo
Santiago Giraldo,Marcos Baghdatis
Santiago Giraldo,Y-H.Lu
...</code></pre>

<hr>
<p>So we make all possible pairs of identifiers!</p>
<p>One of the things that Open Refine gets right is that it doesn't show us humans all the pairs it's looking at.</p>
<p>All these pairs are annoying for a computer, and awful for humans. The computer can calculate a lot of pairwise distances, but I don't want to look at all the pairs.</p>
<p>Do you see the match here? (It's Santiago again!)</p>
<hr>
<pre><code>Karolina Pliskova,K Pliskova</code></pre>

<hr>
<p>Aside from being a drag to look at, there's a bigger problem with verifying equality on a pairwise basis.</p>
<p>Do these two records refer to the same person? (Tennis fans may see where I'm going with this.)</p>
<hr>
<pre><code>Kristyna Pliskova,K Pliskova</code></pre>

<hr>
<p>Karolina has a twin sister, and Kristyna also plays professional tennis! This may well not be obvious if you only look at pairs individually. What matters is the set of names that are transitively judged as equal.</p>
<hr>
<p>sets &gt; pairs</p>
<hr>
<p>Both perceptually and logically, it's better to think in sets than in a bunch of individual pairs.</p>
<hr>
<p>workflow support for reproducible deduplication and merging</p>
<hr>
<p>This is what <code>mergic</code> is for. <code>mergic</code> is a simple tool designed to make it less painful when you need to merge things that don't yet merge.</p>
<hr>
<p>demo: mergic tennis</p>
<hr>
<p>With all that background, let's see how <code>mergic</code> attempts to support a good workflow.</p>
<pre><code class="language-bash">$ pew new pydata</code></pre>

<p>I'll start by making a new <a href="https://virtualenv.pypa.io/">virtual environment</a> using <a href="https://github.com/berdario/pew">pew</a>.</p>
<pre><code class="language-bash">$ pip install mergic</code></pre>

<p><code>mergic</code> is very new (version 0.0.4.1) and it currently installs with no extra dependencies.</p>
<pre><code class="language-bash">$ mergic -h</code></pre>

<p><code>mergic</code> includes a command-line script based on <a href="https://docs.python.org/2/library/argparse.html">argparse</a> that uses a default string distance function.</p>
<pre><code>usage: mergic [-h] {calc,make,check,diff,apply,table} ...

positional arguments:
  {calc,make,check,diff,apply,table}
    calc                calculate all partitions of data
    make                make a JSON partition from data
    check               check validity of JSON partition
    diff                diff two JSON partitions
    apply               apply a patch to a JSON partition
    table               make merge table from JSON partition

optional arguments:
  -h, --help            show this help message and exit</code></pre>

<p>In the tennis data, names appear sometimes with full first names and sometimes with only first initials. To get good comparisons, we should:</p>
<ul>
<li>Transform all the data to the same format, as nearly as possible.</li>
<li>Use a good distance on the transformed data.</li>
</ul>
<p>We can do both of these things with a simple custom script, <a href="tennis/tennis_mergic.py">tennis_mergic.py</a>. It only <a href="requirements.txt">requires</a> the <code>mergic</code> and <code>python-Levenshtein</code> packages.</p>
<pre><code class="language-python">#!/usr/bin/env python

import re
import Levenshtein
import mergic


def first_initial_last(name):
    initial = re.match("^[A-Z]", name).group()
    last = re.search("(?&lt;=[ .])[A-Z].+$", name).group()
    return "{}. {}".format(initial, last)


def distance(x, y):
    x = first_initial_last(x)
    y = first_initial_last(y)
    return Levenshtein.distance(x, y)


mergic.Blender(distance).script()</code></pre>

<p>Note that there's a transformation step in there, normalizing the form of the names to have just a first initial and last name. This kind of normalization can be very important.</p>
<p>As a more extreme example, a friend of mine has used the following transform: Google it. Then you can use a distance on the result set to deduplicate.</p>
<p>Now <a href="tennis/tennis_mergic.py">tennis_mergic.py</a> can be used just like the standard <code>mergic</code> script.</p>
<pre><code class="language-bash">$ ./tennis_mergic.py calc names.txt
## num groups, max group, num pairs, cutoff
## ----------------------------------------
##        669,         1,         0, -1
##        358,         5,       384, 0
##        348,         6,       414, 1
##        332,         6,       470, 2
##        262,        85,      5117, 3
##        165,       324,     52611, 4
##         86,       496,    122899, 5
##         46,       584,    170287, 6
##         24,       624,    194407, 7
##         16,       641,    205138, 8
##         10,       650,    210940, 9
##          4,       663,    219459, 10
##          2,       668,    222778, 11
##          1,       669,    223446, 12</code></pre>

<p>There is a clear best cutoff here, as the size of the max group jumps from 6 items to 85 and the number of within-group comparisons jumps from 470 to 5,117. So we create a partition where the Levenshtein distance between names in our standard first initial and last name format is no more than two, and put the result in a file called <code>groups.json</code>:</p>
<pre><code class="language-bash">$ ./tennis_mergic.py make names.txt 2 &gt; groups.json</code></pre>

<p><strong>This kind of JSON grouping file could be produced and edited by anything, not just <code>mergic</code>.</strong></p>
<p>As expected, the proposed grouping has combined things over-zealously in some places:</p>
<pre><code class="language-bash">$ head -5 groups.json
## {
##     "Yen-Hsun Lu": [
##         "Di Wu",
##         "Yen-Hsun Lu",
##         "Y-H.Lu",</code></pre>

<p>Manual editing can produce a corrected version of the original grouping, which could be saved as <code>edited.json</code>:</p>
<pre><code class="language-bash">$ head -8 edited.json
## {
##     "Yen-Hsun Lu": [
##         "Yen-Hsun Lu",
##         "Y-H.Lu"
##     ],
##     "Di Wu": [
##         "Di Wu"
##     ],</code></pre>

<p>Parts of the review process would be difficult or impossible for a computer to do accurately.</p>
<ul>
<li>There are the Pl&#237;&#353;kov&#225; twins, Karol&#237;na and Krist&#253;na. When we see that <code>K Pliskova</code> appears, we have to go back and see that this occurred in the <code>USOpen-women-2013.csv</code> file, and only Karol&#237;na played in the <a href="http://en.wikipedia.org/wiki/2013_US_Open_%E2%80%93_Women%27s_Singles">2013 US Open</a>.</li>
<li>In a similar but less interesting way, <code>B.Becker</code> turns out to refer to Benjamin, not Brian.</li>
<li>An <code>A Wozniak</code> appears with <code>C Wozniack</code> and <code>C Wozniacki</code>. The first initial does turn out to differentiate the Canadian from the Dane.</li>
<li>The name <code>A.Kuznetsov</code> refers to <em>both</em> Andrey <em>and</em> Alex in <code>Wimbledon-men-2013.csv</code>. This can't be resolved by <code>mergic</code>. One way to resolve the issues is to edit <code>Wimbledon-men-2013.csv</code> so that <code>A.Kuznetsov,I.Sijsling</code> becomes <code>Alex Kuznetsov,I.Sijsling</code>, based on checking <a href="http://en.wikipedia.org/wiki/2013_Wimbledon_Championships_%E2%80%93_Men%27s_Singles">records from that competition</a>.</li>
<li><code>Juan Martin Del Potro</code> is unfortunately too different from <code>J.Del Potro</code> in the current formulation to be grouped automatically, but a human reviewer can correct this. Similarly for <code>Anna Schmiedlova</code> and <code>Anna Karolina Schmiedlova</code>.</li>
</ul>
<p>After editing, you can check that the new grouping is still valid. At this stage we aren't using anything custom any more, so the default <code>mergic</code> is fine:</p>
<pre><code class="language-bash">$ mergic check edited.json
## 669 items in 354 groups</code></pre>

<p>The <code>mergic</code> diffing tools make it easy to make comparisons that would otherwise be difficult, letting us focus on and save only changes that are human reviewers make rather than whole files.</p>
<pre><code class="language-bash">$ mergic diff groups.json edited.json &gt; diff.json</code></pre>

<p>Now <code>diff.json</code> only has the entries that represent changes from the original <code>groups.json</code>.</p>
<p>The edited version can be reconstructed from the original and the diff with <code>mergic apply</code>:</p>
<pre><code class="language-bash">$ mergic apply groups.json diff.json &gt; rebuilt.json</code></pre>

<p>The order of <code>rebuilt.json</code> may not be identical to the original <code>edited.json</code>, but the diff will be empty, meaning the file is equivalent:</p>
<pre><code class="language-bash">$ mergic diff edited.json rebuilt.json
## {}</code></pre>

<p>Finally, to generate a CSV merge table that you'll be able to use with any other tool:</p>
<pre><code class="language-bash">$ mergic table edited.json &gt; merge.csv</code></pre>

<p>Now the file <code>merge.csv</code> has two columns, <code>original</code> and <code>mergic</code>, where <code>original</code> contains all the values that appeared in the original data and <code>mergic</code> contains the deduplicated keys. You can join this on to your original data and go to town.</p>
<p>Here's how we might do that to quickly get a list of who played the most in these 2013 tennis events:</p>
<pre><code class="language-bash">$ join -t, &lt;(sort names.txt) &lt;(sort merge.csv) | cut -d, -f2 | sort | uniq -c | sort -nr | head
##  24 Novak Djokovic
##  22 Rafael Nadal
##  21 Serena Williams
##  21 David Ferrer
##  20 Na Li
##  19 Victoria Azarenka
##  19 Agnieszka Radwanska
##  18 Stanislas Wawrinka
##  17 Tommy Robredo
##  17 Sloane Stephens</code></pre>

<p>Note that this is not the same as the result we got before resolving these name issues:</p>
<pre><code class="language-bash">$ sort names.txt | uniq -c | sort -nr | head
##  21 Rafael Nadal
##  17 Stanislas Wawrinka
##  17 Novak Djokovic
##  17 David Ferrer
##  15 Roger Federer
##  14 Tommy Robredo
##  13 Richard Gasquet
##  11 Victoria Azarenka
##  11 Tomas Berdych
##  11 Serena Williams</code></pre>

<p>As it happens, using a cutoff of 0 and doing no hand editing will still give the correct top ten. In general the desired result and desired level of certainty in its correctness will inform the level of effort that is justified.</p>
<hr>
<p>distance matters</p>
<hr>
<p>Having a good distance function might be the most important thing. It's hard to imagine a machine learning as good a distance function as you could just right based on your human intelligence.</p>
<p>There is work on learnable edit distances; notably there's a current Python project to implement hidden alignment conditional random fields for classifying string pairs: <a href="https://github.com/dirko/pyhacrf">pyhacrf</a>. Python dedupe is <a href="https://github.com/datamade/dedupe/issues/14">eager</a> to incorporate this.</p>
<p>See also: <a href="http://www.cs.utexas.edu/users/ml/papers/marlin-kdd-03.pdf">Adaptive Duplicate Detection Using Learnable String Similarity Measures</a></p>
<hr>
<p>extension to multiple fields</p>
<hr>
<p>We've been talking about single field deduplication.</p>
<hr>
<pre><code class="language-text">name
----
Bob
Rob
Robert</code></pre>

<hr>
<p>This means that we have one field, say name.</p>
<hr>
<pre><code class="language-text">name, name
----------
Bob, Bobby
Bob, Robert
Bobby, Robert</code></pre>

<hr>
<p>And we look at all the possible pairs and calculate those pairwise distances.</p>
<hr>
<p><img width="1000%" title="one dimensional" src="img/one_dimensional.png"></p>
<hr>
<p>While we described it even in Open Refine as &#8220;clustering&#8221;, we've really been doing a classification task: either a pair is in the same group or they aren't. We've had one dimension, and we hope that we can just divide true connections from different items with a simple cutoff.</p>
<hr>
<pre><code class="language-text">name,    hometown
-----------------
Bob,     New York
Rob,     NYC
Robert,  "NY, NY"</code></pre>

<hr>
<p>Often, there's more than one field involved, and it might be good to treat all the fields separately.</p>
<p>So let's calculate distances between each field entry for each pair of rows, in this case.</p>
<hr>
<p><img height="1000%" title="two dimensional" src="img/two_dimensional.png"></p>
<hr>
<p>The reason it might be good to treat the fields separately is that they might be more useful together; we might be able to classify all the true duplicates using the information from both fields.</p>
<p>Maybe you can find two clusters, one for true duplicates and one for different items.</p>
<p>Or maybe you could get some training data and use whatever classification algorithm you like.</p>
<p>Let's look at a couple packages that do these things.</p>
<hr>
<p>R: RecordLinkage</p>
<hr>
<p>The R <a href="http://cran.r-project.org/web/packages/RecordLinkage/index.html">RecordLinkage</a> package, which has a good <a href="http://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf">R Journal article</a> and a number of fine vignettes, does quite a lot of interesting things along the lines of what we've been discussing.</p>
<p>You'll also notice that it imports <code>e1071</code> and <code>rpart</code> and others to plug in machine learning for determining duplicates.</p>
<hr>
<p>demo: mergic on RecordLinkage data</p>
<hr>
<p>Let's look at some of the example data that comes with <code>RecordLinkage</code>.</p>
<p>We write the data out to CSV very simply with <a href="RLdata/RLdata500.R">RLdata500.R</a>:</p>
<pre><code class="language-r"># install.packages('RecordLinkage')
library('RecordLinkage')
data(RLdata500)
write.table(RLdata500, "RLdata500.txt",
            row.names=FALSE, col.names=FALSE,
            quote=FALSE, sep=",", na="")</code></pre>

<p>Then we can take a look at the data:</p>
<pre><code class="language-bash">$ head -4 RLdata500.txt
## CARSTEN,,MEIER,,1949,7,22
## GERD,,BAUER,,1968,7,27
## ROBERT,,HARTMANN,,1930,4,30
## STEFAN,,WOLFF,,1957,9,2</code></pre>

<p>The data is fabricated name and birth date from a hypothetical German hospital. It has a number of columns, but for <code>mergic</code> we'll just treat the rows of CSV as single strings.</p>
<pre><code class="language-bash">$ mergic calc RLdata500.txt
## ...
##        451,         2,        49, 0.111111111111
##        450,         2,        50, 0.115384615385
##        449,         3,        52, 0.125
## ...</code></pre>

<p>Looking through the possible groupings, we see a cutoff of about 0.12 that will produce 50 groups of two items, which looks promising.</p>
<p>This is slightly artificial, but only slightly so; we could well be doing this for two columns to merge on, in which case so we would hope to find groups of two elements.</p>
<pre><code class="language-bash">$ mergic make RLdata500.txt 0.12
## {
##     "MATTHIAS,,HAAS,,1955,7,8": [
##         "MATTHIAS,,HAAS,,1955,7,8",
##         "MATTHIAS,,HAAS,,1955,8,8"
##     ],
##     "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
##         "HELGA,ELFRIEDE,BERGER,,1989,1,18",
##         "HELGA,ELFRIEDE,BERGER,,1989,1,28"
##     ],
## ...</code></pre>

<p>In this example, the partition at a cutoff of 0.12 happens to be exactly right and we correctly group everything. This says something about how realistic this example data set is, something about your tool of choice if it can't easily get perfect performance on this example data set, and also something about information leakage.</p>
<hr>
<p><code>dedupe</code></p>
<hr>
<p>The Python <a href="https://github.com/datamade/dedupe">dedupe</a> project from <a href="http://datamade.us/">DataMade</a> in Chicago is very cool, and I'd better not neglect it.</p>
<p>It's a Python library that implements sophisticated multi-field deduplication and has a lot of connected software. One of these is the <code>csvdedupe</code>.</p>
<hr>
<p>demo: csvdedupe</p>
<hr>
<p>We'll use the RecordLinkage data with a header.</p>
<pre><code class="language-r">write.table(RLdata500, "RLdata500.csv",
            row.names=FALSE,
            quote=FALSE, sep=",", na="")</code></pre>

<p>Then we start the process, specifying which columns of the data to consider for matching.</p>
<pre><code class="language-bash">$ csvdedupe RLdata500.csv --field_names $(head -1 RLdata500.csv | tr ',' ' ')</code></pre>

<p>We go into an interactive supervision stage in which <code>dedupe</code> asks us to clarify things. The hope is that it will learn what matters.</p>
<p>You can build this kind of behavior into your own systems; there is an <a href="http://datamade.github.io/dedupe-examples/docs/csv_example.html">example</a>.</p>
<p>At the end you get output that you can use much like the <code>table</code> output from <code>mergic</code>. It needs some transforming to be easily reviewed by humans though.</p>
<hr>
<p>real clustering?</p>
<hr>
<p>The &#8220;clustering&#8221; that we've been doing hasn't been much like usual clustering.</p>
<hr>
<p><img width="1000%" title="one dimensional" src="img/one_dimensional.png"></p>
<hr>
<p>In part, this is because we we've only had distances between strings without having a real &#8220;string space&#8221;.</p>
<p>I'm going to just sketch out this direction; I think it's interesting but I haven't seen any real results in it yet.</p>
<hr>
<p>dog, doge, kitten, kitteh</p>
<hr>
<p>Say these are the items we're working with. We can use Levenshtein edit distance to make a distance matrix.</p>
<hr>
<pre><code class="language-text">       dog doge kitten kitteh
   dog   0    1      6      6
  doge   1    0      5      5
kitten   6    5      0      1
kitteh   6    5      1      0</code></pre>

<hr>
<p>So here's a distance matrix, and it looks the way we'd expect. But we still don't have <em>coordinates</em> for our words.</p>
<p>Luckily, there is at least one technique for coming up with coordinates when you have a distance matrix. Let's use <a href="http://en.wikipedia.org/wiki/Multidimensional_scaling">multidimensional scaling</a>! There's a nice <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html">implementation in sklearn</a>.</p>
<hr>
<pre><code class="language-python">from sklearn.manifold import MDS
mds = MDS(dissimilarity='precomputed')
coords = mds.fit_transform(distances)</code></pre>

<hr>
<p>Here's all the code it takes.</p>
<hr>
<p><img width="1000%" title="MDS coordinates from distance matrix" src="img/mds_words.png"></p>
<hr>
<p>And here's the result! This is at least a fun visualization, and I wonder if doing clustering in a space like this might sometimes lead to better results.</p>
<p>The key thing here is that we're clustering on the elements themselves, rather than indirectly via the pairwise distances.</p>
<p>There are other ways of getting coordinates for words. This includes the very interesting <a href="https://code.google.com/p/word2vec/">word2vec</a> and related techniques.</p>
<p>Also, you might have items are already naturally coordinates, for example if you have medical data like a person's height or weight.</p>
<hr>
<p>deep thoughts</p>
<hr>
<p>By way of conclusion, I'd like to suggest that this problem of deduplication is no good and we should take steps to:</p>
<ul>
<li>prevent it from being necessary, by having our systems recommend or enforce standard naming</li>
<li>make it possible to do deduplication once and reintegrate the results back into the data system</li>
</ul>
<p>It should be possible to make changes and share them back to data providers. It should be possible to edit data while preserving the data's history. These kind of collaborative data editing are not super easy to implement, and I hope systems emerge that handle it better than current systems.</p>
<hr>
<p>questions for discussion</p>
<hr>
<p>I'd like to ask you to consider and discuss with your peers:</p>
<ul>
<li>What workflows and tools do you use for these kinds of tasks?</li>
<li>Does the JSON partition format used by <code>mergic</code> make sense for your use?</li>
<li>Does the merge table format used by <code>mergic</code> make sense for your use?</li>
<li>What else would make this kind of process better for you?</li>
</ul>
<hr>
<p><img width="1000%" title="Open Data Science Conference" src="img/open_data_sci_con.png"></p>
<hr>
<p>I also hope to see you at <a href="http://opendatascicon.com/">Open Data Science Con</a> in Boston!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>
<p>I'd love to hear from you!</p>
<hr>
<h3>Other interesting things:</h3>
<ul>
<li><a href="http://infolab.stanford.edu/serf/">Stanford Entity Resolution Framework</a></li>
<li><a href="http://infolab.stanford.edu/serf/swoosh_vldbj.pdf">Swoosh: a generic approach to entity resolution</a></li>
<li><a href="http://www.umiacs.umd.edu/~getoor/Tutorials/ER_VLDB2012.pdf">Entity Resolution: Tutorial</a></li>
<li><a href="http://www.datacommunitydc.org/blog/2013/08/entity-resolution-for-big-data">Entity Resolution for Big Data (summary)</a></li>
<li><a href="http://dbs.uni-leipzig.de/file/learning_based_er_with_mr.pdf">Learning-based Entity Resolution with MapReduce</a></li>
<li><a href="http://linqs.cs.umd.edu/projects/ddupe/">D-Dupe: A Novel Tool for Interactive Data Deduplication and Integration</a></li>
</ul><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20150520-practical_mergic/</link>
<guid>http://planspace.org/20150520-practical_mergic/</guid>
<pubDate>Wed, 20 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>mergic</title>
<description><![CDATA[

<p><em>A lightning talk at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>, introducing <a href="https://github.com/ajschumacher/mergic">mergic</a>.</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img height="1000%" title="ermahgerd mergic" src="ermahgerd.png"></p>
<hr>
<p>Down to business!</p>
<hr>
<pre><code>Lukas Lacko             F Pennetta
Leonardo Mayer          S Williams
Marcos Baghdatis        C Wozniacki
Santiago Giraldo        E Bouchard
Juan Monaco             N.Djokovic
Dmitry Tursunov         S.Giraldo
Dudi Sela               Y-H.Lu
Fabio Fognini           T.Robredo
...                     ...</code></pre>

<hr>
<p>The problem often looks like this: You have either two columns with slightly different versions of identifiers, or one long list of things that you need to resolve to common names. These problems are fundamentally the same.</p>
<p>Do you see the match here? (It's Santiago!)</p>
<hr>
<p>workflow support for reproducible deduplication and merging</p>
<hr>
<p>This is what <code>mergic</code> is for. <code>mergic</code> is a simple tool designed to make it less painful when you need to merge things that don't yet merge.</p>
<hr>
<p><img width="1000%" title="big data" src="big_data.png"></p>
<hr>
<p>A quick disclaimer!</p>
<p>This is John Langford's slide, about what big data is. He says that small data is data for which O(n<sup>2</sup>) algorithms are feasible. Currently <code>mergic</code> is strictly for this kind of "artisanal" data, where we want to ensure that our matching is correct but want to reduce the amount of human work to ensure that. And we are about to get very O(n<sup>2</sup>).</p>
<hr>
<pre><code>Santiago Giraldo,Leonardo Mayer
Santiago Giraldo,Dudi Sela
Santiago Giraldo,Juan Monaco
Santiago Giraldo,S Williams
Santiago Giraldo,C Wozniacki
Santiago Giraldo,S.Giraldo
Santiago Giraldo,Marcos Baghdatis
Santiago Giraldo,Y-H.Lu
...</code></pre>

<hr>
<p>So we make all possible pairs of identifiers! This is annoying for a computer, and awful for humans. The computer can calculate a lot of pairwise distances, but I don't want to look at all the pairs.</p>
<p>Do you see the match here? (It's Santiago again!)</p>
<hr>
<pre><code>INFO:dedupe.training:1.0
name : stanislas wawrinka

name : stanislas wawrinka

Do these records refer to the same thing?
(y)es / (n)o / (u)nsure / (f)inished

O.o?</code></pre>

<hr>
<p>The is a "screen shot" of the <a href="https://github.com/datamade/csvdedupe">csvdedupe</a> interface, which is based on the Python <a href="https://github.com/datamade/dedupe">dedupe</a> project, which is very cool. It could be exactly what you want for larger amounts of more complex data. There's even work on getting learnable edit distances implemented now, which would be great to see. But for very simple data sets, <code>dedupe</code> can be overkill. Also, you don't get much sense of the big picture of your data set, and it's still very pair-oriented.</p>
<hr>
<pre><code>Karolina Pliskova,K Pliskova</code></pre>

<hr>
<p>Aside from being a drag to look at, there's a bigger problem with verifying equality on a pairwise basis.</p>
<p>Do these two records refer to the same person? (Tennis fans may see where I'm going with this.)</p>
<hr>
<pre><code>Kristyna Pliskova,K Pliskova</code></pre>

<hr>
<p>Karolina has a twin sister, and Kristyna also plays professional tennis! This may well not be obvious if you only look at pairs individually. What matters is the set of names that are transitively judged as equal.</p>
<hr>
<p>sets &gt; pairs</p>
<hr>
<p>Both perceptually and logically, it's better to think in sets than in a bunch of individual pairs.</p>
<hr>
<p><img width="1000%" title="Open Refine" src="open_refine.png"></p>
<hr>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good. Their interface shows you some useful diagnostics, and you can see sets of things. There's even some idea of repeatable transformations. But there's so much functionality wrapped up in a mostly graphical interface that it's hard to make it part of an easily repeatable workflow. And while there are a bunch of built-in distance functions, I'm not sure whether it's possible to use a custom distance function in Open Refine.</p>
<hr>
<ul>
<li>simple</li>
<li>customizable</li>
<li>reproducible</li>
</ul>
<hr>
<p>So the goals of <code>mergic</code> are to be:</p>
<ul>
<li>simple, meaning largely text-based and obvious</li>
<li>customizable, meaning you can easily use a custom distance function</li>
<li>reproducible, meaning everything you do can be done again automatically</li>
</ul>
<hr>
<p>demo</p>
<hr>
<p>Here's a quick run-through of the <code>mergic</code> workflow. It's similar to the one in the <a href="https://github.com/ajschumacher/mergic">README</a>.</p>
<pre><code class="language-bash">pew new pydata</code></pre>

<p>I'll start by making a new <a href="https://virtualenv.pypa.io/">virtual environment</a> using <a href="https://github.com/berdario/pew">pew</a>.</p>
<pre><code class="language-bash">pip install mergic</code></pre>

<p><code>mergic</code> is very new (version 0.0.4) and it currently installs with no extra dependencies.</p>
<pre><code class="language-bash">mergic -h</code></pre>

<p><code>mergic</code> includes a command-line script based on <a href="https://docs.python.org/2/library/argparse.html">argparse</a> that uses a default string distance function.</p>
<pre><code>usage: mergic [-h] {calc,make,check,diff,apply,table} ...

positional arguments:
  {calc,make,check,diff,apply,table}
    calc                calculate all partitions of data
    make                make a JSON partition from data
    check               check validity of JSON partition
    diff                diff two JSON partitions
    apply               apply a patch to a JSON partition
    table               make merge table from JSON partition

optional arguments:
  -h, --help            show this help message and exit</code></pre>

<p>The command line script has a number of sub-commands that expose its functionality.</p>
<pre><code class="language-bash">head -4 RLdata500.csv</code></pre>

<p>We'll try <code>mergic</code> out with an example data set from <a href="http://www.r-project.org/">R</a>'s <a href="http://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf">RecordLinkage</a> package.</p>
<pre><code>CARSTEN,,MEIER,,1949,7,22
GERD,,BAUER,,1968,7,27
ROBERT,,HARTMANN,,1930,4,30
STEFAN,,WOLFF,,1957,9,2</code></pre>

<p>The data is fabricated name and birth date from a hypothetical German hospital. It has a number of columns, but for <code>mergic</code> we'll just treat the rows of CSV as single strings.</p>
<pre><code class="language-bash">mergic calc RLdata500.csv</code></pre>

<p>The <code>calc</code> subcommand calculates all the pairwise distances and provides diagnostics about possible groupings that could be produced.</p>
<pre><code>num groups, max group, num pairs, cutoff
----------------------------------------
       500,         1,         0, -0.982456140351
       497,         2,         3, 0.0175438596491</code></pre>

<p>With a cutoff lower than any actual encountered string distance, every item stays separate, the maximum group size is one, and there are no pairs within those groups to evaluate.</p>
<pre><code>         2,       499,    124251, 0.416666666667
         1,       500,    124750, 0.418181818182</code></pre>

<p>On the other extreme, we could group every item together in a giant mega-group.</p>
<pre><code>       451,         2,        49, 0.111111111111
       450,         2,        50, 0.115384615385
       449,         3,        52, 0.125</code></pre>

<p><code>mergic</code> gives you a choice about how big the groups it will produce will be. In this case, there's a cutoff of about 0.12 that will produce 50 groups of two items, which looks promising.</p>
<pre><code class="language-bash">mergic make RLdata500.csv 0.12</code></pre>

<p>We can make a grouping with that cutoff, and the result is a JSON-formatted partition.</p>
<pre><code class="language-json">{
    "MATTHIAS,,HAAS,,1955,7,8": [
        "MATTHIAS,,HAAS,,1955,7,8",
        "MATTHIAS,,HAAS,,1955,8,8"
    ],
    "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
        "HELGA,ELFRIEDE,BERGER,,1989,1,18",
        "HELGA,ELFRIEDE,BERGER,,1989,1,28"
    ],</code></pre>

<p>In this example, the partition at a cutoff of 0.12 happens to be exactly right and we correctly group everything. (This says something about how realistic this example data set is, something about your tool of choice if it can't easily get perfect performance on this example data set, and also something about information leakage.)</p>
<pre><code class="language-json">{
    "MATTHIAS,,HAAS,,1955,7,8": [
        "MATTHIAS,,HAAS,,1955,8,8"
    ],
    "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
        "MATTHIAS,,HAAS,,1955,7,8",
        "HELGA,ELFRIEDE,BERGER,,1989,1,18",
        "HELGA,ELFRIEDE,BERGER,,1989,1,28"
    ],</code></pre>

<p>The above would be a strange change to make, but you could make such a change and save your changed version as a new file.</p>
<pre><code class="language-bash">mergic diff base.json edited.json &gt; diff.json
mergic apply base.json diff.json</code></pre>

<p><code>mergic</code> includes functionality for creating and applying diffs that compare two partitions. You can preserve just the changes that you make by hand, which provides a record of the changes that had a human in the loop versus the changes that were computer-generated.</p>
<pre><code class="language-bash">mergic table edited.json</code></pre>

<p>To actually accomplish the desired merge or deduplication after creating a good grouping in JSON, <code>mergic</code> will generate a two-column merge table in CSV that can be used with most any data system.</p>
<pre><code>"HANS,,SCHAEFER,,2003,6,22","HANS,,SCHAEFER,,2003,6,22"
"HARTMHUT,,HOFFMSNN,,1929,12,29","HARTMHUT,,HOFFMSNN,,1929,12,29"
"HARTMUT,,HOFFMANN,,1929,12,29","HARTMHUT,,HOFFMSNN,,1929,12,29"</code></pre>

<p>These merge tables are awful to work with by hand, which is why <code>mergic</code> leaves their generation as a final step after humans work with the more understandable JSON groupings.</p>
<hr>
<p><img width="1000%" title="custom distance function documentation on GitHub" src="custom_distance.png"></p>
<hr>
<p>It's easy to write a script with a custom distance function and immediately use it with all the workflow support of the <code>mergic</code> script.</p>
<p>Often, a custom distance function makes or breaks your effort. It's worth thinking about and experimenting with, and <code>mergic</code> makes it easy!</p>
<hr>
<p><img width="1000%" title="New York Open Statistical Programming Meetup; Practical Mergic: How to Join Anything" src="open_stats_prog_meetup.png"></p>
<hr>
<p>If you're interested in this kind of thing, I'll be doing <a href="http://www.meetup.com/nyhackr/events/222328498/">a longer talk</a> at the <a href="http://www.meetup.com/nyhackr/">New York City Open Statistical Programming Meetup</a> next week Wednesday.</p>
<hr>
<p><img width="1000%" title="Open Data Science Conference" src="open_data_sci_con.png"></p>
<hr>
<p>I also hope to see you at <a href="http://opendatascicon.com/">Open Data Science Con</a> in Boston!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20150514-mergic/</link>
<guid>http://planspace.org/20150514-mergic/</guid>
<pubDate>Thu, 14 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Working with Captured Variables in Python Closures</title>
<description><![CDATA[

<p>You can make <a href="http://en.wikipedia.org/wiki/Closure_%28computer_programming%29">closures</a> in Python like this:</p>
<pre><code class="language-python">def add_some(x=0):
    def adder(y):
        return x+y
    return adder

add3 = add_some(3)

print(add3(7))
## 10</code></pre>

<p>The <code>adder</code> function "remembers" <code>x</code>.</p>
<p>But this will fail:</p>
<pre><code class="language-python">def a_counter(count=0):
    def counter():
        count += 1
        return count
    return counter

the_counter = a_counter()

print(the_counter())
## UnboundLocalError: local variable 'count' referenced before assignment</code></pre>

<p>You can't assign to <code>count</code> in there!</p>
<p>In Python 3, you can resolve this with <code>nonlocal</code>:</p>
<pre><code class="language-python">def a_counter(count=0):
    def counter():
        nonlocal count
        count += 1
        return count
    return counter

the_counter = a_counter()

print(the_counter())
## 1

print(the_counter())
## 2

print(the_counter())
## 3</code></pre>

<p>Great! I'd rather not have to specify <code>nonlocal</code>, but I'm happy to know of another feature unique to Python 3.</p>
<p>In Python 2, you can't get assignment of captured variables inside your closure, but you can mutate a captured variable. <code>&#175;\_(&#12484;)_/&#175;</code></p>
<pre><code class="language-python">def a_counter(count={'val': 0}):
    def counter():
        count['val'] += 1
        return count['val']
    return counter

the_counter = a_counter()

print(the_counter())
## 1

print(the_counter())
## 2

print(the_counter())
## 3</code></pre>

<p>Hooray for closures!</p>    
    ]]></description>
<link>http://planspace.org/20150425-working_with_captured_variables_in_python_closures/</link>
<guid>http://planspace.org/20150425-working_with_captured_variables_in_python_closures/</guid>
<pubDate>Sat, 25 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Two Highlights from NY R Conference 2015</title>
<description><![CDATA[

<p>Here are the two things I thought were most interesting at <a href="http://www.rstats.nyc/">NY R</a> on Saturday April 25, 2015:</p>
<hr>
<p><img alt="MRAN" src="mran.png"></p>
<p><a href="https://twitter.com/revojoe">Joe Rickert</a> showed <a href="http://www.revolutionanalytics.com/">Revolution Analytics</a>' <a href="http://mran.revolutionanalytics.com/">Managed R Archive Network</a> (MRAN). It snapshots all of <a href="http://cran.r-project.org/">CRAN</a> every day so that you can finally do a real lock-in of the versions for all your R dependencies, using the <a href="http://cran.r-project.org/web/packages/checkpoint/index.html">checkpoint</a> package. This was <a href="http://blog.revolutionanalytics.com/2014/10/introducing-rrt.html">announced</a> a while ago, but it was new to me. I love any sort of data store that gives you <code>as-of</code>, even if it's just by snapshotting.</p>
<hr>
<p><a href="https://twitter.com/wesmckinn">Wes McKinney</a> spoke about data frame design and how such tooling should develop further. Here's a transcription of a key <a href="https://twitter.com/planarrowspace/status/591990689635905536">slide</a>:</p>
<ul>
<li><strong>The Great Data Tool Decoupling&#8482;</strong><ul>
<li>Thesis: over time, user interfaces, data storage, and execution engines will decouple and specialize</li>
<li>In fact, you should really want this to happen<ul>
<li>Share systems among languages</li>
<li>Reduce fragmentation and "lock-in"</li>
<li>Shift developer focus to usability</li>
</ul>
</li>
<li>Prediction: we'll be there by 2025; sooner if we all get our act together</li>
</ul>
</li>
</ul>
<p>I've thought about this kind of decoupling for data visualization (see <a href="http://planspace.org/20150119-gog_a_separate_layer_for_visualization/">gog</a>) and I think it's a cool direction in general.</p>
<hr>
<p>There were certainly other interesting and good things as well!</p>    
    ]]></description>
<link>http://planspace.org/20150425-two_highlights_from_ny_r_conference_2015/</link>
<guid>http://planspace.org/20150425-two_highlights_from_ny_r_conference_2015/</guid>
<pubDate>Sat, 25 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Forward Selection with statsmodels</title>
<description><![CDATA[

<p>Python's <a href="http://statsmodels.sourceforge.net/stable/">statsmodels</a> doesn't have a built-in method for choosing a linear model by <a href="http://en.wikipedia.org/wiki/Stepwise_regression">forward selection</a>. Luckily, it isn't impossible to write yourself. So <a href="https://github.com/trevor-smith">Trevor</a> and I sat down and hacked out the following. It tries to optimize <a href="http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">adjusted R-squared</a> by adding features that help the most one at a time until the score goes down or you run out of features.</p>
<pre><code class="language-python">import statsmodels.formula.api as smf

def forward_selected(data, response):
    """Linear model designed by forward selection.

    Parameters:
    -----------
    data : pandas DataFrame with all possible predictors and response

    response: string, name of response column in data

    Returns:
    --------
    model: an "optimal" fitted statsmodels linear model
           with an intercept
           selected by forward selection
           evaluated by adjusted R-squared
    """
    remaining = set(data.columns)
    remaining.remove(response)
    selected = []
    current_score, best_new_score = 0.0, 0.0
    while remaining and current_score == best_new_score:
        scores_with_candidates = []
        for candidate in remaining:
            formula = "{} ~ {} + 1".format(response,
                                           ' + '.join(selected + [candidate]))
            score = smf.ols(formula, data).fit().rsquared_adj
            scores_with_candidates.append((score, candidate))
        scores_with_candidates.sort()
        best_new_score, best_candidate = scores_with_candidates.pop()
        if current_score &lt; best_new_score:
            remaining.remove(best_candidate)
            selected.append(best_candidate)
            current_score = best_new_score
    formula = "{} ~ {} + 1".format(response,
                                   ' + '.join(selected))
    model = smf.ols(formula, data).fit()
    return model</code></pre>

<p>There isn't just one way to design this kind of thing. You could select on some other evaluation metric. You could use internal cross-validation. You might not want to do use forward selection at all. But hey!</p>
<p>Here's how ours can be applied to a classic data set on <a href="http://data.princeton.edu/wws509/datasets/#salary">Discrimination in Salaries</a>:</p>
<pre><code class="language-python">import pandas as pd

url = "http://data.princeton.edu/wws509/datasets/salary.dat"
data = pd.read_csv(url, sep='\\s+')

model = forward_selected(data, 'sl')

print model.model.formula
# sl ~ rk + yr + 1

print model.rsquared_adj
# 0.835190760538</code></pre>    
    ]]></description>
<link>http://planspace.org/20150423-forward_selection_with_statsmodels/</link>
<guid>http://planspace.org/20150423-forward_selection_with_statsmodels/</guid>
<pubDate>Thu, 23 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>R Squared Can Be Negative</title>
<description><![CDATA[

<p>Let's do a little linear regression in Python with <a href="http://scikit-learn.org/">scikit-learn</a>:</p>
<pre><code class="language-python">import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import train_test_split

X, y = np.random.randn(100, 20), np.random.randn(100)
X_train, X_test, y_train, y_test = train_test_split(X, y)

model = LinearRegression()
model.fit(X_train, y_train)</code></pre>

<p>It is a property of <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares regression</a> that for the training data we fit on, the <a href="http://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination R<sup>2</sup></a> and the square of the <a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">correlation coefficient</a> r<sup>2</sup> of the model's predictions with the actual data are equal.</p>
<pre><code class="language-python"># coefficient of determination R^2
print model.score(X_train, y_train)
## 0.203942898079

# squared correlation coefficient r^2
print np.corrcoef(model.predict(X_train), y_train)[0, 1]**2
## 0.203942898079</code></pre>

<p>This does not hold for new data, and if our model is sufficiently bad the coefficient of determination can be negative. The squared correlation coefficient is never negative but can be quite low.</p>
<pre><code class="language-python"># coefficient of determination R^2
print model.score(X_test,  y_test)
## -0.277742673311

# squared correlation coefficient r^2
print np.corrcoef(model.predict(X_test), y_test)[0, 1]**2
## 0.0266856746214</code></pre>

<p>These declines in performance worsen with <a href="http://en.wikipedia.org/wiki/Overfitting">overfitting</a>.</p>    
    ]]></description>
<link>http://planspace.org/20150417-negative_r_squared/</link>
<guid>http://planspace.org/20150417-negative_r_squared/</guid>
<pubDate>Fri, 17 Apr 2015 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
