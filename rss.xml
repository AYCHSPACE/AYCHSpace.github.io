<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Is it worth doing even if it fails?</title>
<description><![CDATA[

<p>Many things can "succeed" or "fail." Avoid anything that isn't worth
doing even if it fails.</p>
<p>If two tasks each have a probability of success and payout, but one is
worth doing regardless of success and one is only about payout, it
almost doesn't matter what the probabilities and payouts are. One
guarantees a good use of time.</p>
<p>This filter also averts unhealthy fixation on "winning" and excessive
disappointment with "losing." It lets you focus on the task itself,
which is the right focus.</p>
<p>You can't always choose what you have to do. But you can try to find
or create value even in endeavors you wouldn't otherwise choose.</p>
<p>Asking "Is it worth doing even if it fails?" encourages healthy things
like sports, discourages antisocial things like fraud, and provides a
positive direction to move everything in between. I think it's a
useful question.</p>    
    ]]></description>
<link>http://planspace.org/20181204-worth_doing_even_if_it_fails/</link>
<guid>http://planspace.org/20181204-worth_doing_even_if_it_fails/</guid>
<pubDate>Tue, 04 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>How To Invent Everything</title>
<description><![CDATA[

<p><a href="http://www.howtoinventeverything.com/">How To Invent Everything</a> is like <a href="https://www.billnye.com/">Bill Nye</a> writing how-tos for <a href="https://www.youtube.com/channel/UCAL3JXZSzSm8AlZyD3nQdBA">Primitive Technology</a>, in the distinctive voice of <a href="https://www.qwantz.com/">Dinosaur Comics</a> writer and actual author <a href="https://en.wikipedia.org/wiki/Ryan_North">Ryan North</a>. It's fun!</p>
<blockquote>
<p>"Scientists are often seen as turbonerds, but the philosophical foundations of science are actually those of pure punk-rock anarchy: never respect authority, never take anyone's word on anything, and test all the things you <em>think</em> you know to confirm or deny them for yourself." (page 34)</p>
</blockquote>
<p>The book is a compendium of "<a href="http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html">You</a> <a href="http://www.ams.org/notices/200601/fea-chow.pdf">could</a> <a href="https://reprog.wordpress.com/2010/05/13/you-could-have-invented-git-and-maybe-you-already-have/">have</a> <a href="http://luis.impa.br/aulas/lectures/inventingTopology.pdf">invented</a>" <a href="http://blog.ezyang.com/2012/02/anatomy-of-you-could-have-invented/">articles</a> for everything from language to selective breeding to steel to the <a href="https://en.wikipedia.org/wiki/Pelton_wheel">pelton wheel</a> to computers. A lot of fun (and some dangerous) science fair projects could be adopted from these pages. Do you know how to make your own charcoal? Or that clover is a <a href="https://en.wikipedia.org/wiki/Legume">legume</a> useful in crop rotation? This book is chockablock with <a href="https://minecraft.gamepedia.com/Recipe">Minecraft recipes</a> for the real world.</p>
<p>The "survival guide for the stranded time traveler" framing is amusing, but it's historical details (like the story of the <a href="https://en.wikipedia.org/wiki/Semmelweis_reflex">Semmelweis reflex</a>) that really add interest. There are a few <a href="http://www.howtoinventeverything.com/errata">errata</a> but overall this is the most easily recommendable book I've read in some time.</p>
<p><img alt="How To Invent Everything cover" src="cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181125-how_to_invent_everything/</link>
<guid>http://planspace.org/20181125-how_to_invent_everything/</guid>
<pubDate>Sun, 25 Nov 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Machine learning / Deep learning</title>
<description><![CDATA[

<p>I participated in a
<a href="https://www.meetup.com/TechinMotionDC/events/252910659/">panel</a> on
deep learning yesterday, so I thought about some of the introductory
questions and wrote out my responses.</p>
<hr>
<h3>How would you define machine learning?</h3>
<p>Machine learning is programming with examples instead of instructions.</p>
<p>It's a way to get computers to do something: in that sense it's
"programming." It's been called
<a href="https://medium.com/@karpathy/software-2-0-a64152b37c35">Software 2.0</a>.</p>
<p>Usually we program by writing explicit instructions, step by step.
With machine learning, we program using examples, by which I mean
data.</p>
<p>Say that programming is writing functions, which is basically right. A
function takes some input and returns some output.</p>
<p><a href="https://en.wikipedia.org/wiki/Higher-order_programming">Higher-order programming</a>
involves functions as the inputs and outputs of other functions, and
machine learning is a specific kind of higher-order programming.</p>
<p>Machine learning is done by a function that takes data (examples) as
input and produces a function as output. Running that higher-order
function is when machine learning takes place. We say that we learn a
function.</p>
<p>Sometimes learned functions are called models, and learning them is
sometimes called training or fitting.</p>
<p>There can also be functions that learn functions that learn functions,
as in <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search</a>,
which involves machine learning at multiple levels.</p>
<h3>What is deep learning? What is the difference between deep learning and machine learning?</h3>
<p>Deep learning is machine learning where the function you're learning
has a particular style of implementation.</p>
<p>In deep learning, the learned function has an internal implementation
involving one or more intermediate representations.</p>
<p>Aspects of these intermediate stages are sometimes called layers or
activations.</p>
<p>The number of layers is how we measure the depth of the model.</p>
<p>I don't care to argue about how many layers you "need" to be "deep."
As far as I'm concerned if there's more than zero intermediate
representations, you can call it deep if you want to. But it's common
to have five, ten, twenty, or more layers.</p>
<p>Usually, but not always, when people talk about deep learning they're
also talking about neural nets, and usually using
<a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>.</p>
<p>The reason people care about deep learning is that it performs better
than many machine learning alternatives on a lot of hard problems
typically involving high-dimensional inputs, like images, audio, and
text.</p>
<hr>
<p>Cassie Kozyrkov's <a href="https://www.youtube.com/watch?v=iLu9XyZ55oI">talk</a>
has a number of useful metaphors for machine learning in practice and
I found it helpful for thinking about how to explain things to a
general audience.</p>    
    ]]></description>
<link>http://planspace.org/20180823-machine_learning_deep_learning/</link>
<guid>http://planspace.org/20180823-machine_learning_deep_learning/</guid>
<pubDate>Thu, 23 Aug 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Land of Lisp</title>
<description><![CDATA[

<p><a href="http://landoflisp.com/">Land of Lisp</a> is an irresistibly fun <a href="https://common-lisp.net/">Common Lisp</a> book. The cartoon illustrations and "Learn to program in Lisp, one game at a time" subtitle don't fully suggest the depth of the content. Recursion is in Chapter 2, you write both <a href="https://www.graphviz.org/">Graphviz</a> and raw <a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> for graphics, you code up a web server from scratch, and you write your own lazy evaluation and multiple <a href="https://en.wikipedia.org/wiki/Domain-specific_language">DSL</a>s with macros before the end of the book. With that much material, some glitches sneak in, but it's still a very nice book.</p>
<p><img alt="Land of Lisp cover" src="land_of_lisp-cover.jpg"></p>
<p>I've done a little bit with <a href="https://en.wikipedia.org/wiki/Emacs_Lisp">Emacs Lisp</a> and <a href="https://clojure.org/">Clojure</a>, but this was my first time learning anything much about Common Lisp. Thinking about languages through contrast (<a href="https://planspace.org/20141129-ruby_symbols_are_not_lisp_symbols/">one old example of mine</a>) is fun, and there's a good deal of that in Land of Lisp.</p>
<p><img alt="Haskell/Lisp map" src="map.png"></p>
<p>I really liked some of the organization/presentation of material. The Periodic Table of the Loop Macro on pages 200 and 201 was one striking example.</p>
<p><img alt="Periodic Table of the Loop Macro" src="loop_table.png"></p>
<p>The way the author <a href="https://en.wikipedia.org/wiki/Memoization">memoized</a> functions, by first defining the un-memoized version and then defining the memoized version with the same name, capturing the original function by <a href="https://en.wikipedia.org/wiki/Closure_(computer_programming)">closure</a>, really tickled me.</p>
<p>There was also reference to a Haskel functional-style "<a href="http://happs.org/">database</a>" that I thought was interesting. Maybe sort of like <a href="https://www.datomic.com/">Datomic</a>? And there were occasional references to other neat things; I may now read <a href="https://cs.brown.edu/~sk/Publications/Papers/Published/khmgpf-impl-use-plt-web-server-journal/">Implementation and Use of the PLT Scheme Web Server</a>, for example.</p>
<p>I've thought about good ways to learn to program a little bit, even <a href="https://planspace.org/20150101-learn_to_code_with_emacs/">suggesting</a> that learning Emacs/Lisp might be good. Then at least you can use Emacs Lisp to customize your editor. I don't think I know of any current Common Lisp projects.</p>
<p>It's nice when learning (and working with) a language to have a handy documentation system. Emacs Lisp <a href="https://planspace.org/20141230-use_info_in_emacs/">has this</a> in spades. In Python, I think beginners should know about, for example, <code>help()</code> and <code>dir()</code> very early. There's nothing like this covered in Land of Lisp. The only documentation referenced, as I recall, was the <a href="http://www.lispworks.com/documentation/HyperSpec/Front/">Common Lisp HyperSpec</a>, which doesn't seem very beginner-friendly.</p>
<p>The most extremely anti-didactic bit of the book, however, has to be on page 382: "The only way to understand [these functions] is to stare at them for a long time." I think non-explanations like this are not likely to be helpful to anyone.</p>
<p>Then there are little glitches in the book. I suspect the book kept expanding and editors were overwhelmed:</p>
<ul>
<li>The intro to Chapter 7 claims to include things that don't start until Chapter 8.</li>
<li><code>sexp</code> is used in code but never <a href="https://en.wikipedia.org/wiki/S-expression">explained</a>, which I don't think is a good idea.</li>
<li><code>pushnew</code> is used on page 149 but not explained until page 368.</li>
<li>On page 445 it says "Restarts are discussed in Chapter 14," but they aren't.</li>
<li>On page 450 a reference to Chapter 16 should be to Chapter 17.</li>
</ul>
<p>In the end the book can't cover everything. For example, the <a href="https://en.wikipedia.org/wiki/Common_Lisp_Object_System">Common Lisp Object System</a> is praised but not explained in enough detail for me to know why it's so great. That was particularly frustrating because I'd still love to see more about managing large projects in languages like Common Lisp or Clojure (with OO concepts or not).</p>
<p>I liked the book a lot; it's great recreational reading for language breadth.</p>    
    ]]></description>
<link>http://planspace.org/20180812-land_of_lisp/</link>
<guid>http://planspace.org/20180812-land_of_lisp/</guid>
<pubDate>Sun, 12 Aug 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Broken Earth trilogy</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/N._K._Jemisin">N. K. Jemisin</a>'s three-book tale is both enjoyable and sufficiently deep to support a lot of conversation. The first <a href="https://en.wikipedia.org/wiki/Hugo_Award">Hugo</a>-winning book, <a href="https://en.wikipedia.org/wiki/The_Fifth_Season_(novel)">The Fifth Season</a>, was the first book I read for a book club. I couldn't stop, and finished <a href="https://en.wikipedia.org/wiki/The_Obelisk_Gate">The Obelisk Gate</a> and <a href="https://en.wikipedia.org/wiki/The_Stone_Sky">The Stone Sky</a> as well.</p>
<p><img alt="Broken Earth covers" src="BrokenEarth.jpg"></p>
<p>Here some aspects of the trilogy that I found interesting:</p>
<ul>
<li>Revolution vs. gradual change or "working within the system": What
   is possible, and when is revolutionary change justified/necessary?</li>
<li>Science fiction vs. fantasy (possibly philosophy of science?):
   Arthur C. Clarke's "Any sufficiently advanced technology is
   indistinguishable from magic" is relevant, but also, what if some
   sufficiently advanced science turns out to be very different from
   what the current popular scientific view thinks is likely?</li>
<li>Gift vs. curse: Having special powers isn't always as great as in
   Harry Potter; this is a little more like X-Men, but more extreme.</li>
<li>Race and slavery: There's a lot to think about; one aspect is an
   R-word that maps pretty directly to the real-world N-word.</li>
</ul>
<p>I noted some quotes from each book, though I started saving more after
the first volume:</p>
<h3>The Fifth Season</h3>
<blockquote>
<p>"It's a gift if it makes us better. It's a curse if we let it
destroy us." (p. 418)</p>
</blockquote>
<h3>The Obelisk Gate</h3>
<blockquote>
<p>"He's never hurt you, though. The world has, but not him. Maybe the
world deserved to be destroyed." (p. 31)</p>
<p>"Something else, neither flesh nor stone. Something immaterial, and
yet it is there for you to perceive. It glimmers in threads strung
between the bits of him, crossing itself in lattices, shifting
constantly. A... tension? An energy, shining and streaming.
Potential. Intention." (p. 101)</p>
<p>"He showed you-again and again, unrelentingly, he would not let you
pretend otherwise-that if obedience did not make one feel safe from
the Guardians or the nodes or the lynchings or the breeding or the
disrespect, then what was the point? The game was too rigged to
bother playing." (p. 159)</p>
<p>"Like those weird cults that crop up from time to time. I heard of one that asks an old man in the sky to keep them alive every time they go to sleep. People need to believe there's more to the world than there is." (p. 166)</p>
<p>"It isn't fair. You just want your life to matter."</p>
</blockquote>
<h3>The Stone Sky</h3>
<blockquote>
<p>'When a slave rebels, it is nothing much to the people who read
about it later. Just thin words on thinner paper worn finer by the
friction of history. ("So you were slaves, so what?" they whisper.
Like it's nothing.)' (p. 7)</p>
<p>"People who are not tuners can perceive magic only in rudimentary
ways; they use machines and instruments to do what is natural for
us." (p. 107)</p>
<p>"But breathing doesn't always mean living, and maybe... maybe
genocide doesn't always leave bodies." (p. 179)</p>
<p>'"Because it must be his choice, first." Harder voice here. A
reprimand. You flinch. "More importantly, because we are fragile at
the beginning, like all new creatures. It takes centuries for us,
the who of us, to...cool. Even the slightest of pressures-like you,
demanding that he fit himself to your needs rather than his own-can
damage the final shape of his personality."' (p. 282)</p>
<p>'"Orogeny," I say, sharply so she will pay attention, "was never the
only way to change the world."' (p. 396)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20180723-broken_earth_trilogy/</link>
<guid>http://planspace.org/20180723-broken_earth_trilogy/</guid>
<pubDate>Mon, 23 Jul 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Geolocation precision by digit</title>
<description><![CDATA[

<p>Latitude and longitude define increasingly fine grids over the Earth
as you add digits after the decimal points, but no matter how fine the
grid, most things still aren't at points of intersection. How far you
can get from a closest point of intersection as the grid gets finer is
a way to describe precision. This precision can be compared to
measurement accuracy and the sizes of objects of interest.</p>
<p>The Earth is not perfectly spherical, and the size of one degree can
vary, especially for longitude. To get an upper bound on error, we'll
use the circumference of the Earth at the equator, around 40,075 km,
to start calculations. With correct rounding, maximum error in one
direction is half a unit. That maximum error could happen in two
directions. Using the usual Pythagorean theorem on a plane for the
worst-case diagonal distance is easy and will be slightly higher than
the actual distance along the Earth's surface, assuming no change in
elevation, so the bound won't even be as tight as it could be. Also
I'll round up.</p>
<hr>
<h3>0.&#176; lat, 0.&#176; lon: 80 km</h3>
<p>Every point that rounds, to the nearest degree, to a particular
latitude and longitude, is within 80 kilometers of the exact
intersection of those lines of latitude and longitude.</p>
<h3>0.1&#176; lat, 0.1&#176; lon: 8 km</h3>
<p>With one decimal place, you're within 8 km. This might already be
precise enough to describe a large city.</p>
<h3>0&#176; 59' lat, 0&#176; 59' lon: 1.4 km</h3>
<p>Any point that is the same to the nearest minute in latitude and
longitude is within 1.4 km of that position exactly.</p>
<p>I recommend sticking with the decimal system.</p>
<h3>0.12&#176; lat, 0.12&#176; lon: 800 m</h3>
<p>Two decimal places of precision gets you within 800 meters.</p>
<h3>0.123&#176; lat, 0.123&#176; lon: 80 m</h3>
<p>Three decimal places will always be within 80 m of the point
described. This is specific enough already for the smallest towns and
even large buildings.</p>
<h3>0&#176; 59' 59" lat, 0&#176; 59&#176; 59" lon: 22 m</h3>
<p>When you have a location to the nearest second, it's within 22 m of
that exact location.</p>
<p>This is the annoying format that you get, for example, from the Apple
Compass app. If your phone's location accuracy
<a href="https://www.gps.gov/systems/gps/performance/accuracy/">is within 5 m</a>,
then you likely have your location correct to the nearest second,
though you may not. (Your phone reports more precise location values
internally, as you can see on your map.)</p>
<h3>0.1234&#176; lat, 0.1234&#176; lon: 8 m</h3>
<p>Four decimal places is enough to specify pretty much any landmark.</p>
<h3>0.12345&#176; lat, 0.12345&#176; lon: 80 cm</h3>
<p>You probably consider everything within your five-digit
latitude/longitude location your personal space. But also, at five
decimal places of precision, my best guess is that error in the
accuracy of your GPS is around the same size or greater than the error
from using this many digits.</p>
<h3>0.123456&#176; lat, 0.123456&#176; lon: 8 cm</h3>
<p>You don't need six digits of precision for the locations of buildings;
you need six digits of precision to describe where each plate is set
on the dinner table.</p>
<p>If you can measure your location as accurately as this, you have
specialized equipment and probably know more about these kinds of
error issues than I do.</p>
<h3>0.1234567&#176; lat, 0.1234567&#176; lon: &lt; 1 cm</h3>
<p>With seven digits of precision you can differentiate one marble from
the marble next to it.</p>
<h3>0.12345678&#176; lat, 0.12345678&#176; lon: &lt; 1 mm</h3>
<p>Is latitude and longitude really your best choice of coordinate
system?</p>
<hr>
<p>You can check out my calculations in <a href="latlon.ipynb">latlon.ipynb</a>.</p>    
    ]]></description>
<link>http://planspace.org/20180719-geolocation_precision_by_digit/</link>
<guid>http://planspace.org/20180719-geolocation_precision_by_digit/</guid>
<pubDate>Thu, 19 Jul 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Portfolios of the Poor</title>
<description><![CDATA[

<blockquote>
<p>"One of the biggest challenges of living on two dollars a day is that it doesn't always come." (page 181)</p>
</blockquote>
<p><a href="http://www.portfoliosofthepoor.com/"><img alt="Portfolios of the Poor (cover)" src="cover.jpg"></a></p>
<blockquote>
<p>"Convenience, flexibility, and reliability are at the heart of building workable financial tools for the poor, and are key to understanding the economic lives of poor households more broadly. Just as we found no households truly living hand to mouth&#8211;even among the very poor&#8211;we found no households so absolutely limited in their resources that price was the overriding determinant of financial choices." (page 153)</p>
</blockquote>
<p><a href="http://www.portfoliosofthepoor.com/">Portfolios of the Poor</a> is a 2009 book referenced in <a href="/20180218-scarcity_by_sendhil_mullainathan_and_eldar_shafir/">Scarcity</a>. While <em>Scarcity</em> positions itself as explaining why poor people make bad financial decisions, <em>Portfolios</em> explores without judgment how people who are poor manage their money: <em>Portfolios</em> doesn't necessarily think a high-interest loan is "bad" if it helps someone solve a pressing problem.</p>
<p>The source data are <a href="http://financialdiaries.com/">financial diaries</a>, collected in a kind of ethnographic process in Bangladesh, India, and <a href="https://www.datafirst.uct.ac.za/dataportal/index.php/catalog/2">South Africa</a>. They follow around 250 families for a year or more each, collecting detailed information that frequently surfaces as personal anecdotes.</p>
<p>You can get a quick summary by reading Chapter Seven, where they present their recommendations (based on observations) in just eleven pages, with this structure:</p>
<ul>
<li>Opportunities: Cash-flow management, building savings, loans for all uses</li>
<li>Principles: Reliability, convenience, flexibility, structure</li>
</ul>
<p>Chapter One is a more complete overview of the book and its methods. Chapter two focuses on cash flow, and the "triple whammy" of low income, irregular income, and lack of really helpful financial instruments. Chapter three is on dealing with risk, chapter four is on building up usefully large sums of money, and chapter five is on the sometimes strange world of pricing financial instruments for the poor.</p>
<p>One interesting service involves having money collected and kept over time for a fee, which seems like a negative interest rate. But in a repeated setting, the difference between saving and borrowing can blur.</p>
<blockquote>
<p>"In Vijaywada, there are customers who simply didn't distinguish between deposit collectors and moneylenders, so similar is the service provided. Both offered repeated money-accumulation cycles for a fee." (page 151)</p>
</blockquote>
<p>The authors explore this and other cases in which the rich-world focus on interest rates may not be the most appropriate, as costs often behave more like fees. Interest is rarely compounded, for example (page 136), and terms are not always as strict as they may seem, though this doesn't necessarily benefit everyone in the same ways.</p>
<blockquote>
<p>"...repayment delays are factored into the nominal price, with the effect that the customer who repays on time pays the highest price. This inverted pattern of incentives can be seen as one of the more unsatisfactory aspects of informal loan finance." (page 141)</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Microfinance">Microfinance</a> is not the largest money-mover in the diaries, but the authors are interested in it, and in improving it. Especially in Bangladesh many diary households were working with microfinance providers like <a href="https://en.wikipedia.org/wiki/Grameen_Bank">Grameen Bank</a>. Author Stuart Rutherford founded <a href="http://www.safesave.org/">SafeSave</a> which is now part of <a href="https://en.wikipedia.org/wiki/BRAC_(organization)">BRAC</a>, and they also mention <a href="https://en.wikipedia.org/wiki/Association_for_Social_Advancement">ASA</a>. The main microfinance recommendation in the book is probably that loans shouldn't be only for micro-entrepreneurial purposes: people have a range of needs that loans can help with.</p>
<p><em>Portfolios</em> was talking about <a href="https://en.wikipedia.org/wiki/Behavioral_economics">behavioral economics</a> before it was cool. It was interesting hearing about rural microfinance administered through weekly group meetings, sometimes with the risks of group liability. They also describe some people's preferences for borrowing on interest rather than spending from savings because of the attendant motivation to pay back quickly, and ideas for commitment savings devices.</p>
<p>There were a lot of things I wasn't familiar with, and the authors provide a helpful glossary in a table footnote on pages 207-208.</p>
<ul>
<li><em>Chit funds</em> are a government regulated form of RoSCA (see below) found only in India.</li>
<li><em>Pro-poor insurers</em> are found only in Bangladesh: they adapt the methods of NGO microcredit banks to offer endowments (savings plans linked to life insurance) to the poor, and to recycle the premiums as loans to the poor.</li>
<li><em>Saving-up clubs</em> are clubs where participants save together toward a particular event, such as a religious festival: they do not recycle the fund as loans.</li>
<li><em>RoSCAs</em>, or rotating savings and credit associations, are a form of savings-and-loan club in which a fixed number of members pay a fixed sum into a pool at a fixed interval, and on each occasion one of the members takes the whole of the pool (there are many variants on this theme).</li>
<li><em>ASCAs</em>, or accumulating savings and credit associations differ from RoSCAs in that regularly depositing members accumulate their fund and lend it out when required to one or more of their members.</li>
<li><em>Burial societies</em>, as found in South Africa, are informal clubs where members insure each other against funeral costs.</li>
<li><em>Stokvels</em> and <em>umgalelos</em> are different names for South African RoSCAs and ASCAs.</li>
<li><em>Salary timing</em> is an agreement with others to share salaries as they arrive.</li>
<li><em>Reciprocal interest-free lending and borrowing</em> are loans between friends, neighbors or family members that are interest-free but bear the implied obligation to reciprocate at some time in the future.</li>
<li><em>Mahajan</em> and <em>mashonisa</em> are South Asian and South African terms for local money-lenders who lend for profit.</li>
<li><em>Moneyguarding</em> is having someone look after your money for you, often a relative, neighbor, employer or shopkeeper.</li>
<li><em>Remitting cash to the village</em> is often practiced by town dwellers as a way of saving and of building up assets in the home village. Note that in the South African study, we treated remitting cash to the village as an expense rather than a financial instrument because we knew that the households receiving the remittances were using it for their own needs rather than saving it. In Bangladesh and India, it was more often (but not always) the case that the money was invested in some way: in land or housing or lent out, for example, and we have treated remittances as savings.</li>
</ul>
<p>One of the interesting RoSCA variants is the auction RoSCA, whereby members effectively achieve different interest rates depending on how quickly they want it to be their turn for the lump payment.</p>
<p>There were also terms that aren't so obscure but were still not familiar to me.</p>
<ul>
<li><a href="https://www.investopedia.com/terms/c/credit_life_insurance.asp">Credit Life Insurance</a><ul>
<li>Pays off your debt if you die.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Endowment_policy">Life-Endowment Savings</a><ul>
<li>Pays a lump sum after a term, or if you die.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Debenture">Debenture</a><ul>
<li>Basically a bond; maybe (even) less asset-secured.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Net_present_value">Net Present Value</a> (NPV)<ul>
<li>For a fixed period, positive if we beat a fixed compounding interest rate, negative if not: "Am I better off over the next three years buying this investment, or putting that money in the bank at 1.5% interest? (And by how much?)"</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Internal_rate_of_return">Internal Rate of Return</a> (IRR)<ul>
<li>Compounding interest rate equivalent of an investment: "If this factory were a bank account, what would it's APR be?"</li>
</ul>
</li>
</ul>
<p>It's pretty far from the purpose of the book, but I did appreciate the excuse to finally understand IRR a little better, along with everything else.</p>
<!-- The book has only a few typographic problems. For example, incorrect parentheses break the equation in the sixth endnote to chapter five, on page 257. -->

<blockquote>
<p>"In human affairs, incremental improvements can provide the basis for broader changes." (page 64)</p>
</blockquote>
<p><em>Thanks to the <a href="https://www.dclibrary.org/">DC Public Library</a> for supporting my access to this book.</em></p>    
    ]]></description>
<link>http://planspace.org/20180429-portfolios_of_the_poor/</link>
<guid>http://planspace.org/20180429-portfolios_of_the_poor/</guid>
<pubDate>Sun, 29 Apr 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Consider Phlebas by Iain M. Banks</title>
<description><![CDATA[

<p>Amazon <a href="http://variety.com/2018/tv/news/consider-phlebas-iain-m-banks-amazon-novel-dennis-kelly-1202706327/">is</a> making a sci-fi TV show based on the 1987 <a href="https://en.wikipedia.org/wiki/Consider_Phlebas">Consider Phlebas</a>. Focusing on certain cultural aspects would be interesting, or it could be just another space opera.</p>
<p>There's a war between <a href="https://en.wikipedia.org/wiki/The_Culture">The Culture</a> and the <a href="https://en.wikipedia.org/wiki/List_of_civilisations_in_the_Culture_series#Idirans">Idirans</a>. They could stand in for modern American society: The Culture is liberal, the Idirans are conservative. In one paragraph Banks uses the word "contempt" eight times, which reminded me of <a href="https://www.amazon.com/Politics-Resentment-Consciousness-Wisconsin-American/dp/022634911X">The Politics of Resentment</a>. The comparison isn't perfect, but it could be explored.</p>
<p>Unfortunately, the bulk of the 514 pages is given to the details of who punched who, and how. Some ideas infiltrate a chapter about a cannibalistic cult, or poker for human lives. The chapters, especially early ones, are very episodic, and I could imagine Amazon following the material closely if they wanted to.</p>
<p>The Culture has sentient artificial Minds that participate in and even guide their society. This is the kind of thing I was hoping to read about, but it's largely swamped by swashbuckling. It would be neat to see Amazon produce a slightly more conceptual adventure based on the source material.</p>
<p>Why the weird title? It's from <a href="https://en.wikipedia.org/wiki/T._S._Eliot">T. S. Eliot</a>'s poem, <a href="https://en.wikipedia.org/wiki/The_Waste_Land">The Waste Land</a>. The relevant section:</p>
<hr>
<pre>
IV. Death by Water

Phlebas the Phoenician, a fortnight dead,
Forgot the cry of gulls, and the deep sea swell
And the profit and loss.
                                   A current under sea
Picked his bones in whispers. As he rose and fell
He passed the stages of his age and youth
Entering the whirlpool.
                                  Gentile or Jew
O you who turn the wheel and look to windward,
Consider Phlebas, who was once handsome and tall as you.
</pre>

<hr>
<p><a href="https://www.poetryfoundation.org/poems/47311/the-waste-land">The Waste Land</a> overflows with allusion; just to read the words as written requires six languages or so. <a href="https://en.wikipedia.org/wiki/Consider_Phlebas">Consider Phlebas</a>, in comparison, is much less dense.</p>
<p><img alt="Consider Phlebas cover" src="consider_phlebas.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180312-consider_phlebas_by_iain_m_banks/</link>
<guid>http://planspace.org/20180312-consider_phlebas_by_iain_m_banks/</guid>
<pubDate>Mon, 12 Mar 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Scarcity by Sendhil Mullainathan and Eldar Shafir</title>
<description><![CDATA[

<p>The promise of <a href="https://scholar.harvard.edu/sendhil/scarcity">Scarcity</a> is an uplifting unified <a href="/20170825-characteristics_of_good_theories/">theory</a> that "accounts for a diverse set of phenomena" (page 162) and has social policy implications. It's a research-based rationale for compassion rather than blame, encapsulated on page 144:</p>
<blockquote>
<p>'Scarcity creates a mindset that perpetuates scarcity. If all this seems bleak, consider the alternative viewpoint: the poor are poor because they lack skills. The lonely are lonely because they are unlikable; dieters lack willpower; and the busy are busy because they lack the capacity to organize their lives. In this alternative view, scarcity is the consequence of deep personal problems, very difficult to change.</p>
<p>'The scarcity mindset, in contrast, is a contextual outcome, more open to remedies. Rather than a personal trait, it is the outcome of environmental conditions brought on by scarcity itself, conditions that can often be managed.'</p>
</blockquote>
<p>The book expands and applies some ideas that were present in <a href="/2011/12/17/selections-from-and-thoughts-on/">Thinking, Fast and Slow</a>:</p>
<blockquote>
<p>'Imagine that you are asked to retain a list of seven digits for a minute or two. While your attention is focused on the digits, you are offered a choice between two desserts: a sinful chocolate cake and a virtuous fruit salad. The evidence suggests that you would be more likely to select the tempting chocolate cake when your mind is loaded with digits.'</p>
</blockquote>
<p>The book is about how scarcity, like (or as) a distracting mental task, makes it harder to make good decisions, which makes it harder to escape scarcity.</p>
<p>The biggest new contributions are probably from the authors' <a href="http://www.sciencemag.org/">Science</a> paper, <a href="http://science.sciencemag.org/content/341/6149/976.long">Poverty Impedes Cognitive Function</a>. They found, for example, that farmers did better on an IQ test shortly after being paid for their yearly harvest, compared to before.</p>
<p>The authors grant that some scarcity, like shortness of time before a deadline, can improve performance by increasing focus. They call this the "focus dividend." But focusing can mean other things are neglected; they speak of "tunneling" and a "bandwidth tax."</p>
<p>They praise the "slack" time and money that some people enjoy. If you're operating without slack, a surprise can do a lot of damage, for example starting a downward spiral of payday loans. If you don't have slack you also need to really trade off in making decisions, which might prevent you from making certain cognitive errors of scale, but requires bandwidth.</p>
<p>Some of the organizational applications remind me of <a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month">The Mythical Man-Month</a>. For example, they describe a NASA project that was behind, so they skipped integration tests, and eventually failed catastrophically.</p>
<blockquote>
<p>'The truly efficient laborer will be found not to crowd his day with work, but will saunter to his task surrounded by a wide halo of ease and leisure.' (Henry David Thoreau, quoted page 194)</p>
</blockquote>
<p>I particularly appreciated that the authors made a noticeable effort to anticipate and address readers' possible objections, but I was already largely in agreement, so it may have been hard for me to judge their success in this regard. They also got me curious about <a href="http://www.portfoliosofthepoor.com/">Portfolios of the Poor</a>, which seems like it could be a good window into how the global poor live. Mullainathan and Shafir inspire one to understand more.</p>
<p><img alt="Scarcity cover" src="scarcity_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180218-scarcity_by_sendhil_mullainathan_and_eldar_shafir/</link>
<guid>http://planspace.org/20180218-scarcity_by_sendhil_mullainathan_and_eldar_shafir/</guid>
<pubDate>Sun, 18 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Failure to replicate Schwartz-Ziv and Tishby</title>
<description><![CDATA[

<p><a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a> didn't appear at any conferences, to my knowledge, but it still built up some <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">buzz</a>. It has been difficult to replicate, for both <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/">bloggers</a> and <a href="https://openreview.net/forum?id=ry_WPG-A-">academics</a>. I attempted to replicate some aspects, and emailed the authors with the message below in an attempt to resolve difficulties. As there has been no response after many weeks, I present the below as an open letter. I hope it can serve as a reference for others who might try to explore these ideas.</p>
<hr>
<p>Hi Ravid! I was intrigued by your paper, <a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a>, so I wanted to replicate your results. I was particularly interested in your Figure 4; I thought that if I could see these kinds of patterns, maybe they could be used to identify when a network is well-trained, for example.</p>
<p>Here is the figure I mean:</p>
<p><img alt="figure 4" src="img/figure_4.png"></p>
<p>I started by trying to work with a simple ReLU network with two hidden layers, on MNIST, and what I found was that the sizes of gradients decrease through training, and so do differences or variances of those gradients. I couldn't see two distinct phases, or indeed any interesting crossing of two metrics.</p>
<p>At first, I wasn't sure about how to interpret the "mean" and "standard deviation" of your Figure 4. So I was happy to find your <a href="https://github.com/ravidziv/IDNNs">code</a>, which I've been using at the 2017-11-02 <a href="https://github.com/ravidziv/IDNNs/tree/c4abb1dad4fbb262315eb8b96eb85dc5c3e98e5c">c4abb1d commit</a>.</p>
<p>Unfortunately, using the definitions of "mean" and "standard deviation" as found in your <code>idnns/plots/plot_gradients.py</code> file, I didn't get results that matched those of your paper any better.</p>
<p>I started to use your code directly, running <code>main.py</code> with default arguments, adding only <code>-save_grads true</code> so I could then use <code>plot_gradients.py</code>. Here is the result:</p>
<p><img alt="default mean std" src="img/default_mean_std.png"></p>
<p>As you can see, there are differences from your published Figure 4.</p>
<p>Can you help me to replicate your results? What should I be doing differently? I would appreciate your feedback. I also posted comments about my process of trying to understand your work on a <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/">"severely theoretical" blog post</a>; if you would like to add public context there as well it would also be appreciated.</p>
<p>One thing that I noticed is that in your published Figure 4, one of the solid "mean" lines crosses the others. With the current calculation of "mean" in your code, in which the value for a layer is the sum of an always-positive calculation on each layer up to and including itself, it doesn't seem possible for lines to ever cross. Does this shine any light on what's going on?</p>
<p>Another thing I noticed is that the caption for your published Figure 4 says "values are normalized by the L2 norms of the weights for each layer" but in the current code, I don't see this happening. Is this perhaps part of what's going on?</p>
<p>I also noticed that the y axis labels in the plot don't seem to correspond to the values being plotted. For example, the first (leftmost) value for the last layer's "standard deviation" is calculated by your code as 0.49 (I check by inserting <code>print</code> messages) but it appears in the plot as less than 0.1. I think this issue doesn't affect the shapes of the curves but only the labeling, as far as I can tell.</p>
<p>More generally, is the cumulative sum approach to calculating statistics over layers appropriate? It strikes me that the pattern of the first layer dominates the plotted results, and it's then difficult to see what's happening uniquely in the higher layers.</p>
<p>I also tried running your <code>main.py</code> with the activation changed to ReLU, using <code>-activation_function 1</code>.</p>
<p><img alt="relu mean std" src="img/relu_mean_std.png"></p>
<p>The result here is mostly the same as the previous one, qualitatively, except that the gradient means seem to increase overall through the training process. Naftali Tishby has <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/#comment-183">said</a> that "results [showing non-decreasing mean gradients] are clearly incorrect as the gradients must decease eventually when the training error saturates." How should this be interpreted?</p>
<p>Let me lay out my understanding of the "mean" and "standard deviation" of your Figure 4, as based on reading your code. Both metrics are calculated for every layer and epoch, so I'll take just one as an example.</p>
<p>Let's say a network has a first layer with two weights, and there are two batches per epoch. Let's say the gradients for the first layer in the first epoch are <code>[2, 3]</code> and <code>[4, 5]</code>.</p>
<p>To get the "mean" we calculate the element-wise mean (<code>[3, 4]</code>) and take the norm, so the "mean" is <code>5</code>.</p>
<p>To get the "standard deviation" we calculate the element-wise variance (<code>[1, 1]</code>) and take the square root of its sum (<code>1.414</code>; call this the "variance") and then take the square root, so the "standard deviation" is <code>1.189</code>.</p>
<p>To show the cumulative sum aspect of the calculation, say there is a second layer with two weights and identical first-epoch gradients. Then for the second layer, first epoch, the "mean" is <code>5+5=10</code> and the "standard deviation" is the square root of the sum of the "variances" <code>sqrt(1.414+1.414)=1.682</code>.</p>
<p>This understanding is based on reading and experimenting with your <code>plot_gradients.py</code>. Does it seem right to you? Am I misinterpreting something? Is the intent other than this?</p>
<p>I would love to replicate the core result of your Figure 4, which I summarize as being "at first, gradients are large but don't vary much, and then later, gradients are small but vary a lot." What's the best way to find this result? I have seen that usually <em>both</em> gradients and the amount they vary decrease through training, but I have been unable to observe the two phases described in your paper.</p>
<p>While my main focus has been on finding the two phases by statistics on the gradients, I am also interested in the mutual information parts of your paper as well. Here are four plots generated by running your <code>main.py</code> (all default arguments) four times:</p>
<p><img alt="default info a" src="img/default_info_a.png"></p>
<p><img alt="default info b" src="img/default_info_b.png"></p>
<p><img alt="default info c" src="img/default_info_c.png"></p>
<p><img alt="default info d" src="img/default_info_d.png"></p>
<p>As you can see, I have not been able to consistently replicate the beautiful plots you have in your paper as Figure 3. In fact, the higher layers consistently fail to show the patterns described in your paper, and the lower layers behave somewhat irregularly. Am I doing something wrong?</p>
<p>One thing I noticed is that in the caption for Figure 3, the architecture is described as "input=12-10-8-6-4-2-1=output". This is somewhat different from the current default in your code, and in particular it doesn't seem possible to have a single output unit with your code in its current version, because for binary classification it uses a softmax over two output nodes. Is this an important difference? Could it be causing the difficulty in seeing compression phases?</p>
<p>I also kept the information plots from two runs of <code>main.py</code> with <code>-activation_function 1</code> for ReLU activations. Here they are:</p>
<p><img alt="relu info a" src="img/relu_info_a.png"></p>
<p><img alt="relu info b" src="img/relu_info_b.png"></p>
<p>I'm not sure if anything can be interpreted from these; maybe the code as I found it doesn't support ReLU at present? I know I've read that there are some different issues concerning binning when calculating mutual information involving ReLUs.</p>
<p>I would appreciate your insight on all these specific issues, but also more broadly: if the phenomena of your paper are general to neural networks, how should I understand the apparent difficulty in seeing similar results with other networks, code, and datasets? How should I understand the apparent difficulty even when using the same dataset, and your code?</p>
<p>Thank you very much for your help with all this,</p>
<p>~ Aaron</p>
<hr>
<p>I should add for completeness that I am aware of Saxe et al.'s Figure 1A, which seems to replicate Schwartz-Ziv and Tishby's information plane result better than Schwartz-Ziv's code does for me. I would like to know how they achieve that, but the point Saxe et al. make is that the information plane result doesn't hold for most neural nets that are used in practice, so it's kind of beside the point. I haven't seen anyone replicate the result on mean and variance of gradients during training, which is what I am most interested in.</p>    
    ]]></description>
<link>http://planspace.org/20180213-failure_to_replicate_schwartz-ziv_and_tishby/</link>
<guid>http://planspace.org/20180213-failure_to_replicate_schwartz-ziv_and_tishby/</guid>
<pubDate>Tue, 13 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Coming Jobs War by Jim Clifton</title>
<description><![CDATA[

<p>I read this book because current populist, protectionist urges seem like nationalistic competition for globally dwindling jobs, which might be called a "jobs war," and I wondered if <a href="https://en.wikipedia.org/wiki/Jim_Clifton">Jim Clifton</a>, the leader of <a href="https://en.wikipedia.org/wiki/Gallup_(company)">Gallup</a>, had unique insight on this phenomenon through the polling and research of his company. Unfortunately, the CEO's <a href="http://www.gallup.com/press/178343/gallup-press.aspx">self-published</a> book is surprisingly light on evidence, heavy on political proselytizing and advertising for Gallup. Jim Clifton is a slightly more sophisticated <a href="/20180122-why_the_rich_are_getting_richer/">Robert Kiyosaki</a>.</p>
<p>I'm surprised nobody, for the sake of Gallup's image, prevented Clifton from publishing his solution to high healthcare costs in the US, which has two prongs: (a) have people die sooner, and (b) fat-shaming. I'm not exaggerating:</p>
<blockquote>
<p>'Somebody has to tell Uncle Louie it's time to cross to the other side and go join his friends, not run doctor to doctor, accepting one low-probability procedure after another.' (pages 155-156)</p>
<p>'Unfit should mean something worse than it currently does. Unfit should mean "intervention required." Unfit should mean less employable because unfit is a cause of lower energy.' (page 160)</p>
</blockquote>
<p>Even if there are germs of reasonable ideas buried in what Clifton espouses, it's hard to get past his caustic victim-blaming, which is most extreme on healthcare, but present throughout.</p>
<p>Clifton sings the praises of entrepreneurs, with a kind of <a href="https://en.wikipedia.org/wiki/Great_man_theory">great man</a> delusion blended with borderline eugenic fatalism.</p>
<blockquote>
<p>'Some leaders even believe that anyone can be trained to be an entrepreneur. This is a mistaken assumption. Entrepreneurs have a rare gift. My estimate is that for every 1,000 people, there are only about three with the potential to develop an organization with $50 million or more in annual revenue.' (page 96)</p>
<p>'Nothing fixes bad managers, not coaching, competency training, incentives, or warnings &#8211; nothing works. A bad manager never gets better.' (page 114)</p>
</blockquote>
<p>Jim Clifton became CEO of Gallup when and because <a href="http://journalstar.com/gallup-s-clifton-dies-at-age-this-story-ran-in/article_cb499250-04a5-5852-b48f-282c047ff505.html">his father, Don Clifton</a>, bought Gallup and installed him as CEO. It's a little hard to take him seriously about individuals' abilities to be totally self-made, or when, with a perfect lack of self-awareness, he goes on to talk about the importance of fixing schools.</p>
<p>Even the core premise of the book, that "What the whole world wants is a good job," isn't well supported with evidence. I had assumed there would be some detailed analysis of polls, maybe some psychological research, but the closest thing to this didn't come very close at all:</p>
<blockquote>
<p>'Of the 7 billion people on Earth, there are 5 billion adults aged 15 and older. Of these 5 billion, 3 billion tell Gallup they work or want to work.' (page 2)</p>
</blockquote>
<p>So the strongest evidence provided that everyone wants to work is that 60% of adults say they <em>do</em> work, <em>or</em> want to work. I have no way of knowing whether "wanting to work" is separated from "wanting to have a decent standard of living." For example, how many people would want to work if they were <a href="/20180125-no_more_work_by_james_livingston/">guaranteed</a> a basic income and social services?</p>
<p>I also expected Clifton would address <a href="/20180121-janesville_an_american_story/">job loss</a> trends due to automation and so on, but there's scarcely a mention. He maintains that people just need to keep making companies, and that will make jobs. He is singularly focused on keeping US GDP higher than China's, never mind per-capita GDP or the strength of the connection between jobs and GDP.</p>
<p>Here are some questions listed in a section called "Defective employees." The questions are supposed to both help evaluate managers and "neatly factor all workers into the three categories of engaged, not engaged, and actively disengaged" (page 104):</p>
<ol>
<li>I know what is expected of me at work.</li>
<li>I have the materials and equipment I need to do my work right.</li>
<li>At work, I have the opportunity to do what I do best every day.</li>
<li>In the last seven days, I have received recognition or praise for doing good work.</li>
<li>My supervisor, or someone at work, seems to care about me as a person.</li>
<li>There is someone at work who encourages my development.</li>
<li>At work, my opinions seem to count.</li>
<li>The mission or purpose of my organization makes me feel my job is important.</li>
<li>My associates or fellow employees are committed to doing quality work.</li>
<li>I have a best friend at work.</li>
<li>In the last six months, someone at work has talked to me about my progress.</li>
<li>This last year, I have had opportunities at work to learn and grow.</li>
</ol>
<p>Most of these items map onto good advice for companies, but number ten's appearance on the list I think is symptomatic of a "<a href="https://www.wired.com/2008/06/pb-theory/">death of science</a>" problem in analysis. Why is "I have a best friend at work" on this list? I suspect the answer is that having a best friend at work is associated with being less likely to quit, and largely independent of other factors, so the item looks good statistically, despite being mostly useless to employees or managers.</p>
<p>Sometimes it's possible to see in Clifton's positions what could be genuine caring for people: a desire that everyone reach their full potential. Supporting people's strengths sounds good, and some of the ideas in the book are not all bad. But there are so many bad ideas, it's like picking through a dumpster.</p>
<p>I had thought that a book from <a href="https://en.wikipedia.org/wiki/Gallup_(company)">Gallup</a> might have interesting, relevant data and analysis. I really like the idea of using poll data to better understand the world. I had only known of Gallup's polling operation, not their management consulting side. Unfortunately, based on this book, it seems management consulting dominates their attention.</p>
<p>The only people who should read this book are those considering working with Gallup, the better to understand what they would be getting into.</p>
<p><img alt="The Coming Jobs War cover" src="coming_jobs_war_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180211-coming_jobs_war/</link>
<guid>http://planspace.org/20180211-coming_jobs_war/</guid>
<pubDate>Sun, 11 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Worm by Mark Bowden</title>
<description><![CDATA[

<p><a href="https://www.amazon.com/Worm-First-Digital-World-War/dp/0802119832">This</a> was the only book on computer security that I could find at my local library, which is a little surprising for DC. It's about the <a href="https://en.wikipedia.org/wiki/Conficker">Conficker worm</a>. It's <a href="http://www.nytimes.com/2011/10/04/books/mark-bowdens-worm-about-conficker-review.html">not</a> a great book.</p>
<p>I had hoped for more expertise. Even I know that <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> packets are not "packets of code" and that <a href="https://en.wikipedia.org/wiki/Denial-of-service_attack#Distributed_attack">DDoS</a> is not "Dedicated Denial of Service." That second error appears once, with later expansions being correct, but it is symptomatic of generally weak quality control.</p>
<p>A scan through the Wikipedia page for <a href="https://en.wikipedia.org/wiki/Conficker">Conficker</a> shows that Worm leaves out a number of interesting technical details: dictionary attacks are not mentioned at all, for example.</p>
<p>I did enjoy learning about the involvement of <a href="https://www.sri.com/">SRI</a> in connection with Conficker and with the development of the internet in general.</p>
<p>Bowden focuses on social aspects, and these mostly ring true: Lots of computers aren't updated, and hackers take advantage of them. Coordination, especially with government, can be difficult. Mischel Kwon in particular is not portrayed glowingly, but many of the cast of characters appear infantile in their own words.</p>
<p>The book succeeds in making security seem pretty inane. Microsoft makes a crummy product, so people get hacked. To stop the hacked machines from doing anything, the good guys try to buy a lot of domain names, and eventually fail. All our technology is both amazing and pitiful.</p>
<p><img alt="Worm cover" src="worm_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180128-worm_by_mark_bowden/</link>
<guid>http://planspace.org/20180128-worm_by_mark_bowden/</guid>
<pubDate>Sun, 28 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>No More Work by James Livingston</title>
<description><![CDATA[

<p><a href="http://history.rutgers.edu/faculty-directory/172-livingston-james">James Livingston</a>'s <a href="https://www.uncpress.org/book/9781469630656/no-more-work/">book</a> feels like <a href="https://en.wikipedia.org/wiki/Drunk_History">drunk history</a> about the future. To paraphrase, with the exception of "Fuck work," which appears multiple times:</p>
<blockquote>
<p>"The robots are taking all the jobs, man. Did you know that Nixon and <em>Cheney</em> were <a href="https://www.jacobinmag.com/2016/05/richard-nixon-ubi-basic-income-welfare/">trying</a> to do universal basic income way back in the 70's? They <em>knew</em>, man. Fuck work. We don't even <em>need</em> jobs. Just take Social Security from every dollar, and tax companies a little more, no problem, man... We all just have to <em>love</em> one another, that's all it is, man, can you even do that?"</p>
</blockquote>
<p>He goes on about Marx and Freud and so on a good deal more, but that's basically it.</p>
<p>As Livingston points out quite a lot, he's not the only one to say that jobs are evaporating. He distinguishes himself by not thinking it's important that people do something "meaningful" like gardening or carpentry, regardless of whether they get paid. He's okay with everybody watching TV. I gather this is similar to his take in <a href="https://www.amazon.com/Against-Thrift-Consumer-Culture-Environment/dp/0465021867">Against Thrift</a>.</p>
<p>It is an interesting hodgepodge of ideas of varying goodness. For example, that if owning people is bad, then also renting people is bad.</p>
<p>I wonder what will happen. Are jobs on the way out? Will the world go post-scarcity, like Star Trek? Will poor people just get trampled on, as seems to largely be the case so far? If you've left the labor force already because you can survive by taking advantage of some existing device, are you the vanguard of the future jobless? What will society be like?</p>
<p><img alt="No More Work cover" src="no_more_work_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180125-no_more_work_by_james_livingston/</link>
<guid>http://planspace.org/20180125-no_more_work_by_james_livingston/</guid>
<pubDate>Thu, 25 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Why the Rich are Getting Richer</title>
<description><![CDATA[

<p>The local public <a href="https://www.dclibrary.org/westend">library</a> reopened, and this <a href="https://www.amazon.com/Why-Rich-Are-Getting-Richer/dp/1612680887/">title</a> caught my eye. It turned out to be the opposite of what I expected: a window into Trump philosophy.</p>
<p><a href="https://en.wikipedia.org/wiki/Robert_Kiyosaki#Criticism_and_controversy">Robert Kiyosaki</a>, also author of <a href="https://en.wikipedia.org/wiki/Rich_Dad_Poor_Dad#Criticism">Rich Dad Poor Dad</a>, is a Trump man. Their names appear together on <a href="https://en.wikipedia.org/wiki/Why_We_Want_You_to_Be_Rich">two</a> <a href="https://en.wikipedia.org/wiki/Midas_Touch_(book)">books</a>, and they have ideas that don't come naturally to me.</p>
<p>For example, Kiyosaki doesn't like <a href="https://en.wikipedia.org/wiki/Federal_Reserve_System">the Fed</a> or taxes. He thinks "Not paying taxes is patriotic." He also says "Taxes Are Fair." He explains that rich people pay little in taxes because "He who has the gold makes the rules." He also says it's because the government wisely incentivizes things the rich do. It's a little dizzying.</p>
<blockquote>
<p>"The good news is that with <em>real</em> financial education, almost everyone can be in the 10% who earn 90% of money."</p>
</blockquote>
<p>I'm reminded of Flynn's <a href="/20170821-downward_spiral_destructive_of_civic_virtue/">downward spiral destructive of civic virtue</a>. To Kiyosaki, it isn't a <em>problem</em> that the rich are getting richer, because you too can get rich, if only you buy his books, come to his seminars, invest like him. You're a smart one.</p>
<p>Kiyosaki has a generalized concern about the dollar not being backed by gold. He recommends cash flow, for example by taking out loans to become a landlord. He's repeatedly dismissive of employees&#8211;of the very idea of work&#8211;but praises job creators.</p>
<p>I wonder which of his contradictions he has in mind when he quotes Fitzgerald's praise of "the ability to hold two opposing ideas in mind at the same time" at the close of the book.</p>
<p>Maybe Kiyosaki really does believe it's wrong to help the poor in ways other than the financial education he sells. Certainly, like Trump, he's most focused on image, on selling.</p>
<p>Maybe the lesson of this book is that some people really do believe all these things.</p>
<p><img alt="Why the Rich are Getting Richer cover" src="why_the_rich_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180122-why_the_rich_are_getting_richer/</link>
<guid>http://planspace.org/20180122-why_the_rich_are_getting_richer/</guid>
<pubDate>Mon, 22 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Janesville: An American Story</title>
<description><![CDATA[

<p>Obama <a href="https://www.facebook.com/barackobama/posts/10155532677446749">recommended</a> this <a href="https://www.amazon.com/Janesville-American-Story-Amy-Goldstein/dp/1501102230">book</a> about <a href="https://en.wikipedia.org/wiki/Janesville,_Wisconsin">Janesville</a>, a town not so far from where I grew up in Wisconsin. So I read the book.</p>
<p>In 2008, the General Motors <a href="https://en.wikipedia.org/wiki/Janesville_Assembly_Plant">factory</a> in Janesville shut down. Janesville is also House speaker <a href="https://en.wikipedia.org/wiki/Paul_Ryan">Paul Ryan</a>'s home. The book follows related issues through 2013.</p>
<p>Many of the lost jobs were union jobs with relatively good pay, health care, and pensions after retirement. Jobs that came to Janesville later tended to be less generous.</p>
<p>Business leaders and politicians tried to help Janesville recover by offering incentives to businesses. They were pro-job in that sense, but anti-union.</p>
<p>Retraining was supposed to help people who lost jobs get new skills and new good jobs. But research showed that among people who lost jobs, those who retrained went on to do worse than those who didn't retrain.</p>
<p>Non-government aid organizations in Janesville were less able to help people when the economy suffered and more people needed help. Teachers faced homeless students.</p>
<p>For some time, businesses have been giving <a href="https://www.brookings.edu/research/thirteen-facts-about-wage-growth/">a decreasing share of income</a> to their workers. I wonder whether the wage growth picture is worse if also considering disappearing benefits like health insurance and pensions.</p>
<p>At the same time, some politicians want to provide fewer benefits to citizens.</p>
<p>There are some good jobs out there with good pay, but not everyone can get them. Regardless of job, people should be able to live decent lives. It would be nice to see a clear path from where we are today to that ideal.</p>
<p><img alt="Janesville cover" src="janesville_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180121-janesville_an_american_story/</link>
<guid>http://planspace.org/20180121-janesville_an_american_story/</guid>
<pubDate>Sun, 21 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Boomerang by Michael Lewis</title>
<description><![CDATA[

<p>Michael Lewis wrote <a href="https://www.amazon.com/Moneyball-Art-Winning-Unfair-Game/dp/0393324818">Moneyball</a> and <a href="https://www.amazon.com/Big-Short-Inside-Doomsday-Machine/dp/0393338827">The Big Short</a>. He also wrote <a href="https://www.amazon.com/Boomerang-Travels-New-Third-World/dp/0393343448">Boomerang: Travels in the New Third World</a>, made up of essays published first in <a href="https://www.vanityfair.com/">Vanity Fair</a>, 2009-2011.</p>
<p>Each chapter focuses on one country:</p>
<ol>
<li>Iceland: dumb young male chauvinist bankers defaulted internationally</li>
<li>Greece: dumb male chauvinist everyone stole, defaulted, lied; also monks are there</li>
<li>Ireland: dumb young male chauvinist bankers defaulted on Irish real estate</li>
<li>Germany: best economy in Europe, still pretty dumb; also lots about poop</li>
<li>USA: cities promised pensions for cops and firefighters; cities now all bankrupt</li>
</ol>
<p>There's a preface with the big shorter from The Big Short, who says that he got himself a million dollars worth of nickels and recommends investing in guns and gold.</p>
<p>Lots of people, cities, banks, and countries are in a lot of debt, which could be bad.</p>
<p><img alt="Boomerang cover" src="boomerang_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180118-boomerang_by_michael_lewis/</link>
<guid>http://planspace.org/20180118-boomerang_by_michael_lewis/</guid>
<pubDate>Thu, 18 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Man in the High Castle</title>
<description><![CDATA[

<p>There is less for Nazis to like in <a href="https://en.wikipedia.org/wiki/Philip_K._Dick">Philip K. Dick</a>'s 1962 <a href="https://en.wikipedia.org/wiki/The_Man_in_the_High_Castle">novel</a> than in the 2015 Netflix <a href="https://en.wikipedia.org/wiki/The_Man_in_the_High_Castle">show</a>.</p>
<p>In both, Nazis won World War II. Also in both the idea of alternate realities is central, so perhaps it is appropriate for the two treatments to vary. And there are different concerns in text versus video. Nevertheless, I wonder about some of the choices in the show.</p>
<p>In the show, Juliana and an emo Nazi operative largely cooperate and have a kind of will-they-won't-they romantic tension. In the book, they have bad sex and then eventually Juliana slits his throat because he's a monster.</p>
<p>In the book, Hitler is out of power, suffering the late effects of syphilis. In the show, Hitler is still in power and collects the same film strips prized by the resistance.</p>
<p>The book spends no time at all in Nazi-controlled settings, while the show luxuriates in Nazi architecture and swastikas. An American who becomes a Nazi is a loving father with a suburban family and home that could just as well appear on <a href="https://en.wikipedia.org/wiki/Leave_It_to_Beaver">Leave It to Beaver</a>.</p>
<p>The show visits a spotless Nazi school where students learn to take pride in genocide and slavery through US history. We see the disabled are killed&#8211;and the Nazi justification.</p>
<p>One reason this is all disturbing is that, in a time when <a href="http://www.wusa9.com/news/local/dc/wwii-vets-shaken-by-nazi-flags-in-charlottesville/465189398">the Nazi flag really is waved within US borders</a>, it's uncomfortably likely that some Americans could watch The Man in the High Castle while longing to make America great as it appears under Nazi control.</p>
<p>The show illustrates the horror of normalizing the horrible, but it may also contribute to such normalization. It is disturbing to consider Nazi arguments, but truly terrifying to think that modern people might take them up as their own.</p>
<p>Comparing the Netflix show to the alternate universe film strips it features, it is not clear whether it is one that helps the side of democracy or the side of fascism.</p>    
    ]]></description>
<link>http://planspace.org/20180115-man_in_high_castle/</link>
<guid>http://planspace.org/20180115-man_in_high_castle/</guid>
<pubDate>Mon, 15 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Your Life</title>
<description><![CDATA[

<p>I heard about <a href="http://designingyour.life/the-book/">Designing Your Life</a> in an <a href="https://www.npr.org/2017/08/28/546716951/you-2-0-how-silicon-valley-can-help-you-get-unstuck">episode</a> of <a href="https://en.wikipedia.org/wiki/Shankar_Vedantam">Shankar Vedantam</a>'s <a href="https://www.npr.org/series/423302056/hidden-brain">Hidden Brain</a>.  The book roughly follows a <a href="http://designingyour.life/resources-authorized/">workshop</a> format of a <a href="http://lifedesignlab.stanford.edu/dyl">Stanford class</a>. It's quick to read&#8212;slower to do&#8212;and full of fun examples and good reminders.</p>
<p><img alt="Designing Your Life" src="designing_your_life_cover.jpg"></p>
<p>There are five core recommendations:</p>
<ol>
<li>be curious (curiosity)</li>
<li>try stuff (bias to action)</li>
<li>reframe problems (reframing)</li>
<li>know it's a process (awareness)</li>
<li>ask for help (radical collaboration)</li>
</ol>
<p>The authors identify two concepts to be wary of:</p>
<ul>
<li>gravity problems: things that can't be productively addressed directly, just as eliminating the force of gravity isn't possible</li>
<li>anchor problems: things that prevent progress because of overcommitment to one issue, solution path, etc.</li>
</ul>
<p>Both of these "problems" may involve "dysfunctional beliefs" that may be better reframed.</p>
<p>The bulk of the text is around a number of <a href="http://designingyour.life/resources-authorized/">activities</a>:</p>
<ul>
<li>Evaluate your life along dimensions of love, play, work, and health.</li>
<li>Write out a "workview" and "lifeview" and think about how they interact.</li>
<li>Keep track of what you actually enjoy doing ("good time journal").</li>
<li>Use mind mapping and brainstorming.</li>
<li>Take incremental steps to learn/try/change things ("prototyping").</li>
<li>Imagine multiple possible futures ("three odyssey plans") not just one "right" path.</li>
<li>Network by meeting people ("life design interviews") don't just apply for listed jobs.</li>
<li>Log and reflect on failures to find growth areas ("failure log").</li>
</ul>
<p>The chapter on "How not to get a job" is very sensible.</p>
<p>Toward the end, the book is somewhat philosophical. It references <a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games">Finite and Infinite Games</a> and talks about a "failure immunity" point of view (positive psychology).</p>
<p>The Hidden Brain episode was called "Getting Unstuck." That describes the book's utility. The ideas are not revolutionary, but it is nice to be reminded, especially when in a rut, to be positive, think creatively, make small improvements, and try new things. The activities in the book help to operationalize these recommendations.</p>    
    ]]></description>
<link>http://planspace.org/20171229-designing_your_life/</link>
<guid>http://planspace.org/20171229-designing_your_life/</guid>
<pubDate>Fri, 29 Dec 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning</title>
<description><![CDATA[

<p><em>This is a presentation given for <a href="https://www.meetup.com/Data-Science-DC/">Data Science DC</a> on <a href="https://www.meetup.com/Data-Science-DC/events/244145151">Tuesday November 14, 2017</a>.</em></p>
<ul>
<li><em><a href="deep_rl.pdf">PDF slides</a></em></li>
<li><em><a href="deep_rl.pptx">PPTX slides</a></em></li>
</ul>
<p>Further resources up front:</p>
<ul>
<li><a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> (paper)</li>
<li>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> (blog post)</li>
<li><a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> (textbook)</li>
<li>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> (videos and slides)</li>
<li><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> (videos, slides, and labs)</li>
<li>OpenAI <a href="https://github.com/openai/gym">gym</a> / <a href="https://github.com/openai/baselines">baselines</a> (software)</li>
<li><a href="http://nationalgocenter.org/">National Go Center</a> (physical place)</li>
<li><a href="http://dc.hackandtell.org/">Hack and Tell</a> (fun meetup)</li>
</ul>
<p>The following goes through all the content of the talk:</p>
<hr>
<p>Aaron Schumacher</p>
<ul>
<li>planspace.org has these slides</li>
</ul>
<hr>
<p>Hi! I'm Aaron. All these slides, and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA). We do deep learning work for government and commercial customers. DLA is a great place to work, and one of the ways it's great is that it sends us to conferences and such things.</p>
<hr>
<p><img alt="Deep RL Bootcamp" src="img/deep_rl_bootcamp.png"></p>
<hr>
<p>DLA sent me to the first UC Berkeley <a href="https://www.deepbootcamp.io/">Deep RL Bootcamp</a> organized by Abbeel, Duan, Chen, and Karpathy. It was a great experience and it largely inspired this talk.</p>
<p>I have a separate <a href="/20170830-berkeley_deep_rl_bootcamp/">summary write-up</a> about my experience at the bootcamp, and they've since put up all the <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">videos, slides, and labs</a>, so you can see everything that was covered there.</p>
<hr>
<p><img alt="Sutton and Barto's textbook" src="img/sutton_and_barto.jpg"></p>
<hr>
<p>The other major source for this talk is Sutton and Barto's textbook, which I like a lot.</p>
<p>The picture shows <a href="https://mitpress.mit.edu/books/reinforcement-learning">the first edition</a>, which is not what you want. The second edition is <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">available free online</a>, and was last updated about a week ago (November 5, 2017).</p>
<p>Sutton and Barto are major figures in reinforcement learning, and they do not follow any <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">no original research rules</a>, making their book really fairly exciting, if you're not put off by the length (over 400 pages).</p>
<p>(The diagrams on the cover are not neural nets, but backup diagrams.)</p>
<hr>
<p>Plan</p>
<ul>
<li>applications: what<ul>
<li>theory</li>
</ul>
</li>
<li>applications: how</li>
<li>onward</li>
</ul>
<hr>
<p>The plan for today is to first mention four successful applications of reinforcement learning. Then we'll go through a core of theory. This will let us then understand pretty completely how each of those applications is achieved. Finally, we'll wrap up, looking at a few other applications and thoughts about how things are going.</p>
<hr>
<p>applications: what</p>
<hr>
<p>The applications here are all going to be games, not because reinforcement learning is only applicable to games, but because games are fun, and these examples are well known and cover a good range of techniques.</p>
<hr>
<p><img alt="backgammon" src="img/backgammon.png"></p>
<hr>
<p>First up, backgammon.</p>
<hr>
<p><img alt="Atari Breakout" src="img/breakout.png"></p>
<hr>
<p>Next, Atari. A lot of Atari games are well played by RL now. The ones shown (Video Pinball, Boxing, Breakout) are some of the ones that RL does the best on.</p>
<hr>
<p><img alt="tetris" src="img/tetris.png"></p>
<hr>
<p>I'm also including Tetris, mostly because it's a chance to talk about an interesting technique.</p>
<hr>
<p><img alt="go" src="img/go.jpg"></p>
<hr>
<p>And in the last two years, Go has been pretty much conquered by RL, so we'll talk about that.</p>
<hr>
<p>theory</p>
<hr>
<p>Let's start to build up the theory of reinforcement learning.</p>
<p>This is going to start very gradually, but I promise that by the end we'll be moving fast.</p>
<hr>
<p>Yann LeCun's cake</p>
<ul>
<li>cake: unsupervised learning</li>
<li>icing: supervised learning</li>
<li>cherry: reinforcement learning</li>
</ul>
<p><img alt="cake" src="img/cake.jpg"></p>
<hr>
<p>Yann LeCun <a href="https://drive.google.com/file/d/0BxKBnD5y2M8NREZod0tVdW5FLTQ/view">introduced</a> this cake idea for relating three main varieties of machine learning. It's largely based on one view of how much information is used at each training step.</p>
<p>I'm going to use it to build up and relate these three kinds of learning, while introducing reinforcement learning notation.</p>
<hr>
<p>unsupervised learning</p>
<ul>
<li>\( s \)</li>
</ul>
<hr>
<p>In unsupervised learning, we have a collection of states, where each individual state can be referred to with \( s \).</p>
<p>I'm using "state" without distinguishing "state" from "observation". You could also call these "examples" or "<a href="https://en.wikipedia.org/wiki/Covariate">covariates</a>" or "data points" or whatever you like.</p>
<p>The symbol "x" is commonly used.</p>
<hr>
<p>state \( s \)</p>
<ul>
<li>numbers</li>
<li>text (as numbers)</li>
<li>image (as numbers)</li>
<li>sound (as numbers)</li>
</ul>
<hr>
<p>States can be anything as long as it can be expressed numerically. So that includes text, images, and sound. Really anything.</p>
<hr>
<p>unsupervised (?) learning</p>
<ul>
<li>given \( s \)</li>
<li>learn \( s \rightarrow \text{cluster_id} \)</li>
<li>learn \( s \rightarrow s \)</li>
</ul>
<hr>
<p>So say we have a set of a thousand images. Each image is an \( s \).</p>
<p>We want to learn something, and that tends to mean unsupervised learning starts to resemble supervised learning.</p>
<p>At two ends of a spectrum we have clustering and <a href="https://en.wikipedia.org/wiki/Autoencoder">autoencoders</a>, and all kinds of dimensionality reduction in between.</p>
<p>Unsupervised learning is sort of the <a href="https://en.wikipedia.org/wiki/Dark_matter">dark matter</a> of machine learning. Even <a href="https://www.youtube.com/watch?v=vdWPQ6iAkT4">Yann LeCun</a> says "We are missing the principles for unsupervised learning."</p>
<hr>
<p>deep unsupervised learning</p>
<ul>
<li>\( s \) with deep neural nets</li>
</ul>
<hr>
<p>Deep unsupervised learning is whenever we do unsupervised learning and somewhere there's a deep neural net.</p>
<hr>
<p>supervised learning</p>
<ul>
<li>\( s \rightarrow a \)</li>
</ul>
<hr>
<p>Up next is supervised learning. We're introducing a new entity \( a \), which I'll call an "action". It's common to call it a "label" or a "target" and to use the symbol "y". Same thing.</p>
<hr>
<p>action a</p>
<ul>
<li>numbers</li>
<li>"cat"/"dog"</li>
<li>"left"/"right"</li>
<li>17.3</li>
<li>[2.0, 11.7, 5]</li>
<li>4.2V</li>
</ul>
<hr>
<p>Whatever you call it, the action is again a numeric thing. It could be anything that \( s \) could be, but it tends to be lower-dimensional.</p>
<p>The cat/dog classifier is a popular example, and a left/right classifier is just the same, but those might feel more like actions.</p>
<hr>
<p>supervised learning</p>
<ul>
<li>given \( s, a \)</li>
<li>learn \( s \rightarrow a \)</li>
</ul>
<hr>
<p>In supervised learning you have a training set of state-action pairs, and you try to learn a function to produce the correct action based on the state alone.</p>
<p>Supervised learning can blur into imitation learning, which can be taken as a kind of reinforcement learning. For example, <a href="https://arxiv.org/abs/1604.07316">NVIDIA's end-to-end self-driving car</a> is based on learning human driving behaviors. (Sergey Levine <a href="https://www.youtube.com/watch?v=Pw1mvoOD-3A&amp;list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm&amp;index=13">explains</a> in some depth.) But I'm not going to talk more about imitation learning, and supervised learning will stand alone.</p>
<p>You can learn this function with linear regression or support vector machines or whatever you like.</p>
<hr>
<p>deep supervised learning</p>
<ul>
<li>\( s \rightarrow a \) with deep neural nets</li>
</ul>
<hr>
<p>Deep supervised learning is whenever we do supervised learning and somewhere there's a deep neural net.</p>
<p>There's also semi-supervised learning, when you have some labeled data and some unlabeled data, which can connect to active learning, which has some relation to reinforcement learning, but that's all I'll say about that.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\(r, s \rightarrow a\)</li>
</ul>
<hr>
<p>Finally, we reach reinforcement learning.</p>
<p>We're adding a new thing \( r \), which is reward.</p>
<hr>
<p>reward \( r \)</p>
<ul>
<li>-3</li>
<li>0</li>
<li>7.4</li>
<li>1</li>
</ul>
<hr>
<p>Reward is a scalar, and we like positive rewards.</p>
<hr>
<p>optimal control / reinforcement learning</p>
<hr>
<p>I'll mention that <a href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a> is closely related to reinforcement learning. It has its own parallel notation and conventions, and I'm going to ignore all that.</p>
<hr>
<p>\( r, s \rightarrow a \)</p>
<hr>
<p>So here's the reinforcement learning setting.</p>
<p>We get a reward and a state, and the agent chooses an action.</p>
<hr>
<p>tick</p>
<hr>
<p>Then, time passes. We're using discrete time, so this is a "tick".</p>
<hr>
<p>\( r', s' \rightarrow a' \)</p>
<hr>
<p>Then we get a new reward and state, which depend on the previous state and action, and the agent chooses a new action.</p>
<hr>
<p>tick</p>
<hr>
<p>And so on.</p>
<hr>
<p><img alt="standard diagram" src="img/standard_diagram.png"></p>
<hr>
<p>This is the standard reinforcement learning diagram, showing the agent and environment. My notation is similar.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>"given" \( r, s, a, r', s', a', ... \)</li>
<li>learn "good" \( s \rightarrow a \)</li>
</ul>
<hr>
<p>So here's reinforcement learning.</p>
<p>"Given" is in quotes because the rewards and states that we see depend on the actions we choose.</p>
<p>"Good" is in quotes because we haven't defined "good" beyond that we like positive rewards.</p>
<hr>
<p>Question:</p>
<p>Can you formulate supervised learning as reinforcement learning?</p>
<hr>
<p>Here's a question to think about.</p>
<hr>
<p>supervised as reinforcement?</p>
<ul>
<li>reward 0 for incorrect, reward 1 for correct<ul>
<li>beyond binary classification?</li>
</ul>
</li>
<li>reward deterministic</li>
<li>next state random</li>
</ul>
<hr>
<p>You can re-cast supervised learning as reinforcement learning, but it isn't necessarily as efficient: it's better to be told what the right answer is than to just be told that you're wrong. This is one sense in which Yann LeCun means that reinforcement learning is using fewer bits of information per example.</p>
<hr>
<p>Question:</p>
<p>Why is the Sarsa algorithm called Sarsa?</p>
<hr>
<p>Here's a question that you can already figure out!</p>
<p>(Sarsa is an on-policy TD control algorithm due to Sutton, 1996.)</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\( r, s \rightarrow a \)</li>
</ul>
<hr>
<p>Sarsa: State, Action, Reward, State, Action.</p>
<p>We'll see the closely related Q-learning algorithm in some detail later on.</p>
<hr>
<p>deep reinforcement learning</p>
<ul>
<li>\( r, s \rightarrow a \) with deep neural nets</li>
</ul>
<hr>
<p>Deep reinforcement learning is whenever we do reinforcement learning and somewhere there's a deep neural net.</p>
<p>The one twist is that in reinforcement learning, the network(s) may not be in the obvious place. We'll need to develop a few more ideas to see more places to put a neural net.</p>
<hr>
<p>Markov property</p>
<ul>
<li>\( s \) is enough</li>
</ul>
<hr>
<p>We're going to assume <a href="https://en.wikipedia.org/wiki/Markov_property">the Markov property</a>, which means that the state we observe tells us as much as we can know about what will happen next.</p>
<p>This is frequently not true, but we'll assume it anyway.</p>
<hr>
<pre><code>                   Make choices?
                   no                        yes
Completely    no   Markov chain              MDP: Markov Decision Process
observable?   yes  HMM: Hidden Markov Model  POMDP: Partially Observable MDP</code></pre>

<hr>
<p>This chart relates a bunch of Markov things, to hopefully help you orient yourself.</p>
<p>We're working with Markov Decision Processes (MDPs). Even though things are frequently not really completely observable, we'll usually just ignore that. Often this is fine (think about blackjack, for example).</p>
<p>It's often easiest to think about finite spaces, but this isn't always necessary.</p>
<p>(The chart is from <a href="http://www.pomdp.org/faq.html">POMDP.org</a>.)</p>
<hr>
<p>usual RL problem elaboration</p>
<hr>
<p>We'll introduce some more notation and a few ideas that help us work on reinforcement learning problems.</p>
<hr>
<p>policy \( \pi \)</p>
<ul>
<li>\( \pi: s \rightarrow a \)</li>
</ul>
<hr>
<p>This thing that takes a state and produces an action we'll call a "policy" \( \pi \). This effectively <em>is</em> our reinforcement learning agent, and the name of the game is figuring out how to get a good policy.</p>
<p>But how do we even know whether a policy is good?</p>
<p>(If you don't like spelling in Greek, you can imagine it's a little walker!)</p>
<hr>
<p>return</p>
<ul>
<li>\( \sum{r} \)<ul>
<li>into the future (episodic or ongoing)</li>
</ul>
</li>
</ul>
<hr>
<p>We want a policy that maximizes "return" \( \sum{r} \). Return is just the sum of all the rewards we'll get into the future. The symbol "G" is also used, but I'll keep writing the sum.</p>
<p>Reinforcement learning can consider finite-length episodes that always come to an end, or the ongoing setting in which the agent keeps on going. (You can see that there could be a problem with infinity in the ongoing setting.)</p>
<hr>
<p><img alt="gridworld" src="img/gridworld.png"></p>
<hr>
<p>Here's a typical toy example for understanding reinforcement learning: a gridworld.</p>
<p>There are sixteen states, each uniquely identifiable to the agent.</p>
<p>There are four actions: go up, down, left, right.</p>
<p>We want the agent to learn to go from the start to the goal.</p>
<p>We'll say this is episodic, with each episode ending after the agent makes it to the goal.</p>
<p>So how do we set up the rewards?</p>
<hr>
<p><img alt="gridworld reward" src="img/gridworld_reward.png"></p>
<hr>
<p>Here's a natural way to set up reward. You get a reward of one when you complete the task. What does the return at each state look like, then?</p>
<hr>
<p><img alt="gridworld return" src="img/gridworld_return.png"></p>
<hr>
<p>The return, unfortunately, is just one everywhere. And this means that there isn't any reason to go directly to the goal. We have all the time in the world, so we could just as well meander randomly until we happen to hit the goal. This isn't very good.</p>
<hr>
<p>Question:</p>
<p>How do we keep our agent from dawdling?</p>
<hr>
<p>How can we set things up so that going directly to the goal position is incentivized?</p>
<p>Consider changing rewards, or changing how return is calculated, or anything else.</p>
<p>There are at least three approaches that are commonly used.</p>
<hr>
<p>negative reward at each time step</p>
<hr>
<p>If we get a negative reward at each time step, the only way to maximize return is to minimize episode length, so we're motivated to end the episode as soon as possible. This certainly makes sense for the episodic setting.</p>
<p>(It's like existing is pain; the agent is like <a href="http://rickandmorty.wikia.com/wiki/Mr._Meeseeks">Mr. Meeseeks</a>!)</p>
<hr>
<p>discounted rewards</p>
<ul>
<li>\( \sum{\gamma^tr} \)</li>
</ul>
<hr>
<p>It's very common to use a "discount factor" \( \gamma \), which might be 0.9, for example. Then a reward soon is worth more than a reward later, and again the agent is motivated to do things more directly.</p>
<hr>
<p>average rate of reward</p>
<ul>
<li>\( \frac{\sum{r}}{t} \)</li>
</ul>
<hr>
<p>Average rate of reward is also used in some settings, where it can make things more stable.</p>
<hr>
<p>value functions</p>
<ul>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>I already snuck in a value function when we saw green values for every state in the gridworld.</p>
<p>The state value function \( v \) tells us the return from any state, and the state-action value function \( q \) gives the return from any state-action combination. It might not yet be clear why the two are importantly different, so let's get into that.</p>
<hr>
<p>Question:</p>
<p>Does a value function specify a policy (if you want to maximize return)?</p>
<hr>
<p>Here's the relevant question to consider.</p>
<hr>
<p>trick question?</p>
<ul>
<li>\( v_\pi \)</li>
<li>\( q_\pi \)</li>
</ul>
<hr>
<p>Is it a trick question? Really, values depend on what you do, on your policy. But let's assume I just give you a value function and you trust it. Can you use it to induce a policy?</p>
<hr>
<p>value functions</p>
<ul>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>The state-action value function \( q \) certainly implies a policy. For whatever state we're in, we check the return for all available actions, and choose the action that's best.</p>
<p>For the state value function \( v \), we can get a policy in a similar way, but only if we know what state we'll be in after taking an action. That may or may not be true.</p>
<hr>
<p>environment dynamics</p>
<ul>
<li>\( s, a \rightarrow r', s' \)</li>
</ul>
<hr>
<p>The next state and reward depend on the previous state and action, and that's determined by the environment. We don't necessarily know what's going on inside the environment.</p>
<hr>
<p><img alt="gridworld" src="img/gridworld.png"></p>
<hr>
<p>When you, a smart human with lots of ideas about physics, look at this picture of a gridworld, you assume that going "right" will do what you expect.</p>
<p>But in general, we don't know in advance how the environment behaves. Maybe going "right" always takes you to the upper left state, for example.</p>
<p>Sometimes you do know the environment dynamics, like with games with well-defined rules. But sometimes you don't. You could try to learn the environment dynamics with a model.</p>
<hr>
<p>model</p>
<ul>
<li>\( s, a \rightarrow r', s' \)</li>
</ul>
<hr>
<p>Reinforcement learning uses "model" to refer to a model that you learn for the environment dynamics. (Often, just the next state is predicted.)</p>
<p>This model is something that the agent has to learn, and depending on whether the agent is learning a model determines whether you're said to be doing model-based vs. model-free reinforcement learning.</p>
<p>(In adaptive control this "model learning" is called "system identification".)</p>
<hr>
<p>learning and acting</p>
<hr>
<p>Everything to this point has been elaborating the problem setup for reinforcement learning. Now we get into how we actually learn and go forth with RL.</p>
<hr>
<p>learn model of dynamics</p>
<ul>
<li>from experience</li>
<li>difficulty varies</li>
</ul>
<hr>
<p>As mentioned, you could try to learn your environment's dynamics. If you can do this well, it's great, but it may not be easy. Model-based RL is an exciting area of research.</p>
<hr>
<p>learn value function(s) with planning</p>
<ul>
<li>assuming we have the environment dynamics or a good model already</li>
</ul>
<hr>
<p>To introduce value function learning, let's consider the situation where you know nothing yet but you have the environment dynamics, so you can make decisions by looking ahead. This is a familiar thought process, conceptually.</p>
<hr>
<p><img alt="planning 0" src="img/planning0.png"></p>
<hr>
<p>You're in state \( s_1 \) and you have to choose between \( a_1 \) and \( a_2 \).</p>
<p>Because you have the environment dynamics, you can see what would happen next.</p>
<hr>
<p><img alt="planning 1" src="img/planning1.png"></p>
<hr>
<p>Looking one time step ahead, you can already start to evaluate your action choices by the rewards you'd get immediately. This is the state-action value view, \( q \): you're evaluating the value of the <em>actions</em> directly.</p>
<hr>
<p><img alt="planning 2" src="img/planning2.png"></p>
<hr>
<p>Looking two time steps ahead, you can similarly start to evaluate how good the states you would get to are. This is the state value view, \( v \).</p>
<p>This example is simple and assumes states and actions never recur. We also haven't introduced a couple more wrinkles. You can think about differences between thinking in terms of \( v \) and \( q \) later, for example in the case of backgammon.</p>
<p>This kind of planning can happen continuously in the background, or it can be done on demand at decision time. The search space can get quite large.</p>
<hr>
<p>connections</p>
<ul>
<li>Dijkstra's algorithm</li>
<li>A* algorithm</li>
<li>minimax search<ul>
<li>two-player games not a problem</li>
</ul>
</li>
<li>Monte Carlo tree search</li>
</ul>
<hr>
<p>The graph structure on the previous slide might make you think of a range of algorithms that you could already be familiar with.</p>
<p>You can think of these as smart ways of exploring the possibly very large branching structures that can spring up. <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a> is particularly clever, and it will appear again later.</p>
<hr>
<p><img alt="rollout diagram" src="img/rollout_diagram.png"></p>
<hr>
<p>One way of dealing with all the possible tree paths is just to sample from it and try to make estimates on the basis of these.</p>
<hr>
<p><img alt="backgammon roll" src="img/rollout.jpg"></p>
<hr>
<p>I thought it was interesting to learn that the term "roll-out" comes from work on backgammon, where you literally roll dice to move a game along.</p>
<hr>
<p>everything is stochastic</p>
<hr>
<p>Backgammon has randomness in the environment, from the dice, but randomness can enter all over the place.</p>
<hr>
<p><img alt="stochastic s'" src="img/stochastic_s.png"></p>
<hr>
<p>The next state can be random.</p>
<hr>
<p><img alt="stochastic r'" src="img/stochastic_r.png"></p>
<hr>
<p>And the next reward can be random, as in the case of <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandits</a>.</p>
<hr>
<p>exploration vs. exploitation</p>
<ul>
<li>\( \epsilon \)-greedy policy<ul>
<li>usually do what looks best</li>
<li>\( \epsilon \) of the time, choose random action</li>
</ul>
</li>
</ul>
<hr>
<p>This can always be an issue in unknown environments, but especially with randomness, we encounter the issue of exploration vs. exploitation. The \( \epsilon \)-greedy approach is one well-known way to ensure that an agent keeps exploring.</p>
<hr>
<p>Question:</p>
<p>How does randomness affect \( \pi \), \( v \), and \( q \)?</p>
<ul>
<li>\( \pi: s \rightarrow a \)</li>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>Now acknowledging that there's randomness all over the place, I can show another way that my notation is shorthand.</p>
<hr>
<p>Question:</p>
<p>How does randomness affect \( \pi \), \( v \), and \( q \)?</p>
<ul>
<li>\( \pi: \mathbb{P}(a|s) \)</li>
<li>\( v: s \rightarrow \mathbb{E} \sum{r} \)</li>
<li>\( q: s, a \rightarrow \mathbb{E} \sum{r} \)</li>
</ul>
<hr>
<p>I haven't been hiding too much. A policy is a probability distribution over possible actions, and value functions give expectations. No problem.</p>
<hr>
<p>Monte Carlo returns</p>
<ul>
<li>\( v(s) = ? \)</li>
<li>keep track and average</li>
<li>like "planning" from experienced "roll-outs"</li>
</ul>
<hr>
<p>Here's our first pass at a model-free learning algorithm. We interact with the environment, and use our experiences in the past just like model-based roll-outs.</p>
<hr>
<p>non-stationarity</p>
<ul>
<li>\( v(s) \) changes over time!</li>
</ul>
<hr>
<p>The environment may change, and we want our methods to be able to deal with this.</p>
<hr>
<p>moving average</p>
<ul>
<li>new mean = old mean + \( \alpha \)(new sample - old mean)</li>
</ul>
<hr>
<p>This kind of moving average is used all over the place.</p>
<p>The parameter \( \alpha \) is a learning rate, and the whole thing can be seen to be a case of stochastic gradient descent, which is a nice early connection to neural nets which start turning up.</p>
<p>(Sutton and Barto use \( \alpha \) for learning rate, not the otherwise popular \( \eta \). This is a sensible choice for them especially since they sometimes have multiple learning rates in the same algorithm, and the second learning rate can then be \( \beta \).)</p>
<hr>
<p><img alt="MDP diagram" src="img/mdp_diagram.png"></p>
<hr>
<p>We can think about this question, which is not a trick: the difference is \( r' \).</p>
<hr>
<p>Bellman equation</p>
<ul>
<li>\( v(s) = r' + v(s') \)</li>
</ul>
<hr>
<p>That relation gives rise to a bunch of Bellman equations, which are quite useful.</p>
<p><a href="https://en.wikipedia.org/wiki/Richard_E._Bellman">Bellman</a> came up with a couple neat things. <a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programming</a> (despite being <a href="http://arcanesentiment.blogspot.com/2010/04/why-dynamic-programming.html">poorly named</a>) is great for lots of problems, including those involving Bellman equations.</p>
<p>Bellman also introduced the phrase "<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>".</p>
<hr>
<p>temporal difference (TD)</p>
<hr>
<p>Bellman equations give rise to temporal difference (TD) learning (Sutton, 1988).</p>
<p>It's sometimes described as "bootstrap" learning.</p>
<hr>
<p>Q-learning</p>
<ul>
<li>\( \text{new } q(s, a) = q(s, a) + \alpha (r' + \gamma \max_{a} q(s', a) - q(s, a)) \)</li>
</ul>
<hr>
<p>Q-learning is a temporal difference method that combines a lot of the things we've just seen.</p>
<p>You can see in the equation that Sarsa is closely related.</p>
<hr>
<p>on-policy / off-policy</p>
<hr>
<p>Reinforcement learning differentiates between on-policy and off-policy learning. On-policy learning is when you're learning about the policy that you're currently following, and it's generally easier than off-policy learning.</p>
<p>Importance sampling is one way to achieve off-policy learning.</p>
<p>Q-learning is off-policy, which is nice. You can look at the equation and see why.</p>
<hr>
<p>estimate \( v \), \( q \)</p>
<ul>
<li>with a deep neural network</li>
</ul>
<hr>
<p>We've sort of implicitly been doing what's called tabular learning, where each state or state-action has its own estimated return. But we can plug in any kind of supervised learning algorithm, including deep learning. We'll see this in applications.</p>
<hr>
<p>back to the \( \pi \)</p>
<ul>
<li>parameterize \( \pi \) directly</li>
<li>update based on how well it works</li>
<li>REINFORCE<ul>
<li>REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility</li>
</ul>
</li>
</ul>
<hr>
<p>Stepping back from value functions, we can work with a parameterized policy directly.</p>
<p>REINFORCE is not necessarily a great acronym.</p>
<hr>
<p>policy gradient</p>
<ul>
<li>\( \nabla \log (\pi(a|s))\)</li>
</ul>
<hr>
<p>This is the gradient that you use.</p>
<p>You may see the connection to score functions and the so-called <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">log-derivative trick</a>, but we can interpret without all that.</p>
<p>If an action contributes to high return, you can encourage it, and if it contributes to low return, you can discourage it.</p>
<hr>
<p>actor-critic</p>
<ul>
<li>\( \pi \) is the actor</li>
<li>\( v \) is the critic</li>
<li>train \( \pi \) by policy gradient to encourage actions that work out better than \( v \) expected</li>
</ul>
<hr>
<p>Actor-critic algorithms combine policy gradient and value function methods to reduce variance, and are pretty popular.</p>
<hr>
<p>applications: how</p>
<hr>
<p>We now know enough to understand a lot of the details of big reinforcement learning applications!</p>
<hr>
<p>TD-gammon (1992)</p>
<ul>
<li>\( s \) is custom features</li>
<li>\( v \) with shallow neural net</li>
<li>\( r \) is 1 for a win, 0 otherwise</li>
<li>\( TD(\lambda) \)<ul>
<li>eligibility traces</li>
</ul>
</li>
<li>self play</li>
<li>shallow forward search</li>
</ul>
<hr>
<p>Backgammon was dealt with pretty completely about 25 years ago already.</p>
<p>We know what state the board will be in after any move, and the state of the board is what matters, so it makes sense to use a state value function \( v \) here.</p>
<hr>
<p>DQN (2015)</p>
<ul>
<li>\( s \) is four frames of video</li>
<li>\( v \) with deep convolutional neural net</li>
<li>\( r \) is 1 if score increases, -1 if decreases, 0 otherwise</li>
<li>Q-learning<ul>
<li>usually-frozen target network</li>
<li>clipping update size, etc.</li>
</ul>
</li>
<li>\( \epsilon \)-greedy</li>
<li>experience replay</li>
</ul>
<hr>
<p>Deep Q-Networks (DQN) is a well-known algorithm that works well for many Atari games. See:</p>
<ul>
<li><a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> (2013)</li>
<li><a href="https://deepmind.com/research/dqn/">Human-level control through Deep Reinforcement Learning</a> (2015)</li>
</ul>
<hr>
<p>evolution (2006)</p>
<ul>
<li>\( s \) is custom features</li>
<li>\( \pi \) has a simple parameterization</li>
<li>evaluate by final score</li>
<li>cross-entropy method</li>
</ul>
<hr>
<p>Surprise! Tetris is well-solved by evolutionary methods, which I didn't mention at all. These methods are sneaky that way. They can work better than you'd think. See:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Learning Tetris Using the Noisy Cross-Entropy Method</a> (2006)</li>
<li><a href="https://blog.openai.com/evolution-strategies/">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></li>
</ul>
<hr>
<p>AlphaGo (2016)</p>
<ul>
<li>\( s \) is custom features over geometry<ul>
<li>big and small variants</li>
</ul>
</li>
<li>\( \pi_{SL} \) with deep convolutional neural net, supervised training</li>
<li>\( \pi_{rollout} \) with smaller convolutional neural net, supervised training</li>
<li>\( r \) is 1 for a win, -1 for a loss, 0 otherwise</li>
<li>\( \pi_{RL} \) is \( \pi_{SL} \) refined with policy gradient peer-play reinforcement learning</li>
<li>\( v \) with deep convolutional neural net, trained based on \( \pi_{RL} \) games</li>
<li>asynchronous policy and value Monte Carlo tree search<ul>
<li>expand tree with \( \pi_{SL} \)</li>
<li>evaluate positions with blend of \( v \) and \( \pi_{rollout} \) rollouts</li>
</ul>
</li>
</ul>
<hr>
<p>AlphaGo surprised a lot of people by reaching super-human levels of Go play. It was a bit complicated. See:</p>
<ul>
<li><a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">Mastering the game of Go with deep neural networks and tree search</a></li>
</ul>
<hr>
<p>AlphaGo Zero (2017)</p>
<ul>
<li>\( s \) is simple features over time and geometry</li>
<li>\( \pi, v \) with deep residual convolutional neural net</li>
<li>\( r \) is 1 for a win, -1 for a loss, 0 otherwise</li>
<li>Monte Carlo tree search for self-play training and play</li>
</ul>
<hr>
<p>AlphaGo Zero improved on AlphaGo in performance and by requiring no human training data. It reaches incredibly good performance with only its self-play learning method. See:</p>
<ul>
<li><a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">Mastering the game of Go without human knowledge</a><ul>
<li>Interesting quote on reach/limitations: "Our approach is most directly applicable to Zero-sum games of perfect information."</li>
</ul>
</li>
</ul>
<p>(AlphaGo Zero can only play legal moves, but stupid moves (like filling eyes) aren't forbidden, so it is less constrained than the original AlphaGo.)</p>
<hr>
<p>onward</p>
<hr>
<p>Let's look at some more applications to paint out more of the area that reinforcement learning can influence.</p>
<hr>
<p>Neural Architecture Search (NAS)</p>
<hr>
<p>Neural Architecture Search (NAS) uses policy gradient where the actions design a neural net. The reward is validation set performance. See:</p>
<ul>
<li><a href="https://research.googleblog.com/2017/11/automl-for-large-scale-image.html">AutoML for large scale image classification and object detection</a></li>
<li><a href="https://arxiv.org/abs/1707.07012">Learning Transferable Architectures for Scalable Image Recognition</a></li>
<li><a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning</a></li>
</ul>
<hr>
<p><img alt="Amazon Echo" src="img/echo.jpg"></p>
<hr>
<p>A lot of people are interested in natural language processing. A lot of chatbots are still big nests of <code>if</code> statements, but there is interest in using RL for language. See:</p>
<ul>
<li><a href="https://arxiv.org/abs/1709.02349">A Deep Reinforcement Learning Chatbot</a><ul>
<li><a href="https://developer.amazon.com/alexaprize">Amazon Alexa Prize</a></li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1703.04908">Emergence of Grounded Compositional Language in Multi-Agent Populations</a></li>
</ul>
<p>(In particular in the multi-agent setting, there might be some connection to <a href="https://en.wikipedia.org/wiki/Evolutionary_game_theory">evolutionary game theory</a> and the work of <a href="https://en.wikipedia.org/wiki/Peyton_Young">Peyton Young</a>.)</p>
<hr>
<p><img alt="robot" src="img/robot.jpg"></p>
<hr>
<p>Lots of people think using RL for robot control is pretty neat.</p>
<p>(left to right: Chelsea Finn, Pieter Abbeel, <a href="http://www.willowgarage.com/pages/pr2/overview">PR2</a>, Trevor Darrell, Sergey Levine)</p>
<p>Pieter Abbeel, Peter Chen, Rocky Duan, and Tianhao Zhang <a href="https://nyti.ms/2hLYbGQ">founded Embodied Intelligence</a> to work on robot stuff, it seems.</p>
<hr>
<p>self-driving cars</p>
<ul>
<li>interest</li>
<li>results?</li>
</ul>
<hr>
<p>People seem to want to use RL for self-driving cars.</p>
<hr>
<p><img alt="OpenAI DotA 2" src="img/openai_dota2.jpg"></p>
<hr>
<p>OpenAI has <a href="https://blog.openai.com/dota-2/">beaten</a> top human players in one-on-one DotA 2. This is pretty neat. They haven't released details of their methods as they say they're working on the full five-on-five game. We'll see!</p>
<hr>
<p>conclusion</p>
<hr>
<p>Wrapping up!</p>
<hr>
<p><img alt="bootcamp diagram" src="img/annotated.jpg"></p>
<hr>
<p>There are <a href="/20170830-berkeley_deep_rl_bootcamp/">a lot of RL algorithms</a> with horrible acronyms.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\( r,s \rightarrow a \)</li>
<li>\( \pi: s \rightarrow a\)</li>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s,a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>But the core ideas are pretty concise, and lots of work in the field can be quickly understood to a first approximation by relating it back to these core ideas.</p>
<hr>
<p>network architectures for deep RL</p>
<ul>
<li>feature engineering can still matter</li>
<li>if using pixels<ul>
<li>often simpler than state-of-the-art for supervised</li>
<li>don't pool away location information if you need it</li>
</ul>
</li>
<li>consider using multiple/auxiliary outputs</li>
<li>consider phrasing regression as classification</li>
<li>room for big advancements</li>
</ul>
<hr>
<p>Lots of exciting developments in RL are coming from the use of deep neural nets, and I wanted to say a few things specific to doing these applications of deep learning.</p>
<hr>
<p>the lure and limits of RL</p>
<ul>
<li>seems like AI (?)</li>
<li>needs so much data</li>
</ul>
<hr>
<p>There's progress in RL now. The limits of that progress are not yet known.</p>
<p>One hot take <a href="https://twitter.com/dennybritz/status/925028640001105920">from Denny Britz</a>: "Ironically, the major advances in RL over the past few years all boil down to making RL look less like RL and more like supervised learning."</p>
<hr>
<p>Question</p>
<ul>
<li>Should you use reinforcement learning?</li>
</ul>
<hr>
<p>It's for you to decide!</p>
<hr>
<p>Thank you!</p>
<hr>
<p>Thank you for coming!</p>
<p>Thanks again to <a href="https://www.deeplearninganalytics.com/">DLA</a> for supporting me in learning more about reinforcement learning, and in being the audience for the earliest version of this talk, providing lots of valuable feedback.</p>
<p>Thanks to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for working through the second iteration of this talk with me, providing even more good feedback.</p>
<p>Thanks to <a href="https://twitter.com/EricHaengel">Eric Haengel</a> for help with Go and in thinking about the <a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">AlphaGo Zero paper</a>.</p>
<hr>
<p>further resources</p>
<hr>
<ul>
<li><a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> (paper)</li>
<li>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> (blog post)</li>
<li><a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> (textbook)</li>
<li>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> (videos and slides)</li>
<li><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> (videos, slides, and labs)</li>
<li>OpenAI <a href="https://github.com/openai/gym">gym</a> / <a href="https://github.com/openai/baselines">baselines</a> (software)</li>
<li><a href="http://nationalgocenter.org/">National Go Center</a> (physical place)</li>
<li><a href="http://dc.hackandtell.org/">Hack and Tell</a> (fun meetup)</li>
</ul>
<p>Here are my top resources for learning more about deep reinforcement
learning (and having a little fun).</p>
<p>The content of <a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> is similar to this talk. It does a quick intro to a lot of deep reinforcement learning.</p>
<p>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> is a readable introduction as well, focused on policy gradient, with readable code.</p>
<p>If you can afford the time, I recommend all of Sutton and Barto's <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a>. The authors are two major reinforcement learning pioneers, to the extent that they sometimes introduce new concepts in the pages of their textbook.</p>
<p>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> is quite good. It largely follows Sutton and Barto's text, and also references Csaba Szepesv&#225;ri's <a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning</a>.</p>
<p>The <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> that inspired this talk has lots of materials online, so you can get the full treatment if you like!</p>
<p>If you want to get into code, <a href="https://openai.com/">OpenAI</a>'s <a href="https://github.com/openai/gym">gym</a> and <a href="https://github.com/openai/baselines">baselines</a> could be nice places to get started.</p>
<p>If you want to have fun, you can play Go with humans at the <a href="http://nationalgocenter.org/">National Go Center</a>, and I always recommend the <a href="http://dc.hackandtell.org/">Hack and Tell</a> meetup for a good time nerding out with people over a range of fun projects.</p>
<p>Juergen Schmidhuber's <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Deep Learning in Neural Networks: An Overview</a> didn't make my top list, but it has an interesting section specific to using LSTMs in RL.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20171114-deep_rl/</link>
<guid>http://planspace.org/20171114-deep_rl/</guid>
<pubDate>Tue, 14 Nov 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Mounting Disks on AWS</title>
<description><![CDATA[

<p>I don't hand-mount disks much these days, but sometimes I want to spin up a quick <a href="https://aws.amazon.com/ec2/">EC2</a> instance to hammer something out, and I want to use the fast <a href="https://en.wikipedia.org/wiki/Solid-state_drive">SSD</a> storage that comes with some instances, and those disks don't auto-mount.</p>
<p>Amazon has <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html#making-instance-stores-available-on-your-instances">some documentation</a> but it isn't quite enough for me by itself. Here are commands that have served me well on Ubuntu:</p>
<pre><code class="language-bash">df -h  # see what's already mounted
lsblk  # find out the /dev/ paths of devices, like say /dev/xvdb
mkfs.ext4 /dev/xvdb  # format a disk using the Ext4 filesystem
mkdir disk  # make a mount point
mount /dev/xvdb disk  # mount the disk
df -h  # check for success and size of the new disk
chown ubuntu disk1  # make the disk accessible to the admin account</code></pre>

<p>Some of those will need <code>sudo</code>.</p>
<p>I don't really know that Ext4 is the best filesystem to use; let me know if some other one is better!</p>
<p>Some instances come with multiple SSDs, in which case it might be fun to use them in a <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_0">RAID-0</a> configuration for even more speed. I haven't tried this yet, but there is some <a href="http://www.tldp.org/HOWTO/Software-RAID-HOWTO-5.html#ss5.5">documentation</a> to start from. Has anyone tried this?</p>    
    ]]></description>
<link>http://planspace.org/20171022-mounting_disks_on_aws/</link>
<guid>http://planspace.org/20171022-mounting_disks_on_aws/</guid>
<pubDate>Sun, 22 Oct 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
