<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Failure to replicate Schwartz-Ziv and Tishby</title>
<description><![CDATA[

<p><a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a> didn't appear at any conferences, to my knowledge, but it still built up some <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">buzz</a>. It has been difficult to replicate, for both <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/">bloggers</a> and <a href="https://openreview.net/forum?id=ry_WPG-A-">academics</a>. I attempted to replicate some aspects, and emailed the authors with the message below in an attempt to resolve difficulties. As there has been no response after many weeks, I present the below as an open letter. I hope it can serve as a reference for others who might try to explore these ideas.</p>
<hr>
<p>Hi Ravid! I was intrigued by your paper, <a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a>, so I wanted to replicate your results. I was particularly interested in your Figure 4; I thought that if I could see these kinds of patterns, maybe they could be used to identify when a network is well-trained, for example.</p>
<p>Here is the figure I mean:</p>
<p><img alt="figure 4" src="img/figure_4.png"></p>
<p>I started by trying to work with a simple ReLU network with two hidden layers, on MNIST, and what I found was that the sizes of gradients decrease through training, and so do differences or variances of those gradients. I couldn't see two distinct phases, or indeed any interesting crossing of two metrics.</p>
<p>At first, I wasn't sure about how to interpret the "mean" and "standard deviation" of your Figure 4. So I was happy to find your <a href="https://github.com/ravidziv/IDNNs">code</a>, which I've been using at the 2017-11-02 <a href="https://github.com/ravidziv/IDNNs/tree/c4abb1dad4fbb262315eb8b96eb85dc5c3e98e5c">c4abb1d commit</a>.</p>
<p>Unfortunately, using the definitions of "mean" and "standard deviation" as found in your <code>idnns/plots/plot_gradients.py</code> file, I didn't get results that matched those of your paper any better.</p>
<p>I started to use your code directly, running <code>main.py</code> with default arguments, adding only <code>-save_grads true</code> so I could then use <code>plot_gradients.py</code>. Here is the result:</p>
<p><img alt="default mean std" src="img/default_mean_std.png"></p>
<p>As you can see, there are differences from your published Figure 4.</p>
<p>Can you help me to replicate your results? What should I be doing differently? I would appreciate your feedback. I also posted comments about my process of trying to understand your work on a <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/">"severely theoretical" blog post</a>; if you would like to add public context there as well it would also be appreciated.</p>
<p>One thing that I noticed is that in your published Figure 4, one of the solid "mean" lines crosses the others. With the current calculation of "mean" in your code, in which the value for a layer is the sum of an always-positive calculation on each layer up to and including itself, it doesn't seem possible for lines to ever cross. Does this shine any light on what's going on?</p>
<p>Another thing I noticed is that the caption for your published Figure 4 says "values are normalized by the L2 norms of the weights for each layer" but in the current code, I don't see this happening. Is this perhaps part of what's going on?</p>
<p>I also noticed that the y axis labels in the plot don't seem to correspond to the values being plotted. For example, the first (leftmost) value for the last layer's "standard deviation" is calculated by your code as 0.49 (I check by inserting <code>print</code> messages) but it appears in the plot as less than 0.1. I think this issue doesn't affect the shapes of the curves but only the labeling, as far as I can tell.</p>
<p>More generally, is the cumulative sum approach to calculating statistics over layers appropriate? It strikes me that the pattern of the first layer dominates the plotted results, and it's then difficult to see what's happening uniquely in the higher layers.</p>
<p>I also tried running your <code>main.py</code> with the activation changed to ReLU, using <code>-activation_function 1</code>.</p>
<p><img alt="relu mean std" src="img/relu_mean_std.png"></p>
<p>The result here is mostly the same as the previous one, qualitatively, except that the gradient means seem to increase overall through the training process. Naftali Tishby has <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/#comment-183">said</a> that "results [showing non-decreasing mean gradients] are clearly incorrect as the gradients must decease eventually when the training error saturates." How should this be interpreted?</p>
<p>Let me lay out my understanding of the "mean" and "standard deviation" of your Figure 4, as based on reading your code. Both metrics are calculated for every layer and epoch, so I'll take just one as an example.</p>
<p>Let's say a network has a first layer with two weights, and there are two batches per epoch. Let's say the gradients for the first layer in the first epoch are <code>[2, 3]</code> and <code>[4, 5]</code>.</p>
<p>To get the "mean" we calculate the element-wise mean (<code>[3, 4]</code>) and take the norm, so the "mean" is <code>5</code>.</p>
<p>To get the "standard deviation" we calculate the element-wise variance (<code>[1, 1]</code>) and take the square root of its sum (<code>1.414</code>; call this the "variance") and then take the square root, so the "standard deviation" is <code>1.189</code>.</p>
<p>To show the cumulative sum aspect of the calculation, say there is a second layer with two weights and identical first-epoch gradients. Then for the second layer, first epoch, the "mean" is <code>5+5=10</code> and the "standard deviation" is the square root of the sum of the "variances" <code>sqrt(1.414+1.414)=1.682</code>.</p>
<p>This understanding is based on reading and experimenting with your <code>plot_gradients.py</code>. Does it seem right to you? Am I misinterpreting something? Is the intent other than this?</p>
<p>I would love to replicate the core result of your Figure 4, which I summarize as being "at first, gradients are large but don't vary much, and then later, gradients are small but vary a lot." What's the best way to find this result? I have seen that usually <em>both</em> gradients and the amount they vary decrease through training, but I have been unable to observe the two phases described in your paper.</p>
<p>While my main focus has been on finding the two phases by statistics on the gradients, I am also interested in the mutual information parts of your paper as well. Here are four plots generated by running your <code>main.py</code> (all default arguments) four times:</p>
<p><img alt="default info a" src="img/default_info_a.png"></p>
<p><img alt="default info b" src="img/default_info_b.png"></p>
<p><img alt="default info c" src="img/default_info_c.png"></p>
<p><img alt="default info d" src="img/default_info_d.png"></p>
<p>As you can see, I have not been able to consistently replicate the beautiful plots you have in your paper as Figure 3. In fact, the higher layers consistently fail to show the patterns described in your paper, and the lower layers behave somewhat irregularly. Am I doing something wrong?</p>
<p>One thing I noticed is that in the caption for Figure 3, the architecture is described as "input=12-10-8-6-4-2-1=output". This is somewhat different from the current default in your code, and in particular it doesn't seem possible to have a single output unit with your code in its current version, because for binary classification it uses a softmax over two output nodes. Is this an important difference? Could it be causing the difficulty in seeing compression phases?</p>
<p>I also kept the information plots from two runs of <code>main.py</code> with <code>-activation_function 1</code> for ReLU activations. Here they are:</p>
<p><img alt="relu info a" src="img/relu_info_a.png"></p>
<p><img alt="relu info b" src="img/relu_info_b.png"></p>
<p>I'm not sure if anything can be interpreted from these; maybe the code as I found it doesn't support ReLU at present? I know I've read that there are some different issues concerning binning when calculating mutual information involving ReLUs.</p>
<p>I would appreciate your insight on all these specific issues, but also more broadly: if the phenomena of your paper are general to neural networks, how should I understand the apparent difficulty in seeing similar results with other networks, code, and datasets? How should I understand the apparent difficulty even when using the same dataset, and your code?</p>
<p>Thank you very much for your help with all this,</p>
<p>~ Aaron</p>
<hr>
<p>I should add for completeness that I am aware of Saxe et al.'s Figure 1A, which seems to replicate Schwartz-Ziv and Tishby's information plane result better than Schwartz-Ziv's code does for me. I would like to know how they achieve that, but the point Saxe et al. make is that the information plane result doesn't hold for most neural nets that are used in practice, so it's kind of beside the point. I haven't seen anyone replicate the result on mean and variance of gradients during training, which is what I am most interested in.</p>    
    ]]></description>
<link>http://planspace.org/20180213-failure_to_replicate_schwartz-ziv_and_tishby/</link>
<guid>http://planspace.org/20180213-failure_to_replicate_schwartz-ziv_and_tishby/</guid>
<pubDate>Tue, 13 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Coming Jobs War by Jim Clifton</title>
<description><![CDATA[

<p>I read this book because current populist, protectionist urges seem like nationalistic competition for globally dwindling jobs, which might be called a "jobs war," and I wondered if <a href="https://en.wikipedia.org/wiki/Jim_Clifton">Jim Clifton</a>, the leader of <a href="https://en.wikipedia.org/wiki/Gallup_(company)">Gallup</a>, had unique insight on this phenomenon through the polling and research of his company. Unfortunately, the CEO's <a href="http://www.gallup.com/press/178343/gallup-press.aspx">self-published</a> book is surprisingly light on evidence, heavy on political proselytizing and advertising for Gallup. Jim Clifton is a slightly more sophisticated <a href="/20180122-why_the_rich_are_getting_richer/">Robert Kiyosaki</a>.</p>
<p>I'm surprised nobody, for the sake of Gallup's image, prevented Clifton from publishing his solution to high healthcare costs in the US, which has two prongs: (a) have people die sooner, and (b) fat-shaming. I'm not exaggerating:</p>
<blockquote>
<p>'Somebody has to tell Uncle Louie it's time to cross to the other side and go join his friends, not run doctor to doctor, accepting one low-probability procedure after another.' (pages 155-156)</p>
<p>'Unfit should mean something worse than it currently does. Unfit should mean "intervention required." Unfit should mean less employable because unfit is a cause of lower energy.' (page 160)</p>
</blockquote>
<p>Even if there are germs of reasonable ideas buried in what Clifton espouses, it's hard to get past his caustic victim-blaming, which is most extreme on healthcare, but present throughout.</p>
<p>Clifton sings the praises of entrepreneurs, with a kind of <a href="https://en.wikipedia.org/wiki/Great_man_theory">great man</a> delusion blended with borderline eugenic fatalism.</p>
<blockquote>
<p>'Some leaders even believe that anyone can be trained to be an entrepreneur. This is a mistaken assumption. Entrepreneurs have a rare gift. My estimate is that for every 1,000 people, there are only about three with the potential to develop an organization with $50 million or more in annual revenue.' (page 96)</p>
<p>'Nothing fixes bad managers, not coaching, competency training, incentives, or warnings &#8211; nothing works. A bad manager never gets better.' (page 114)</p>
</blockquote>
<p>Jim Clifton became CEO of Gallup when and because <a href="http://journalstar.com/gallup-s-clifton-dies-at-age-this-story-ran-in/article_cb499250-04a5-5852-b48f-282c047ff505.html">his father, Don Clifton</a>, bought Gallup and installed him as CEO. It's a little hard to take him seriously about individuals' abilities to be totally self-made, or when, with a perfect lack of self-awareness, he goes on to talk about the importance of fixing schools.</p>
<p>Even the core premise of the book, that "What the whole world wants is a good job," isn't well supported with evidence. I had assumed there would be some detailed analysis of polls, maybe some psychological research, but the closest thing to this didn't come very close at all:</p>
<blockquote>
<p>'Of the 7 billion people on Earth, there are 5 billion adults aged 15 and older. Of these 5 billion, 3 billion tell Gallup they work or want to work.' (page 2)</p>
</blockquote>
<p>So the strongest evidence provided that everyone wants to work is that 60% of adults say they <em>do</em> work, <em>or</em> want to work. I have no way of knowing whether "wanting to work" is separated from "wanting to have a decent standard of living." For example, how many people would want to work if they were <a href="/20180125-no_more_work_by_james_livingston/">guaranteed</a> a basic income and social services?</p>
<p>I also expected Clifton would address <a href="/20180121-janesville_an_american_story/">job loss</a> trends due to automation and so on, but there's scarcely a mention. He maintains that people just need to keep making companies, and that will make jobs. He is singularly focused on keeping US GDP higher than China's, never mind per-capita GDP or the strength of the connection between jobs and GDP.</p>
<p>Here are some questions listed in a section called "Defective employees." The questions are supposed to both help evaluate managers and "neatly factor all workers into the three categories of engaged, not engaged, and actively disengaged" (page 104):</p>
<ol>
<li>I know what is expected of me at work.</li>
<li>I have the materials and equipment I need to do my work right.</li>
<li>At work, I have the opportunity to do what I do best every day.</li>
<li>In the last seven days, I have received recognition or praise for doing good work.</li>
<li>My supervisor, or someone at work, seems to care about me as a person.</li>
<li>There is someone at work who encourages my development.</li>
<li>At work, my opinions seem to count.</li>
<li>The mission or purpose of my organization makes me feel my job is important.</li>
<li>My associates or fellow employees are committed to doing quality work.</li>
<li>I have a best friend at work.</li>
<li>In the last six months, someone at work has talked to me about my progress.</li>
<li>This last year, I have had opportunities at work to learn and grow.</li>
</ol>
<p>Most of these items map onto good advice for companies, but number ten's appearance on the list I think is symptomatic of a "<a href="https://www.wired.com/2008/06/pb-theory/">death of science</a>" problem in analysis. Why is "I have a best friend at work" on this list? I suspect the answer is that having a best friend at work is associated with being less likely to quit, and largely independent of other factors, so the item looks good statistically, despite being mostly useless to employees or managers.</p>
<p>Sometimes it's possible to see in Clifton's positions what could be genuine caring for people: a desire that everyone reach their full potential. Supporting people's strengths sounds good, and some of the ideas in the book are not all bad. But there are so many bad ideas, it's like picking through a dumpster.</p>
<p>I had thought that a book from <a href="https://en.wikipedia.org/wiki/Gallup_(company)">Gallup</a> might have interesting, relevant data and analysis. I really like the idea of using poll data to better understand the world. I had only known of Gallup's polling operation, not their management consulting side. Unfortunately, based on this book, it seems management consulting dominates their attention.</p>
<p>The only people who should read this book are those considering working with Gallup, the better to understand what they would be getting into.</p>
<p><img alt="The Coming Jobs War cover" src="coming_jobs_war_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180211-coming_jobs_war/</link>
<guid>http://planspace.org/20180211-coming_jobs_war/</guid>
<pubDate>Sun, 11 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Worm by Mark Bowden</title>
<description><![CDATA[

<p><a href="https://www.amazon.com/Worm-First-Digital-World-War/dp/0802119832">This</a> was the only book on computer security that I could find at my local library, which is a little surprising for DC. It's about the <a href="https://en.wikipedia.org/wiki/Conficker">Conficker worm</a>. It's <a href="http://www.nytimes.com/2011/10/04/books/mark-bowdens-worm-about-conficker-review.html">not</a> a great book.</p>
<p>I had hoped for more expertise. Even I know that <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> packets are not "packets of code" and that <a href="https://en.wikipedia.org/wiki/Denial-of-service_attack#Distributed_attack">DDoS</a> is not "Dedicated Denial of Service." That second error appears once, with later expansions being correct, but it is symptomatic of generally weak quality control.</p>
<p>A scan through the Wikipedia page for <a href="https://en.wikipedia.org/wiki/Conficker">Conficker</a> shows that Worm leaves out a number of interesting technical details: dictionary attacks are not mentioned at all, for example.</p>
<p>I did enjoy learning about the involvement of <a href="https://www.sri.com/">SRI</a> in connection with Conficker and with the development of the internet in general.</p>
<p>Bowden focuses on social aspects, and these mostly ring true: Lots of computers aren't updated, and hackers take advantage of them. Coordination, especially with government, can be difficult. Mischel Kwon in particular is not portrayed glowingly, but many of the cast of characters appear infantile in their own words.</p>
<p>The book succeeds in making security seem pretty inane. Microsoft makes a crummy product, so people get hacked. To stop the hacked machines from doing anything, the good guys try to buy a lot of domain names, and eventually fail. All our technology is both amazing and pitiful.</p>
<p><img alt="Worm cover" src="worm_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180128-worm_by_mark_bowden/</link>
<guid>http://planspace.org/20180128-worm_by_mark_bowden/</guid>
<pubDate>Sun, 28 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>No More Work by James Livingston</title>
<description><![CDATA[

<p><a href="http://history.rutgers.edu/faculty-directory/172-livingston-james">James Livingston</a>'s <a href="https://www.uncpress.org/book/9781469630656/no-more-work/">book</a> feels like <a href="https://en.wikipedia.org/wiki/Drunk_History">drunk history</a> about the future. To paraphrase, with the exception of "Fuck work," which appears multiple times:</p>
<blockquote>
<p>"The robots are taking all the jobs, man. Did you know that Nixon and <em>Cheney</em> were <a href="https://www.jacobinmag.com/2016/05/richard-nixon-ubi-basic-income-welfare/">trying</a> to do universal basic income way back in the 70's? They <em>knew</em>, man. Fuck work. We don't even <em>need</em> jobs. Just take Social Security from every dollar, and tax companies a little more, no problem, man... We all just have to <em>love</em> one another, that's all it is, man, can you even do that?"</p>
</blockquote>
<p>He goes on about Marx and Freud and so on a good deal more, but that's basically it.</p>
<p>As Livingston points out quite a lot, he's not the only one to say that jobs are evaporating. He distinguishes himself by not thinking it's important that people do something "meaningful" like gardening or carpentry, regardless of whether they get paid. He's okay with everybody watching TV. I gather this is similar to his take in <a href="https://www.amazon.com/Against-Thrift-Consumer-Culture-Environment/dp/0465021867">Against Thrift</a>.</p>
<p>It is an interesting hodgepodge of ideas of varying goodness. For example, that if owning people is bad, then also renting people is bad.</p>
<p>I wonder what will happen. Are jobs on the way out? Will the world go post-scarcity, like Star Trek? Will poor people just get trampled on, as seems to largely be the case so far? If you've left the labor force already because you can survive by taking advantage of some existing device, are you the vanguard of the future jobless? What will society be like?</p>
<p><img alt="No More Work cover" src="no_more_work_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180125-no_more_work_by_james_livingston/</link>
<guid>http://planspace.org/20180125-no_more_work_by_james_livingston/</guid>
<pubDate>Thu, 25 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Why the Rich are Getting Richer</title>
<description><![CDATA[

<p>The local public <a href="https://www.dclibrary.org/westend">library</a> reopened, and this <a href="https://www.amazon.com/Why-Rich-Are-Getting-Richer/dp/1612680887/">title</a> caught my eye. It turned out to be the opposite of what I expected: a window into Trump philosophy.</p>
<p><a href="https://en.wikipedia.org/wiki/Robert_Kiyosaki#Criticism_and_controversy">Robert Kiyosaki</a>, also author of <a href="https://en.wikipedia.org/wiki/Rich_Dad_Poor_Dad#Criticism">Rich Dad Poor Dad</a>, is a Trump man. Their names appear together on <a href="https://en.wikipedia.org/wiki/Why_We_Want_You_to_Be_Rich">two</a> <a href="https://en.wikipedia.org/wiki/Midas_Touch_(book)">books</a>, and they have ideas that don't come naturally to me.</p>
<p>For example, Kiyosaki doesn't like <a href="https://en.wikipedia.org/wiki/Federal_Reserve_System">the Fed</a> or taxes. He thinks "Not paying taxes is patriotic." He also says "Taxes Are Fair." He explains that rich people pay little in taxes because "He who has the gold makes the rules." He also says it's because the government wisely incentivizes things the rich do. It's a little dizzying.</p>
<blockquote>
<p>"The good news is that with <em>real</em> financial education, almost everyone can be in the 10% who earn 90% of money."</p>
</blockquote>
<p>I'm reminded of Flynn's <a href="/20170821-downward_spiral_destructive_of_civic_virtue/">downward spiral destructive of civic virtue</a>. To Kiyosaki, it isn't a <em>problem</em> that the rich are getting richer, because you too can get rich, if only you buy his books, come to his seminars, invest like him. You're a smart one.</p>
<p>Kiyosaki has a generalized concern about the dollar not being backed by gold. He recommends cash flow, for example by taking out loans to become a landlord. He's repeatedly dismissive of employees&#8211;of the very idea of work&#8211;but praises job creators.</p>
<p>I wonder which of his contradictions he has in mind when he quotes Fitzgerald's praise of "the ability to hold two opposing ideas in mind at the same time" at the close of the book.</p>
<p>Maybe Kiyosaki really does believe it's wrong to help the poor in ways other than the financial education he sells. Certainly, like Trump, he's most focused on image, on selling.</p>
<p>Maybe the lesson of this book is that some people really do believe all these things.</p>
<p><img alt="Why the Rich are Getting Richer cover" src="why_the_rich_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180122-why_the_rich_are_getting_richer/</link>
<guid>http://planspace.org/20180122-why_the_rich_are_getting_richer/</guid>
<pubDate>Mon, 22 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Janesville: An American Story</title>
<description><![CDATA[

<p>Obama <a href="https://www.facebook.com/barackobama/posts/10155532677446749">recommended</a> this <a href="https://www.amazon.com/Janesville-American-Story-Amy-Goldstein/dp/1501102230">book</a> about <a href="https://en.wikipedia.org/wiki/Janesville,_Wisconsin">Janesville</a>, a town not so far from where I grew up in Wisconsin. So I read the book.</p>
<p>In 2008, the General Motors <a href="https://en.wikipedia.org/wiki/Janesville_Assembly_Plant">factory</a> in Janesville shut down. Janesville is also House speaker <a href="https://en.wikipedia.org/wiki/Paul_Ryan">Paul Ryan</a>'s home. The book follows related issues through 2013.</p>
<p>Many of the lost jobs were union jobs with relatively good pay, health care, and pensions after retirement. Jobs that came to Janesville later tended to be less generous.</p>
<p>Business leaders and politicians tried to help Janesville recover by offering incentives to businesses. They were pro-job in that sense, but anti-union.</p>
<p>Retraining was supposed to help people who lost jobs get new skills and new good jobs. But research showed that among people who lost jobs, those who retrained went on to do worse than those who didn't retrain.</p>
<p>Non-government aid organizations in Janesville were less able to help people when the economy suffered and more people needed help. Teachers faced homeless students.</p>
<p>For some time, businesses have been giving <a href="https://www.brookings.edu/research/thirteen-facts-about-wage-growth/">a decreasing share of income</a> to their workers. I wonder whether the wage growth picture is worse if also considering disappearing benefits like health insurance and pensions.</p>
<p>At the same time, some politicians want to provide fewer benefits to citizens.</p>
<p>There are some good jobs out there with good pay, but not everyone can get them. Regardless of job, people should be able to live decent lives. It would be nice to see a clear path from where we are today to that ideal.</p>
<p><img alt="Janesville cover" src="janesville_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180121-janesville_an_american_story/</link>
<guid>http://planspace.org/20180121-janesville_an_american_story/</guid>
<pubDate>Sun, 21 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Boomerang by Michael Lewis</title>
<description><![CDATA[

<p>Michael Lewis wrote <a href="https://www.amazon.com/Moneyball-Art-Winning-Unfair-Game/dp/0393324818">Moneyball</a> and <a href="https://www.amazon.com/Big-Short-Inside-Doomsday-Machine/dp/0393338827">The Big Short</a>. He also wrote <a href="https://www.amazon.com/Boomerang-Travels-New-Third-World/dp/0393343448">Boomerang: Travels in the New Third World</a>, made up of essays published first in <a href="https://www.vanityfair.com/">Vanity Fair</a>, 2009-2011.</p>
<p>Each chapter focuses on one country:</p>
<ol>
<li>Iceland: dumb young male chauvinist bankers defaulted internationally</li>
<li>Greece: dumb male chauvinist everyone stole, defaulted, lied; also monks are there</li>
<li>Ireland: dumb young male chauvinist bankers defaulted on Irish real estate</li>
<li>Germany: best economy in Europe, still pretty dumb; also lots about poop</li>
<li>USA: cities promised pensions for cops and firefighters; cities now all bankrupt</li>
</ol>
<p>There's a preface with the big shorter from The Big Short, who says that he got himself a million dollars worth of nickels and recommends investing in guns and gold.</p>
<p>Lots of people, cities, banks, and countries are in a lot of debt, which could be bad.</p>
<p><img alt="Boomerang cover" src="boomerang_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180118-boomerang_by_michael_lewis/</link>
<guid>http://planspace.org/20180118-boomerang_by_michael_lewis/</guid>
<pubDate>Thu, 18 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Man in the High Castle</title>
<description><![CDATA[

<p>There is less for Nazis to like in <a href="https://en.wikipedia.org/wiki/Philip_K._Dick">Philip K. Dick</a>'s 1962 <a href="https://en.wikipedia.org/wiki/The_Man_in_the_High_Castle">novel</a> than in the 2015 Netflix <a href="https://en.wikipedia.org/wiki/The_Man_in_the_High_Castle">show</a>.</p>
<p>In both, Nazis won World War II. Also in both the idea of alternate realities is central, so perhaps it is appropriate for the two treatments to vary. And there are different concerns in text versus video. Nevertheless, I wonder about some of the choices in the show.</p>
<p>In the show, Juliana and an emo Nazi operative largely cooperate and have a kind of will-they-won't-they romantic tension. In the book, they have bad sex and then eventually Juliana slits his throat because he's a monster.</p>
<p>In the book, Hitler is out of power, suffering the late effects of syphilis. In the show, Hitler is still in power and collects the same film strips prized by the resistance.</p>
<p>The book spends no time at all in Nazi-controlled settings, while the show luxuriates in Nazi architecture and swastikas. An American who becomes a Nazi is a loving father with a suburban family and home that could just as well appear on <a href="https://en.wikipedia.org/wiki/Leave_It_to_Beaver">Leave It to Beaver</a>.</p>
<p>The show visits a spotless Nazi school where students learn to take pride in genocide and slavery through US history. We see the disabled are killed&#8211;and the Nazi justification.</p>
<p>One reason this is all disturbing is that, in a time when <a href="http://www.wusa9.com/news/local/dc/wwii-vets-shaken-by-nazi-flags-in-charlottesville/465189398">the Nazi flag really is waved within US borders</a>, it's uncomfortably likely that some Americans could watch The Man in the High Castle while longing to make America great as it appears under Nazi control.</p>
<p>The show illustrates the horror of normalizing the horrible, but it may also contribute to such normalization. It is disturbing to consider Nazi arguments, but truly terrifying to think that modern people might take them up as their own.</p>
<p>Comparing the Netflix show to the alternate universe film strips it features, it is not clear whether it is one that helps the side of democracy or the side of fascism.</p>    
    ]]></description>
<link>http://planspace.org/20180115-man_in_high_castle/</link>
<guid>http://planspace.org/20180115-man_in_high_castle/</guid>
<pubDate>Mon, 15 Jan 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Your Life</title>
<description><![CDATA[

<p>I heard about <a href="http://designingyour.life/the-book/">Designing Your Life</a> in an <a href="https://www.npr.org/2017/08/28/546716951/you-2-0-how-silicon-valley-can-help-you-get-unstuck">episode</a> of <a href="https://en.wikipedia.org/wiki/Shankar_Vedantam">Shankar Vedantam</a>'s <a href="https://www.npr.org/series/423302056/hidden-brain">Hidden Brain</a>.  The book roughly follows a <a href="http://designingyour.life/resources-authorized/">workshop</a> format of a <a href="http://lifedesignlab.stanford.edu/dyl">Stanford class</a>. It's quick to read&#8212;slower to do&#8212;and full of fun examples and good reminders.</p>
<p><img alt="Designing Your Life" src="designing_your_life_cover.jpg"></p>
<p>There are five core recommendations:</p>
<ol>
<li>be curious (curiosity)</li>
<li>try stuff (bias to action)</li>
<li>reframe problems (reframing)</li>
<li>know it's a process (awareness)</li>
<li>ask for help (radical collaboration)</li>
</ol>
<p>The authors identify two concepts to be wary of:</p>
<ul>
<li>gravity problems: things that can't be productively addressed directly, just as eliminating the force of gravity isn't possible</li>
<li>anchor problems: things that prevent progress because of overcommitment to one issue, solution path, etc.</li>
</ul>
<p>Both of these "problems" may involve "dysfunctional beliefs" that may be better reframed.</p>
<p>The bulk of the text is around a number of <a href="http://designingyour.life/resources-authorized/">activities</a>:</p>
<ul>
<li>Evaluate your life along dimensions of love, play, work, and health.</li>
<li>Write out a "workview" and "lifeview" and think about how they interact.</li>
<li>Keep track of what you actually enjoy doing ("good time journal").</li>
<li>Use mind mapping and brainstorming.</li>
<li>Take incremental steps to learn/try/change things ("prototyping").</li>
<li>Imagine multiple possible futures ("three odyssey plans") not just one "right" path.</li>
<li>Network by meeting people ("life design interviews") don't just apply for listed jobs.</li>
<li>Log and reflect on failures to find growth areas ("failure log").</li>
</ul>
<p>The chapter on "How not to get a job" is very sensible.</p>
<p>Toward the end, the book is somewhat philosophical. It references <a href="https://en.wikipedia.org/wiki/Finite_and_Infinite_Games">Finite and Infinite Games</a> and talks about a "failure immunity" point of view (positive psychology).</p>
<p>The Hidden Brain episode was called "Getting Unstuck." That describes the book's utility. The ideas are not revolutionary, but it is nice to be reminded, especially when in a rut, to be positive, think creatively, make small improvements, and try new things. The activities in the book help to operationalize these recommendations.</p>    
    ]]></description>
<link>http://planspace.org/20171229-designing_your_life/</link>
<guid>http://planspace.org/20171229-designing_your_life/</guid>
<pubDate>Fri, 29 Dec 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Deep Reinforcement Learning</title>
<description><![CDATA[

<p><em>This is a presentation given for <a href="https://www.meetup.com/Data-Science-DC/">Data Science DC</a> on <a href="https://www.meetup.com/Data-Science-DC/events/244145151">Tuesday November 14, 2017</a>.</em></p>
<ul>
<li><em><a href="deep_rl.pdf">PDF slides</a></em></li>
<li><em><a href="deep_rl.pptx">PPTX slides</a></em></li>
</ul>
<p>Further resources up front:</p>
<ul>
<li><a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> (paper)</li>
<li>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> (blog post)</li>
<li><a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> (textbook)</li>
<li>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> (videos and slides)</li>
<li><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> (videos, slides, and labs)</li>
<li>OpenAI <a href="https://github.com/openai/gym">gym</a> / <a href="https://github.com/openai/baselines">baselines</a> (software)</li>
<li><a href="http://nationalgocenter.org/">National Go Center</a> (physical place)</li>
<li><a href="http://dc.hackandtell.org/">Hack and Tell</a> (fun meetup)</li>
</ul>
<p>The following goes through all the content of the talk:</p>
<hr>
<p>Aaron Schumacher</p>
<ul>
<li>planspace.org has these slides</li>
</ul>
<hr>
<p>Hi! I'm Aaron. All these slides, and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work at <a href="http://www.deeplearninganalytics.com/">Deep Learning Analytics</a> (DLA). We do deep learning work for government and commercial customers. DLA is a great place to work, and one of the ways it's great is that it sends us to conferences and such things.</p>
<hr>
<p><img alt="Deep RL Bootcamp" src="img/deep_rl_bootcamp.png"></p>
<hr>
<p>DLA sent me to the first UC Berkeley <a href="https://www.deepbootcamp.io/">Deep RL Bootcamp</a> organized by Abbeel, Duan, Chen, and Karpathy. It was a great experience and it largely inspired this talk.</p>
<p>I have a separate <a href="/20170830-berkeley_deep_rl_bootcamp/">summary write-up</a> about my experience at the bootcamp, and they've since put up all the <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">videos, slides, and labs</a>, so you can see everything that was covered there.</p>
<hr>
<p><img alt="Sutton and Barto's textbook" src="img/sutton_and_barto.jpg"></p>
<hr>
<p>The other major source for this talk is Sutton and Barto's textbook, which I like a lot.</p>
<p>The picture shows <a href="https://mitpress.mit.edu/books/reinforcement-learning">the first edition</a>, which is not what you want. The second edition is <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">available free online</a>, and was last updated about a week ago (November 5, 2017).</p>
<p>Sutton and Barto are major figures in reinforcement learning, and they do not follow any <a href="https://en.wikipedia.org/wiki/Wikipedia:No_original_research">no original research rules</a>, making their book really fairly exciting, if you're not put off by the length (over 400 pages).</p>
<p>(The diagrams on the cover are not neural nets, but backup diagrams.)</p>
<hr>
<p>Plan</p>
<ul>
<li>applications: what<ul>
<li>theory</li>
</ul>
</li>
<li>applications: how</li>
<li>onward</li>
</ul>
<hr>
<p>The plan for today is to first mention four successful applications of reinforcement learning. Then we'll go through a core of theory. This will let us then understand pretty completely how each of those applications is achieved. Finally, we'll wrap up, looking at a few other applications and thoughts about how things are going.</p>
<hr>
<p>applications: what</p>
<hr>
<p>The applications here are all going to be games, not because reinforcement learning is only applicable to games, but because games are fun, and these examples are well known and cover a good range of techniques.</p>
<hr>
<p><img alt="backgammon" src="img/backgammon.png"></p>
<hr>
<p>First up, backgammon.</p>
<hr>
<p><img alt="Atari Breakout" src="img/breakout.png"></p>
<hr>
<p>Next, Atari. A lot of Atari games are well played by RL now. The ones shown (Video Pinball, Boxing, Breakout) are some of the ones that RL does the best on.</p>
<hr>
<p><img alt="tetris" src="img/tetris.png"></p>
<hr>
<p>I'm also including Tetris, mostly because it's a chance to talk about an interesting technique.</p>
<hr>
<p><img alt="go" src="img/go.jpg"></p>
<hr>
<p>And in the last two years, Go has been pretty much conquered by RL, so we'll talk about that.</p>
<hr>
<p>theory</p>
<hr>
<p>Let's start to build up the theory of reinforcement learning.</p>
<p>This is going to start very gradually, but I promise that by the end we'll be moving fast.</p>
<hr>
<p>Yann LeCun's cake</p>
<ul>
<li>cake: unsupervised learning</li>
<li>icing: supervised learning</li>
<li>cherry: reinforcement learning</li>
</ul>
<p><img alt="cake" src="img/cake.jpg"></p>
<hr>
<p>Yann LeCun <a href="https://drive.google.com/file/d/0BxKBnD5y2M8NREZod0tVdW5FLTQ/view">introduced</a> this cake idea for relating three main varieties of machine learning. It's largely based on one view of how much information is used at each training step.</p>
<p>I'm going to use it to build up and relate these three kinds of learning, while introducing reinforcement learning notation.</p>
<hr>
<p>unsupervised learning</p>
<ul>
<li>\( s \)</li>
</ul>
<hr>
<p>In unsupervised learning, we have a collection of states, where each individual state can be referred to with \( s \).</p>
<p>I'm using "state" without distinguishing "state" from "observation". You could also call these "examples" or "<a href="https://en.wikipedia.org/wiki/Covariate">covariates</a>" or "data points" or whatever you like.</p>
<p>The symbol "x" is commonly used.</p>
<hr>
<p>state \( s \)</p>
<ul>
<li>numbers</li>
<li>text (as numbers)</li>
<li>image (as numbers)</li>
<li>sound (as numbers)</li>
</ul>
<hr>
<p>States can be anything as long as it can be expressed numerically. So that includes text, images, and sound. Really anything.</p>
<hr>
<p>unsupervised (?) learning</p>
<ul>
<li>given \( s \)</li>
<li>learn \( s \rightarrow \text{cluster_id} \)</li>
<li>learn \( s \rightarrow s \)</li>
</ul>
<hr>
<p>So say we have a set of a thousand images. Each image is an \( s \).</p>
<p>We want to learn something, and that tends to mean unsupervised learning starts to resemble supervised learning.</p>
<p>At two ends of a spectrum we have clustering and <a href="https://en.wikipedia.org/wiki/Autoencoder">autoencoders</a>, and all kinds of dimensionality reduction in between.</p>
<p>Unsupervised learning is sort of the <a href="https://en.wikipedia.org/wiki/Dark_matter">dark matter</a> of machine learning. Even <a href="https://www.youtube.com/watch?v=vdWPQ6iAkT4">Yann LeCun</a> says "We are missing the principles for unsupervised learning."</p>
<hr>
<p>deep unsupervised learning</p>
<ul>
<li>\( s \) with deep neural nets</li>
</ul>
<hr>
<p>Deep unsupervised learning is whenever we do unsupervised learning and somewhere there's a deep neural net.</p>
<hr>
<p>supervised learning</p>
<ul>
<li>\( s \rightarrow a \)</li>
</ul>
<hr>
<p>Up next is supervised learning. We're introducing a new entity \( a \), which I'll call an "action". It's common to call it a "label" or a "target" and to use the symbol "y". Same thing.</p>
<hr>
<p>action a</p>
<ul>
<li>numbers</li>
<li>"cat"/"dog"</li>
<li>"left"/"right"</li>
<li>17.3</li>
<li>[2.0, 11.7, 5]</li>
<li>4.2V</li>
</ul>
<hr>
<p>Whatever you call it, the action is again a numeric thing. It could be anything that \( s \) could be, but it tends to be lower-dimensional.</p>
<p>The cat/dog classifier is a popular example, and a left/right classifier is just the same, but those might feel more like actions.</p>
<hr>
<p>supervised learning</p>
<ul>
<li>given \( s, a \)</li>
<li>learn \( s \rightarrow a \)</li>
</ul>
<hr>
<p>In supervised learning you have a training set of state-action pairs, and you try to learn a function to produce the correct action based on the state alone.</p>
<p>Supervised learning can blur into imitation learning, which can be taken as a kind of reinforcement learning. For example, <a href="https://arxiv.org/abs/1604.07316">NVIDIA's end-to-end self-driving car</a> is based on learning human driving behaviors. (Sergey Levine <a href="https://www.youtube.com/watch?v=Pw1mvoOD-3A&amp;list=PLkFD6_40KJIxopmdJF_CLNqG3QuDFHQUm&amp;index=13">explains</a> in some depth.) But I'm not going to talk more about imitation learning, and supervised learning will stand alone.</p>
<p>You can learn this function with linear regression or support vector machines or whatever you like.</p>
<hr>
<p>deep supervised learning</p>
<ul>
<li>\( s \rightarrow a \) with deep neural nets</li>
</ul>
<hr>
<p>Deep supervised learning is whenever we do supervised learning and somewhere there's a deep neural net.</p>
<p>There's also semi-supervised learning, when you have some labeled data and some unlabeled data, which can connect to active learning, which has some relation to reinforcement learning, but that's all I'll say about that.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\(r, s \rightarrow a\)</li>
</ul>
<hr>
<p>Finally, we reach reinforcement learning.</p>
<p>We're adding a new thing \( r \), which is reward.</p>
<hr>
<p>reward \( r \)</p>
<ul>
<li>-3</li>
<li>0</li>
<li>7.4</li>
<li>1</li>
</ul>
<hr>
<p>Reward is a scalar, and we like positive rewards.</p>
<hr>
<p>optimal control / reinforcement learning</p>
<hr>
<p>I'll mention that <a href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a> is closely related to reinforcement learning. It has its own parallel notation and conventions, and I'm going to ignore all that.</p>
<hr>
<p>\( r, s \rightarrow a \)</p>
<hr>
<p>So here's the reinforcement learning setting.</p>
<p>We get a reward and a state, and the agent chooses an action.</p>
<hr>
<p>tick</p>
<hr>
<p>Then, time passes. We're using discrete time, so this is a "tick".</p>
<hr>
<p>\( r', s' \rightarrow a' \)</p>
<hr>
<p>Then we get a new reward and state, which depend on the previous state and action, and the agent chooses a new action.</p>
<hr>
<p>tick</p>
<hr>
<p>And so on.</p>
<hr>
<p><img alt="standard diagram" src="img/standard_diagram.png"></p>
<hr>
<p>This is the standard reinforcement learning diagram, showing the agent and environment. My notation is similar.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>"given" \( r, s, a, r', s', a', ... \)</li>
<li>learn "good" \( s \rightarrow a \)</li>
</ul>
<hr>
<p>So here's reinforcement learning.</p>
<p>"Given" is in quotes because the rewards and states that we see depend on the actions we choose.</p>
<p>"Good" is in quotes because we haven't defined "good" beyond that we like positive rewards.</p>
<hr>
<p>Question:</p>
<p>Can you formulate supervised learning as reinforcement learning?</p>
<hr>
<p>Here's a question to think about.</p>
<hr>
<p>supervised as reinforcement?</p>
<ul>
<li>reward 0 for incorrect, reward 1 for correct<ul>
<li>beyond binary classification?</li>
</ul>
</li>
<li>reward deterministic</li>
<li>next state random</li>
</ul>
<hr>
<p>You can re-cast supervised learning as reinforcement learning, but it isn't necessarily as efficient: it's better to be told what the right answer is than to just be told that you're wrong. This is one sense in which Yann LeCun means that reinforcement learning is using fewer bits of information per example.</p>
<hr>
<p>Question:</p>
<p>Why is the Sarsa algorithm called Sarsa?</p>
<hr>
<p>Here's a question that you can already figure out!</p>
<p>(Sarsa is an on-policy TD control algorithm due to Sutton, 1996.)</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\( r, s \rightarrow a \)</li>
</ul>
<hr>
<p>Sarsa: State, Action, Reward, State, Action.</p>
<p>We'll see the closely related Q-learning algorithm in some detail later on.</p>
<hr>
<p>deep reinforcement learning</p>
<ul>
<li>\( r, s \rightarrow a \) with deep neural nets</li>
</ul>
<hr>
<p>Deep reinforcement learning is whenever we do reinforcement learning and somewhere there's a deep neural net.</p>
<p>The one twist is that in reinforcement learning, the network(s) may not be in the obvious place. We'll need to develop a few more ideas to see more places to put a neural net.</p>
<hr>
<p>Markov property</p>
<ul>
<li>\( s \) is enough</li>
</ul>
<hr>
<p>We're going to assume <a href="https://en.wikipedia.org/wiki/Markov_property">the Markov property</a>, which means that the state we observe tells us as much as we can know about what will happen next.</p>
<p>This is frequently not true, but we'll assume it anyway.</p>
<hr>
<pre><code>                   Make choices?
                   no                        yes
Completely    no   Markov chain              MDP: Markov Decision Process
observable?   yes  HMM: Hidden Markov Model  POMDP: Partially Observable MDP</code></pre>

<hr>
<p>This chart relates a bunch of Markov things, to hopefully help you orient yourself.</p>
<p>We're working with Markov Decision Processes (MDPs). Even though things are frequently not really completely observable, we'll usually just ignore that. Often this is fine (think about blackjack, for example).</p>
<p>It's often easiest to think about finite spaces, but this isn't always necessary.</p>
<p>(The chart is from <a href="http://www.pomdp.org/faq.html">POMDP.org</a>.)</p>
<hr>
<p>usual RL problem elaboration</p>
<hr>
<p>We'll introduce some more notation and a few ideas that help us work on reinforcement learning problems.</p>
<hr>
<p>policy \( \pi \)</p>
<ul>
<li>\( \pi: s \rightarrow a \)</li>
</ul>
<hr>
<p>This thing that takes a state and produces an action we'll call a "policy" \( \pi \). This effectively <em>is</em> our reinforcement learning agent, and the name of the game is figuring out how to get a good policy.</p>
<p>But how do we even know whether a policy is good?</p>
<p>(If you don't like spelling in Greek, you can imagine it's a little walker!)</p>
<hr>
<p>return</p>
<ul>
<li>\( \sum{r} \)<ul>
<li>into the future (episodic or ongoing)</li>
</ul>
</li>
</ul>
<hr>
<p>We want a policy that maximizes "return" \( \sum{r} \). Return is just the sum of all the rewards we'll get into the future. The symbol "G" is also used, but I'll keep writing the sum.</p>
<p>Reinforcement learning can consider finite-length episodes that always come to an end, or the ongoing setting in which the agent keeps on going. (You can see that there could be a problem with infinity in the ongoing setting.)</p>
<hr>
<p><img alt="gridworld" src="img/gridworld.png"></p>
<hr>
<p>Here's a typical toy example for understanding reinforcement learning: a gridworld.</p>
<p>There are sixteen states, each uniquely identifiable to the agent.</p>
<p>There are four actions: go up, down, left, right.</p>
<p>We want the agent to learn to go from the start to the goal.</p>
<p>We'll say this is episodic, with each episode ending after the agent makes it to the goal.</p>
<p>So how do we set up the rewards?</p>
<hr>
<p><img alt="gridworld reward" src="img/gridworld_reward.png"></p>
<hr>
<p>Here's a natural way to set up reward. You get a reward of one when you complete the task. What does the return at each state look like, then?</p>
<hr>
<p><img alt="gridworld return" src="img/gridworld_return.png"></p>
<hr>
<p>The return, unfortunately, is just one everywhere. And this means that there isn't any reason to go directly to the goal. We have all the time in the world, so we could just as well meander randomly until we happen to hit the goal. This isn't very good.</p>
<hr>
<p>Question:</p>
<p>How do we keep our agent from dawdling?</p>
<hr>
<p>How can we set things up so that going directly to the goal position is incentivized?</p>
<p>Consider changing rewards, or changing how return is calculated, or anything else.</p>
<p>There are at least three approaches that are commonly used.</p>
<hr>
<p>negative reward at each time step</p>
<hr>
<p>If we get a negative reward at each time step, the only way to maximize return is to minimize episode length, so we're motivated to end the episode as soon as possible. This certainly makes sense for the episodic setting.</p>
<p>(It's like existing is pain; the agent is like <a href="http://rickandmorty.wikia.com/wiki/Mr._Meeseeks">Mr. Meeseeks</a>!)</p>
<hr>
<p>discounted rewards</p>
<ul>
<li>\( \sum{\gamma^tr} \)</li>
</ul>
<hr>
<p>It's very common to use a "discount factor" \( \gamma \), which might be 0.9, for example. Then a reward soon is worth more than a reward later, and again the agent is motivated to do things more directly.</p>
<hr>
<p>average rate of reward</p>
<ul>
<li>\( \frac{\sum{r}}{t} \)</li>
</ul>
<hr>
<p>Average rate of reward is also used in some settings, where it can make things more stable.</p>
<hr>
<p>value functions</p>
<ul>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>I already snuck in a value function when we saw green values for every state in the gridworld.</p>
<p>The state value function \( v \) tells us the return from any state, and the state-action value function \( q \) gives the return from any state-action combination. It might not yet be clear why the two are importantly different, so let's get into that.</p>
<hr>
<p>Question:</p>
<p>Does a value function specify a policy (if you want to maximize return)?</p>
<hr>
<p>Here's the relevant question to consider.</p>
<hr>
<p>trick question?</p>
<ul>
<li>\( v_\pi \)</li>
<li>\( q_\pi \)</li>
</ul>
<hr>
<p>Is it a trick question? Really, values depend on what you do, on your policy. But let's assume I just give you a value function and you trust it. Can you use it to induce a policy?</p>
<hr>
<p>value functions</p>
<ul>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>The state-action value function \( q \) certainly implies a policy. For whatever state we're in, we check the return for all available actions, and choose the action that's best.</p>
<p>For the state value function \( v \), we can get a policy in a similar way, but only if we know what state we'll be in after taking an action. That may or may not be true.</p>
<hr>
<p>environment dynamics</p>
<ul>
<li>\( s, a \rightarrow r', s' \)</li>
</ul>
<hr>
<p>The next state and reward depend on the previous state and action, and that's determined by the environment. We don't necessarily know what's going on inside the environment.</p>
<hr>
<p><img alt="gridworld" src="img/gridworld.png"></p>
<hr>
<p>When you, a smart human with lots of ideas about physics, look at this picture of a gridworld, you assume that going "right" will do what you expect.</p>
<p>But in general, we don't know in advance how the environment behaves. Maybe going "right" always takes you to the upper left state, for example.</p>
<p>Sometimes you do know the environment dynamics, like with games with well-defined rules. But sometimes you don't. You could try to learn the environment dynamics with a model.</p>
<hr>
<p>model</p>
<ul>
<li>\( s, a \rightarrow r', s' \)</li>
</ul>
<hr>
<p>Reinforcement learning uses "model" to refer to a model that you learn for the environment dynamics. (Often, just the next state is predicted.)</p>
<p>This model is something that the agent has to learn, and depending on whether the agent is learning a model determines whether you're said to be doing model-based vs. model-free reinforcement learning.</p>
<p>(In adaptive control this "model learning" is called "system identification".)</p>
<hr>
<p>learning and acting</p>
<hr>
<p>Everything to this point has been elaborating the problem setup for reinforcement learning. Now we get into how we actually learn and go forth with RL.</p>
<hr>
<p>learn model of dynamics</p>
<ul>
<li>from experience</li>
<li>difficulty varies</li>
</ul>
<hr>
<p>As mentioned, you could try to learn your environment's dynamics. If you can do this well, it's great, but it may not be easy. Model-based RL is an exciting area of research.</p>
<hr>
<p>learn value function(s) with planning</p>
<ul>
<li>assuming we have the environment dynamics or a good model already</li>
</ul>
<hr>
<p>To introduce value function learning, let's consider the situation where you know nothing yet but you have the environment dynamics, so you can make decisions by looking ahead. This is a familiar thought process, conceptually.</p>
<hr>
<p><img alt="planning 0" src="img/planning0.png"></p>
<hr>
<p>You're in state \( s_1 \) and you have to choose between \( a_1 \) and \( a_2 \).</p>
<p>Because you have the environment dynamics, you can see what would happen next.</p>
<hr>
<p><img alt="planning 1" src="img/planning1.png"></p>
<hr>
<p>Looking one time step ahead, you can already start to evaluate your action choices by the rewards you'd get immediately. This is the state-action value view, \( q \): you're evaluating the value of the <em>actions</em> directly.</p>
<hr>
<p><img alt="planning 2" src="img/planning2.png"></p>
<hr>
<p>Looking two time steps ahead, you can similarly start to evaluate how good the states you would get to are. This is the state value view, \( v \).</p>
<p>This example is simple and assumes states and actions never recur. We also haven't introduced a couple more wrinkles. You can think about differences between thinking in terms of \( v \) and \( q \) later, for example in the case of backgammon.</p>
<p>This kind of planning can happen continuously in the background, or it can be done on demand at decision time. The search space can get quite large.</p>
<hr>
<p>connections</p>
<ul>
<li>Dijkstra's algorithm</li>
<li>A* algorithm</li>
<li>minimax search<ul>
<li>two-player games not a problem</li>
</ul>
</li>
<li>Monte Carlo tree search</li>
</ul>
<hr>
<p>The graph structure on the previous slide might make you think of a range of algorithms that you could already be familiar with.</p>
<p>You can think of these as smart ways of exploring the possibly very large branching structures that can spring up. <a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo tree search</a> is particularly clever, and it will appear again later.</p>
<hr>
<p><img alt="rollout diagram" src="img/rollout_diagram.png"></p>
<hr>
<p>One way of dealing with all the possible tree paths is just to sample from it and try to make estimates on the basis of these.</p>
<hr>
<p><img alt="backgammon roll" src="img/rollout.jpg"></p>
<hr>
<p>I thought it was interesting to learn that the term "roll-out" comes from work on backgammon, where you literally roll dice to move a game along.</p>
<hr>
<p>everything is stochastic</p>
<hr>
<p>Backgammon has randomness in the environment, from the dice, but randomness can enter all over the place.</p>
<hr>
<p><img alt="stochastic s'" src="img/stochastic_s.png"></p>
<hr>
<p>The next state can be random.</p>
<hr>
<p><img alt="stochastic r'" src="img/stochastic_r.png"></p>
<hr>
<p>And the next reward can be random, as in the case of <a href="https://en.wikipedia.org/wiki/Multi-armed_bandit">multi-armed bandits</a>.</p>
<hr>
<p>exploration vs. exploitation</p>
<ul>
<li>\( \epsilon \)-greedy policy<ul>
<li>usually do what looks best</li>
<li>\( \epsilon \) of the time, choose random action</li>
</ul>
</li>
</ul>
<hr>
<p>This can always be an issue in unknown environments, but especially with randomness, we encounter the issue of exploration vs. exploitation. The \( \epsilon \)-greedy approach is one well-known way to ensure that an agent keeps exploring.</p>
<hr>
<p>Question:</p>
<p>How does randomness affect \( \pi \), \( v \), and \( q \)?</p>
<ul>
<li>\( \pi: s \rightarrow a \)</li>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s, a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>Now acknowledging that there's randomness all over the place, I can show another way that my notation is shorthand.</p>
<hr>
<p>Question:</p>
<p>How does randomness affect \( \pi \), \( v \), and \( q \)?</p>
<ul>
<li>\( \pi: \mathbb{P}(a|s) \)</li>
<li>\( v: s \rightarrow \mathbb{E} \sum{r} \)</li>
<li>\( q: s, a \rightarrow \mathbb{E} \sum{r} \)</li>
</ul>
<hr>
<p>I haven't been hiding too much. A policy is a probability distribution over possible actions, and value functions give expectations. No problem.</p>
<hr>
<p>Monte Carlo returns</p>
<ul>
<li>\( v(s) = ? \)</li>
<li>keep track and average</li>
<li>like "planning" from experienced "roll-outs"</li>
</ul>
<hr>
<p>Here's our first pass at a model-free learning algorithm. We interact with the environment, and use our experiences in the past just like model-based roll-outs.</p>
<hr>
<p>non-stationarity</p>
<ul>
<li>\( v(s) \) changes over time!</li>
</ul>
<hr>
<p>The environment may change, and we want our methods to be able to deal with this.</p>
<hr>
<p>moving average</p>
<ul>
<li>new mean = old mean + \( \alpha \)(new sample - old mean)</li>
</ul>
<hr>
<p>This kind of moving average is used all over the place.</p>
<p>The parameter \( \alpha \) is a learning rate, and the whole thing can be seen to be a case of stochastic gradient descent, which is a nice early connection to neural nets which start turning up.</p>
<p>(Sutton and Barto use \( \alpha \) for learning rate, not the otherwise popular \( \eta \). This is a sensible choice for them especially since they sometimes have multiple learning rates in the same algorithm, and the second learning rate can then be \( \beta \).)</p>
<hr>
<p><img alt="MDP diagram" src="img/mdp_diagram.png"></p>
<hr>
<p>We can think about this question, which is not a trick: the difference is \( r' \).</p>
<hr>
<p>Bellman equation</p>
<ul>
<li>\( v(s) = r' + v(s') \)</li>
</ul>
<hr>
<p>That relation gives rise to a bunch of Bellman equations, which are quite useful.</p>
<p><a href="https://en.wikipedia.org/wiki/Richard_E._Bellman">Bellman</a> came up with a couple neat things. <a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programming</a> (despite being <a href="http://arcanesentiment.blogspot.com/2010/04/why-dynamic-programming.html">poorly named</a>) is great for lots of problems, including those involving Bellman equations.</p>
<p>Bellman also introduced the phrase "<a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">curse of dimensionality</a>".</p>
<hr>
<p>temporal difference (TD)</p>
<hr>
<p>Bellman equations give rise to temporal difference (TD) learning (Sutton, 1988).</p>
<p>It's sometimes described as "bootstrap" learning.</p>
<hr>
<p>Q-learning</p>
<ul>
<li>\( \text{new } q(s, a) = q(s, a) + \alpha (r' + \gamma \max_{a} q(s', a) - q(s, a)) \)</li>
</ul>
<hr>
<p>Q-learning is a temporal difference method that combines a lot of the things we've just seen.</p>
<p>You can see in the equation that Sarsa is closely related.</p>
<hr>
<p>on-policy / off-policy</p>
<hr>
<p>Reinforcement learning differentiates between on-policy and off-policy learning. On-policy learning is when you're learning about the policy that you're currently following, and it's generally easier than off-policy learning.</p>
<p>Importance sampling is one way to achieve off-policy learning.</p>
<p>Q-learning is off-policy, which is nice. You can look at the equation and see why.</p>
<hr>
<p>estimate \( v \), \( q \)</p>
<ul>
<li>with a deep neural network</li>
</ul>
<hr>
<p>We've sort of implicitly been doing what's called tabular learning, where each state or state-action has its own estimated return. But we can plug in any kind of supervised learning algorithm, including deep learning. We'll see this in applications.</p>
<hr>
<p>back to the \( \pi \)</p>
<ul>
<li>parameterize \( \pi \) directly</li>
<li>update based on how well it works</li>
<li>REINFORCE<ul>
<li>REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility</li>
</ul>
</li>
</ul>
<hr>
<p>Stepping back from value functions, we can work with a parameterized policy directly.</p>
<p>REINFORCE is not necessarily a great acronym.</p>
<hr>
<p>policy gradient</p>
<ul>
<li>\( \nabla \log (\pi(a|s))\)</li>
</ul>
<hr>
<p>This is the gradient that you use.</p>
<p>You may see the connection to score functions and the so-called <a href="http://blog.shakirm.com/2015/11/machine-learning-trick-of-the-day-5-log-derivative-trick/">log-derivative trick</a>, but we can interpret without all that.</p>
<p>If an action contributes to high return, you can encourage it, and if it contributes to low return, you can discourage it.</p>
<hr>
<p>actor-critic</p>
<ul>
<li>\( \pi \) is the actor</li>
<li>\( v \) is the critic</li>
<li>train \( \pi \) by policy gradient to encourage actions that work out better than \( v \) expected</li>
</ul>
<hr>
<p>Actor-critic algorithms combine policy gradient and value function methods to reduce variance, and are pretty popular.</p>
<hr>
<p>applications: how</p>
<hr>
<p>We now know enough to understand a lot of the details of big reinforcement learning applications!</p>
<hr>
<p>TD-gammon (1992)</p>
<ul>
<li>\( s \) is custom features</li>
<li>\( v \) with shallow neural net</li>
<li>\( r \) is 1 for a win, 0 otherwise</li>
<li>\( TD(\lambda) \)<ul>
<li>eligibility traces</li>
</ul>
</li>
<li>self play</li>
<li>shallow forward search</li>
</ul>
<hr>
<p>Backgammon was dealt with pretty completely about 25 years ago already.</p>
<p>We know what state the board will be in after any move, and the state of the board is what matters, so it makes sense to use a state value function \( v \) here.</p>
<hr>
<p>DQN (2015)</p>
<ul>
<li>\( s \) is four frames of video</li>
<li>\( v \) with deep convolutional neural net</li>
<li>\( r \) is 1 if score increases, -1 if decreases, 0 otherwise</li>
<li>Q-learning<ul>
<li>usually-frozen target network</li>
<li>clipping update size, etc.</li>
</ul>
</li>
<li>\( \epsilon \)-greedy</li>
<li>experience replay</li>
</ul>
<hr>
<p>Deep Q-Networks (DQN) is a well-known algorithm that works well for many Atari games. See:</p>
<ul>
<li><a href="https://arxiv.org/abs/1312.5602">Playing Atari with Deep Reinforcement Learning</a> (2013)</li>
<li><a href="https://deepmind.com/research/dqn/">Human-level control through Deep Reinforcement Learning</a> (2015)</li>
</ul>
<hr>
<p>evolution (2006)</p>
<ul>
<li>\( s \) is custom features</li>
<li>\( \pi \) has a simple parameterization</li>
<li>evaluate by final score</li>
<li>cross-entropy method</li>
</ul>
<hr>
<p>Surprise! Tetris is well-solved by evolutionary methods, which I didn't mention at all. These methods are sneaky that way. They can work better than you'd think. See:</p>
<ul>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Learning Tetris Using the Noisy Cross-Entropy Method</a> (2006)</li>
<li><a href="https://blog.openai.com/evolution-strategies/">Evolution Strategies as a Scalable Alternative to Reinforcement Learning</a></li>
</ul>
<hr>
<p>AlphaGo (2016)</p>
<ul>
<li>\( s \) is custom features over geometry<ul>
<li>big and small variants</li>
</ul>
</li>
<li>\( \pi_{SL} \) with deep convolutional neural net, supervised training</li>
<li>\( \pi_{rollout} \) with smaller convolutional neural net, supervised training</li>
<li>\( r \) is 1 for a win, -1 for a loss, 0 otherwise</li>
<li>\( \pi_{RL} \) is \( \pi_{SL} \) refined with policy gradient peer-play reinforcement learning</li>
<li>\( v \) with deep convolutional neural net, trained based on \( \pi_{RL} \) games</li>
<li>asynchronous policy and value Monte Carlo tree search<ul>
<li>expand tree with \( \pi_{SL} \)</li>
<li>evaluate positions with blend of \( v \) and \( \pi_{rollout} \) rollouts</li>
</ul>
</li>
</ul>
<hr>
<p>AlphaGo surprised a lot of people by reaching super-human levels of Go play. It was a bit complicated. See:</p>
<ul>
<li><a href="https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf">Mastering the game of Go with deep neural networks and tree search</a></li>
</ul>
<hr>
<p>AlphaGo Zero (2017)</p>
<ul>
<li>\( s \) is simple features over time and geometry</li>
<li>\( \pi, v \) with deep residual convolutional neural net</li>
<li>\( r \) is 1 for a win, -1 for a loss, 0 otherwise</li>
<li>Monte Carlo tree search for self-play training and play</li>
</ul>
<hr>
<p>AlphaGo Zero improved on AlphaGo in performance and by requiring no human training data. It reaches incredibly good performance with only its self-play learning method. See:</p>
<ul>
<li><a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">Mastering the game of Go without human knowledge</a><ul>
<li>Interesting quote on reach/limitations: "Our approach is most directly applicable to Zero-sum games of perfect information."</li>
</ul>
</li>
</ul>
<p>(AlphaGo Zero can only play legal moves, but stupid moves (like filling eyes) aren't forbidden, so it is less constrained than the original AlphaGo.)</p>
<hr>
<p>onward</p>
<hr>
<p>Let's look at some more applications to paint out more of the area that reinforcement learning can influence.</p>
<hr>
<p>Neural Architecture Search (NAS)</p>
<hr>
<p>Neural Architecture Search (NAS) uses policy gradient where the actions design a neural net. The reward is validation set performance. See:</p>
<ul>
<li><a href="https://research.googleblog.com/2017/11/automl-for-large-scale-image.html">AutoML for large scale image classification and object detection</a></li>
<li><a href="https://arxiv.org/abs/1707.07012">Learning Transferable Architectures for Scalable Image Recognition</a></li>
<li><a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search with Reinforcement Learning</a></li>
</ul>
<hr>
<p><img alt="Amazon Echo" src="img/echo.jpg"></p>
<hr>
<p>A lot of people are interested in natural language processing. A lot of chatbots are still big nests of <code>if</code> statements, but there is interest in using RL for language. See:</p>
<ul>
<li><a href="https://arxiv.org/abs/1709.02349">A Deep Reinforcement Learning Chatbot</a><ul>
<li><a href="https://developer.amazon.com/alexaprize">Amazon Alexa Prize</a></li>
</ul>
</li>
<li><a href="https://arxiv.org/abs/1703.04908">Emergence of Grounded Compositional Language in Multi-Agent Populations</a></li>
</ul>
<p>(In particular in the multi-agent setting, there might be some connection to <a href="https://en.wikipedia.org/wiki/Evolutionary_game_theory">evolutionary game theory</a> and the work of <a href="https://en.wikipedia.org/wiki/Peyton_Young">Peyton Young</a>.)</p>
<hr>
<p><img alt="robot" src="img/robot.jpg"></p>
<hr>
<p>Lots of people think using RL for robot control is pretty neat.</p>
<p>(left to right: Chelsea Finn, Pieter Abbeel, <a href="http://www.willowgarage.com/pages/pr2/overview">PR2</a>, Trevor Darrell, Sergey Levine)</p>
<p>Pieter Abbeel, Peter Chen, Rocky Duan, and Tianhao Zhang <a href="https://nyti.ms/2hLYbGQ">founded Embodied Intelligence</a> to work on robot stuff, it seems.</p>
<hr>
<p>self-driving cars</p>
<ul>
<li>interest</li>
<li>results?</li>
</ul>
<hr>
<p>People seem to want to use RL for self-driving cars.</p>
<hr>
<p><img alt="OpenAI DotA 2" src="img/openai_dota2.jpg"></p>
<hr>
<p>OpenAI has <a href="https://blog.openai.com/dota-2/">beaten</a> top human players in one-on-one DotA 2. This is pretty neat. They haven't released details of their methods as they say they're working on the full five-on-five game. We'll see!</p>
<hr>
<p>conclusion</p>
<hr>
<p>Wrapping up!</p>
<hr>
<p><img alt="bootcamp diagram" src="img/annotated.jpg"></p>
<hr>
<p>There are <a href="/20170830-berkeley_deep_rl_bootcamp/">a lot of RL algorithms</a> with horrible acronyms.</p>
<hr>
<p>reinforcement learning</p>
<ul>
<li>\( r,s \rightarrow a \)</li>
<li>\( \pi: s \rightarrow a\)</li>
<li>\( v: s \rightarrow \sum{r} \)</li>
<li>\( q: s,a \rightarrow \sum{r} \)</li>
</ul>
<hr>
<p>But the core ideas are pretty concise, and lots of work in the field can be quickly understood to a first approximation by relating it back to these core ideas.</p>
<hr>
<p>network architectures for deep RL</p>
<ul>
<li>feature engineering can still matter</li>
<li>if using pixels<ul>
<li>often simpler than state-of-the-art for supervised</li>
<li>don't pool away location information if you need it</li>
</ul>
</li>
<li>consider using multiple/auxiliary outputs</li>
<li>consider phrasing regression as classification</li>
<li>room for big advancements</li>
</ul>
<hr>
<p>Lots of exciting developments in RL are coming from the use of deep neural nets, and I wanted to say a few things specific to doing these applications of deep learning.</p>
<hr>
<p>the lure and limits of RL</p>
<ul>
<li>seems like AI (?)</li>
<li>needs so much data</li>
</ul>
<hr>
<p>There's progress in RL now. The limits of that progress are not yet known.</p>
<p>One hot take <a href="https://twitter.com/dennybritz/status/925028640001105920">from Denny Britz</a>: "Ironically, the major advances in RL over the past few years all boil down to making RL look less like RL and more like supervised learning."</p>
<hr>
<p>Question</p>
<ul>
<li>Should you use reinforcement learning?</li>
</ul>
<hr>
<p>It's for you to decide!</p>
<hr>
<p>Thank you!</p>
<hr>
<p>Thank you for coming!</p>
<p>Thanks again to <a href="https://www.deeplearninganalytics.com/">DLA</a> for supporting me in learning more about reinforcement learning, and in being the audience for the earliest version of this talk, providing lots of valuable feedback.</p>
<p>Thanks to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for working through the second iteration of this talk with me, providing even more good feedback.</p>
<p>Thanks to <a href="https://twitter.com/EricHaengel">Eric Haengel</a> for help with Go and in thinking about the <a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">AlphaGo Zero paper</a>.</p>
<hr>
<p>further resources</p>
<hr>
<ul>
<li><a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> (paper)</li>
<li>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> (blog post)</li>
<li><a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a> (textbook)</li>
<li>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> (videos and slides)</li>
<li><a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> (videos, slides, and labs)</li>
<li>OpenAI <a href="https://github.com/openai/gym">gym</a> / <a href="https://github.com/openai/baselines">baselines</a> (software)</li>
<li><a href="http://nationalgocenter.org/">National Go Center</a> (physical place)</li>
<li><a href="http://dc.hackandtell.org/">Hack and Tell</a> (fun meetup)</li>
</ul>
<p>Here are my top resources for learning more about deep reinforcement
learning (and having a little fun).</p>
<p>The content of <a href="https://arxiv.org/abs/1708.05866">A Brief Survey of Deep Reinforcement Learning</a> is similar to this talk. It does a quick intro to a lot of deep reinforcement learning.</p>
<p>Karpathy's <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a> is a readable introduction as well, focused on policy gradient, with readable code.</p>
<p>If you can afford the time, I recommend all of Sutton and Barto's <a href="http://incompleteideas.net/sutton/book/the-book-2nd.html">Reinforcement Learning: An Introduction</a>. The authors are two major reinforcement learning pioneers, to the extent that they sometimes introduce new concepts in the pages of their textbook.</p>
<p>David Silver's <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html">course</a> is quite good. It largely follows Sutton and Barto's text, and also references Csaba Szepesv&#225;ri's <a href="https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf">Algorithms for Reinforcement Learning</a>.</p>
<p>The <a href="https://sites.google.com/view/deep-rl-bootcamp/lectures">Deep Reinforcement Learning Bootcamp</a> that inspired this talk has lots of materials online, so you can get the full treatment if you like!</p>
<p>If you want to get into code, <a href="https://openai.com/">OpenAI</a>'s <a href="https://github.com/openai/gym">gym</a> and <a href="https://github.com/openai/baselines">baselines</a> could be nice places to get started.</p>
<p>If you want to have fun, you can play Go with humans at the <a href="http://nationalgocenter.org/">National Go Center</a>, and I always recommend the <a href="http://dc.hackandtell.org/">Hack and Tell</a> meetup for a good time nerding out with people over a range of fun projects.</p>
<p>Juergen Schmidhuber's <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&amp;rep=rep1&amp;type=pdf">Deep Learning in Neural Networks: An Overview</a> didn't make my top list, but it has an interesting section specific to using LSTMs in RL.</p><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20171114-deep_rl/</link>
<guid>http://planspace.org/20171114-deep_rl/</guid>
<pubDate>Tue, 14 Nov 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Mounting Disks on AWS</title>
<description><![CDATA[

<p>I don't hand-mount disks much these days, but sometimes I want to spin up a quick <a href="https://aws.amazon.com/ec2/">EC2</a> instance to hammer something out, and I want to use the fast <a href="https://en.wikipedia.org/wiki/Solid-state_drive">SSD</a> storage that comes with some instances, and those disks don't auto-mount.</p>
<p>Amazon has <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/add-instance-store-volumes.html#making-instance-stores-available-on-your-instances">some documentation</a> but it isn't quite enough for me by itself. Here are commands that have served me well on Ubuntu:</p>
<pre><code class="language-bash">df -h  # see what's already mounted
lsblk  # find out the /dev/ paths of devices, like say /dev/xvdb
mkfs.ext4 /dev/xvdb  # format a disk using the Ext4 filesystem
mkdir disk  # make a mount point
mount /dev/xvdb disk  # mount the disk
df -h  # check for success and size of the new disk
chown ubuntu disk1  # make the disk accessible to the admin account</code></pre>

<p>Some of those will need <code>sudo</code>.</p>
<p>I don't really know that Ext4 is the best filesystem to use; let me know if some other one is better!</p>
<p>Some instances come with multiple SSDs, in which case it might be fun to use them in a <a href="https://en.wikipedia.org/wiki/Standard_RAID_levels#RAID_0">RAID-0</a> configuration for even more speed. I haven't tried this yet, but there is some <a href="http://www.tldp.org/HOWTO/Software-RAID-HOWTO-5.html#ss5.5">documentation</a> to start from. Has anyone tried this?</p>    
    ]]></description>
<link>http://planspace.org/20171022-mounting_disks_on_aws/</link>
<guid>http://planspace.org/20171022-mounting_disks_on_aws/</guid>
<pubDate>Sun, 22 Oct 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Science as Mountaineering</title>
<description><![CDATA[

<p>In <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a> addresses "textbook science" that ironically distances students from science.</p>
<p>For example, a textbook might use <a href="https://en.wikipedia.org/wiki/Foucault_pendulum">Foucault's pendulum</a> as evidence that the Earth rotates. The issue with "experiments" like this, Kuhn says, is that "By the time they were performed no scientist still needed to be convinced of the validity of the theory their outcome is now used to demonstrate. Those decisions had long since been made on the basis of significantly more equivocal evidence."</p>
<p>Textbook science becomes a series of just-so stories, sanitized of the actual messy scientific process. Brief clear explanations are not necessarily bad, but they may not prepare a student for the very different mode of reading (and doing) research. Textbooks can be trusted, for the most part, but new work may be relatively poorly explained, not yet clearly correct, or even just incorrect.</p>
<p>I like the comparison to mountaineering. Science teachers want to get students up the mountain, so they rely on well-trod trails. Discoverers may have originally gone by entirely different paths. Some routes were only evident when looking down from higher ground.</p>
<p>Students take stairs up quickly to very steep terrain. If they miss the opportunity to learn how to climb on the easier hills, they face a difficult transition if they want to do work without guard rails.</p>
<p>I think this analysis isn't specific to science; the focus could be seen as critical thinking, evaluating evidence, and so on. Even within the <a href="https://www.nextgenscience.org/">Next Generation Science Standards</a>, however, the skills I think are important seem surprisingly minimized, as the "science and engineering practices" focus on "engineering design" rather than figuring out what's true.</p>
<p>Some of <a href="http://modeling.asu.edu/R&amp;E/ModelingThryPhysics.pdf">Hestene's work</a> is in the direction of teaching more appropriate skills, and I know many science teachers emphasize the scientific method. I think in both cases practices that could be helpful even outside of science can seem isolated and academic, and I wonder what other approaches might help more people handle themselves on the mountain.</p>    
    ]]></description>
<link>http://planspace.org/20170917-science_as_mountaineering/</link>
<guid>http://planspace.org/20170917-science_as_mountaineering/</guid>
<pubDate>Sun, 17 Sep 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Problems with ImageNet and its Solutions</title>
<description><![CDATA[

<p>The <a href="http://www.image-net.org/">ImageNet</a> challenges play an important role in the development of computer vision. The great <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks">success</a> of neural nets on ImageNet has contributed to general fervor around artificial intelligence. While the applied breakthroughs are real, issues with ImageNet and modern networks indicate gaps between current practice and intelligent perception.</p>
<p>For this investigation I used a <a href="/20170430-sampling_imagenet/">subset</a> of ImageNet consisting of 5 images from each of 200 classes and a <a href="https://arxiv.org/abs/1512.03385">ResNet</a>50 network pre-trained on the 1,000-class ImageNet recognition task, as included with <a href="https://keras.io/">Keras</a>. The overlap is 150 classes, and on those the network has 92.3% top-5 accuracy, while it can't ever be correct for the other 50 subset classes. I also averaged the last layer's activations for each set of five examples to make class centroids, inducing a nearest-neighbor model.</p>
<p><img alt="not a croquet ball" src="img/n03134739_1193_croquet_ball.jpg"></p>
<p>The least important problem with ImageNet is that sometimes the ground truth labels are bad. My favorite example is an image labeled "<a href="https://en.wikipedia.org/wiki/Croquet">croquet</a> ball," which the centroid model more correctly labels "tennis ball." Perhaps the original labeler saw what looks like a <a href="https://en.wikipedia.org/wiki/Polo">Polo</a> <a href="https://en.wikipedia.org/wiki/Ralph_Lauren_Corporation">brand</a> wristband and thought it was close enough to call croquet.</p>
<p>Neural nets are pretty robust to dirty training labels, so this is not the end of the world for model performance, but the occasional obviously incorrect label does highlight that "ground truth" is not absolute.</p>
<p>Apart from correctness, using a single label for an object is also a choice of representation that may not be optimal. ImageNet is labeled with <a href="https://wordnet.princeton.edu/">WordNet</a>, which is hierarchical. It might be beneficial for a system to know that croquet balls and tennis balls are both balls, for example, but this kind of information is not usually used.</p>
<p><img alt="occluded unicycle(s)?" src="img/n04509417_4503_unicycle.jpg"></p>
<p>Another interesting example is an image which the centroid model labels "person" but which is supposed to be "unicycle." The unicycles aren't clearly visible in the image, so the only way to get "unicycle" is by context.</p>
<p><img alt="fish, cat, goblet" src="img/n01443537_4691_goldfish.jpg"></p>
<p>Images frequently include multiple distinct entities, as in an image that is labeled "goldfish" but which also includes a cat in a glass bowl. The ResNet model's best guess is "goblet," which is not entirely baseless. Should "cat" be incorrect?</p>
<p>The problem of multiple subjects can be addressed by labeling bounding boxes within images, or even labeling every pixel. Recent ImageNet challenges seem to be focusing more on this kind of object detection. But even when there's clearly one main object in view, there are problems not well addressed by ImageNet models.</p>
<p><img alt="jellyfish... statue? light?" src="img/n01910747_13396_jellyfish.jpg"></p>
<p>A garden statue of a jellyfish is labeled "jellyfish" by ImageNet. The centroid model guesses it's "mushroom." This is likely due at least in part to context clues. It's also indicative of the ResNet model's broader failure to have a flexible conception of jellyfish.</p>
<p><img alt="man, not dumbbell" src="img/n00007846_98724_person.jpg"></p>
<p>Overreliance on context is a known problem with image classifiers. For example, Google has <a href="https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">shown</a> that some of its high-performing recognizers didn't distinguish dumbbells from the arms that held them. I saw something like this when the ResNet's top three guesses for a picture of a man were "dumbbell," "water bottle," and "hammer."</p>
<p>Focusing on the parts of images that contain only a specific object can treat symptoms like the dumbbell/arm problem, but they don't address the deeper issue that current neural nets don't really have a mechanism for understanding objects in visual scenes abstractly. Caption-generators have similar failure modes. I only know of <a href="https://arxiv.org/abs/1609.05518">one explicit approach</a> to the problem, and it's not particularly elegant.</p>
<p><a href="https://arxiv.org/abs/1604.00289"><img alt="bad captions" src="img/bad_captions.png"></a></p>
<p>Neural net research seems to mostly hope that systems will learn an appropriate internal representation for a given task. For image classification, the representation is effective, but it seems different from human perception. If we knew better how thoughts are represented in the brain, we could emulate those representations directly. For the moment, we can only evaluate artificial representations indirectly.</p>
<p>I started this investigation hoping to find visual analogies with neural net activations, similar to <a href="/20170705-word_vectors_and_sat_analogies/">word analogies</a> with word vectors. I didn't expect it to work great, but I was disappointed by just how poorly it worked.</p>
<p>Word vectors are trained based on how words appear in text. One common method puts words near each other in space if they could be substituted in a sentence. Syntax and semantics dictate word placement, so word vectors pick up these characteristics. For example, phrases like "She pet the ___" will tend to put "cat" and "dog" close in word vector representation. Image activations in a neural net are based on how things look, so it's less clear how much "meaning" they should pick up.</p>
<p>I was also using a very small collection of just 200 classes, so the space is super sparse. But I still have both "bicycle" and "motorcycle" (among others) so I was hoping to find some interesting analogies.</p>
<p>Unfortunately, for my 200 categories, there are no analogies by closest item with four distinct items. That is, when you find the closest thing to x + y - z, it's always one of x, y, or z. This is <a href="http://anthology.aclweb.org/W16-2503">a known problem</a> with word vector analogies as well. The standard cheat is to just find the nearest new thing, which tends to mean you're just finding nearby items.</p>
<p>Even cheating to get uniqueness, the analogies I can come up with are not super compelling. Here are the best ones I found:</p>
<ul>
<li>sheep : camel :: seal : hippopotamus</li>
<li>flute : oboe :: domestic cat : lion</li>
<li>brassiere : maillot :: guacamole : burrito</li>
</ul>
<p>You could try to convince yourself that there's some meaning there, but it's really just that some things are close to each other in space; the relations are not super meaningful. Here are some examples with similar goodness in terms of distance:</p>
<ul>
<li>flute : oboe :: snake : centipede</li>
<li>lemon : orange :: lizard : frog</li>
<li>lemon : orange :: pitcher : bowl</li>
</ul>
<p>What about the bicycle and motorcycle?</p>
<ul>
<li>bicycle : motorcycle :: flute : oboe</li>
<li>bicycle : motorcycle :: guacamole : burrito</li>
</ul>
<p>What we see is that some things are close together; bicycle is close to motorcycle and flute is close to oboe. So when we ask for something close to bicycle + flute - oboe, for example, flute - oboe is basically zero and we find the closest thing to bicycle. The representations are successfully putting things that look similar close together.</p>
<p><img alt="burrito? guacamole?" src="img/n07880968_4922_burrito.jpg"></p>
<p>In the case of "guacamole" and "burrito" images can frequently be used for either class. At best, the representation learns about co-occurrence. So the best case is the dumbbell/arm problem again.</p>
<p>It isn't really fair to ask activations trained for visual classification to also learn interesting semantics and give fun analogies, just as it isn't reasonable to anthropomorphize neural nets and imagine that they're close to emulating human perception or approaching general artificial intelligence.</p>
<p>One direction that aims to address the limitations of simple classification tasks, for example, is sometimes described as grounded perception or embodied cognition. The idea is that an agent needs to interact in an environment to learn things like "this is used for that" and so on. This might be part of an advance.</p>
<p>In my estimation, however, the core problem of neural-symbolic integration seems unlikely to be solved by some kind of accidental emergent property. We struggle to represent input to an intelligent system, and we have very little idea how to represent the internals. Perhaps further inspiration will come from neuroscience.</p>
<hr>
<p>I presented a version of this material at the <a href="https://www.meetup.com/DC-Hack-and-Tell/events/241002526/">2017-09-14 Hack and Tell</a> using some slides (<a href="hack_and_tell.pptx">PPTX</a>, <a href="hack_and_tell.pdf">PDF</a>).</p>    
    ]]></description>
<link>http://planspace.org/20170911-problems_with_imagenet_and_its_solutions/</link>
<guid>http://planspace.org/20170911-problems_with_imagenet_and_its_solutions/</guid>
<pubDate>Mon, 11 Sep 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Berkeley Deep RL Bootcamp</title>
<description><![CDATA[

<p>At its conclusion, <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> said a major goal of his 2017 <a href="https://www.deeprlbootcamp.berkeley.edu/">Deep Reinforcement Learning Bootcamp</a> was to broaden the application of RL techniques. Around 250 representatives from research and industry had just emerged from 22 scheduled hours over a Saturday and Sunday in Berkeley. Abbeel asked the attendees to report back with tales of applying algorithms they may not have known existed previously.</p>
<p>Instruction came from leaders of modern reinforcement learning research, all delivering their expertise within a framework that highlighted the large-scale structure of the field. I found their organizing diagram to be particularly helpful.</p>
<p><img alt="landscape" src="img/landscape.png"></p>
<p>The reinforcement learning problem statement is simple: at every time step, an agent gets some observation of state \( s \), some reward \( r \), and chooses an action \( a \). So an agent is \( s,r \rightarrow a \), and reinforcement learning is largely about rearranging these three letters.</p>
<p>For example, the Q function is \( s,a \rightarrow \sum{r} \). You <a href="https://en.wikipedia.org/wiki/Bellman_equation">can</a> learn that function from experience, and it nicely induces a policy \( s \rightarrow a \). If you use a <a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">ConvNet</a> to learn the Q function, you can then <a href="https://deepmind.com/research/dqn/">publish</a> in <a href="https://www.nature.com/">Nature</a>.</p>
<p>Of the many good things about the bootcamp, I most valued getting a better conceptual feel for RL. It was also great to hear from experts about practical details, intuitions, and future directions. For all of these I particularly appreciated <a href="https://www.cs.toronto.edu/~vmnih/">Vlad Mnih</a>'s sessions.</p>
<p>The concerns of RL illuminate deep architectures from unique angles. The big networks that win classification challenges don't win in RL, for example, possibly getting at something about the nature of neural net training and generalization. Vlad described how fixing their target Q-network was more important with the smaller nets they used in development than on their final networks. Wins with <a href="https://arxiv.org/abs/1707.06887">distributional RL</a> may connect to a fundamental affinity of neural nets for categorical problems over regression problems.</p>
<p>Approaches like <a href="https://arxiv.org/abs/1611.05397">unsupervised auxiliary tasks</a> prompt comparisons with how the brain might work. But even cutting-edge techniques with models (\( s,a \rightarrow s \)) and planning, like <a href="https://arxiv.org/abs/1707.06203">imagination-augmented agents</a> and the <a href="https://arxiv.org/abs/1612.08810">predictron</a>, do as much to highlight differences as similarities with how we think. RL is at least as close to <a href="https://en.wikipedia.org/wiki/Optimal_control">optimal control</a> as it is to <a href="https://en.wikipedia.org/wiki/Artificial_general_intelligence">AGI</a>.</p>
<p>Reinforcement learning is home to a profusion of acronyms and initialisms that can be intimidating. As best I can, I've extended the Deep RL Bootcamp's diagram to include everything that was covered, together with a few terms common elsewhere and a set of expansions and links. For didactic resources, start from <a href="https://twitter.com/karpathy">Karpathy</a>'s <a href="http://karpathy.github.io/2016/05/31/rl/">Pong from Pixels</a>.</p>
<p><img alt="annotated" src="img/annotated.jpg"></p>
<ul>
<li>(<a href="https://en.wikipedia.org/wiki/Partially_observable_Markov_decision_process">PO</a>)<a href="https://en.wikipedia.org/wiki/Markov_decision_process">MDP</a>: (Partially Observable) Markov Decision Process</li>
<li><a href="https://en.wikipedia.org/wiki/Derivative-free_optimization">DFO</a>: Derivative-Free Optimization</li>
<li><a href="https://en.wikipedia.org/wiki/Cross-entropy_method">cross-entropy method</a><ul>
<li><a href="http://iew3.technion.ac.il/CE/files/papers/Learning%20Tetris%20Using%20the%20Noisy%20Cross-Entropy%20Method.pdf">Learning Tetris using the noisy cross-entropy method</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/CMA-ES">CMA</a>: Covariance Matrix Adaptation</li>
<li><a href="https://en.wikipedia.org/wiki/Natural_evolution_strategy">NES</a>: Natural Evolution Strategy<ul>
<li><a href="https://blog.openai.com/evolution-strategies/">Evolution strategies as a scalable alternative to reinforcement learning</a></li>
</ul>
</li>
<li><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.2545&amp;rep=rep1&amp;type=pdf">REINFORCE</a>: REward Increment = Nonnegative Factor times Offset Reinforcement times Characteristic Eligibility</li>
<li><a href="https://arxiv.org/abs/1502.05477">TRPO</a>: Trust Region Policy Optimization</li>
<li><a href="https://blog.openai.com/openai-baselines-ppo/">PPO</a>: Proximal Policy Optimization</li>
<li><a href="https://arxiv.org/abs/1510.09142">SVG</a>: Stochastic Value Gradients</li>
<li><a href="https://arxiv.org/abs/1602.01783">A3C</a>: Asynchronous Advantage Actor Critic</li>
<li><a href="https://blog.openai.com/baselines-acktr-a2c/">A2C</a>: (Synchronous) Advantage Actor Critic</li>
<li><a href="https://arxiv.org/abs/1708.05144">ACKTR</a>: Actor Critic using Kronecker-Factored Trust Region</li>
<li><a href="https://arxiv.org/abs/1611.05397">UNREAL</a>: UNsupervised REinforcement and Auxiliary Learning</li>
<li><a href="https://en.wikipedia.org/wiki/Temporal_difference_learning">TD</a>: Temporal Difference (also \( TD(\lambda) \))</li>
<li>FQI: Fitted Q Iteration</li>
<li><a href="https://deepmind.com/research/dqn/">DQN</a>: Deep Q-Network</li>
<li><a href="https://arxiv.org/abs/1509.02971">DDPG</a>: Deep Deterministic Policy Gradient</li>
<li><a href="https://arxiv.org/abs/1603.00748">NAF</a>: Normalized Advantage Functions</li>
<li><a href="http://realai.org/imitation-learning/">Imitation</a> Learning</li>
<li><a href="https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf">IRL</a>: Inverse Reinforcement Learning</li>
<li><a href="https://arxiv.org/abs/1603.00448">GCL</a>: Guided Cost Learning</li>
<li><a href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_363">HRL</a>: Hierarchical Reinforcement Learning</li>
<li>FuN: FeUdal Networks<ul>
<li><a href="http://www.cs.toronto.edu/~fritz/absps/dh93.pdf">Feudal reinforcement learning</a></li>
<li><a href="https://arxiv.org/abs/1703.01161">FeUdal Networks for hierarchical reinforcement learning</a></li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Model_predictive_control">MPC</a>: Model Predictive Control</li>
<li><a href="http://mlg.eng.cam.ac.uk/pub/pdf/DeiRas11.pdf">PILCO</a>: Probabilistic Inference
for Learning COntrol</li>
<li>local models specifically as in <a href="https://arxiv.org/abs/1501.05611">Learning contact-rich manipulation skills with guided policy search</a></li>
<li><a href="https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator">LQR</a>: Linear-Quadratic Regulator</li>
<li><a href="https://arxiv.org/abs/1612.08810">The Predictron: End-to-end learning and planning</a></li>
<li><a href="https://arxiv.org/abs/1707.06203">I2A</a>: Imagination-Augmented Agent</li>
<li>(<a href="https://en.wikipedia.org/wiki/Monte_Carlo_tree_search">Monte Carlo</a>) (<a href="https://en.wikipedia.org/wiki/Tree_traversal">tree</a>) <a href="https://en.wikipedia.org/wiki/Artificial_intelligence#Search_and_optimization">search</a>, <a href="https://en.wikipedia.org/wiki/Minimax">minimax</a>, etc.</li>
<li><a href="https://en.wikipedia.org/wiki/Gaussian_process">GP</a>: Gaussian Process</li>
<li><a href="https://en.wikipedia.org/wiki/Mixture_model#Gaussian_mixture_model">GMM</a>: Gaussian Mixture Model</li>
<li>(<a href="https://en.wikipedia.org/wiki/Deep_learning">D</a>/<a href="https://en.wikipedia.org/wiki/Convolutional_neural_network">C</a>/<a href="https://en.wikipedia.org/wiki/Recurrent_neural_network">R</a>)<a href="https://en.wikipedia.org/wiki/Artificial_neural_network">NN</a>: (Deep/Convolutional/Recurrent) Neural Network</li>
<li><a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a>: Long Short-Term Memory</li>
</ul><!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20170830-berkeley_deep_rl_bootcamp/</link>
<guid>http://planspace.org/20170830-berkeley_deep_rl_bootcamp/</guid>
<pubDate>Wed, 30 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Characteristics of good theories</title>
<description><![CDATA[

<p>In <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a> gives a partial list of characteristics that good theories share:</p>
<ol>
<li><strong>Accurate</strong>: within its domain, consequences deducible from a theory should be in demonstrated agreement with the results of existing experiments and observations</li>
<li><strong>Consistent</strong>: internally (with itself) and with other currently accepted theories applicable to related aspects of nature</li>
<li><strong>Broad scope</strong>: consequences should extend far beyond the particular observations, laws, or subtheories it was originally designed to explain</li>
<li><strong>Simple</strong>: bringing order to phenomena that in its absence would be individually isolated and, as a set, confused</li>
<li><strong>Fruitful</strong>: leading to new research findings; disclosing new phenomena or previously unnoted relationships among those already known</li>
</ol>
<p>In <a href="https://en.wikipedia.org/wiki/The_Beginning_of_Infinity">The Beginning of Infinity</a>, <a href="https://en.wikipedia.org/wiki/David_Deutsch">David Deutsch</a> objects to positivism or anti-realism, but has not so different criteria to Kuhn. Deutsch focuses on these two:</p>
<ol>
<li><strong>Hard to vary</strong>: having no extraneous or non-explanatory characteristics; possibly related to Kuhn's "consistent" and "simple"</li>
<li><strong>Reach</strong>: similar to Kuhn's "broad scope," "fruitful," and possibly even "simple"</li>
</ol>
<p>Deutsch mostly takes "accurate" as a given or obvious requirement. Even this seemingly simple goal becomes complicated when people <a href="/20170823-transfer_of_allegiance_from_theory_to_theory/">disagree</a> about "the results of existing experiments and observations."</p>
<p>The focus of both authors is on scientific theories, as in (for example) physics. When explaining human affairs, the same criteria may not always be perfect guides. For example, many non-simple conspiracy theories are not true, but there can really be conspiracies, and things may not be obvious.</p>    
    ]]></description>
<link>http://planspace.org/20170825-characteristics_of_good_theories/</link>
<guid>http://planspace.org/20170825-characteristics_of_good_theories/</guid>
<pubDate>Fri, 25 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Unique IDs and Nielsen abuse</title>
<description><![CDATA[

<p>The Wall Street Journal ran an article online with the headline <a href="https://www.wsj.com/articles/in-tv-ratings-game-networks-try-to-dissguys-bad-newz-from-nielsen-1499350955">In TV Ratings Game, Networks Try to Dissguys Bad Newz from Nielsen</a>. It got picked up by <a href="https://www.theverge.com/2017/7/6/15923722/tv-network-ratings-nielsen-viewership">other outlets</a>, and ran in the print WSJ the next day, on Friday July 7, 2017, on the front page, below the fold.</p>
<p>The implication is that networks identify their programs to Nielsen via a free text field with no validation; programs don't have unique IDs. Not having unique IDs is a huge pain even when everyone is trying to be consistent. Inevitably you have to do some kind of deduplication and merging. In this case, it leads to abuse (possibly sanctioned unofficially).</p>
<p><img alt="page 1" src="img/page1.png"></p>
<p><img alt="page 9" src="img/page9.png"></p>    
    ]]></description>
<link>http://planspace.org/20170724-unique_ids_and_nielsen_abuse/</link>
<guid>http://planspace.org/20170724-unique_ids_and_nielsen_abuse/</guid>
<pubDate>Mon, 24 Jul 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Transfer of allegiance from theory to theory</title>
<description><![CDATA[

<p>Toward the end of <a href="https://en.wikipedia.org/wiki/Thomas_Kuhn">Kuhn</a>'s <a href="https://www.andrew.cmu.edu/user/kk3n/philsciclass/kuhn.pdf">Objectivity, Value Judgment, and Theory Choice</a>, he says something that feels relevant to modern popular debate:</p>
<blockquote>
<p>...communication between proponents of different theories is
inevitably partial, that what each takes to be facts depends in part
on the theory he espouses, and that an individual's transfer of
allegiance from theory to theory is often better described as
conversion than as choice.</p>
</blockquote>
<p>Kuhn describes the investigation and development of non-mainstream
ideas as a benefit resulting from diversity of opinion. At the level
of conspiracy theory, and when choice of what to believe is based on
problematic desiderata like greed and racism, it seems to become a
pathology.</p>    
    ]]></description>
<link>http://planspace.org/20170823-transfer_of_allegiance_from_theory_to_theory/</link>
<guid>http://planspace.org/20170823-transfer_of_allegiance_from_theory_to_theory/</guid>
<pubDate>Wed, 23 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Scientific and historical levels of explanation</title>
<description><![CDATA[

<p>From page 1 of <a href="https://en.wikipedia.org/wiki/James_Flynn_(academic)">Flynn</a>'s <a href="https://en.wikipedia.org/wiki/What_Is_Intelligence%3F">What is intelligence?</a>:</p>
<blockquote>
<p>A warning for everyone: there are problems that can simply be
settled by evidence, for example, whether some swans are black. But
there are deeper problems that pose paradoxes. Sometimes the
evidence that would solve them lies in an inaccessible past. That
means we have to retreat from the scientific level of explanation to
the historical level where we demand only plausibility that conforms
to the known facts. I believe that my efforts to resolve the
historical paradoxes we will discuss should be judged by whether
someone has a more satisfactory resolution to offer. The reader
should be wary throughout to distinguish the contentions I evidence
from the contentions to which I lend only plausibility.</p>
</blockquote>
<p>I'm not sure whether an existence claim (about black swans) is the
best example, or whether it matters much if other problems are deeper
or pose paradoxes, but I do think this is an interesting way to phrase
the epistemological difference between a claim supported
experimentally, as by a randomized controlled trial, and a claim based
on observational evidence that is not direct. It's the kind of thing
that I think is worth considering, especially as there seems to be so
much disagreement about how to settle problems.</p>    
    ]]></description>
<link>http://planspace.org/20170822-scientific_and_historical_levels_of_explanation/</link>
<guid>http://planspace.org/20170822-scientific_and_historical_levels_of_explanation/</guid>
<pubDate>Tue, 22 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>A downward spiral destructive of civic virtue</title>
<description><![CDATA[

<p>From page 166 of <a href="https://en.wikipedia.org/wiki/James_Flynn_(academic)">Flynn</a>'s <a href="https://en.wikipedia.org/wiki/What_Is_Intelligence%3F">What is intelligence?</a>:</p>
<blockquote>
<p>Competition for possessions without a rationally imposed limit
engenders pessimism about acceptance of the restraints necessary to
avoid ecological disaster.</p>
<p>Competition for possessions also creates a downward spiral
destructive of civic virtue. Those who wish to maximize their
economic status are reluctant to pay taxes and this diminishes state
provision of health, education, and security against misfortune. As
the quality of state provision declines, it becomes imperative to
maximize private wealth for reasons of security even if status
seeking is set aside. Even principled socialists will pay fees to
jump the queue for medical care and to get education for their
children in schools that are not a test of physical survival. The
more that is true, the more you resent any dollar leaving your
pocket in tax, so public provision drops further, so willingness to
be taxed drops further, and so forth. Indeed, since only a few can
amass the fortune needed to provide self-security, no amount of
money you can realistically hope to acquire is enough.</p>
</blockquote>
<p>I think this passage touches a lot of modern debates.</p>    
    ]]></description>
<link>http://planspace.org/20170821-downward_spiral_destructive_of_civic_virtue/</link>
<guid>http://planspace.org/20170821-downward_spiral_destructive_of_civic_virtue/</guid>
<pubDate>Mon, 21 Aug 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Word Vectors and SAT Analogies</title>
<description><![CDATA[

<p>In 2013, <a href="https://code.google.com/archive/p/word2vec/">word2vec</a> popularized <a href="http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">word vectors</a> and <em>king - man + woman = queen</em>.</p>
<p><a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"><img alt="king queen etc." src="img/king_queen.png"></a></p>
<p>This reminded me of <a href="https://en.wikipedia.org/wiki/SAT">SAT</a> analogy questions, which <a href="http://blog.prepscholar.com/sat-analogies-and-comparisons-why-removed-what-replaced-them">disappeared from the SAT in 2005</a>, but looked like this:</p>
<pre><code>PALTRY : SIGNIFICANCE ::

A. redundant : discussion
B. austere : landscape
C. opulent : wealth
D. oblique : familiarity
E. banal : originality</code></pre>

<p>The king/queen example is not difficult, and I don't know whether it was tested or discovered. A better evaluation would use a set of challenging pre-determined questions.</p>
<p>There is a Google set of analogy questions, but all the relationships are grammatical, geographical, or by gender. Typical: "fast : fastest :: old : oldest." (<a href="http://download.tensorflow.org/data/questions-words.txt">dataset</a>, <a href="https://arxiv.org/abs/1301.3781">paper</a>, <a href="https://aclweb.org/aclwiki/Google_analogy_test_set_(State_of_the_art)">context</a>)</p>
<p>SAT questions are more interesting. Selecting from fixed answer choices provides a nice guessing baseline (1/5 is 20%) and using a human test means it's easier to get human performance levels (average US college applicant is 57%; human voting is 81.5%).</p>
<p>Michael Littman and Peter Turney have made available <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">a set of 374 SAT analogy questions</a> since 2003. You have to email Turney to get them, and I appreciate that he helped me out.</p>
<p>Littman and Turney <a href="http://cogprints.org/4518/1/NRC-48273.pdf">used</a> a vector-based approach on their dataset back in 2005. They achieved 47% accuracy (state of the art at the time) which is a nice benchmark.</p>
<p>They made vectors for each word pair using web data. To get one value for "banal : originality" they would search <a href="https://en.wikipedia.org/wiki/AltaVista">AltaVista</a> for "banal and not originality" and take the log of the number of hits. With a list of 64 connectives they made vectors with 128 components.</p>
<p>I'm using <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> and word2vec word vectors that are per-word and based on various text corpora directly. Since the vectors are not specific to particular pairings, they may at a relative disadvantage for the SAT task. To get a vector for a word pair, I just subtract.</p>
<p>Stanford provides a variety of GloVe vectors pre-trained on three different corpora:</p>
<ul>
<li>Twitter (<code>glove.twitter</code>)<ul>
<li>2B tweets, 27B tokens, 1.2M vocab, uncased</li>
<li>as each of 25d, 50d, 100d, &amp; 200d vectors</li>
</ul>
</li>
<li>Wikipedia 2014 + Gigaword 5 (<code>glove.6B</code>)<ul>
<li>6B tokens, 400K vocab, uncased</li>
<li>as each of 50d, 100d, 200d, &amp; 300d vectors</li>
</ul>
</li>
<li>Common Crawl<ul>
<li>uncased (<code>glove.42B</code>) 42B tokens, 1.9M vocab, 300d vectors</li>
<li>cased (<code>glove.840B</code>) 840B tokens, 2.2M vocab, 300d vectors</li>
</ul>
</li>
</ul>
<p>I have one word2vec model: <code>GoogleNews-vectors</code> trained on a Google News dataset, providing 300d vectors.</p>
<p><img src="img/accuracy.png" width="480"></p>
<ul>
<li>The best word vectors for the SAT analogies task (<code>glove.840B</code>) achieve 49% accuracy. This outperforms the 2005 47% result, but is still within the confidence bounds of 42.2% to 52.5% that they report.</li>
<li>Holding the training set constant and varying the size of the vectors affects performance on the SAT analogies task: bigger vectors work better, though performance may be plateauing around 300d.</li>
<li>Twitter data does markedly worse on the SAT analogies task. This is consistent with Twitter's limitations and reputation for being less than erudite.</li>
</ul>
<p>Trained word vectors provide a fixed vocabulary. When a word was missing, I used a vector of zeros. Most embeddings only missed one to eight words from the 374 questions, but <code>glove.twitter</code> missed 105. I also checked accuracies when excluding questions that had unsupported words for a set of embeddings, and the results were remarkably close, even with the Twitter embeddings. So the accuracies are due to the embeddings and not just gaps in the vocabularies.</p>
<p>It is pretty impressive that word vectors work as well as they do on this task, but nobody should consider word vectors a solution to natural language understanding. Problems with word vectors have been pointed out (<a href="http://www.aclweb.org/anthology/N15-1098">source</a>, <a href="http://www.aclweb.org/anthology/C/C16/C16-1262.pdf">source</a>, <a href="http://anthology.aclweb.org/W16-2503">source</a>, <a href="https://arxiv.org/abs/1705.11168">source</a>).</p>
<p>Still, the idea of word vectors (translating sparse data to a learned dense representation) is super useful, and not just for words. See implementations <a href="https://keras.io/layers/embeddings/">in Keras</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/layers/embedding_column">in TensorFlow</a>.</p>
<p>These methods are not the best-performing non-human technique for these SAT analogy questions. Littman and Turney <a href="https://aclweb.org/aclwiki/SAT_Analogy_Questions_(State_of_the_art)">report</a> several. Latent Relational Analysis comes in at 56% accuracy, against the average US college applicant at 57%.</p>
<p>This is a case in which "the average human" is not a good bar for measuring AI success. The average human has a remarkably small vocabulary relative to what a computer should easily handle, not to mention that the average human would not be admitted to most colleges you could name.</p>
<hr>
<p>I'm grateful to the <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/">DC Deep Learning Working Group</a> for helpful discussions, and particularly Shabnam Tafreshi and Dmitrijs Milajevs for sharing references at <a href="https://www.meetup.com/DC-Deep-Learning-Working-Group/events/237114317/">the January 26, 2017 meeting</a>. Thanks also to <a href="https://twitter.com/metasemantic">Travis Hoppe</a> of <a href="http://dc.hackandtell.org/">DC Hack and Tell</a> for always doing cool things with NLP. Thanks to Peter Turney for providing the dataset and commenting on distances.</p>
<p>Notes:</p>
<ul>
<li>An <a href="https://www.technologyreview.com/s/541356/king-man-woman-queen-the-marvelous-mathematics-of-computational-linguistics/">example</a> of word2vec popularization.</li>
<li>My code is available at <a href="https://github.com/ajschumacher/sat_analogies">ajschumacher/sat_analogies</a>.</li>
<li>All but 20 of the 374 questions have five designed answer choices; those 20 have four and "no : choice" as the last option. To maintain comparability I kept those questions in, though it means a human guessing baseline should be slightly over 20%.</li>
<li>For efficiency, I used GloVe values as 2-byte floats, which is lossy. I also tested with 4-byte floats, and the results varied by at most one question, while being much slower to generate.</li>
<li>Cosine distance is more effective than Euclidean distance for this task, and the advantage increases with dimensionality.</li>
</ul>
<p><img src="img/cosine_advantage.png" width="480"></p>    
    ]]></description>
<link>http://planspace.org/20170705-word_vectors_and_sat_analogies/</link>
<guid>http://planspace.org/20170705-word_vectors_and_sat_analogies/</guid>
<pubDate>Wed, 05 Jul 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
