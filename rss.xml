<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Parsing TFRecords inside the TensorFlow Graph</title>
<description><![CDATA[

<p>You can parse TFRecords using the standard protocol buffer <code>.FromString</code> method, but you can also parse them inside the TensorFlow graph.</p>
<p>The examples here assume you have in memory the serialized Example <code>my_example_str</code> and SequenceExample <code>my_seq_ex_str</code> from <a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a>. You could create them, or read them from <a href="/20170323-tfrecords_for_humans/my_example.tfrecords">my_example.tfrecords</a> and <a href="/20170323-tfrecords_for_humans/my_seq_ex.tfrecords">my_seq_ex.tfrecords</a>. That loading could be via <code>tf.python_io.tf_record_iterator</code> or via <code>tf.TFRecordReader</code> following the pattern shown in <a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a>.</p>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/parse_single_example"><code>tf.parse_single_example</code></a> decoder works like <code>tf.decode_csv</code>: it takes a string of raw data and turns it into structured data, based on the options it's created with. The structured data it turns it into is <em>not</em> a protocol buffer message object, but a dictionary that is hopefully easier to work with.</p>
<pre><code class="language-python">import tensorflow as tf

serialized = tf.placeholder(tf.string)

my_example_features = {'my_ints': tf.FixedLenFeature(shape=[2], dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session = tf.Session()

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': array([5, 6]),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>The <code>shape</code> parameter is part of the schema we're defining. A <code>shape</code> of <code>[]</code> means a single element, so the result returned won't be in an array, as for <code>my_float</code>. The <code>shape</code> of <code>[1]</code> means an array containing one element, like for <code>my_bytes</code>. Within a Feature, things are always listed, so the choice of how to get a single element back out is decided by the choice of <code>shape</code> argument. A <code>shape</code> of <code>[2]</code> means a list of two elements, naturally enough, and there's no alternative.</p>
<p>The <code>dtype=object</code> is how NumPy works with strings.</p>
<p>When some feature might have differing numbers of values across records, they can all be read with <code>tf.VarLenFeature</code>. This distinction is made only when parsing. Records are made with however many values you put in; you don't specify <code>FixedLen</code> or <code>VarLen</code> when you're making an <code>Example</code>. So the <code>my_ints</code> feature just parsed as <code>FixedLen</code> can also be parsed as <code>VarLen</code>.</p>
<pre><code class="language-python">my_example_features = {'my_ints': tf.VarLenFeature(dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': SparseTensorValue(indices=array([[0], [1]]),
##                               values=array([5, 6]),
##                               dense_shape=array([2])),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>When parsing as a <code>VarLenFeature</code>, the result is a sparse representation. This can seem a little silly, because features here will always be dense from left to right. Early versions of TensorFlow <a href="https://github.com/tensorflow/tensorflow/issues/976">didn't</a> have the current behavior. But this sparseness is a mechanism by which TensorFlow can support non-rectangular data, for example when forming batches from multiple variable length features, or as seen next with a <code>SequenceExample</code>:</p>
<pre><code class="language-python">my_context_features = {'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_sequence_features = {'my_ints': tf.VarLenFeature(shape=[2], dtype=tf.int64)}
my_seq_ex = tf.parse_single_sequence_example(
                serialized,
                context_features=my_context_features,
                sequence_features=my_sequence_features)

result = session.run(my_seq_ex, feed_dict={serialized: my_seq_ex_str})
result
## ({'my_bytes': array(['data'], dtype=object)},
##  {'my_ints': SparseTensorValue(
##                  indices=array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 2]]),
##                  values=array([5, 6, 7, 8, 9]),
##                  dense_shape=array([2, 3]))})</code></pre>

<p>The result is a tuple of two dicts: the context data and the sequence data.</p>
<p>Since the <code>my_ints</code> sequence feature is parsed as a <code>VarLenFeature</code>, it's returned as a sparse tensor. This example has to be parsed as a <code>VarLenFeature</code>, because the two entries in <code>my_ints</code> are of different lengths (<code>[5, 6]</code> and <code>[7, 8, 9]</code>).</p>
<p>The way the <code>my_ints</code> values get combined into one sparse tensor is the same as the way it would be done when making a batch from multiple records each containing a <code>VarLenFeature</code>.</p>
<p>To make it clearer what's going on, we can look at the sparse tensor in dense form:</p>
<pre><code class="language-python">session.run(tf.sparse_tensor_to_dense(result[1]['my_ints']))
## array([[5, 6, 0],
##        [7, 8, 9]])</code></pre>

<p>The other option for parsing sequence features is <code>tf.FixedLenSequenceFeature</code>, which will work if each entry of the sequence feature is the same length. The result then is a dense tensor.</p>
<p>To parse multiple <code>Example</code> records in one op, there's <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/parse_example"><code>tf.parse_example</code></a>. This returns a dict with the same keys you'd get from parsing a single <code>Example</code>, with the values combining the values from all the parsed examples, in a batch-like fashion. There isn't a corresponding op for <code>SequenceExample</code> records.</p>
<p>More could be said about sparse tensors and TFRecords. The <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_merge">tf.sparse_merge</a> op is one way to combine sparse tensors, similar to the combination that happened for <code>my_ints</code> in the <code>SequenceExample</code> above. And there's <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a> for parsing out general sparse features directly from TFRecords (better documentation in <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parsing_ops.py">source</a>).</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 26 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>mystery.tfrecords</title>
<description><![CDATA[

<p>Here's a practical puzzle: what's in the file <a href="mystery.tfrecords"><code>mystery.tfrecords</code></a>?</p>
<hr>
<p>You don't need any more extra information specific to that file. Here are a few posts about the format and related issues, some of which might be helpful:</p>
<ul>
<li><a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a></li>
<li><a href="/20170330-tfrecords_via_proto/">TFRecords via Protocol Buffer Definitions</a></li>
<li><a href="/20170403-images_and_tfrecords/">Images and TFRecords</a></li>
<li><a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a></li>
</ul>
<!--

Welcome to hidden additional notes! Finding this is a way of solving
the puzzle, I suppose...

Here's how `mystery.tfrecords` was made:

<pre><code class="language-python">import tensorflow as tf

with open('success.jpg') as f:
    success = f.read()

example = tf.train.Example(features=tf.train.Features(feature={
    'jpg': tf.train.Feature(bytes_list=tf.train.BytesList(value=[success]))
}))

example_str = example.SerializeToString()

with tf.python_io.TFRecordWriter('mystery.tfrecords') as writer:
    writer.write(example_str)</code></pre>

Here's one way to get the contents back out:

<pre><code class="language-python">reader = tf.python_io.tf_record_iterator('mystery.tfrecords')

examples = [tf.train.Example().FromString(example_str)
            for example_str in reader]
# Using `SequenceExample` rather than `Example` also works.

len(examples)  # 1
# So we know there's just one example in there.

example = examples[0]

example.features.feature.keys()  # 'jpg'
# If parsed as a SequenceExample, this would instead be:
# `example.context.feature.keys()`

# It should be clear from the 'jpg' key, but you can also check:
len(example.features.feature['jpg'].int64_list.value)  # 0
len(example.features.feature['jpg'].float_list.value)  # 0

len(example.features.feature['jpg'].bytes_list.value)  # 1

jpg = example.features.feature['jpg'].bytes_list.value[0]

# If you don't trust the key, you can check the magic number:
jpg[:2]  # '\xff\xd8'
# That's the JPG magic number, FFD8.

with open('success.jpg', 'wb') as f:
    f.write(jpg)</code></pre>

Success!

-->

<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170425-mystery.tfrecords/</link>
<guid>http://planspace.org/20170425-mystery.tfrecords/</guid>
<pubDate>Tue, 25 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow as Automatic MPI</title>
<description><![CDATA[

<p>TensorFlow raises the level of abstraction in distributed programs from message-passing to data structures and operations directly on those data structures. The difference is analogous to the difference between programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a> and programming in a high-level language. TensorFlow may not have every possible high-performance feature for cluster computing, but what it offers is compelling.</p>
<hr>
<h2>Message-Passing</h2>
<p>Leaving aside shared filesystems or directly accessed shared memory, systems that run across multiple computers have to communicate by sending messages to one another. A low-level approach might use <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> sockets directly. Some convenience could be obtained by using Remote Procedure Calls (<a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a>).</p>
<p>The Message-Passing Interface (<a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>) was created in the early 1990s to make distributed programming easier within the High Performance Computing (HPC) community. It standardized interfaces like <code>MPI_Send</code> and <code>MPI_Recv</code> to facilitate exchanging data between processes of a distributed system.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Actor_model">actor model</a> also focuses on message-passing, as for example in <a href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">Erlang</a> or with <a href="http://akka.io/">Akka</a> in Java or Scala.</p>
<p>Message-passing gives you fine-grained control over how nodes communicate. You can use message-passing to build up a variety of algorithms and new systems. For example, <a href="http://spark.apache.org/">Spark</a> was built in part with Akka agents for some time, before switching to an RPC-based implementation. TensorFlow also builds its own abstractions using message-passing.</p>
<hr>
<h2>Automatic Message-Passing with TensorFlow</h2>
<p>TensorFlow uses message-passing, but it's largely invisible to TensorFlow users. The TensorFlow API lets you say where data and operations should live, and TensorFlow automatically handles any necessary messaging.</p>
<p>Programming with explicit messages is like the low-level memory shifting necessary when programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a>. For example, here is some assembly pseudo-code:</p>
<pre><code>copy value from position `a` to register 1
copy value from position `b` to register 2
add register 1 and 2
copy result to position `c`</code></pre>

<p>In a high-level language, this could be:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>Similarly, with TensorFlow, you focus on the computation you want performed, and don't worry about how values may need to be moved around to make it happen. A distributed analog to the assembly above might look like this:</p>
<pre><code>send value of `a` from machine A to machine D
send value of `b` from machine B to machine D
add values on machine D
send result to machine C</code></pre>

<p>And with TensorFlow:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>At least, that's the ideal. TensorFlow may still need explicit direction on where things should be placed (<a href="https://www.tensorflow.org/api_docs/python/tf/device">with tf.device</a>) especially when multiple machines are involved. But that direction is more succinct and declarative than the full imperative detail of message-passing.</p>
<hr>
<h2>Faster Messages and Collective Communications</h2>
<p>TensorFlow's user API is essentially message-less, but TensorFlow still uses messages under the hood. The contents of the messages are serialized protocol buffers sent via <a href="http://www.grpc.io/">gRPC</a>, which uses <a href="https://en.wikipedia.org/wiki/HTTP/2">HTTP/2</a>.</p>
<p>A message-based approach can be made faster by using a faster transport, and by using faster algorithms. TensorFlow has some early support for both, but other frameworks may offer more. You may or may not care, as the simplicity of TensorFlow's approach may or may not outweigh possible performance boosts.</p>
<p>One easy transport speedup could come in hardware with NVIDIA's <a href="http://www.nvidia.com/object/nvlink.html">NVLink</a> interconnect, which is faster than <a href="https://en.wikipedia.org/wiki/PCI_Express">PCIe</a>. As far as I can tell NVLink won't require code changes, but it looks like it will be pretty rare: it may be that only some supercomputers (<a href="https://www.olcf.ornl.gov/summit/">Summit</a>, <a href="https://asc.llnl.gov/coral-info">Sierra</a>) will use NVLink, with IBM's <a href="https://en.wikipedia.org/wiki/IBM_POWER_microprocessors">POWER</a> processors.</p>
<p>Some clusters connect machines with <a href="https://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a> rather than <a href="https://en.wikipedia.org/wiki/Ethernet">ethernet</a>. InfiniBand Remote Direct Memory Access (<a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">RDMA</a>) is supposed to be pretty fast.</p>
<p>Jun Shi at Yahoo contributed an <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md">implementation</a>, <a href="https://github.com/tensorflow/tensorflow/pull/8943">merged</a> four days ago, that begins to let TensorFlow use RDMA for transport via IB verbs, in addition to gRPC.</p>
<p>MPI implementations often support InfiniBand, and an advantage of using MPI is that you can let that implementation worry about InfiniBand support without writing your own RDMA code. There's an <a href="https://github.com/tensorflow/tensorflow/pull/7710">open pull request</a> to give TensorFlow this kind of MPI integration.</p>
<p>On the algorithm side, there are <a href="https://computing.llnl.gov/tutorials/mpi/#Collective_Communication_Routines">collective communication</a> operations that can be optimized, and MPI implementations tend to be good at these. For example, <a href="http://research.baidu.com/bringing-hpc-techniques-deep-learning/">ring allreduce</a> is more efficient for syncing up model parameters than sending them all to a central parameter server and then sending them all back out. Baidu has a <a href="https://github.com/baidu-research/tensorflow-allreduce">fork</a> of TensorFlow that adds their <a href="https://github.com/baidu-research/baidu-allreduce">implementation</a> of ring allreduce using MPI at <a href="https://github.com/baidu-research/tensorflow-allreduce/tree/master/tensorflow/contrib/mpi"><code>tf.contrib.mpi</code></a>.</p>
<p>Outside the MPI world, the NVIDIA Collective Communications Library (<a href="https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/">NCCL</a>) works with multiple GPUs on the same machine. TensorFlow has some support for this via <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/nccl"><code>tf.contrib.nccl</code></a>.</p>
<p>Quite distinct from TensorFlow, the Facebook Incubator project <a href="https://github.com/facebookincubator/gloo">Gloo</a> focuses on collective communication algorithms and supports transport via TCP/IP and InfiniBand without MPI. Gloo is used by <a href="https://caffe2.ai/">Caffe2</a>; the Caffe2 example <a href="https://github.com/caffe2/caffe2/blob/master/caffe2/python/examples/resnet50_trainer.py">resnet50 trainer</a> uses Gloo's ring allreduce.</p>
<!--

Just for fun, here's an Akka implementation of allreduce that I found:
https://github.com/brianmartin/akka-allreduce
I believe that's more of a tree allreduce than a ring allreduce.

-->

<hr>
<h2>Single-Machine Parallelism</h2>
<p>The HPC community has an approach called "MPI everywhere" which means running one single-threaded MPI processes per CPU core. The simplicity of writing single-threaded programs with all parallelism via MPI is attractive, but it's not necessarily the best way to use a machine with a lot of cores and possibly GPUs.</p>
<p>The hybrid approach is to have parallelism both across multiple machines and within each individual machine, usually via threads. HPC folks seem to like to use <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> for multithreading.</p>
<p>TensorFlow supports this kind of within-process parallelism. TensorFlow uses threads pretty freely internally, and you can add your own additional parallelism with Python's <a href="https://docs.python.org/3/library/threading.html">threading</a> or <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a> libraries, among many possible options. Just as with MPI though, when programs are run by a cluster manager, resources may be restricted.</p>
<hr>
<h2>Code organization</h2>
<p>It isn't strictly required for either, but it seems most common to write MPI programs and distributed TensorFlow programs with one entry point that quickly specializes based on the particular role in the cluster for that node. As this implies, every invocation of the program has to know something about the cluster and its place in it.</p>
<hr>
<h2>Cluster Topology</h2>
<p>MPI implementations provide a mechanism in starting a group of processes by which every process knows how many there are in its group and its position or <em>rank</em> in the group. The rank 0 process, called the <em>root</em>, is generally special.</p>
<p>TensorFlow leaves the starting of distributed processes to a cluster manager like Kubernetes. It's common to have several task groups like <code>ps</code> and <code>worker</code>, with a given process having a task type and a number within that group.</p>
<p>Gloo is interestingly different in that individual processes may not start with knowledge of the cluster topology, but only with a reference to a central broker which could be a file on a shared filesystem or a Redis server. Each process checks in with and gets information about other members of the cluster via the central broker. Gloo calls this process <em>rendezvous</em>. As with MPI, processes find out about just one group of processes, and in fact Gloo can use MPI for its initial rendezvous setup.</p>
<hr>
<h2>See also</h2>
<ul>
<li><a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">TensorFlow as a Distributed Virtual Machine</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
</ul>
<hr>
<p>Thanks to <a href="https://twitter.com/danfujita123">Dan Fujita</a> for suggesting more connections between TensorFlow and MPI, and sharing some MPI code that was helpful. Thanks also to <a href="https://twitter.com/ljdursi">Jonathan Dursi</a>, whose <a href="https://www.dursi.ca/">blog</a> I read with interest. And thanks to <a href="https://github.com/cliffwoolley">Cliff Woolley</a> of NVIDIA for <a href="https://github.com/NVIDIA/nccl/issues/86">clarifying</a> the meaning of NCCL.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</link>
<guid>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</guid>
<pubDate>Sun, 23 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Reading from Disk inside the TensorFlow Graph</title>
<description><![CDATA[

<p>As I've <a href="/20170312-use_only_what_you_need_from_tensorflow/">noted</a>, the TensorFlow input pipeline misleadingly <a href="https://www.tensorflow.org/programmers_guide/reading_data">described</a> as "reading from files" is far more complicated than many people need or want to deal with. Indeed, TensorFlow developers realize this and are <a href="https://github.com/tensorflow/tensorflow/issues/7951">thinking about</a> adding alternative interfaces for getting data in to TensorFlow programs.</p>
<p>You can still use any method you like for loading data in Python and then put it in your TensorFlow graph via the <code>feed_dict</code> method. But say you do want to do your file reading with ops that live inside the TensorFlow graph. How does it work?</p>
<p>TensorFlow does have <a href="https://www.tensorflow.org/api_docs/python/tf/read_file"><code>tf.read_file</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/write_file"><code>tf.write_file</code></a>, which let you read and write whole files at once, something like regular Python <code>file.read()</code> and <code>file.write()</code>. But you likely want something a little more helpful.</p>
<p>In Python, you can read lines from a file like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; reader = open('filename.txt'):
&gt;&gt;&gt; for line in reader:
&gt;&gt;&gt;    print(line)</code></pre>

<p>TensorFlow has readers that generalize this idea of getting multiple things, like lines, from files. The immediate example is the <a href="https://www.tensorflow.org/api_docs/python/tf/TextLineReader"><code>tf.TextLineReader</code></a>. There are also <a href="https://www.tensorflow.org/api_guides/python/io_ops#Readers">readers</a> that read records from TFRecords files, or just fixed number of bytes at a time from arbitrary files. Reading a whole file at a time is sort of degenerate case.</p>
<p>Instead of taking a filename, however, TensorFlow readers take a queue of filenames. This can be useful:</p>
<ul>
<li>You may have lots of files that you want to read from, possibly using multiple machines.</li>
<li>You may want to read through one or more files multiple times, for multiple epochs of training.</li>
</ul>
<p>It's a little awkward to use a queue when you just want to read from one file, but that's how TensorFlow works.</p>
<p>TensorFlow's file reading gets tangled up with its threading <code>QueueRunner</code> because they're often shown together, but this is not necessary. We <a href="/20170327-tensorflow_and_queues/">can</a> set up a quick and dirty one-item queue like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; filename_queue = tf.FIFOQueue(capacity=1, dtypes=[tf.string])
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(filename_queue.enqueue('limerick.txt'))
&gt;&gt;&gt; session.run(filename_queue.close())</code></pre>

<p>We make a reader and tell it to read from files in the queue. There are two outputs that update every time you evaluate either of them:</p>
<ul>
<li>A <code>key</code>, based on the current filename, which you may not need and don't need to explicitly evaluate.</li>
<li>A <code>value</code>, which is the bit of data we're probably interested in.</li>
</ul>
<pre><code class="language-python">&gt;&gt;&gt; reader = tf.TextLineReader()
&gt;&gt;&gt; key, value = reader.read(filename_queue)
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:1', 'This limerick goes in reverse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:2', "Unless I'm remiss"]
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:3', 'The neat thing is this:']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:4', 'If you start from the bottom-most verse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:5', "This limerick's not any worse"]
&gt;&gt;&gt; session.run([key, value])
# raises `OutOfRangeError`</code></pre>

<p>The <a href="limerick.txt">limerick</a> is <a href="http://www.smbc-comics.com/?id=3201">due to</a> Zach Weinersmith.</p>
<p>If we hadn't closed the filename queue, that last <code>session.run</code> would block, waiting for somebody to add another filename to the filename queue.</p>
<p>If we had added more filenames to the filename queue, the reader would continue happily reading from the next, and the next.</p>
<p>The records here are lines of simple ASCII text, so we can immediately see them clearly in the REPL. But for many types of data you'll need a decoder.</p>
<p>To read CSV files, you would read text lines just as above, and then use the <a href="https://www.tensorflow.org/api_docs/python/tf/decode_csv"><code>tf.decode_csv</code></a> decoder on  <code>value</code>. The <a href="https://www.tensorflow.org/api_docs/python/tf/decode_raw"><code>tf.decode_raw</code></a> decoder turns raw bytes into standard TensorFlow datatypes. And you can parse TFRecords examples.</p>
<p>Boom! Reading files inside the graph!</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 12 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed MapReduce with TensorFlow</title>
<description><![CDATA[

<p>Using many computers to count words is a tired Hadoop example, but might be unexpected with TensorFlow. In 50 lines, a TensorFlow program can implement not only map and reduce steps, but a whole MapReduce system.</p>
<hr>
<h2>Set up the cluster</h2>
<p>The design will have three roles, or jobs. There will be one task in the <code>files</code> job, distributing units of work. There will be one task for the <code>reduce</code> role, keeping track of results. There could be arbitrarily many tasks doing the <code>map</code> job, but for this example there will be two.</p>
<p>TensorFlow programs often use <code>ps</code> (parameter server) and <code>worker</code> tasks, but this is largely a convention. The program here won't follow the convention.</p>
<p>The four tasks can run on four computers, or on fewer. To stay local, here's a cluster definition that runs them all on your local machine.</p>
<pre><code class="language-python">cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}</code></pre>

<p>I have a <a href="make_configs.py"><code>make_configs.py</code></a> script that produces four shell scripts (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>). Each should be sourced (like <code>source config_map_0.sh</code>) in a separate shell. This setting of environment variables is work that could be handled by a cluster manager like <a href="https://kubernetes.io/">Kubernetes</a>, but these scripts will get it done.</p>
<p>Every task will run <a href="count.py"><code>count.py</code></a>. The first few lines establish the cluster.</p>
<pre><code class="language-python">import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)</code></pre>

<p>For more on TensorFlow clusters, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a>.</p>
<p>To avoid lots of indentation, I'm not using <code>tf.Session</code> as a context manager, which is fine for this example. I'll similarly avoid <code>with</code> blocks when reading files later.</p>
<hr>
<h2>Set up the data</h2>
<p>For distributed data processing to make sense, you likely want a distributed file system like HDFS providing a way for workers to grab chunks of data to work on. You might have hundred-megabyte <a href="/20170323-tfrecords_for_humans/">TFRecords files</a> prepared, for example.</p>
<p>For this example, we'll use <a href="/20170331-on_tyranny/">a dataset of 22 small text files</a>. We'll generate a <a href="filenames.txt">list of filenames</a>. Then, assuming every member of the cluster can access the file system, we can give a worker a filename and have it read the file.</p>
<pre><code class="language-bash">$ wget http://planspace.org/20170331-on_tyranny/on_tyranny.tar.gz
$ tar zxvf on_tyranny.tar.gz
$ find on_tyranny -type f &gt; filenames.txt</code></pre>

<hr>
<h2>Make a filename distributor</h2>
<p>The <code>files</code> task will host a <a href="/20170327-tensorflow_and_queues/">queue</a> of filenames.</p>
<pre><code class="language-python">with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')</code></pre>

<p>This code will be executed by every member of the cluster, but it won't make multiple queues. The <code>tf.device</code> context specifies that the queue lives on the <code>files</code> task machine, and the <code>shared_name</code> uniquely identifies this queue. So just one queue gets made, but every member of the cluster can refer to it with the Python variable name <code>filename_queue</code>.</p>
<p>The next part of the code only runs on the <code>files</code> task:</p>
<pre><code class="language-python">if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()</code></pre>

<p>Device assignment here is left up to TensorFlow, which is fine.</p>
<p>The <code>file</code> task uses normal Python file reading to get filenames from <a href="filenames.txt"><code>filenames.txt</code></a> and put them in the queue.</p>
<p>I'm loading the queue explicitly to avoid getting into <code>Coordinator</code> and <code>QueueRunner</code>; you could also use a <code>string_input_producer</code>, for example.</p>
<p>Then the <code>files</code> task runs <code>server.join()</code>, which keeps it running so that the queue doesn't disappear. We'll have to kill the process eventually because it won't know when to stop. This is another thing a cluster manager could be responsible for.</p>
<hr>
<h2>Make a reduce node</h2>
<p>There's just going to be one variable storing the total word count.</p>
<pre><code class="language-python">with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')</code></pre>

<p>Like the code defining the queue, every task in the cluster will run this code, so every task in the cluster can refer to this variable. Variables in specific places are "de-duplicated" using <code>name</code> instead of <code>shared_name</code>.</p>
<p>There's very little code that only the <code>reduce</code> task runs.</p>
<pre><code class="language-python">if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)</code></pre>

<p>The <code>reduce</code> task initializes and then displays the current value of <code>total_word_count</code> every two seconds.</p>
<p>It would be a bit more like Hadoop MapReduce to have the reducer explicitly receive data emitted from mappers, perhaps via another queue. Then the reducer would have to run some code to reduce down data from that queue.</p>
<p>The absence of any reducing code in the <code>reduce</code> task demonstrates the way distribution works in TensorFlow. The <code>reduce</code> task owns a variable, but we can add to that variable from another machine.</p>
<p>Like the <code>files</code> task, the <code>reduce</code> task doesn't have any way of knowing when the counting process is done and then shutting down, which I think is okay for this example.</p>
<hr>
<h2>Make map nodes</h2>
<p>The <code>map</code> task has already run code establishing the filename queue and the total word count variable. Here's what each <code>map</code> task does:</p>
<pre><code class="language-python">if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign_add(total_word_count,
                                 word_count_to_add,
                                 use_locking=True)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>Each <code>map</code> task pulls a filename from the queue, reads the file, counts its words, and then adds its count to the total.</p>
<p>A <code>map</code> task will run until the queue is empty, and then it will die with an <code>OutOfRangeError</code>. This would be a little more careful:</p>
<pre><code class="language-python">        try:
            filename = session.run(filename_from_queue)
        except tf.errors.OutOfRangeError:
            break</code></pre>

<p>Inside the <code>map</code> task, the actual work that's done is not part of the TensorFlow graph. We can execute arbitrary Python here.</p>
<p>There's a five second pause in the loop so that things don't happen too fast to watch.</p>
<p>The total word count is stored off in the <code>reduce</code> task, possibly on a different computer, but that doesn't matter. This is part of what's cool about TensorFlow.</p>
<hr>
<h2>Run</h2>
<p>To execute, we run <a href="count.py"><code>count.py</code></a> in four places with the appropriate environment variables set. That's it. We've counted words with many computers.</p>
<hr>
<h2>So what?</h2>
<p>Programming a cluster with TensorFlow is just like programming a single computer with TensorFlow. This is pretty neat, and it makes a lot of things possible beyond just distributed stochastic gradient descent.</p>
<p>The example above demonstrates a distributed queue. If you can do that, do you need a separate queueing system like <a href="https://www.rabbitmq.com/">RabbitMQ</a>? Maybe not in every situation.</p>
<p>The example above sends filenames to workers, which is a pretty general model. What if you feel comfortable sending executable filenames, or some representation of code? You might implement something like <a href="http://www.celeryproject.org/">Celery</a> pretty quickly.</p>
<p>The example I've shown can probably fail in more ways than I even realize. It would be more work to make it robust. It would be again more work to make it more general. But it's pretty exciting to be able to write something like this at all.</p>
<p>And while even the typical TensorFlow distributed training is itself a kind of MapReduce process, TensorFlow is general enough that it could be used for wildly different architectures. TensorFlow is an amazing tool.</p>
<hr>
<h2>Code</h2>
<p>Here's <a href="make_configs.py"><code>make_configs.py</code></a>:</p>
<pre><code class="language-python">import json

cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}
for task_type, addresses in cluster.items():
    for index in range(len(addresses)):
        tf_config = {'cluster': cluster,
                     'task': {'type': task_type,
                              'index': index}}
        tf_config_string = json.dumps(tf_config, indent=2)
        with open('config_{}_{}.sh'.format(task_type, index), 'w') as f:
            f.write("export TF_CONFIG='{}'\n".format(tf_config_string))
            # GPUs won't be needed, so prevent accessing GPU memory.
            f.write('export CUDA_VISIBLE_DEVICES=-1\n')</code></pre>

<p>The files produced by <a href="make_configs.py"><code>make_configs.py</code></a> (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>) look like this:</p>
<pre><code class="language-json">export TF_CONFIG='{
  "cluster": {
    "files": [
      "localhost:2222"
    ], 
    "map": [
      "localhost:2224", 
      "localhost:2225"
    ], 
    "reduce": [
      "localhost:2223"
    ]
  }, 
  "task": {
    "index": 0, 
    "type": "map"
  }
}'
export CUDA_VISIBLE_DEVICES=-1</code></pre>

<p>And here's <a href="count.py"><code>count.py</code></a> all together:</p>
<pre><code class="language-python">from __future__ import print_function

import time
import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)

with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')

if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()

with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')

if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)

if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign(total_word_count,
                             total_word_count + word_count_to_add)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>All the files needed to demo this are together in a <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">repo on GitHub</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</link>
<guid>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</guid>
<pubDate>Tue, 11 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow Clusters: Questions and Code</title>
<description><![CDATA[

<p>One way to think about TensorFlow is as a framework for <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed computing</a>. I've suggested that TensorFlow is a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">distributed virtual machine</a>. As such, it offers a lot of flexibility. TensorFlow also suggests some conventions that make writing programs for distributed computation tractable.</p>
<h2>When is there a cluster?</h2>
<p>A <a href="http://hadoop.apache.org/">Hadoop</a> or <a href="http://spark.apache.org/">Spark</a> cluster is generally long-lived. The cluster runs, processing jobs as they come to it. A company might have a thousand-node Spark cluster, for example, used by everyone in a division.</p>
<p>In contrast, TensorFlow clusters generally spring into being for the purpose of running a particular TensorFlow program. There are computers on the network, and they become members of TensorFlow clusters based on what they're running.</p>
<p>To make this process manageable, you might use a system like <a href="https://kubernetes.io/">Kubernetes</a> or <a href="https://cloud.google.com/ml-engine/">Google Cloud ML</a> to intelligently run TensorFlow programs on specific machines in a larger pool.</p>
<h2>What does the cluster do for me?</h2>
<p>In systems like Hadoop MapReduce and Spark, there's usually considerable distance between the programming interface and how the computation actually gets distributed. If you're writing your own map and reduce steps for Hadoop, you're close to that mechanism, but you're still plugging in to the pre-built larger architecture. Most users prefer higher-level APIs like <a href="https://pig.apache.org/">Pig</a> on Hadoop or the standard Spark APIs. And both Hadoop and Spark support at least one SQL-style interface, even farther from the underlying implementations. As a user of Hadoop or Spark, you don't think about putting computation on particular machines, or worry about the different roles that different machines might play.</p>
<p>With TensorFlow, the abstraction for distributed computing is the same as the abstraction for local computing: the computation graph. Distributing TensorFlow programs means having graphs that span multiple computers. You're responsible for what parts of the graph go where, and what every computer in the cluster does.</p>
<h2>How many programs do I write?</h2>
<p>One familiar model of distributed computing is client-server, like the web. Web servers and browsers are quite different programs.</p>
<p>Closer to TensorFlow applications, a central server could do some computation on request, or it could offer data to be processed on the client side, like <a href="https://setiathome.berkeley.edu/">SETI@home</a> or <a href="https://folding.stanford.edu/">Folding@home</a>. Again, server and client code are distinct.</p>
<p>Nothing prevents you from writing separate "server" and "client" TensorFlow programs, but it isn't necessary, natural, or recommended. One of TensorFlow's design goals was to get away from the client-server divide in DistBelief.</p>
<p>The TensorFlow distributed runtime is peer-to-peer: every machine can accept graph nodes from any other machine, and every machine can put graph nodes on any other machine. Generally, every machine will run the same program. Whether the system behaves like a client-server system or something else entirely is up to you.</p>
<h2>Which computer does what?</h2>
<p>Distributed TensorFlow works by running the same program on multiple machines, but that doesn't mean that every machine does exactly the same thing.</p>
<p>If a system with separate client and server programs is like a system of two symbiotic species, your TensorFlow program is like the DNA of an organism whose cells specialize based on their environment.</p>
<p>The dominant convention is to have two main roles: <em>parameter servers</em> (<code>ps</code>) and <em>workers</em>. You might also have a <em>master</em> role, and one of your workers can be the <em>chief</em> worker, but <code>ps</code> and <code>worker</code> is usually the main division.</p>
<p>Usually a machine running your TensorFlow program will learn what its role should be based on the <code>TF_CONFIG</code> environment variable, which should be set by your cluster manager.</p>
<h2>Who knows what about the cluster topology?</h2>
<p>In Hadoop and Spark, the system is keeping track of all the machines in the cluster. On the web, servers generally only know about client addresses long enough to provide a response.</p>
<p>Usually every machine in a TensorFlow cluster will be given a complete listing of machines in the cluster.</p>
<p>If some machines really don't need to know about others in the same cluster, for example if workers never communicate with one another, it's also possible to provide less complete information.</p>
<p>The cluster topology is also most often provided via the <code>TF_CONFIG</code> environment variable.</p>
<h2>Code?</h2>
<p>Let's say you have a network which includes the following machines:</p>
<ul>
<li><code>192.168.0.10</code></li>
<li><code>192.168.0.11</code></li>
<li><code>192.168.0.12</code></li>
</ul>
<p>The machines are all running the same version of TensorFlow. Let's see how we could get them set up to run a distributed TensorFlow program.</p>
<p>We'll have one parameter server (<code>ps</code>) and two workers. Each machine will know about the cluster's topology.</p>
<pre><code class="language-python">import tensorflow as tf

cluster = {'ps': ['192.168.0.10:2222'],
           'worker': ['192.168.0.11:2222', '192.168.0.12:2222']}
cluster_spec = tf.train.ClusterSpec(cluster)
server = tf.train.Server(cluster_spec)</code></pre>

<p>In the strings like <code>'192.168.0.10:2222'</code>, the protocol (gRPC) is omitted, and the port (<code>2222</code>) is the default for TensorFlow communication.</p>
<p>The <code>server</code> that every machine starts is what enables the communication of the TensorFlow distributed runtime; its behavior is largely at a lower level than the code we'll write.</p>
<p>This code gets the network topology going, but it doesn't tell an individual machine which role it should have. Working out which IP address or hostname refers to the current machine is not necessarily straightforward, but hard-coding different values into different copies of the code would be an even worse idea than hard-coding the topology once. This is where the environment variable <code>TF_CONFIG</code> becomes very useful.</p>
<p>We'll put cluster and task information into the <code>TF_CONFIG</code> environment variable as a JSON serialization. On the machine that will be our parameter server, the data will look like this as a Python dictionary:</p>
<pre><code class="language-python">tf_config = {'cluster': {'ps': ['192.168.0.10:2222'],
                         'worker': ['192.168.0.11:2222', '192.168.0.12:2222']},
             'task': {'type': 'ps',
                      'index': 0}}</code></pre>

<p>You can set the environment variable in the usual way in a shell, making small changes to achieve JSON syntax.</p>
<pre><code class="language-bash">$ export TF_CONFIG='{"cluster": {"ps": ["192.168.0.10:2222"], "worker": ["192.168.0.11:2222", "192.168.0.12:2222"]}, "task": {"type": "ps", "index": 0}}'</code></pre>

<p>To avoid fussing with that directly, you could do it in Python.</p>
<pre><code class="language-python">import os
import json

os.environ['TF_CONFIG'] = json.dumps(tf_config)</code></pre>

<p>Really, it'll be best if your cluster manager sets <code>TF_CONFIG</code> for you.</p>
<p>Once <code>TF_CONFIG</code> is set, you can read it and get to work.</p>
<pre><code class="language-python">tf_config = json.loads(os.environ['TF_CONFIG'])
cluster_spec = tf.train.ClusterSpec(tf_config['cluster'])
task_type = tf_config['task']['type']
task_id = tf_config['task']['index']
server = tf.train.Server(cluster_spec,
                         job_name=task_type,
                         task_index=task_id)</code></pre>

<p>Another way to do that is with <code>tf.contrib.learn.RunConfig</code>, which automatically checks the <code>TF_CONFIG</code> environment variable.</p>
<pre><code class="language-python">config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)</code></pre>

<p>At this point, you are ready to begin writing a distributed TensorFlow program.</p>
<!-- helpful references:

https://github.com/tensorflow/tensorflow/blob/17c47804b86e340203d451125a721310033710f1/tensorflow/contrib/learn/python/learn/estimators/run_config.py

https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/tensorflowcore/trainer/task.py

-->

<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</link>
<guid>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</guid>
<pubDate>Mon, 10 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>How Not To Program the TensorFlow Graph</title>
<description><![CDATA[

<p>Using TensorFlow from Python is like using Python to program <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">another computer</a>. Some Python statements build your TensorFlow program, some Python statements execute that program, and of course some Python statements aren't involved with TensorFlow at all.</p>
<p>Being thoughtful about the graphs you construct can help you avoid confusion and performance pitfalls. Here are a few considerations.</p>
<h2>Avoid having many identical ops</h2>
<!-- Better re-write; maybe separate out a section about Python names for TF ops, losing references to your ops, etc. -->

<p>Lots of methods in TensorFlow create ops in the computation graph, but do not execute them. You may want to execute multiple times, but that doesn't mean you should create lots of copies of the same ops.</p>
<p>A clear example is <code>tf.global_variables_initializer()</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
# Create some variables...
&gt;&gt;&gt; initializer = tf.global_variables_initializer()
# Variables are not yet initialized.
&gt;&gt;&gt; session.run(initializer)
# Now variables are initialized.
# Do some more work...
&gt;&gt;&gt; session.run(initializer)
# Now variables are re-initialized.</code></pre>

<p>If the call to <code>tf.global_variables_initializer()</code> is repeated, for example directly as the argument to <code>session.run()</code>, there are downsides.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.global_variables_initializer())
&gt;&gt;&gt; session.run(tf.global_variables_initializer())</code></pre>

<p>A new initializer op is created every time the argument to <code>session.run()</code> here is evaluated. This creates multiple initializer ops in the graph. Having multiple copies isn't a big deal for small ops in an interactive session, and you might even want to do it in the case of the initializer if you've created more variables that need to be included in initialization. But think about whether you need lots of duplicate ops.</p>
<p>Creating ops inside <code>session.run()</code>, you also don't have a Python variable referring to those ops, so you can't easily reuse them.</p>
<p>In Python, if you create an object that nothing refers to, it can be garbage collected. The abandoned object will be deleted and and memory it used will be freed. That doesn't happen to things in the TensorFlow graph; everything you put in the graph stays there.</p>
<p>It's pretty clear that <code>tf.global_variables_initializer()</code> returns an op. But ops are also created in less obvious ways.</p>
<p>Let's compare to how NumPy works.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>At this point there are two arrays in memory, <code>x</code> and <code>y</code>. The <code>y</code> has the value 2.0, but there's no record of <em>how</em> it came to have that value. The addition has left no record of itself.</p>
<p>TensorFlow is different.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Now only <code>x</code> is a TensorFlow variable; <code>y</code> is an <code>add</code> op, which can return the result of that addition if we ever run it.</p>
<p>One more comparison.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Here <code>y</code> is assigned to refer to one result array <code>x + 1.0</code>, and then reassigned to point to a different one. The first one will be garbage collected and disappear.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>In this case, <code>y</code> refers to one <code>add</code> op in the TensorFlow graph, and then <code>y</code> is reassigned to point to a different <code>add</code> op in the graph. Since <code>y</code> only points to the second <code>add</code> now, we don't have a convenient way to work with the first one. But both the <code>add</code> ops are still around, in the graph, and will stay there.</p>
<p>(As an aside, Python's mechanism for defining class-specific addition <a href="http://www.python-course.eu/python3_magic_methods.php">and so on</a>, which is how <code>+</code> is made to create TensorFlow ops, is pretty neat.)</p>
<p>Especially if you're just working with the default graph and running interactively in a regular REPL or a notebook, you can end up with a lot of abandoned ops in your graph. Every time you re-run a notebook cell that defines any graph ops, you aren't just redefining ops&#8212;you're creating new ones.</p>
<p>Often it's okay to have a few extra ops floating around when you're experimenting. But things can get out of hand.</p>
<pre><code class="language-python">for _ in range(1e6):
    x = x + 1</code></pre>

<p>If <code>x</code> is a NumPy array, or just a regular Python number, this will run in constant memory and finish with one value for x.</p>
<p>But if <code>x</code> is a TensorFlow variable, there will be over a million ops in your TensorFlow graph, just defining a computation and not even <em>doing</em> it.</p>
<p>One immediate fix for TensorFlow is to use a <code>tf.assign</code> op, which gives behavior more like what you might expect.</p>
<pre><code class="language-python">increment_x = tf.assign(x, x + 1)
for _ in range(1e6):
    session.run(increment_x)</code></pre>

<p>This revised version does not create any ops inside the loop, which is generally good advice. TensorFlow does have <a href="https://www.tensorflow.org/api_guides/python/control_flow_ops">control flow constructs</a> including <a href="https://www.tensorflow.org/api_docs/python/tf/while_loop">while loops</a>. But only use these when really needed.</p>
<p>Be conscious of when you're creating ops, and only create the ones you need. Try to keep op creation distinct from op execution. And after interactive experimentation, eventually get to a state, probably in a script, where you're only creating the ops that you need.</p>
<h2>Avoid constants in the graph</h2>
<p>A particularly unfortunate op to needlessly add to a graph is accidental constant ops, especially large ones.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones = np.ones((1000, 1000))</code></pre>

<p>There are a million ones in the NumPy array <code>many_ones</code>. We can add them up.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones.sum()
## 1000000.0</code></pre>

<p>What if we add them up with TensorFlow?</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.reduce_sum(many_ones))
## 1000000.0</code></pre>

<p>The result is the same, but the mechanism is quite different. This not only added some ops to the graph&#8212;it put a copy of the entire million-element array into the graph as a constant.</p>
<p>Variations on this pattern can result in accidentally loading an entire data set into the graph as constants. A program might still run, for small data sets. Or your system might fail.</p>
<p>One simple way to avoid storing data in the graph is to use the <code>feed_dict</code> mechanism.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_things = tf.placeholder(tf.float64)
&gt;&gt;&gt; adder = tf.reduce_sum(many_things)
&gt;&gt;&gt; session.run(adder, feed_dict={many_things: many_ones})
## 1000000.0</code></pre>

<p>As before, be clear about what you're adding to the graph and when. Concrete data usually only enters the graph at moments of evaluation.</p>
<h2>TensorFlow as Functional Programming</h2>
<p>TensorFlow ops are like functions. This is especially clear when an op has one or more placeholder inputs; evaluating the op in a session is like calling a function with those arguments. So Python functions that return TensorFlow ops are like <a href="https://en.wikipedia.org/wiki/Higher-order_function">higher-order functions</a>.</p>
<p>You might decide that it's worth putting a constant into the graph. For example, if you have to reshape a lot of tensors to 28x28, you might make an op that does that.</p>
<pre><code class="language-python">&gt;&gt;&gt; input_tensor = tf.placeholder(tf.float32)
&gt;&gt;&gt; reshape_to_28 = tf.reshape(input_tensor, shape=[28, 28])</code></pre>

<p>This is like <a href="https://en.wikipedia.org/wiki/Currying">currying</a> in that the <code>shape</code> argument has now been set. The <code>[28, 28]</code> has become a constant in the graph and specified that argument. Now to evaluate <code>reshape_to_28</code> we only have to provide <code>input_tensor</code>.</p>
<p>Broader connections have been suggested between <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">neural networks, types, and functional programming</a>. It's interesting to think of TensorFlow as a system that supports this kind of construction.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</guid>
<pubDate>Tue, 04 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Images and TFRecords</title>
<description><![CDATA[

<p>There are a number of ways to work with images in TensorFlow and, if you wish, with TFRecords. These methods aren't so mysterious if you <a href="/20170323-tfrecords_for_humans/">understand TFRecords</a> and a little bit about how digital images work.</p>
<hr>
<h2>Representations for Images</h2>
<!--

(I kind of like this paragraph, at least in that it gives credit for the image source, but it's not really key to the development here.)

Wikipedia [tells me](https://en.wikipedia.org/wiki/Keep_Calm_and_Carry_On) that this [poster](https://commons.wikimedia.org/wiki/File:Freedom_Is_In_Peril_Defend_It_With_All_Your_Might.svg), and the more widely known "Keep Calm and Carry On," were not very well received [in 1939](img/freedom_in_tube.jpg). Perhaps its time has come.

-->

<p>For example, this image is 600 pixels tall and 400 pixels wide. Every pixel has some intensity in red, green, and blue: three values, or channels, for every pixel. This image has shape [600, 400, 3]. (The order of the dimensions doesn't matter as long as everybody agrees on the convention.) The display on your screen is like a dense matrix; it is a <a href="https://en.wikipedia.org/wiki/Raster_graphics">raster graphic</a>.</p>
<p><img alt="FREEDOM IS IN PERIL / DEFEND IT WITH ALL YOUR MIGHT" src="img/freedom.png"></p>
<p>Neural networks that work with images typically work with this kind of dense matrix representation. For this image, the matrix will have 600 * 400 * 3 = 720,000 values. If each value is an unsigned 8-bit integer, that's 720,000 bytes, or about three quarters of a megabyte.</p>
<p>Images sometimes also have an alpha transparency channel, which is a fourth channel in a color image, but not necessary if there's nothing else "underneath" the image. And not all images are color; greyscale (black and white) images can have just one channel.</p>
<p>Using unsigned 8-bit integers (256 possible values) for each value in the image array is enough for displaying images to humans. But when working with image data, it isn't uncommon to switch to 32-bit floats, for example. This quadruples the size of the data in memory. As always, remain aware of your data types.</p>
<h3>Dense Array</h3>
<p>One way to store complete raster image data is by serializing a NumPy array to disk with <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html"><code>numpy.save</code></a>.</p>
<pre>720,080 bytes  <a href="img/freedom.npy">freedom.npy</a></pre>

<p>The file <code>freedom.npy</code> has 80 more bytes than the ones required to store the array values. Those extra bytes specify things like the dimensions of the array. If we save raw array values in TFRecords, we'll also have to keep track of this additional information.</p>
<p>Because storing one or more value for every pixel takes so many bytes, file formats for images typically do something cleverer.</p>
<h3>JPEG</h3>
<p><a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a> is lossy. When you save an array as JPG and then read from the JPG, it will generally look about the same, but you won't necessarily get back exactly the same values for your array. JPEG is like <a href="https://en.wikipedia.org/wiki/MP3">MP3</a> for images. JPEG is good for photographs.</p>
<pre> 25,136 bytes  <a href="img/freedom.jpg">freedom.jpg</a></pre>

<p><code>freedom.jpg</code> is less than 4% the size of the NumPy array, and it still looks pretty good. It does have some ringing artifacts around the letter edges. JPEG file size depends on compression parameters when saving, and on the encoder used. For example, Google <a href="https://research.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html">released</a> their <a href="https://github.com/google/guetzli/">Guetzli</a> JPEG encoder, which can produce smaller files but takes more computation to do so.</p>
<pre> 46,567 bytes  <a href="img/freedom2.jpg">freedom2.jpg</a></pre>

<p><code>freedom2.jpg</code> is another JPEG file, saved with higher quality. Ringing artifacts are pretty much gone. The file is still under 7% the size of the NumPy arrray.</p>
<h3>PNG</h3>
<p><a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics">PNG</a> is lossless. If you save as PNG and then read from the PNG, you can get back exactly what you had originally. PNG is like <a href="https://en.wikipedia.org/wiki/Zip_(file_format)">ZIP</a> for images, but image viewers do the "un-zipping" automatically so they can put a raster image on the screen. PNG is like <a href="https://en.wikipedia.org/wiki/GIF">GIF</a> without animation.</p>
<pre> 33,192 bytes  <a href="img/freedom.png">freedom.png</a></pre>

<p><code>freedom.png</code> is under 5% the size of the NumPy array, and it preserves the image perfectly. It's less often used, but compression parameters can also affect PNG size, and the encoder can make a difference too. PNG uses deflate (zlip) compression, so Google's <a href="https://github.com/google/zopfli">Zopfli</a> algorithm can be used, for example, while <a href="https://github.com/google/brotli">Brotli</a> cannot.</p>
<h3>SVG</h3>
<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> doesn't represent pixels directly at all, but represents in a text format the geometry of shapes in the image. SVG images can look good at any zoom level; they don't suffer from pixelization. They can also be edited with different tools than raster images. And for simple images, an SVG can be quite small. SVG is like <a href="https://en.wikipedia.org/wiki/HTML">HTML</a>; you can look at it as text, but to see it as intended requires a program like a web browser.</p>
<pre> 42,721 bytes  <a href="img/freedom.svg">freedom.svg</a></pre>

<p><code>freedom.svg</code> turns out not be a super efficient encoding of the image; it represents all the letter outlines instead of using plain text in some font, for example. But it represents the image fundamentally differently from just recreating pixels, and it's still under 6% the size of the NumPy array file.</p>
<h3>TFRecords?</h3>
<p>TFRecords don't know anything about image formats. You just put bytes in them. So you have your choice of whether that means you store dense arrays of values or a well-known image format. TensorFlow provides <a href="https://www.tensorflow.org/api_guides/python/image">image format support</a> for JPEG, PNG, and GIF in the computation graph.</p>
<hr>
<h2>Images without TensorFlow</h2>
<p>As always, <a href="/20170312-use_only_what_you_need_from_tensorflow/">you have a choice about whether you need to do everything with TensorFlow</a>. If you're loading data batches with a <code>feed_dict</code>, in particular, feel free to use whatever Python functionality you're comfortable with. Even if you're not, you'll probably want to do some work with your data outside of TensorFlow before you move all your computation into the graph.</p>
<p>The Matplotlib <a href="http://matplotlib.org/users/image_tutorial.html">image tutorial</a> recommends using <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imread"><code>matplotlib.image.imread</code></a> to read image formats from disk. This function will automatically change image array values to floats between zero and one, and it doesn't give you options about how to read the image.</p>
<p>The <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imread.html">scipy.misc.imread</a> function uses the Python Imaging Library (PIL) to support many image formats, including PNG and JPEG. This function also has some useful options. The original <code>freedom.png</code> has an alpha channel which we don't need. Passing <code>mode='RGB'</code> tells the reader to give us just three color channels.</p>
<pre><code class="language-python">&gt;&gt;&gt; import scipy.misc
&gt;&gt;&gt; image = scipy.misc.imread('freedom.png', mode='RGB')
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>Matplotlib's <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.imshow">imshow</a> is good for checking out what image arrays look like. (Also specify <code>%matplotlib inline</code> if you're in a notebook.)</p>
<pre><code class="language-python">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.imshow(image)</code></pre>

<p>NumPy gives us a way to save and load its arrays.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.save('freedom.npy', image)
&gt;&gt;&gt; same_image = np.load('freedom.npy')</code></pre>

<p>As long as we have <code>scipy.misc</code> imported, we can use <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imsave.html">scipy.misc.imsave</a> to write various image formats as well. For saving, <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imsave">matplotlib.image.imsave</a> actually has more options. Neither of these offer control over compression level, for example.</p>
<pre><code class="language-python">&gt;&gt;&gt; scipy.misc.imsave('freedom.jpg', image)</code></pre>

<hr>
<h2>PNG and JPEG with TensorFlow</h2>
<p>TensorFlow has ops for decoding and encoding PNGs and JPEGs, so we can put these operations into the computation graph.</p>
<p>Above, <code>imread</code> both read a file from disk and decoded it, and <code>imsave</code> both encoded an image and wrote it to disk. The TensorFlow functionality decouples the decoding and encoding from file reading and writing. This example will avoid using TensorFlow's file reading and writing, to focus just on the encoding and decoding.</p>
<p>The <code>channels=3</code> passed to <code>tf.image.decode_image()</code> is the equivalent of <code>mode='RGB'</code> above. We don't want to get the alpha channel from the PNG file.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; with open('freedom.png', 'rb') as f:
...     png_bytes = f.read()
&gt;&gt;&gt; bytes = tf.placeholder(tf.string)
&gt;&gt;&gt; decode_png = tf.image.decode_image(bytes, channels=3)
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; image = session.run(decode_png, feed_dict={bytes: png_bytes})
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_image"><code>tf.image.decode_image</code></a> here intelligently uses <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_png"><code>tf.image.decode_png</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_jpeg"><code>tf.image.decode_jpeg</code></a>, or <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_gif"><code>tf.image.decode_gif</code></a>, similar to how above <code>imread</code> can handle a variety of formats. You can also use the appropriate function directly.</p>
<p>Above, <code>imsave</code> supported writing various formats, choosing the one appropriate for the filename specified. In TensorFlow, you have to use <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_jpeg"><code>tf.image.encode_jpeg</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_png"><code>tf.image.encode_png</code></a> directly, and both provide extra arguments controlling compression and more.</p>
<pre><code class="language-python">&gt;&gt;&gt; tensor = tf.placeholder(tf.uint8)
&gt;&gt;&gt; encode_jpeg = tf.image.encode_jpeg(tensor)
&gt;&gt;&gt; jpeg_bytes = session.run(encode_jpeg, feed_dict={tensor: image})
&gt;&gt;&gt; with open('freedom.jpg', 'wb') as f:
...     f.write(jpeg_bytes)</code></pre>

<p>With the default settings, TensorFlow encodes a larger JPEG than <code>imsave</code>, coming in at 46,567 bytes. It looks pretty good.</p>
<hr>
<h2>PNG and JPEG in TFRecords</h2>
<p>We can put plain bytes into <code>Example</code> TFRecords <a href="/20170323-tfrecords_for_humans/">without too much trouble</a>. So PNG or JPEG images are easily handled.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'png_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=[png_bytes]))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_png_bytes = same_example.features.feature['png_bytes'].bytes_list.value[0]</code></pre>

<p>When the <code>same_png_bytes</code> is decoded by <code>tf.image.decode_image</code>, as above, or <code>tf.image.decode_png</code> directly, you'll get back a tensor with the correct dimensions, because PNG (and JPEG) include that information in their encodings.</p>
<hr>
<h2>Image Arrays in TFRecords</h2>
<p>If you want to save dense matrix representations in TFRecords, there's a little bit of bookkeeping to do, but it isn't too bad.</p>
<pre><code>image_bytes = image.tostring()
image_shape = image.shape

my_example = tf.train.Example(features=tf.train.Features(feature={
    'image_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes])),
    'image_shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image_shape))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_image_bytes = same_example.features.feature['image_bytes'].bytes_list.value[0]
same_image_shape = list(
    same_example.features.feature['image_shape'].int64_list.value)</code></pre>

<p>With the information recovered from TFRecord form, it's easy to use NumPy to put the image back together.</p>
<pre><code class="language-python">same_image = np.fromstring(same_image_bytes, dtype=np.uint8)
same_image.shape = same_image_shape</code></pre>

<p>You can do the same using TensorFlow.</p>
<pre><code class="language-python">shape = tf.placeholder(tf.int32)
new_image = tf.reshape(tf.decode_raw(bytes, tf.uint8), shape)
same_image = session.run(encode_jpeg, feed_dict={bytes: same_image_bytes,
                                                 shape: same_image_shape})</code></pre>

<p>In this example, however, the parsing of the <code>Example</code> was already done outside the TensorFlow graph, so there isn't a strong reason to stay inside the graph here.</p>
<hr>
<h3>Comparison to Caffe and LMDB</h3>
<p><a href="http://caffe.berkeleyvision.org/">Caffe</a> is an older deep learning framework that can work with data stored in <a href="https://symas.com/offerings/lightning-memory-mapped-database/">LMDB</a> on-disk databases, similar to how TensorFlow can work with data stored in TFRecords files.</p>
<p>Like TensorFlow, Caffe defines a protocol buffer message for training examples. In Caffe, it's called <code>Datum</code>. These are saved just like in TensorFlow, by serializing them and putting them in a file on disk, but instead of a TFRecords file, which just puts records in a row and reads them out in that order, here Caffe will work with LMDB, which has the semantics of a key-value store.</p>
<p>TensorFlow's <code>Example</code> format is super flexible, but the trade-off is that it doesn't automatically do things for you. Caffe's <code>Datum</code>, on the other hand, expects you to put in a dense image array, and an integer class label. So here there isn't any fiddling with array size, for example, but the trade-off is that it won't work easily for arbitrary data structures that we might eventually want to store.</p>
<p>In the TFRecords examples above, we stored only image data, and said nothing about a class label or anything else. This is because TFRecords lets you decide what you want to save, rather than defining a format in advance. You could save an integer label, or a float regression label, or a string of text, or an image mask, and on and on. Here, we're just using the built-in Caffe integer label.</p>
<pre><code class="language-python">import caffe
import lmdb

# `image` was read above
label = 9  # for a classification problem

datum = caffe.io.array_to_datum(arr=image, label=label)
datum_str = datum.SerializeToString()

env = lmdb.open('lmdb_data')
txn = env.begin(write=True)
txn.put(key='my datum', value=datum_str)

cur = txn.cursor()
same_datum_str = cur.get('my datum')

same_datum = caffe.proto.caffe_pb2.Datum().FromString(same_datum_str)
same_image = caffe.io.datum_to_array(same_datum)
same_label = datum.label</code></pre>

<p>To actually work with the Caffe training process, there are some other conditions for the data to satisfy. Caffe expects [channels, height, width] instead of [height, width, channels], for example.</p>
<hr>
<h2>What should you use?</h2>
<p>Prefer doing fewer separate manipulation steps to whatever your original data is, if you can help it: try not to have lots of different versions of your data in different places on disk. This will make your life easier.</p>
<p>If you are choosing a format, JPEG is good for photos.</p>
<p>Think about whether you need to put everything into the TensorFlow computation graph. Think about whether you need to use TFRecords. Try to spend your time on things that help solve your problems.</p>
<p>For two more complete <em>in situ</em> examples of converting images to TFRecords, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py">code for MNIST images</a> and <a href="https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py">code for ImageNet images</a>. The ImageNet code can be run on the command-line.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170403-images_and_tfrecords/</link>
<guid>http://planspace.org/20170403-images_and_tfrecords/</guid>
<pubDate>Mon, 03 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>On Tyranny: Twenty Lessons from the Twentieth Century</title>
<description><![CDATA[

<p>Shortly after the U.S. presidential election of 2016, <a href="http://history.yale.edu/people/timothy-snyder">Timothy Snyder</a> wrote a popular post that was widely <a href="https://qz.com/846940/a-yale-history-professors-20-point-guide-to-defending-democracy-under-a-trump-presidency/">re-published</a> and then expanded to become the book <a href="https://www.amazon.com/Tyranny-Twenty-Lessons-Twentieth-Century/dp/0804190119">On Tyranny: Twenty Lessons from the Twentieth Century</a>.</p>
<p>I've put together text versions of the post, in one file and in 22 files, because I think the ideas are important and so that I can use these versions in text data processing examples.</p>
<ul>
<li>Whole post in a single file:<ul>
<li><a href="on_tyranny.txt"><code>on_tyranny.txt</code></a></li>
</ul>
</li>
<li>Compressed directory of 22 files:<ul>
<li><a href="on_tyranny.tar.gz"><code>on_tyranny.tar.gz</code></a></li>
<li><a href="on_tyranny.zip"><code>on_tyranny.zip</code></a></li>
</ul>
</li>
<li>Each section individually:<ul>
<li><a href="on_tyranny/00_introduction.txt"><code>00_introduction.txt</code></a></li>
<li><a href="on_tyranny/01_do_not_obey_in_advance.txt"><code>01_do_not_obey_in_advance.txt</code></a></li>
<li><a href="on_tyranny/02_defend_an_institution.txt"><code>02_defend_an_institution.txt</code></a></li>
<li><a href="on_tyranny/03_recall_professional_ethics.txt"><code>03_recall_professional_ethics.txt</code></a></li>
<li><a href="on_tyranny/04_distinguish_certain_words.txt"><code>04_distinguish_certain_words.txt</code></a></li>
<li><a href="on_tyranny/05_be_calm_when_the_unthinkable_arrives.txt"><code>05_be_calm_when_the_unthinkable_arrives.txt</code></a></li>
<li><a href="on_tyranny/06_be_kind_to_our_language.txt"><code>06_be_kind_to_our_language.txt</code></a></li>
<li><a href="on_tyranny/07_stand_out.txt"><code>07_stand_out.txt</code></a></li>
<li><a href="on_tyranny/08_believe_in_truth.txt"><code>08_believe_in_truth.txt</code></a></li>
<li><a href="on_tyranny/09_investigate.txt"><code>09_investigate.txt</code></a></li>
<li><a href="on_tyranny/10_practice_corporeal_politics.txt"><code>10_practice_corporeal_politics.txt</code></a></li>
<li><a href="on_tyranny/11_make_eye_contact_and_small_talk.txt"><code>11_make_eye_contact_and_small_talk.txt</code></a></li>
<li><a href="on_tyranny/12_take_responsibility_for_the_face_of_the_world.txt"><code>12_take_responsibility_for_the_face_of_the_world.txt</code></a></li>
<li><a href="on_tyranny/13_hinder_the_one-party_state.txt"><code>13_hinder_the_one-party_state.txt</code></a></li>
<li><a href="on_tyranny/14_give_regularly_to_good_causes.txt"><code>14_give_regularly_to_good_causes.txt</code></a></li>
<li><a href="on_tyranny/15_establish_a_private_life.txt"><code>15_establish_a_private_life.txt</code></a></li>
<li><a href="on_tyranny/16_learn_from_others_in_other_countries.txt"><code>16_learn_from_others_in_other_countries.txt</code></a></li>
<li><a href="on_tyranny/17_watch_out_for_the_paramilitaries.txt"><code>17_watch_out_for_the_paramilitaries.txt</code></a></li>
<li><a href="on_tyranny/18_be_reflective_if_you_must_be_armed.txt"><code>18_be_reflective_if_you_must_be_armed.txt</code></a></li>
<li><a href="on_tyranny/19_be_as_courageous_as_you_can.txt"><code>19_be_as_courageous_as_you_can.txt</code></a></li>
<li><a href="on_tyranny/20_be_a_patriot.txt"><code>20_be_a_patriot.txt</code></a></li>
<li><a href="on_tyranny/21_credits.txt"><code>21_credits.txt</code></a></li>
</ul>
</li>
</ul>
<p>The contents of <a href="on_tyranny.txt"><code>on_tyranny.txt</code></a> inline:</p>
<pre>
Americans are no wiser than the Europeans who saw democracy yield to
fascism, Nazism, or communism. Our one advantage is that we might
learn from their experience. Now is a good time to do so. Here are
twenty lessons from the twentieth century, adapted to the
circumstances of today.

Lesson 1. Do not obey in advance.

Much of the power of authoritarianism is freely given. In times like
these, individuals think ahead about what a more repressive government
will want, and then start to do it without being asked. You&#8217;ve already
done this, haven&#8217;t you? Stop. Anticipatory obedience teaches
authorities what is possible and accelerates unfreedom.


Lesson 2. Defend an institution.

Defend an institution. Follow the courts or the media, or a court or a
newspaper. Do not speak of &#8220;our institutions&#8221; unless you are making
them yours by acting on their behalf. Institutions don&#8217;t protect
themselves. They go down like dominoes unless each is defended from
the beginning.


Lesson 3. Recall professional ethics.

When the leaders of state set a negative example, professional
commitments to just practice become much more important. It is hard to
break a rule-of-law state without lawyers, and it is hard to have show
trials without judges.


Lesson 4. When listening to politicians, distinguish certain words.

Look out for the expansive use of &#8220;terrorism&#8221; and &#8220;extremism.&#8221; Be
alive to the fatal notions of &#8220;exception&#8221; and &#8220;emergency.&#8221; Be angry
about the treacherous use of patriotic vocabulary.


Lesson 5: Be calm when the unthinkable arrives.

When the terrorist attack comes, remember that all authoritarians at
all times either await or plan such events in order to consolidate
power. Think of the Reichstag fire. The sudden disaster that requires
the end of the balance of power, the end of opposition parties, and so
on, is the oldest trick in the Hitlerian book. Don&#8217;t fall for it.


Lesson 6: Be kind to our language.

Avoid pronouncing the phrases everyone else does. Think up your own
way of speaking, even if only to convey that thing you think everyone
is saying. (Don&#8217;t use the internet before bed. Charge your gadgets
away from your bedroom, and read.) What to read? Perhaps The Power of
the Powerless by V&#225;clav Havel, 1984 by George Orwell, The Captive Mind
by Czes&#322;aw Milosz, The Rebel by Albert Camus, The Origins of
Totalitarianism by Hannah Arendt, or Nothing is True and Everything is
Possible by Peter Pomerantsev.


Lesson 7: Stand out.

Someone has to. It is easy, in words and deeds, to follow along. It
can feel strange to do or say something different. But without that
unease, there is no freedom. And the moment you set an example, the
spell of the status quo is broken, and others will follow.


Lesson 8: Believe in truth.

To abandon facts is to abandon freedom. If nothing is true, then no
one can criticize power, because there is no basis upon which to do
so. If nothing is true, then all is spectacle. The biggest wallet pays
for the most blinding lights.


Lesson 9: Investigate.

Figure things out for yourself. Spend more time with long articles.
Subsidize investigative journalism by subscribing to print media.
Realize that some of what is on your screen is there to harm you.
Learn about sites that investigate foreign propaganda pushes.


Lesson 10: Practice corporeal politics.

Power wants your body softening in your chair and your emotions
dissipating on the screen. Get outside. Put your body in unfamiliar
places with unfamiliar people. Make new friends and march with them.


Lesson 11: Make eye contact and small talk.

This is not just polite. It is a way to stay in touch with your
surroundings, break down unnecessary social barriers, and come to
understand whom you should and should not trust. If we enter a culture
of denunciation, you will want to know the psychological landscape of
your daily life.


Lesson 12: Take responsibility for the face of the world.

Notice the swastikas and the other signs of hate. Do not look away and
do not get used to them. Remove them yourself and set an example for
others to do so.


Lesson 13: Hinder the one-party state.

The parties that took over states were once something else. They
exploited a historical moment to make political life impossible for
their rivals. Vote in local and state elections while you can.


Lesson 14: Give regularly to good causes, if you can.

Pick a charity and set up autopay. Then you will know that you have
made a free choice that is supporting civil society helping others
doing something good.


Lesson 15: Establish a private life.

Nastier rulers will use what they know about you to push you around.
Scrub your computer of malware. Remember that email is skywriting.
Consider using alternative forms of the internet, or simply using it
less. Have personal exchanges in person. For the same reason, resolve
any legal trouble. Authoritarianism works as a blackmail state,
looking for the hook on which to hang you. Try not to have too many
hooks.


Lesson 16: Learn from others in other countries.

Keep up your friendships abroad, or make new friends abroad. The
present difficulties here are an element of a general trend. And no
country is going to find a solution by itself. Make sure you and your
family have passports.


Lesson 17: Watch out for the paramilitaries.

When the men with guns who have always claimed to be against the
system start wearing uniforms and marching around with torches and
pictures of a Leader, the end is nigh. When the pro-Leader
paramilitary and the official police and military intermingle, the
game is over.


Lesson 18: Be reflective if you must be armed.

If you carry a weapon in public service, God bless you and keep you.
But know that evils of the past involved policemen and soldiers
finding themselves, one day, doing irregular things. Be ready to say
no. (If you do not know what this means, contact the United States
Holocaust Memorial Museum and ask about training in professional
ethics.)


Lesson 19: Be as courageous as you can.

If none of us is prepared to die for freedom, then all of us will die
in unfreedom.


Lesson 20: Be a patriot.

The incoming president is not. Set a good example of what America
means for the generations to come. They will need it.


This was written by Timothy Snyder:
http://history.yale.edu/people/timothy-snyder

This text is from the version published here:
https://qz.com/846940/a-yale-history-professors-20-point-guide-to-defending-democracy-under-a-trump-presidency/

The short post version was expanded to a book:
https://www.amazon.com/Tyranny-Twenty-Lessons-Twentieth-Century/dp/0804190119
</pre>    
    ]]></description>
<link>http://planspace.org/20170331-on_tyranny/</link>
<guid>http://planspace.org/20170331-on_tyranny/</guid>
<pubDate>Fri, 31 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TFRecords via Protocol Buffer Definitions</title>
<description><![CDATA[

<p>I've written about <a href="/20170323-tfrecords_for_humans/">how to use TFRecords</a>, and I've written about <a href="/20170329-protocol_buffers_in_python/">how protocol buffer messages are defined</a>. Since the data structures in TFRecords are defined as protocol buffer messages, we can use their definitions to understand them.</p>
<p>Protocol buffer definitions remind me a little bit of <a href="https://en.wikipedia.org/wiki/Syntax_diagram">railroad diagrams</a> like the ones in Douglas Crockford's <a href="http://shop.oreilly.com/product/9780596517748.do">JavaScript: The Good Parts</a>. Here's how Crockford builds up number literals in JavaScript, starting from digits:</p>
<p><img alt="integer railroad diagram" src="img/railroad_integer.png"></p>
<p><img alt="fraction railroad diagram" src="img/railroad_fraction.png"></p>
<p><img alt="exponent railroad diagram" src="img/railroad_exponent.png"></p>
<p><img alt="number railroad diagram" src="img/railroad_number.png"></p>
<p>Small components are fully defined, which can then be built into more complex constructs. (<a href="http://archive.oreilly.com/pub/a/javascript/excerpts/javascript-good-parts/syntax-diagrams.html">More examples</a>.) Protocol buffer definitions are built in much the same way.</p>
<p>With comments, the TensorFlow source files <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto">feature.proto</a> and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto">example.proto</a> total 400 lines. There are only about 30 lines are not comments. These lines (rearranged slightly) do all the defining of the <code>Example</code> and <code>SequenceExample</code> formats that are used for TFRecords.</p>
<pre><code>message BytesList {
  repeated bytes value = 1;
}
message FloatList {
  repeated float value = 1 [packed = true];
}
message Int64List {
  repeated int64 value = 1 [packed = true];
}
message Feature {
  oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
  }
}
message Features {
  map&lt;string, Feature&gt; feature = 1;
}
message Example {
  Features features = 1;
}
message FeatureList {
  repeated Feature feature = 1;
}
message FeatureLists {
  map&lt;string, FeatureList&gt; feature_list = 1;
}
message SequenceExample {
  Features context = 1;
  FeatureLists feature_lists = 2;
}</code></pre>

<p>These brief definitions give rise to most of the functionality <a href="/20170329-protocol_buffers_in_python/">described</a> for creating, writing, and reading the TFRecords formats.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170330-tfrecords_via_proto/</link>
<guid>http://planspace.org/20170330-tfrecords_via_proto/</guid>
<pubDate>Thu, 30 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Protocol Buffers in Python</title>
<description><![CDATA[

<p>Google's data interchange format, <a href="https://en.wikipedia.org/wiki/Protocol_Buffers">Protocol Buffers</a>, is pretty straightforward.</p>
<hr>
<h3>Installing Protobuf</h3>
<p>Python support for protocol buffers can be installed with <code>pip</code>:</p>
<pre><code class="language-bash">pip install protobuf</code></pre>

<p>The package is imported as <code>google.protobuf</code>, but you likely won't need to import it.</p>
<p>To define new protocol buffer formats, you'll also want the <code>protoc</code> tool. One way to get it is to find and install a system-specific package, but you can also get it by installing another Google Python package, <code>grpcio-tools</code>:</p>
<pre><code class="language-bash">pip install grpcio-tools</code></pre>

<p>This won't put a <code>protoc</code> executable in your <code>PATH</code>, but it will let you run <code>protoc</code> via Python, as <code>python -m grpc_tools.protoc</code>. For convenience, you can add an alias in your shell:</p>
<pre><code class="language-bash">alias protoc='python -m grpc_tools.protoc'</code></pre>

<hr>
<h3>Defining Protobuf Messages</h3>
<p>The details of protocol buffer messages types are defined in <code>.proto</code> files like <a href="my_example.proto"><code>my_example.proto</code></a>.</p>
<pre><code>syntax = "proto3";

message Bottle {
  string note = 1;
}</code></pre>

<p>Syntax version 3 has to be specified, as the default is still version 2.</p>
<p>We're defining a fairly dull message. A <code>Bottle</code> can contain one <code>note</code>, which is a string.</p>
<p>The number one there is not setting a default value, but specifying a numbering that's used internally when reading and writing binary representations of our messages.</p>
<hr>
<h3>Generating Code for our Protobuf</h3>
<p>Assuming we're in the same directory as <code>my_example.proto</code>, we can use <code>protoc</code> to generate some Python code corresponding to the message type we defined:</p>
<pre><code class="language-bash">protoc --proto_path=./ --python_out=./ my_example.proto</code></pre>

<p>This will produce a new file <code>my_example_pb2.py</code>. (Syntax version 3 does not affect the filename here.)</p>
<hr>
<h3>Using Protobuf Messages in Python</h3>
<p>With the generated <code>my_example_pb2.py</code> code, we can use our message type in Python, and take advantage of features like serialization and deserialization.</p>
<pre><code class="language-python">import my_example_pb2

my_bottle = my_example_pb2.Bottle(note='Ahoy!')

with open('my_bottle.pb', 'wb') as f:
    f.write(my_bottle.SerializeToString())

with open('my_bottle.pb', 'rb') as f:
    new_bottle = my_example_pb2.Bottle().FromString(f.read())</code></pre>

<p>This looks a lot like <a href="/20170323-tfrecords_for_humans/">examples with TFRecords</a>, because they use the same mechanism.</p>
<hr>
<h3>More Information</h3>
<p>The example here is deliberately minimal. For more detail, the <a href="https://developers.google.com/protocol-buffers/">Google protocol buffers site</a> is quite good, with a very nice <a href="https://developers.google.com/protocol-buffers/docs/pythontutorial">Python tutorial</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170329-protocol_buffers_in_python/</link>
<guid>http://planspace.org/20170329-protocol_buffers_in_python/</guid>
<pubDate>Wed, 29 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow as a Distributed Virtual Machine</title>
<description><![CDATA[

<p>TensorFlow has a flexible API, and it has automatic differentiation, and it can run on GPUs. But the thing that's really neat about TensorFlow is that it gives you a fairly general way to easily program across multiple computers.</p>
<p>TensorFlow's distributed runtime, the big bottom box in this figure from the <a href="https://research.google.com/pubs/pub45381.html">2016 paper</a> "TensorFlow: A system for large-scale machine learning", is the part of TensorFlow that runs the computation graph.</p>
<p><img alt="layered TensorFlow architecture" src="img/tensorflow_layers.png"></p>
<p>The computation graph, specified with <a href="https://en.wikipedia.org/wiki/Protocol_Buffers">protocol buffers</a>, is much higher level than <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java virtual machine</a> (JVM) bytecode. But I think it's interesting to think of the TensorFlow distributed runtime as a sort of <a href="https://en.wikipedia.org/wiki/Virtual_machine">virtual machine</a>. This is not a whole system virtual machine, but a process virtual machine, like the JVM, to "execute computer programs in a platform-independent environment." The Python API can be like a <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific language</a> for programming the TensorFlow graph.</p>
<p>Unlike the JVM, and unlike any other system I know, TensorFlow lets you directly put computation on multiple machines, pretty much however you want, and then it quietly handles all the details for you. Wherever it needs to, TensorFlow adds send and receive nodes to allow the graph to be executed as specified. This is shown in this figure from the <a href="https://research.google.com/pubs/pub45166.html">2015 paper</a> "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems" (also <a href="https://arxiv.org/abs/1603.04467">available on arXiv</a> and commonly linked as the <a href="http://download.tensorflow.org/paper/whitepaper2015.pdf">PDF on tensorflow.org</a>).</p>
<p><img alt="distributing a TensorFlow graph" src="img/distributed_graph.png"></p>
<p>The box labels in the figure are for devices, but with TensorFlow they could be on the same machine or on different machines, and it hardly matters.</p>
<p>The flexibility and ease that result are incredible. It isn't terribly hard to think of implementing your own map-reduce system using TensorFlow. It might even be reasonably performant, if you have a distributed file system! The common TensorFlow distributed model training techniques are a kind of map-reduce, after all. But you could do pretty near anything! This is pretty cool.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/</link>
<guid>http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/</guid>
<pubDate>Tue, 28 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow and Queues</title>
<description><![CDATA[

<p>There are many ways to implement <a href="https://en.wikipedia.org/wiki/Queue_(abstract_data_type)">queue data structures</a>, and TensorFlow has some of its own.</p>
<p><img alt="queue" src="img/queue.png"></p>
<hr>
<h3>FIFO Queue with a list</h3>
<p>In Python, a <a href="https://docs.python.org/3/library/stdtypes.html?highlight=list#list">list</a> can implement a first-in first-out (FIFO) queue, with slightly awkward syntax:</p>
<pre><code class="language-python">&gt;&gt;&gt; my_list = []
&gt;&gt;&gt; my_list.insert(0, 'a')
&gt;&gt;&gt; my_list.insert(0, 'b')
&gt;&gt;&gt; my_list.insert(0, 'c')
&gt;&gt;&gt; my_list.pop()
'a'
&gt;&gt;&gt; my_list.pop()
'b'
&gt;&gt;&gt; my_list.pop()
'c'</code></pre>

<p>A Python list is not a very efficient implementation for a first-in first-out queue, and it doesn't offer mechanisms for limiting the total length of the queue at any given time, for example. But also, queues are a common way to implement communication between threads, and not everything you might do with a list is perfectly thread-safe (though it is surprisingly close; thanks <a href="https://en.wikipedia.org/wiki/Global_interpreter_lock">GIL</a>).</p>
<hr>
<h3>FIFO Queue with the Python standard library</h3>
<p>The Python standard library's <a href="https://docs.python.org/3/library/queue.html">queue</a> (<code>Queue</code> in Python 2) provides several queue options. The <code>queue.Queue</code> class implements a thread-safe FIFO queue:</p>
<pre><code class="language-python">&gt;&gt;&gt; import queue
&gt;&gt;&gt; my_queue = queue.Queue()
&gt;&gt;&gt; my_queue.put('a')
&gt;&gt;&gt; my_queue.put('b')
&gt;&gt;&gt; my_queue.put('c')
&gt;&gt;&gt; my_queue.get()
'a'
&gt;&gt;&gt; my_queue.get()
'b'
&gt;&gt;&gt; my_queue.get()
'c'</code></pre>

<p>When making a <code>queue.Queue</code>, you can specify an integer <code>maxsize</code> argument to set a bound on how many things can be in the queue at any given time. The default is zero, which makes a queue that can store (theoretically) any number of things. And there are no restrictions on what you can put in; anything in Python can be added to one of these queues.</p>
<hr>
<h3>Why use TensorFlow Queues?</h3>
<p>TensorFlow also offers a number of queue options. There are a couple reasons to use TensorFlow queues over standard Python queues:</p>
<ul>
<li>TensorFlow queues live in TensorFlow computation graphs, with the attendant benefits of unifying things there and allowing distributed graph computation.</li>
<li>TensorFlow queues offer a few more methods than standard Python queues, like <code>dequeue_many</code>, which is good for getting training batches.</li>
<li>TensorFlow queues work with additional TensorFlow constructs, like the <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a>.</li>
<li>TensorFlow offers queue variants not in the Python standard library: the <a href="https://www.tensorflow.org/api_docs/python/tf/PaddingFIFOQueue">PaddingFIFOQueue</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue">RandomShuffleQueue</a>.</li>
</ul>
<hr>
<h3>FIFO Queue with TensorFlow</h3>
<p>Here's a standard TensorFlow <a href="https://www.tensorflow.org/api_docs/python/tf/FIFOQueue">FIFOQueue</a>:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; letter = tf.placeholder(tf.string)
&gt;&gt;&gt; queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string])
&gt;&gt;&gt; enqueue = queue.enqueue(letter)
&gt;&gt;&gt; dequeue = queue.dequeue()
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'a'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'b'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'c'})
&gt;&gt;&gt; session.run(dequeue)
'a'
&gt;&gt;&gt; session.run(dequeue)
'b'
&gt;&gt;&gt; session.run(dequeue)
'c'</code></pre>

<p>There's a little extra code to deal with the computation graph, and the TensorFlow queue requires a <code>capacity</code> argument and a <code>dtypes</code> argument.</p>
<p>The <code>capacity</code> is like the <code>maxsize</code> of a regular Python queue.</p>
<p>The <code>dtypes</code> argument is a list of <a href="https://www.tensorflow.org/programmers_guide/dims_types#data_types">Tensorflow data types</a>. The elements added to the queue will always be lists of tensors with the specified data types. Here, we add a single string tensor at a time to the queue.</p>
<hr>
<h3>Comparing Python standard library and TensorFlow queue offerings</h3>
<p>Here's a listing of queue types in the Python queue library and in TensorFlow.</p>
<ul>
<li>Python standard: <a href="https://docs.python.org/3/library/queue.html#queue.Queue">queue.Queue</a> / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/FIFOQueue">FIFOQueue</a></li>
<li>Python standard: <a href="https://docs.python.org/3/library/queue.html#queue.LifoQueue">queue.LifoQueue</a> / TensorFlow: no close equivalent</li>
<li>Python standard: <a href="https://docs.python.org/3/library/queue.html#queue.PriorityQueue">queue.PriorityQueue</a> / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/PriorityQueue">PriorityQueue</a></li>
<li>Python standard: no close equivalent / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/PaddingFIFOQueue">PaddingFIFOQueue</a></li>
<li>Python standard: no close equivalent / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue">RandomShuffleQueue</a></li>
</ul>
<hr>
<h3>Priority Queue with the Python standard library and TensorFlow</h3>
<p>A priority queue lets you assign a priority to each item as it is added, and the item that comes out when you next de-queue is the item with the highest priority currently in the queue. Higher priority is represented by lower numbers.</p>
<p>Both the standard Python queue library and TensorFlow have priority queues.</p>
<pre><code class="language-python">&gt;&gt;&gt; import queue
&gt;&gt;&gt; my_queue = queue.PriorityQueue()
&gt;&gt;&gt; my_queue.put([4, 'a'])
&gt;&gt;&gt; my_queue.put([1, 'b'])
&gt;&gt;&gt; my_queue.put([2, 'c'])
&gt;&gt;&gt; my_queue.get()
[1, 'b']
&gt;&gt;&gt; my_queue.get()
[2, 'c']
&gt;&gt;&gt; my_queue.get()
[4, 'a']</code></pre>

<p>The TensorFlow priority queue is just like the standard Python one, but with a little more strictness on types. When adding to the queue, the first tensor provided must be <code>tf.int64</code>, the priority.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; priority = tf.placeholder(tf.int64)
&gt;&gt;&gt; letter = tf.placeholder(tf.string)
&gt;&gt;&gt; queue = tf.PriorityQueue(capacity=10, types=[tf.string], shapes=[[]])
&gt;&gt;&gt; enqueue = queue.enqueue([priority, letter])
&gt;&gt;&gt; dequeue = queue.dequeue()
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(enqueue, feed_dict={priority: 4, letter: 'a'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={priority: 1, letter: 'b'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={priority: 2, letter: 'c'})
&gt;&gt;&gt; session.run(dequeue)
[1, 'b']
&gt;&gt;&gt; session.run(dequeue)
[2, 'c']
&gt;&gt;&gt; session.run(dequeue)
[4, 'a']</code></pre>

<p>As of TensorFlow 1.0.1 at least, <code>tf.PriorityQueue</code> calls its argument <code>types</code> rather than <code>dtypes</code>, and it seems to be requiring the <code>shapes</code> argument in spite of the documentation.</p>
<hr>
<h3>TensorFlow's <code>PaddingFIFOQueue</code></h3>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/PaddingFIFOQueue">PaddingFIFOQueue</a> has mini-batch model training in mind. It allows you to put in tensors of variable size, and when using <code>dequeue_many</code>, shorter ones get zero-padded to the maximum size in the batch.</p>
<pre><code class="language-python">import tensorflow as tf
numbers = tf.placeholder(tf.int64)
queue = tf.PaddingFIFOQueue(capacity=10, dtypes=[tf.int64], shapes=[[]])
enqueue = queue.enqueue(numbers)
dequeue_many = queue.dequeue_many(n=3)
session = tf.Session()
session.run(enqueue, feed_dict={numbers: [1]})
session.run(enqueue, feed_dict={numbers: [2, 3]})
session.run(enqueue, feed_dict={numbers: [3, 4, 5]})
session.run(dequeue_many)
array([[1, 0, 0],
       [2, 3, 0],
       [3, 4, 5]])</code></pre>

<hr>
<h3>TensorFlow's <code>RandomShuffleQueue</code></h3>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue">RandomShuffleQueue</a> also has batching in mind. It draws randomly from items currently in the queue.</p>
<p>To prevent pulling out items as they go in (not at all randomly) the <code>RandomShuffleQueue</code> has an argument <code>min_after_dequeue</code> which ensures that there are at least that many (plus the number being drawn) to randomly draw from. This requirement is dropped after the queue eventually has <code>.close()</code> called on it, so that all items can be drawn.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; letter = tf.placeholder(tf.string)
&gt;&gt;&gt; queue = tf.RandomShuffleQueue(capacity=10, dtypes=[tf.string],
...                               min_after_dequeue=2)
&gt;&gt;&gt; enqueue = queue.enqueue(letter)
&gt;&gt;&gt; dequeue = queue.dequeue()
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'a'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'b'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'c'})
&gt;&gt;&gt; session.run(dequeue)
'b'  # or 'a', or 'c'</code></pre>

<hr>
<h3>Queues inside TensorFlow</h3>
<p>Some parts of TensorFlow automatically create and use queues. For example, <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/train/string_input_producer">tf.train.string_input_producer</a> (among others in <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/input.py">input.py</a>) makes a queue, and a <a href="https://www.tensorflow.org/api_docs/python/tf/train/QueueRunner">QueueRunner</a>, and sets up a TensorBoard summary op.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170327-tensorflow_and_queues/</link>
<guid>http://planspace.org/20170327-tensorflow_and_queues/</guid>
<pubDate>Mon, 27 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>The TensorFlow Coordinator for Python Threading</title>
<description><![CDATA[

<p>A lot of Python code doesn't bother with <a href="https://en.wikipedia.org/wiki/Thread_(computing)">threads</a> at all, but TensorFlow <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues">encourages</a> using threads, especially for loading data.</p>
<p>Threading as TensorFlow recommends still uses the standard Python <a href="https://docs.python.org/3/library/threading.html">threading</a> library, but TensorFlow adds a <a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator">Coordinator</a> class that makes some good practices easier and fits with the rest of TensorFlow.</p>
<p>This <code>Coordinator</code> is for within-process thread coordination on one machine. It doesn't touch the TensorFlow computation graph or "coordinate" across multiple machines.</p>
<p>The three scripts below show how TensorFlow's <code>Coordinator</code> functionality relates to standard Python functionality.</p>
<hr>
<h3>Script <a href="code/base.py">1</a>: Python standard library</h3>
<p>Four threads are going to run the function <code>sleep_politely</code>. The main script will sleep for five seconds and then ask that all the threads stop, by setting <code>SHOULD_STOP</code> to <code>True</code>. The script then waits for the threads to stop, with <code>thread.join()</code>, before finishing.</p>
<pre><code class="language-python">def sleep_politely(should_stop):
    while not should_stop():
        time.sleep(2)

SHOULD_STOP = False
should_stop = lambda: SHOULD_STOP
threads = [threading.Thread(target=sleep_politely, args=(should_stop,))
           for _ in range(4)]

for thread in threads:
    thread.start()
time.sleep(5)
SHOULD_STOP = True
for thread in threads:
    thread.join()</code></pre>

<p>This could be set up to allow requesting stoppage inside <code>sleep_politely</code>, but the setup here parallels the TensorFlow example that comes next.</p>
<hr>
<h3>Script <a href="code/tf.py">2</a>: with TensorFlow Coordinator</h3>
<p>This script behaves just as the above, but uses <code>tf.train.Coordinator</code>.</p>
<pre><code class="language-python">def sleep_politely(coord):
    while not coord.should_stop():
        time.sleep(2)

coord = tf.train.Coordinator()
threads = [threading.Thread(target=sleep_politely, args=(coord,))
           for _ in range(4)]

for thread in threads:
    thread.start()
time.sleep(5)
coord.request_stop()
coord.join(threads)</code></pre>

<p>If appropriate, <code>coord.request_stop()</code> could also be called inside <code>sleep_politely</code> here.</p>
<hr>
<p>Coordinating threads is not always easy, and there are some problems with both scripts above.</p>
<ul>
<li>If there's an exception in the main script, the threads won't get stopped, and the program will effectively hang forever.</li>
<li>If there's an exception in any of the threads, everything else will carry on, whether it should or not.</li>
</ul>
<hr>
<h3>Script <a href="code/tf_full.py">3</a>: more complete TensorFlow</h3>
<pre><code class="language-python">def sleep_politely(coord):
    with coord.stop_on_exception():
        while not coord.should_stop():
            time.sleep(2)

coord = tf.train.Coordinator()
threads = [threading.Thread(target=sleep_politely, args=(coord,))
           for _ in range(4)]

try:
    for thread in threads:
        thread.start()
    time.sleep(5)
except Exception as exception:
    coord.request_stop(exception)
finally:
    coord.request_stop()
    coord.join(threads)</code></pre>

<p>The <code>try</code>/<code>except</code>/<code>finally</code> here means that even if there's an exception in the main script, the threads should get shut down. We could also do the same without TensorFlow.</p>
<p>The addition of the <code>coord.stop_on_exception()</code> context manager in <code>sleep_politely</code> means that if there's an exception in one of the threads, this will also shut everything down appropriately and pass along the exception. This would be a little more work to implement without TensorFlow, but it could be done.</p>
<hr>
<p>There are two reasons to use TensorFlow's <code>Coordinator</code>:</p>
<ul>
<li><code>Coordinator</code> makes some nice things particularly convenient.</li>
<li><code>Coordinator</code> fits with more pieces of TensorFlow, like the <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a>.</li>
</ul>
<p>The <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/coordinator.py">Python <code>Coordinator</code> code</a> is in one Python file and not super entangled with the rest of TensorFlow. For the threading convenience it provides, one could imagine extracting it for use elsewhere. The TensorFlow project seems to like the functionality well enough that it's one of the few components of <code>tf.training</code> that appears in both a Python and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/training/coordinator.cc">C++ version</a>.</p>
<hr>
<p>Even with the full <code>Coordinator</code> setup, the SIGINT signal (as from pressing <code>ctrl-c</code>) and the SIGTERM signal (as from <code>kill</code>) will interrupt the main script as exceptions. An additional step would be to set up <a href="https://docs.python.org/3/library/signal.html">signal</a> handlers to orchestrate a more orderly shutdown for these cases, if the main script is doing anything very involved.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/</link>
<guid>http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/</guid>
<pubDate>Fri, 24 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TFRecords for Humans</title>
<description><![CDATA[

<p>TensorFlow <a href="https://www.tensorflow.org/programmers_guide/reading_data#standard_tensorflow_format">recommends</a> its TFRecords format as the standard TensorFlow format for data on disk.</p>
<p>You <a href="/20170312-use_only_what_you_need_from_tensorflow/">don't have to</a> use TFRecords with TensorFlow. But if you need to read data inside your TensorFlow graph, and a reader op doesn't exist for your data, it might be easier to transform your data to TFRecords than to write a <a href="https://www.tensorflow.org/extend/new_data_formats">custom data reader op</a>.</p>
<p>Before using TFRecords in a distributed setting, you probably want to understand and work with them locally.</p>
<hr>
<h3>The TFRecords Format</h3>
<p>TFRecords is a <a href="https://www.tensorflow.org/api_guides/python/python_io#TFRecords_Format_Details">simple binary file format</a>. It lets you put one or more strings of bytes into a file. You could put any bytes you like in a TFRecords file, but it'll be more useful to use the formats provided in TensorFlow.</p>
<p>TensorFlow defines two <a href="https://developers.google.com/protocol-buffers/">protocol buffer</a> message types for use with TFRecords: the <a href="https://www.tensorflow.org/api_docs/python/tf/train/Example">Example</a> message type and the <a href="https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample">SequenceExample</a> message type.</p>
<p>The pre-defined protocol buffer message types offer flexibility by letting you arrange your data as a map from string keys to values that are lists of integers, lists of 32-bit floats, or lists of bytes.</p>
<hr>
<h3>Writing and Reading in the style of <code>Example</code> Records without TensorFlow</h3>
<p>An equivalent representation of an <code>Example</code> TFRecord using Python dictionaries might look like this:</p>
<pre><code class="language-python">my_dict = {'features' : {
    'my_ints': [5, 6],
    'my_float': [2.7],
    'my_bytes': ['data']
}}</code></pre>

<p>In Python 2, the string literal <code>'data'</code> is bytes. The equivalent in Python 3 is <code>bytes('data', 'utf-8')</code>. And Python uses 64-bit floats rather than the 32-bit floats that TFRecords uses, so we have more precision in Python.</p>
<p>The values in this <code>dict</code> are accessed like this:</p>
<ul>
<li><code>my_dict['features']['my_ints']</code></li>
<li><code>my_dict['features']['my_float']</code></li>
<li><code>my_dict['features']['my_bytes']</code></li>
</ul>
<p>Ordinarily, to save this data (serialize and write to disk) and then read it again (read from disk and deserialize) in Python you might use the <a href="https://docs.python.org/3/library/pickle.html">pickle</a> module. For example:</p>
<pre><code class="language-python">import pickle

my_dict_str = pickle.dumps(my_dict)
with open('my_dict.pkl', 'w') as f:
    f.write(my_dict_str)

with open('my_dict.pkl', 'r') as f:
    that_dict_str = f.read()
that_dict = pickle.loads(that_dict_str)</code></pre>

<hr>
<h3>Writing and Reading <code>Example</code> Records with TensorFlow</h3>
<p>The TFRecords <code>Example</code> format defines things in detail: An <code>Example</code> contains one <code>Features</code>, which is a map from strings to <code>Feature</code> elements, which can each be <code>Int64List</code>, <code>FloatList</code>, or <code>BytesList</code>. (See also: <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto">example.proto</a> and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto">feature.proto</a>)</p>
<pre><code class="language-python">import tensorflow as tf

my_example = tf.train.Example(features=tf.train.Features(feature={
    'my_ints': tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 6])),
    'my_float': tf.train.Feature(float_list=tf.train.FloatList(value=[2.7])),
    'my_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=['data']))
}))</code></pre>

<p>The values in the <code>Example</code> can be accessed then like this:</p>
<ul>
<li><code>my_example.features.feature['my_ints'].int64_list.value</code></li>
<li><code>my_example.features.feature['my_float'].float_list.value</code></li>
<li><code>my_example.features.feature['my_bytes'].bytes_list.value</code></li>
</ul>
<p>Writing to and reading from disk are much like with <code>pickle</code>, except that the reader here provides all the records from a TFRecords file. In this example, there's only one record in the file.</p>
<pre><code class="language-python">my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]</code></pre>

<p>The file written: <a href="my_example.tfrecords"><code>my_example.tfrecords</code></a>.</p>
<hr>
<h3>Images in <code>Example</code> Records</h3>
<p>The <code>Example</code> format lets you store pretty much any kind of data, including images. But the mechanism for arranging the data into serialized bytes, and then reconstructing the original format again, is left up to you. For more on this, see my post on <a href="/20170403-images_and_tfrecords/">Images and TFRecords</a>.</p>
<p>For two more complete <em>in situ</em> examples of converting images to TFRecords, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py">code for MNIST images</a> and <a href="https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py">code for ImageNet images</a>. The ImageNet code can be run on the command-line.</p>
<hr>
<h3>The <code>SequenceExample</code> Record</h3>
<p>The <code>SequenceExample</code> message type essentially extends <code>Example</code> for sequence data. (You could imagine achieving the same effect with just <code>Example</code>, but it would be awkward.)</p>
<p>A <code>SequenceExample</code> keeps the same kind of map as <code>Example</code>, but calls it <code>context</code>, because it's thought of as the static context for the dynamic sequence data. And it adds another map, called <code>feature_lists</code>, that maps from string keys to lists of lists.</p>
<p>In Python dictionaries, a <code>SequenceExample</code> is like this:</p>
<pre><code class="language-python">my_seq_dict = {
    'context' : {
        'my_bytes':
            ['data']},
    'feature_lists' : {
        'my_ints': [
            [5, 6],
            [7, 8, 9]]}}</code></pre>

<p>A corresponding full <code>SequenceExample</code> is a bit more verbose:</p>
<pre><code class="language-python">my_seq_ex = tf.train.SequenceExample(
    context=tf.train.Features(feature={
        'my_bytes':
            tf.train.Feature(bytes_list=tf.train.BytesList(value=['data']))}),
    feature_lists=tf.train.FeatureLists(feature_list={
        'my_ints': tf.train.FeatureList(feature=[
            tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 6])),
            tf.train.Feature(int64_list=tf.train.Int64List(value=[7, 8, 9]))])}))</code></pre>

<p>In a file: <a href="my_seq_ex.tfrecords"><code>my_seq_ex.tfrecords</code></a>.</p>
<p>You probably don't want to mix <code>Example</code> and <code>SequenceExample</code> records in the same TFRecords file.</p>
<!--

Good look at `SequenceExample` with different way of making them...
http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/

-->

<hr>
<h3>Reading TFRecords in a TensorFlow Graph</h3>
<p>You may eventually want to read TFRecords files with ops in a TensorFlow graph, using <a href="https://www.tensorflow.org/api_docs/python/tf/TFRecordReader">tf.TFRecordReader</a>. This will involve a filename queue; for an example, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py">some MNIST tutorial code</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170323-tfrecords_for_humans/</link>
<guid>http://planspace.org/20170323-tfrecords_for_humans/</guid>
<pubDate>Thu, 23 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow Logging</title>
<description><![CDATA[

<p>TensorFlow wraps the standard Python <a href="https://docs.python.org/3/library/logging.html">logging library</a>, exposing functionality at <code>tf.logging</code>.</p>
<p>This means that you can start logging things immediately after importing TensorFlow:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.logging.info('Things are good!')
INFO:tensorflow:Things are good!</code></pre>

<p>TensorFlow also automatically logs things using this functionality. For example, estimators will log out information about the many things they're up to.</p>
<p>The TensorFlow <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py">logging code</a> also adds some non-standard functionality. A comment notes that some of the code "is taken from pyglib/logging". I assume pyglib is a Google-internal Python library. I think it's interesting to see these Google tools <a href="/20170315-tensorflow_and_a_googleverse_like_the_hadleyverse/">appearing here and there</a>.</p>
<p>Here's an example of TensorFlow logging's added functionality: <code>log_every_n</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
INFO:tensorflow:Common error!
&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
INFO:tensorflow:Common error!</code></pre>

<p>(This example won't work in an <a href="https://ipython.org/">IPython</a> shell because every new REPL "read" there looks like a new file to Python.)</p>
<hr>
<p>If you want to use regular Python logging techniques in addition to TensorFlow's mechanism, everything should generally work, but it is possible to get into a situation that seems surprising in a REPL.</p>
<pre><code class="language-python">&gt;&gt;&gt; import logging
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; logging.warn('Something interesting!')
WARNING:root:Something interesting!
&gt;&gt;&gt; tf.logging.info('1+1=2')
INFO:tensorflow:1+1=2
INFO:tensorflow:1+1=2</code></pre>

<p>This doubling is because TensorFlow's log messages are bubbling up to the root logger as intended, but both the root logger and TensorFlow's logger are getting handled in the same place, so we see the message twice.</p>
<p>Shortly after the initial release of TensorFlow this kind of thing <a href="http://stackoverflow.com/questions/33662648/tensorflow-causes-logging-messages-to-double">was happening more than it should</a>, but the current behavior is correct.</p>
<p>The situation shown above happens because both the base logging package and TensorFlow's logging are trying to automatically be helpful by writing things out to the interactive session.</p>
<p>In practice you should set up log handlers yourself, <a href="http://docs.python-guide.org/en/latest/writing/logging/">as appropriate</a>. And of course if you only use one or the other of <code>logging</code> or <code>tf.logging</code> you'll be particularly unlikely to have any conflicts.</p>
<p>A poor solution to the doubling above would be to turn off log message propagation from the TensorFlow logger.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.logging._logger.propagate = False
&gt;&gt;&gt; tf.logging.info('1+1=2')
INFO:tensorflow:1+1=2</code></pre>

<p>But I can't recommend using that as anything but a quick fix in a REPL if double messages happen to be bothering you.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170322-tensorflow_logging/</link>
<guid>http://planspace.org/20170322-tensorflow_logging/</guid>
<pubDate>Wed, 22 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Various TensorFlow APIs for Python</title>
<description><![CDATA[

<p>An early <a href="http://fastml.com/what-you-wanted-to-know-about-tensorflow/">perception</a> of TensorFlow was that the API was quite low-level.</p>
<blockquote>
<p>"meaning you&#8217;ll be multiplying matrices and vectors."</p>
</blockquote>
<p>TensorFlow offers ops for low-level operations, and from the beginning programmers used those low-level ops to build higher-level APIs.</p>
<p>To paraphrase <a href="http://www.paulgraham.com/avg.html">Paul Graham</a>:</p>
<blockquote>
<p>"if you have a choice of several [APIs], it is, all other things being equal, a mistake to program in anything but the [highest-level] one."</p>
</blockquote>
<p>You don't need to be a Lisp hacker to make it easier to use TensorFlow the way you want by writing new functions and classes, and you probably should.</p>
<p>But because TensorFlow had already done the obviously hard work of making low-level things work, it was relatively easy for lots of people to attempt the subtly hard work of designing higher-level APIs.</p>
<p>This led to a proliferation of Python APIs, which can sometimes be confusing to track.</p>
<p>Things are still moving around and settling a bit, but Google is communicating a pretty clear plan for official Python APIs for TensorFlow.</p>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<p>This plan changes some names and rearranges things slightly, but it's an incremental development of things that have existed for a while. We can look at how things have developed over time both for what's becoming official TensorFlow API and some of the projects that still exist outside official TensorFlow.</p>
<hr>
<h3>Python Frontend (op level)</h3>
<p>You can continue to use TensorFlow's low-level APIs.</p>
<p>There are ops, like <code>tf.matmul</code> and <code>tf.nn.relu</code>, which you might use to build a neural network architecture in full detail. To do really novel things, you may want this level of control. But you may also prefer to work with larger building blocks. The other other APIs below will mostly specialize in this kind of application.</p>
<p>There are also ops like <code>tf.image.decode_jpeg</code> (and many others) which may be necessary but don't necessarily relate to what is usually considered the architecture of neural networks. Some higher-level APIs wrap some of this functionality, but they usually stay close to the building of network architectures and the training of such networks once defined.</p>
<hr>
<h3>Layers (as from TF-Slim)</h3>
<p><a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html">TF-Slim</a> has a number of <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">components</a> but it looks like essentially we're seeing the following: <code>tf.contrib.slim.layers</code> became <code>tf.contrib.layers</code> becomes <code>tf.layers</code>.</p>
<p>Sergio Guadarrama, one of the TF-Slim authors, generously confirmed that TensorFlow is converging on a layers API and implementation along these lines, but warns that there can still be some differences. He points out that <code>tf.contrib.layers</code> have <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/contrib/framework/arg_scope"><code>arg_scope</code></a>, while <code>tf.layers</code> don't.</p>
<p>This <code>layers</code> API provides a first higher level of abstraction over writing things out by individual ops. For example, <code>tf.layers.conv2d</code> implements a convolution layer that involves multiple individual ops.</p>
<p>Other parts of TF-Slim are likely also worth using, and there is a collection of <a href="https://github.com/tensorflow/models/tree/master/slim">models that use TF-Slim</a> in the <a href="https://github.com/tensorflow/models">TensorFlow models repository</a>.</p>
<p>Historical note: It looks like before calling them layers, TF-Slim overloaded the word "op" for their layer concept (see <a href="https://github.com/tensorflow/models/tree/master/inception/inception/slim">earlier documentation</a>).</p>
<p>TF-Slim is in the TensorFlow codebase as <code>tf.contrib.slim</code>.</p>
<p><img alt="slim... shady?" src="img/slim_shady.png"></p>
<hr>
<h3>Estimator (as from TF Learn)</h3>
<p>Distinct from TensorFlow, the <a href="http://scikit-learn.org/">scikit-learn</a> project makes a lot of machine learning models conveniently available in Python.</p>
<p><img alt="scikit-learn logo" src="img/sklearn.png"></p>
<p>A key design element of scikit is that many different models offer the same simple API: they are all implemented as <em>estimators</em>. So regardless of whether the model is linear regression or a GBM, if your instantiated scikit-learn model object is called <code>estimator</code>, you can do all of these:</p>
<pre><code class="language-python">estimator.fit(data, labels)       # train
estimator.score(data, labels)     # test
estimator.predict(data)           # run on new data</code></pre>

<p>The estimator API for TensorFlow is directly inspired by scikit-learn, though implemented quite independently. Here's nearly equivalent TensorFlow code, assuming you have a TensorFlow estimator model instantiated as <code>estimator</code>:</p>
<pre><code class="language-python">estimator.fit(data, labels)       # train
estimator.evaluate(data, labels)  # test
estimator.predict(data)           # run on new data</code></pre>

<p>The only thing that changed in the code is scikit-learn's <code>score</code> method becomes <code>evaluate</code>, which I think is going to do a lot of good for the self esteem of TensorFlow models.</p>
<p>There are other differences too. In scikit-learn, usually models have an idea of training "until done"&#8212;but in TensorFlow, if you don't specify a number of steps to train for, the model will go on training forever. TensorFlow estimators also do a lot of things by default, like adding TensorBoard summaries and saving model checkpoints.</p>
<p>The estimators API started as <a href="https://github.com/tensorflow/skflow">skflow</a> ("scikit-flow") before moving into the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">tf.contrib.learn</a> ("TF Learn") and now the base estimator code is getting situated in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator">tf.estimator</a> and <a href="https://www.tensorflow.org/extend/estimators">documentation</a> is accumulating.</p>
<hr>
<h3>Keras Model (absorbing Keras)</h3>
<p><a href="https://keras.io/">Keras</a> was around before TensorFlow. It was always a high-level API for neural nets, originally running with <a href="http://www.deeplearning.net/software/theano/">Theano</a> as its backend.</p>
<p>After the release of TensorFlow, Keras moved to also work with TensorFlow as a backend. And now TensorFlow is absorbing at least some aspects of the Keras project into the TensorFlow codebase, though <a href="https://twitter.com/fchollet">Fran&#231;ois Chollet</a> seems likely to continue championing Keras as a very fine project in its own right. (See also his response to a <a href="https://www.quora.com/What-will-Keras-do-with-TensorFlow-Slim">Quora question</a> about any possible relationship between TF-Slim and Keras.)</p>
<p>Once specified, Keras models offer basically the same API (<code>fit</code>/<code>evaluate</code>/<code>predict</code>) as estimators (above).</p>
<p>Keras is appearing in the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/keras">tf.contrib.keras</a> and should move to <code>tf.keras</code> at TensorFlow 1.2.</p>
<p><img alt="Keras" src="img/keras.jpg"></p>
<hr>
<h3>Canned Estimators</h3>
<p>Canned estimators are concrete pre-defined models that follow the estimator conventions. Currently there are a bunch right in <code>tf.contrib.learn</code>, such as <code>LinearRegressor</code> and <code>DNNClassifier</code>. There are some elsewhere. For example, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/g3doc/kmeans.md">kmeans</a> is in <code>tf.contrib.factorization</code>. It isn't clear to me exactly where all the canned estimators will eventually settle down in the API.</p>
<hr>
<h3>Other APIs</h3>
<p>All the APIs that follow are not part of the official TensorFlow project. I guess you could use them.</p>
<p><img alt="Bucksstar Coffee" src="img/bucksstar.png"></p>
<hr>
<h3>Pretty Tensor</h3>
<p><a href="https://github.com/google/prettytensor">Pretty Tensor</a> is still a Google project, so it might be worth checking out if you really like <a href="https://en.wikipedia.org/wiki/Fluent_interface">fluent interfaces</a> with lots of chaining, like <a href="https://d3js.org/">d3</a>.</p>
<hr>
<h3>TFLearn</h3>
<p>The confusingly named <a href="https://github.com/tflearn/tflearn">TFLearn</a> (no space; perhaps more clearly identified as <a href="(http://tflearn.org/)">TFLearn.org</a>) is not at all the same thing as the TF Learn (with a space) that appears in TensorFlow at <code>tf.contrib.learn</code>. TFLearn is a <a href="http://stackoverflow.com/questions/38859354/what-is-the-difference-between-tf-learn-aka-scikit-flow-and-tflearn-aka-tflea">separate</a> Python package that uses TensorFlow. It seems like it aspires to be like Keras. Here is the TFLearn logo:</p>
<p><img alt="TFLearn logo" src="img/tflearn.png"></p>
<hr>
<h3>TensorLayer</h3>
<p>Another one with a confusing name is <a href="https://github.com/zsdonghao/tensorlayer/">TensorLayer</a>. This is a separate package from TensorFlow and it's different from TensorFlow's layers API.</p>
<p><img alt="TensorLayer logo" src="img/tensorlayer.png"></p>
<hr>
<p>My recommendation is to use the rich APIs available inside TensorFlow. You shouldn't have to import anything else, with the possible exception of Keras if you like it and are working before the Keras integration into TensorFlow is complete.</p>
<hr>
<p>Thanks to Googler and TF-Slim developer Sergio Guadarrama for clarifying the development of the layers APIs, and thanks to <a href="https://twitter.com/amuellerml">Andreas Mueller</a> of scikit-learn for his <a href="https://twitter.com/amuellerml/status/844300337666240514">summary</a> of the relation between scikit-learn and TensorFlow.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170321-various_tensorflow_apis_for_python/</link>
<guid>http://planspace.org/20170321-various_tensorflow_apis_for_python/</guid>
<pubDate>Tue, 21 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow APIs for Various Languages</title>
<description><![CDATA[

<p>You couldn't be blamed for thinking that <a href="https://www.tensorflow.org/">TensorFlow</a> is a Python package. And a lot of TensorFlow functionality is unique to Python. But the core of TensorFlow&#8212;the part of TensorFlow that most truly <em>is</em> TensorFlow&#8212;is the distributed runtime ("TensorFlow Distributed Execution Engine") and Python is just one way to talk to it.</p>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<p>The runtime itself is written in C++, but it has APIs ("frontends") in many languages.</p>
<hr>
<h3>C</h3>
<p>The only TensorFlow APIs with any <a href="https://www.tensorflow.org/programmers_guide/version_semantics">official stability</a> are the <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h">C API</a> and (parts of) the Python API.</p>
<p>TensorFlow <a href="https://www.tensorflow.org/extend/language_bindings">suggests</a> that you use the C API if you are making a TensorFlow API for some other language. Lots of programming languages have mechanisms for connecting with <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a>. In a very real sense the TensorFlow C API exists <em>so that</em> other APIs can be built.</p>
<p>There are a number of non-Python <a href="https://www.tensorflow.org/api_docs/">languages with TensorFlow APIs</a> but for the most part I will consider them "deploy only" ("inference only") and largely ignore them.</p>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<h3>C++</h3>
<p>The <a href="https://www.tensorflow.org/api_docs/cc/">C++ API</a> is "exposed through header files in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc">tensorflow/cc</a>" and <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a> is the language that the TensorFlow runtime is written in, but the C++ API is still marked as experimental and has fewer features than are accessible via the Python API. It may be where you want to be for some deploy scenarios.</p>
<p>As a possibly interesting historical note, <a href="https://research.google.com/archive/mapreduce.html">the MapReduce paper</a> was another place where the world saw Google (and Jeff Dean) making something cool with C++. Yahoo! (and others) eventually implemented those concepts in Java, as <a href="http://hadoop.apache.org/">Hadoop</a>. Google seems to continue to do a lot of work in C++.</p>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<h3>Java</h3>
<p>Possibly recognizing <a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Java's</a> presence in industry, TensorFlow now has a <a href="https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/package-summary">Java API</a>.</p>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<h3>Go</h3>
<p><a href="https://en.wikipedia.org/wiki/Go_(programming_language)">Go</a> is a Google programming language, and so it seems appropriate that there is a <a href="https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go">Go API</a> for TensorFlow.</p>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<h3>Rust</h3>
<p><a href="https://en.wikipedia.org/wiki/Rust_(programming_language)">Rust</a> is a neat language, and it has a TensorFlow <a href="https://github.com/tensorflow/rust">api</a> too (<a href="https://tensorflow.github.io/rust/tensorflow/">docs</a>).</p>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<h3>Haskell</h3>
<p><a href="https://en.wikipedia.org/wiki/Haskell_(programming_language)">Haskell</a> is not a language I would have guessed would have a TensorFlow API, but <a href="https://github.com/tensorflow/haskell">it does</a> (<a href="https://tensorflow.github.io/haskell/haddock/">docs</a>).</p>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<h3>Python</h3>
<p><a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> is where the action is. TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/">Python API documentation</a> is headed simply "All symbols in TensorFlow" and it really does <a href="https://www.tensorflow.org/extend/language_bindings">seem to be</a> that TensorFlow functionality is prototyped in Python and then moved into the C++ core:</p>
<blockquote>
<p>Python was the first client language supported by TensorFlow and currently supports the most features. More and more of that functionality is being moved into the core of TensorFlow (implemented in C++) and exposed via a C API.</p>
</blockquote>
<p>The Python API is so rich, you'll have to choose which levels of Python API you'll want to use.</p>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<h3>R</h3>
<p>The <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R</a> TensorFlow API made by <a href="https://www.rstudio.com/">RStudio</a> takes a different approach than the APIs that use TensorFlow's C API to connect with TensorFlow. The <a href="https://rstudio.github.io/tensorflow/">R API</a> (<a href="https://github.com/rstudio/tensorflow">on github</a>) wraps the entire Python API. This is not exactly what the TensorFlow project recommends, but it gives R users access to all the features in the Python API, which would be quite a lot of work to replicate using only the C API.</p>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>Thanks to <a href="https://blog.rstudio.org/author/kevinusheyrstudio/">Kevin Ushey</a> for <a href="https://support.rstudio.com/hc/en-us/community/posts/115005611307">directing</a> me to the right venue, and to <a href="https://github.com/jjallaire">@jjallaire</a> and <a href="https://github.com/pourzanj">@pourzanj</a> for an illuminating <a href="https://github.com/rstudio/tensorflow/issues/105">discussion</a> about the R TensorFlow API.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170320-tensorflow_apis_for_various_languages/</link>
<guid>http://planspace.org/20170320-tensorflow_apis_for_various_languages/</guid>
<pubDate>Mon, 20 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Thank You for Reaching Out</title>
<description><![CDATA[

<p>I make myself pretty <a href="/aaron/">easy to contact</a>, and sometimes people contact me.</p>
<h3>1. Will I hire you?</h3>
<p>I'm not in a position to hire you (or give you an internship). Please don't send me your r&#233;sum&#233;. If you want to work where I work, or anywhere I've worked before, please go through official channels.</p>
<h3>2. Will I recommend you for a job?</h3>
<p>If I already know your work and I know it to be good, then yes.</p>
<h3>3. How can you find a job?</h3>
<p>I've found jobs mostly through people I know, and once through a recruiter.</p>
<p>Don't meet people for the sake of finding a job. Meet people by being active in your community and doing good work. Go to a <a href="https://www.meetup.com/">meetup</a>. Talk to people. Help where you can. Give a talk about something you know or are learning about. Help out on a <a href="http://brigade.codeforamerica.org/brigade/">community project</a>. Keep developing new skills.</p>
<p>The majority of the cold contacts I get are from recruiters. I don't have an answer for them on this page because they don't <em>care</em>; it is their <em>job</em> to contact people all the time&#8212;even people who don't seem to be looking for work&#8212;on the off chance that they make a contact at exactly the right time. Can they find you? They will send you jobs.</p>
<h3>4. How can you develop skills?</h3>
<p>I have a presentation about my career path and things I think are very generally important. It's called <a href="/20151206-how_to_eat_computers/">How to Eat Computers</a> and it was made for children, but even such a very mature person as you might enjoy it.</p>
<p>I have two short lists of recommended books:</p>
<ul>
<li><a href="/20160322-books_for_programmers/">Books for Programmers</a></li>
<li><a href="/20160320-books_for_professionals/">Books for Professionals</a></li>
</ul>
<p>You should read and write a lot. Develop interests and pursue them.</p>
<h3>5. What educational program should you do?</h3>
<p>You should go for the best program(s) you can, but remember that individual (student) variability is much larger than variability between programs.</p>
<h3>6. Should you do online courses?</h3>
<p>This is like asking "Should I read books?" Put them in the same category. Choose what will best help you learn what you want to learn.</p>
<h3>7. Should you do a coding bootcamp?</h3>
<p>Don't think of a bootcamp as a credential. They don't play in the same world as graduate degrees. If you go for a bootcamp, you are saying to the world that you are <em>scrappy</em>; you are going to <em>work hard</em> and you are going to <em>make something happen</em>. The structures of the program may help you to do more than you would on your own, but no program can do everything for you.</p>
<p>At the end of the program you are going to have your work to show and talk about. Could you have done that work without the program? Yes. Would you have?</p>
<h3>8. What's the best coding bootcamp?</h3>
<p>I've taught data science evening classes for <a href="https://generalassemb.ly/">General Assembly</a> and the full-time program for <a href="https://www.thisismetis.com/">Metis</a>. I developed some materials and I hope I helped both programs, but things vary a lot with locations, staff, and time: I don't know which programs are better than others.</p>
<h3>9. Will I collaborate with you? Will I be your mentor?</h3>
<p>If you have a project or question that you think I'm uniquely positioned to help with, let's talk about that project or question.</p>
<p>If you're looking for people in general but don't yet have a project or question to talk about, thank you for thinking of me, and please contact me if I am still the right person to talk to after you do. I keep some project ideas in my blog's <a href="https://github.com/ajschumacher/ajschumacher.github.io/issues">github issues</a>; you could look there to see if anything interests you.</p>    
    ]]></description>
<link>http://planspace.org/20170319-thank_you_for_reaching_out/</link>
<guid>http://planspace.org/20170319-thank_you_for_reaching_out/</guid>
<pubDate>Sun, 19 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Much Ado about the TensorFlow Logo</title>
<description><![CDATA[

<p>In the beginning, there was <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">G&#246;del, Escher, Bach: An Eternal Golden Braid</a> by Douglas Hofstadter.</p>
<p><img alt="G&#246;del, Escher, Bach" src="img/geb.jpg"></p>
<p>This "metaphorical fugue on minds and machines in the spirit of Lewis Carroll" includes the <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> idea and related discussion of consciousness and formal systems. Hofstadter's influence can be seen, for example, <a href="https://cs.illinois.edu/news/strange-loop-conference">in the name</a> of the <a href="http://www.thestrangeloop.com/">Strange Loop</a> tech conference.</p>
<p>It seems likely that many people working in artificial intelligence and machine learning have encountered G&#246;del, Escher, Bach.</p>
<p>Of course, there's also <a href="https://en.wikipedia.org/wiki/Chernin_Entertainment">Chernin Entertainment</a>, the production company.</p>
<p><img alt="Chernin Entertainment" src="img/chernin.jpg"></p>
<p>So we shouldn't rule out the possibility that Google engineers are fans of <a href="http://www.imdb.com/company/co0286257/">Chernin's work</a>. I hear <a href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden Figures</a> is quite good. And I guess a lot of people like <a href="https://en.wikipedia.org/wiki/New_Girl">New Girl</a>?</p>
<p>In any event, somehow we get to this TensorFlow logo:</p>
<p><img alt="TensorFlow logo - old?" src="img/tf-old.png"></p>
<p>If you look carefully, does it seem like the right side of the "T" view is too short?</p>
<p>This very serious concern appears as <a href="https://github.com/tensorflow/tensorflow/issues/1922">issue #1922</a> on the TensorFlow github, and <a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XhO1sqp4l4g">on the TensorFlow mailing list</a> complete with ASCII art illustration.</p>
<p>The consensus response seemed to be some variant of "won't fix" (it wouldn't look as cool, anyway) until...</p>
<p><img alt="TensorFlow logo - new?" src="img/tf-new.jpg"></p>
<p>As of around the 1.0 release of TensorFlow, which was around the first <a href="https://events.withgoogle.com/tensorflow-dev-summit/">TensorFlow Dev Summit</a>, this logo variant seems to be in vogue. It removes the (possibly contentious) shadows, and adds a picture of a computation graph, in case you were about to forget that TensorFlow is about computation graphs. (If you want to think of it as a neural network, you're free to do so.)</p>
<p>Logos are fun!</p>
<hr>
<p>Thanks to <a href="https://twitter.com/divergentdave">David Cook</a> and <a href="https://twitter.com/philipashlock">Philip Ashlock</a> for helping me find Chernin again based on a very murky memory.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170318-much_ado_about_the_tensorflow_logo/</link>
<guid>http://planspace.org/20170318-much_ado_about_the_tensorflow_logo/</guid>
<pubDate>Sat, 18 Mar 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
