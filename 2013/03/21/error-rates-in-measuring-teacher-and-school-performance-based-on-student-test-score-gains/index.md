# Error Rates in Measuring Teacher and School Performance Based on Student Test Score Gains

<div>
<p>I was recently pointed to the 2010 USDOE/Mathematica <a href="http://www.mathematica-mpr.com/publications/pdfs/education/error_rates.pdf">paper</a>&#160;that shares this post's name. One has to think that <a href="http://planspace.org/2013/03/06/value-added-measures-unstable-responses-from-rockoff-and-kane/">Rockoff and Kane</a>&#160;et al. have seen it, but nobody seems to ever mention it. From the abstract:<br>
</p>
<blockquote>Simulation results suggest that value-added estimates are likely to be noisy using the amount of data&#160;that are typically used in practice. Type I and II error rates for comparing a teacher&#8217;s performance to the average&#160;are likely to be about 25 percent with three years of data and 35 percent with one year of data.</blockquote>
<br>
Those numbers are awful. What's interesting to me is that they aren't even that much better when using three years of data, as compared to one year. I had <a href="http://planspace.org/2013/03/06/value-added-measures-unstable-responses-from-rockoff-and-kane/">thought</a> they would probably improve a lot with triple the data. They just stay bad.<br>
<br>
This study used a fairly simple VAM, similar to what they do in Tennessee, so that's one possible critique. But the fact is, this is the only research I've seen that seriously attempts to address the trustworthiness of VAM at the teacher and school level. <a href="http://www.metproject.org/">Everybody</a> <a href="http://obs.rc.fas.harvard.edu/chetty/value_added.pdf">else</a> seems to be ignoring it, as if there's no cost to making arbitrarily incorrect judgements about teachers' work. This <a href="http://www.mathematica-mpr.com/publications/pdfs/education/error_rates.pdf">paper</a> is worth a look.<br>
</div>


*This post was originally hosted elsewhere.*
