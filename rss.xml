<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Navigating TensorFlow Estimator Documentation</title>
<description><![CDATA[

<p>The TensorFlow documentation keeps improving, but it can still be hard to find what you're looking for. Here's a way of organizing <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator">Estimator</a>s or <a href="https://www.tensorflow.org/api_guides/python/contrib.learn"><code>tf.contrib.learn</code></a> documentation, from high-level to low-level:</p>
<ul>
<li>Using Estimators (already made or "canned")<ul>
<li><a href="https://www.tensorflow.org/get_started/tflearn"><code>DNNClassifier</code> on iris dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide"><code>LinearClassifier</code> (logistic) on census income dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/wide_and_deep"><code>DNNLinearCombinedClassifier</code> (extends census income example)</a></li>
</ul>
</li>
<li>Aspects of using Estimators<ul>
<li><a href="https://www.tensorflow.org/tutorials/linear">Feature columns (in context of linear models)</a></li>
<li><a href="https://www.tensorflow.org/get_started/input_fn">Using <code>input_fn</code>; <code>DNNRegressor</code> on Boston housing dataset</a></li>
<li><a href="https://www.tensorflow.org/get_started/monitors">Logging etc. with <code>ValidationMonitor</code> (extends iris example)</a></li>
</ul>
</li>
<li>Making your own Estimator<ul>
<li><a href="https://www.tensorflow.org/extend/estimators">With <code>layers</code> in a <code>model_fn</code> on the abalones dataset</a></li>
<li><a href="https://www.tensorflow.org/tutorials/layers">With <code>layers</code> in a <code>model_fn</code> on MNIST</a></li>
</ul>
</li>
<li>Lower-level API than Estimators<ul>
<li><a href="https://www.tensorflow.org/get_started/mnist/pros#build_a_multilayer_convolutional_network">Use the lower-level API to make a system for MNIST</a></li>
</ul>
</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</link>
<guid>http://planspace.org/20170521-navigating_tensorflow_estimator_documentation/</guid>
<pubDate>Sun, 21 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Code Reading Questions at OSCON</title>
<description><![CDATA[

<p><a href="https://www.oreilly.com/">O'Reilly</a>'s 2017 <a href="https://conferences.oreilly.com/oscon/">OSCON</a> had a "<a href="https://conferences.oreilly.com/oscon/oscon-tx/public/content/game">code game</a>" with ten questions covering nine different languages. It was supposed to get people engaged in the expo hall, but the quiz-like gamification content reminded me of my old <a href="/20150616-code_reading_question/">code reading question</a> idea. The questions are available in a <a href="https://cdn.oreillystatic.com/en/assets/1/event/214/oscon2017_code_game.pdf">PDF</a> (<a href="oscon2017_code_game.pdf">mirror</a>). I'll put the questions here as well.</p>
<hr>
<h3>1. Rust</h3>
<pre><code class="language-rust">use std::collections::HashSet;
use std::io::{BufRead, Result};

fn f&lt;I: BufRead&gt;(input: &amp;mut I) -&gt; Result&lt;usize&gt; {
    Ok(input.lines()
       .map(|r| r.expect(&#8220;ara ara&#8221;))
       .flat_map(|l| l.split_whitespace()
                     .map(str::to_owned)
                     .collect::&lt;Vec&lt;_&gt;&gt;())
       .collect::&lt;HashSet&lt;_&gt;&gt;()
       .len())
}</code></pre>

<p>This function reads input. What else does it do?</p>
<ul>
<li>A. Counts the number of white-space-separated &#8220;words&#8221;</li>
<li>B. Counts the number of distinct &#8220;words&#8221;</li>
<li>C. Finds the &#8220;word&#8221; that appears most frequently</li>
<li>D. Finds the longest line</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>2. JavaScript</h3>
<pre><code class="language-javascript">function search(values, target) {
  for(var i = 0; i &lt; values.length; ++i){
    if (values[i] == target) { return i; }
  }
  return -1;
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Depth-first search</li>
<li>B. Binary search</li>
<li>C. Merge search</li>
<li>D. Linear search</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>3. Go</h3>
<pre><code class="language-go">func function(s []float64) float64 {
        var sum float64 = 0.0
        for _, n := range s {
                sum += n
        }
        return sum / float64(len(s))
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Sums the contents of a slice</li>
<li>B. Finds the maximum value in a slice</li>
<li>C. Averages the contents of a slice</li>
<li>D. Appends values to a slice</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>4. Perl 5</h3>
<pre><code class="language-perl">sub mystery {
    return @_ if @_ &lt; 2;
    my $p = pop;
    mystery(grep $_ &lt; $p, @_), $p,
    mystery(grep $_ &gt;= $p, @_);
}</code></pre>

<p>What does the mystery subroutine do?</p>
<ul>
<li>A. Binary search</li>
<li>B. Merge sort</li>
<li>C. Removes items that are too large or too small</li>
<li>D. Quick sort</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>5. Java 8</h3>
<pre><code class="language-java">static void function(int[] ar)
 {
   Random rnd = ThreadLocalRandom.current();
   for (int i = ar.length - 1; i &gt; 0; i--)
   {
     int index = rnd.nextInt(i + 1);
     int a = ar[index];
     ar[index] = ar[i];
     ar[i] = a;
   }
 }</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Merge sort</li>
<li>B. Shuffle</li>
<li>C. Increases size of array</li>
<li>D. Decreases size of array</li>
</ul>
<!-- Correct: B -->

<hr>
<h3>6. Ruby</h3>
<pre><code class="language-ruby">def f(hash)
  prs = hash.inject({}) do |hsh, pr|
    k, v = yield pr
    hsh.merge(k =&gt; v)
  end
  Hash[prs]
end</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Reverses an array</li>
<li>B. Administers a booster shot</li>
<li>C. Enters a freeway safely</li>
<li>D. Transforms a hash</li>
</ul>
<!-- Correct: D -->

<hr>
<h3>7. Python</h3>
<pre><code class="language-python">def function(list):
     return [x for x in list if x == x[::-1]]</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Finds and returns all palindromes within the given list</li>
<li>B. Reverses all strings in the given list</li>
<li>C. Swaps the first and last letter in each word in the given list</li>
<li>D. Returns a list of anagrams for each word in the given list</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>8. Scala</h3>
<pre><code class="language-scala">object Op {
  val r1: Regex = &#8220;&#8221;&#8221;([^aeiouAEIOU\d\s]+)([^\d\s]*)$&#8221;&#8221;&#8221;.r
  val r2: Regex = &#8220;&#8221;&#8221;[aeiouAEIOU][^\d\s]*$&#8221;&#8221;&#8221;.r
  val s1:String = &#8220;\u0061\u0079&#8221;
  val s2:String = &#8220;\u0077&#8221; + s1
  def apply(s: String): String = {
    s.toList match {
      case Nil =&gt; &#8220;&#8221;
      case _ =&gt; s match {
        case r1(c, r) =&gt; r ++ c ++ s1 case r2(_*) =&gt; s ++ s2
        case _ =&gt; throw new
RuntimeException(&#8220;Sorry&#8221;)
      }
    }
  }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Converts a given String to a JavaScript-based String</li>
<li>B. Converts a Unix/MacOSX String format into a Windows String format</li>
<li>C. Converts a word into the children&#8217;s language equivalent called &#8220;Pig Latin&#8221;</li>
<li>D. Converts a given String to IPV6 format since IP numbers are running out</li>
</ul>
<!-- Correct: C -->

<hr>
<h3>9. Swift</h3>
<pre><code class="language-swift">import Foundation

let i = &#8220;Hell&#248;, Swift&#8221;

let t = i.precomposedStringWithCanonicalMapping

let c = t.utf8.map({UnicodeScalar($0+2)})
let j = i.utf8.map({UnicodeScalar($0+1)}).count / 2

var d = String(repeating: String(describing: c[j]), count: j)

d.append(Character(&#8220;&#127462;&#127482;&#127482;&#127480;&#8221;))

let result = &#8220;\(d): \(d.characters.count)&#8221;</code></pre>

<p>What is the value of &#8220;result&#8221;?</p>
<ul>
<li>A. ......&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>B. ......&#127462;&#127482;&#127482;&#127480;: 8</li>
<li>C. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 7</li>
<li>D. &#8220;&#8221;&#8221;&#8221;&#8221;&#8221;&#127462;&#127482;&#127482;&#127480;: 8</li>
</ul>
<!-- Correct: A -->

<hr>
<h3>10. JavaScript</h3>
<pre><code class="language-javascript">function thing (n) {
    for (var i = 0; i &lt; n; i++) {
        setTimeout(function () {console.log(i);}, 0);
    }
}</code></pre>

<p>What does this function do?</p>
<ul>
<li>A. Prints numbers 0 through n</li>
<li>B. Prints n n time</li>
<li>C. Prints 0 n times</li>
<li>D. Prints nothing</li>
</ul>
<!-- Correct: B -->

<hr>
<p>View HTML source to see the answers.</p>
<p>I tried to keep the above close to what appeared on the physical
cards. I haven't tried to correct the little mistakes and bizarre
indentation throughout.</p>
<p>It's fun!</p>    
    ]]></description>
<link>http://planspace.org/20170517-code_reading_questions_at_oscon/</link>
<guid>http://planspace.org/20170517-code_reading_questions_at_oscon/</guid>
<pubDate>Wed, 17 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Caffe and TensorFlow at Deep Learning Analytics</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">presentation</a> given Wednesday May 10, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a> as part of <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/full/tensorflow-day">TensorFlow Day</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<p>Thank you!</p>
<hr>
<p>Thanks to all the OSCON organizers and participants, and especially to the Google folks organizing TensorFlow Day, and everyone coming out for TensorFlow events! What fun!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png" width="100%"></p>
<hr>
<p>I work at a company called <a href="https://www.deeplearninganalytics.com/">Deep Learning Analytics</a>, or DLA.</p>
<p>The advantage of a name like Deep Learning Analytics is that, assuming you've heard of deep learning, you immediately know something about what we do.</p>
<hr>
<p>deeplearninganalytics.com</p>
<hr>
<p>The disadvantage of a name like Deep Learning Analytics is that our URL is really long.</p>
<hr>
<p><img alt="some of the Deep Learning Analytics team" src="img/dla_team.jpg" width="100%"></p>
<hr>
<p>This is most of us at our Arlington location, just outside DC. We do a mix of government contracting and commercial work.</p>
<hr>
<p><img alt="DarLA" src="img/darla.png" height="100%"></p>
<hr>
<p>We have a cute robot mascot called DarLA.</p>
<p>DarLA the robot could be a metaphor for any machine learning system. You have to build the arms and the legs, and that's a good deal of work. There's also generally a lot of work in getting data to train the system with. But you also need a brain, and that's inevitably sort of the interesting part.</p>
<p>So what do you use for the brain?</p>
<hr>
<p>warning: opinions</p>
<hr>
<p>I'm going to comment on a few aspects of a few systems, based on what I've seen. Everybody involved in this space is incredibly smart, and probably correct in some ways even when I disagree.</p>
<hr>
<ul>
<li>Caffe?</li>
<li>TensorFlow?</li>
</ul>
<hr>
<p>The two systems I'll talk about are Caffe and TensorFlow.</p>
<p>I'm aware of Caffe 2 but I don't have anything to say about it today, aside from that it seems a lot like Caffe.</p>
<hr>
<p><img src="img/nvidia_comparison.jpg" alt="DL framework comparison" height="100%"></p>
<hr>
<p>This is a picture of a slide from a talk some NVIDIA folks gave a couple weeks ago. I will now agree and disagree with it.</p>
<p>Caffe easy to start? I disagree, and I'll talk about why.</p>
<p>Caffe easy to develop? I disagree, and I'll talk about why.</p>
<p>Caffe limited capability? I agree, and I'll talk about why.</p>
<p>TensorFlow portable and nice documentation? I mostly agree.</p>
<p>TensorFlow slow? I think to the extent this is true, it doesn't matter, and I'll talk about some related considerations.</p>
<p>My colleagues have done some work in Torch as well, but I'm not going to talk about these others.</p>
<hr>
<p>Easy to start?</p>
<hr>
<p>So which framework makes it easier to get started?</p>
<hr>
<p><code>pip install:</code><br>
&#10008; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>This is a convenience, but it's also really nice. There are advantages to compiling things yourself too. But for quickly getting started, this is great.</p>
<p>Even if you're happy to compile an optimized build for your system, having easy installation options available can be nice for setting up test environments, for example.</p>
<hr>
<p>"Easy" models:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>By "easy" models, I mean models that are either completely pre-specified and pre-trained, or very easily specified by a very high-level API.</p>
<p>The Caffe model zoo might have been the first recognized collection of pre-trained models and model architecture specifications. One or two years ago, it might have been the best way to start working with an interesting deep net without going through the whole training process yourself.</p>
<p>Now, though, I think TensorFlow is really passing Caffe on this. You can get a number of pre-trained models already with just one line of Python, using <code>tf.contrib.keras</code>. Hopefully others will be able to match that ease of redistribution.</p>
<p>Further, TensorFlow's "canned models" in <code>tf.contrib.learn</code> provide a scikit-learn-like interface that Caffe never attempts.</p>
<hr>
<p>Easy to develop?</p>
<hr>
<p>What about ease of development?</p>
<hr>
<p>API level:</p>
<ul>
<li>high? middle? low?</li>
</ul>
<hr>
<p>For development, it matters what level you want to work at. You can do a lot quickly if you can stay at a high level. But sometimes you need to get low-level to control details or do something slightly non-standard.</p>
<p>I think TensorFlow now is doing a better job than Caffe of providing API surfaces across a range of levels.</p>
<p>Caffe's API, at the protocol buffer text format level you have to eventually get to, is sort of a middle-low level. The vocabulary is more limited than what you get with TensorFlow ops.</p>
<p>You can build higher-level APIs with Caffe, and DLA has an in-house library that we use to make Caffe easier to work with.</p>
<p>TensorFlow has Keras.</p>
<hr>
<p>Python integration:<br>
&#189; Caffe<br>
&#10004; TensorFlow</p>
<hr>
<p>Both Caffe and TensorFlow are written with C++, but interfacing with Caffe can feel like interfacing with the separate free-standing program, whereas the TensorFlow interface is seamless.</p>
<p>If there's a way to use Caffe without at some point writing protobuf text files to disk and then having Caffe read them, I don't know it.</p>
<hr>
<p>modular design</p>
<hr>
<p>I'll show an example of a Caffe layer in order to talk about some of the design issues.</p>
<hr>
<pre><code>layer {name: "data"
       type: "Data"
       top: "data"
       top: "label"
       transform_param {crop_size: 227}
       data_param {source: "train_lmdb_path"
                   batch_size: 256
                   backend: LMDB}}</code></pre>

<hr>
<p>I condensed a Caffe data layer spec a bit.</p>
<p>Caffe likes reading data from LMDB databases. So this layer includes a path to a location on disk, where an LMDB database of a particular form needs to be. So this is a layer that reads from disk.</p>
<p>But wait - this layer spec also specifies the training batch size.</p>
<p>Does it do anything else? What's that "<code>crop_size</code>"?</p>
<p>This layer also implicitly takes random crops from the images it reads, 227 pixels by 227 pixels. So the images in the database should be at least that big.</p>
<p>That's a lot happening in one layer!</p>
<p>On the one hand, this is pretty neat. Caffe does a lot for you. And once you've made a lot of choices, you can optimize the implementation, which is part of how Caffe gets pretty fast.</p>
<p>On the other hand, Caffe is making a lot of decisions for us, and it isn't particularly happy if we want to change those decisions. Caffe can feel more like final application code than framework code.</p>
<p>What if we want non-square crops? What if we want to introduce random distortions? Or if we want to read data from JPG files instead of LMDB? None of these are supported by this LMDB layer, and these changes aren't easily composable. We can switch to a memory data layer to get training batches in, but then we have to independently implement cropping if we want it, and so on.</p>
<p>In contrast, TensorFlow doesn't have such tight integrations. The closest analog in TensorFlow might be reading from TFRecords files, but that functionality is isolated, and doesn't bundle in choices about batch size or cropping.</p>
<p>You get a more modular system with TensorFlow, so you shouldn't find yourself having to pick apart tightly-coupled functionality later on.</p>
<p>At the same time, TensorFlow does include the higher-level APIs mentioned earlier, so it isn't completely a choice between low-level and high-level either.</p>
<hr>
<p>multi-GPU</p>
<hr>
<p>How do we get multi-GPU systems?</p>
<hr>
<p><img src="img/alexnet.png" alt="AlexNet diagram" width="100%"></p>
<hr>
<p>This is the diagram from <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>, the 2012 paper that popularized deep learning. It has over 11,000 citations now.</p>
<p>This was a multi-GPU implementation!</p>
<p>I want to point out how crazy it is. Alex Krizhevsky split individual convolutional layers between two GPUs. Whoa.</p>
<hr>
<p>Caffe</p>
<hr>
<p>So how do we do multi-GPU with Caffe?</p>
<hr>
<p><img alt="multi-GPU Caffe docs" src="img/caffe_multi.png"></p>
<hr>
<p>For Caffe, there's this page of documentation. As far as I know there is no link to this page of documentation. I found it <a href="https://github.com/BVLC/caffe/blob/master/docs/multigpu.md">via</a> the GitHub repo that backs the website.</p>
<p>There are some limitations in Caffe's multi-GPU support.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>Well, how do we do multi-GPU with TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same way we do single-GPU! The API stays the same, and you just say where you want things to run. This is also more flexible in terms of the ways you can parallelize things. You could have two copies of a model on two GPUs, or one really big model across two GPUs, for example.</p>
<hr>
<p><img alt="Jenn" src="img/jenn.jpg"></p>
<hr>
<p>My colleague Jenn parallelized some GAN (Generative Adversarial Network) code to use multiple GPUs. I was super curious to see what she had to do. She was really puzzled about why I was so curious, because TensorFlow made it so easy.</p>
<p>Also, if you're curious about GANs, you should really check out Jenn's <a href="https://github.com/jennsleeman/introtogans_dcdatascience_2017">talk on the subject</a>.</p>
<hr>
<p>multi-machine</p>
<hr>
<p>What if we want to train across multiple machines now?</p>
<hr>
<p>Caffe</p>
<hr>
<p>How about Caffe?</p>
<hr>
<p><img src="img/trivial_mpi.png" alt="trivial MPI" width="100%"></p>
<hr>
<p>Caffe doesn't do it, but Yangqing helpfully <a href="https://github.com/BVLC/caffe/issues/876">points out</a> that you could make your own distributed Caffe system by adding MPI. Some people have actually <a href="https://software.intel.com/en-us/articles/caffe-training-on-multi-node-distributed-memory-systems-based-on-intel-xeon-processor-e5">done</a> this. I'm not sure it's trivial.</p>
<hr>
<p>TensorFlow</p>
<hr>
<p>And TensorFlow?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>It's the same API! You just put computation on various machines!</p>
<p>There is a little bit more to it than that, but I think it's easier than MPI.</p>
<hr>
<p><img alt="infrastructure" src="img/infr.png"></p>
<hr>
<p>Another thing we're excited about is possibilities for scaling out our work with TensorFlow. OpenAI <a href="https://blog.openai.com/infrastructure-for-deep-learning/">published</a> information and code on their system, and it seems like a pretty great model.</p>
<hr>
<p>responsiveness</p>
<hr>
<p>I'm going to talk about one anecdotal measures of developer responsiveness. It isn't totally fair, because Caffe development is basically over as Caffe 2 emerges, but it may still be interesting.</p>
<p>I occasionally submit very minor pull requests, for things like ensuring a variable is the correct type, or fixing typos in documentation. These are simple PRs that don't generally require discussion or alteration; they just get merged in.</p>
<p>So how quickly does that happen with Caffe and with TensorFlow?</p>
<hr>
<p>merge time:</p>
<ul>
<li>TensorFlow: 12 hours</li>
<li>Caffe: 329 days</li>
</ul>
<hr>
<p>TensorFlow developers seem more responsive.</p>
<p>(<a href="https://github.com/tensorflow/tensorflow/pull/9451">TensorFlow PR</a>)</p>
<p>(<a href="https://github.com/BVLC/caffe/pull/4182">Caffe PR</a>)</p>
<hr>
<p><img alt="TensorFlow logo" src="img/tf.jpg" height="100%"></p>
<hr>
<p>And I think that, along with many other things, bodes well for the future of TensorFlow.</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>    
    ]]></description>
<link>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</link>
<guid>http://planspace.org/20170510-caffe_and_tensorflow_at_dla/</guid>
<pubDate>Wed, 10 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Building TensorFlow Systems from Components</title>
<description><![CDATA[

<p><em>These are materials for a <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">workshop</a> given Tuesday May 9, 2017 at <a href="https://conferences.oreilly.com/oscon/">OSCON</a>. (<a href="big.html">slides</a>)</em></p>
<hr>
<h3>Workshop participant materials</h3>
<p>These are things you can use during the workshop. Use them when you need them.</p>
<ul>
<li>Focus one: <em>getting data in</em><ul>
<li>Download notebook: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
<li><a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a></li>
</ul>
</li>
<li>Focus two: <em>distributed programs</em><ul>
<li><a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/"><code>mapreduce_with_tensorflow</code> code and data</a></li>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
</li>
<li>Focus three: <em>high-level ML APIs</em><ul>
<li>Download notebook: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View notebook on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
</li>
</ul>
<p>The whole slide deck and everything follows, and it's long.</p>
<hr>
<p>Thank you!</p>
<hr>
<p>Before starting, I want to take a moment to notice how great it is to have a conference. Thank you for being here! We are all really lucky. If this isn't nice, I don't know what is.</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<p>If you want to participate in the workshop, <em>really go</em> to <code>planspace.org</code> and pull up these materials!</p>
<hr>
<p><img alt="Deep Learning Analytics" src="img/dla.png"></p>
<hr>
<p>I work for a company called Deep Learning Analytics.</p>
<p>I'll talk more about DLA tomorrow <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/62149">as part of TensorFlow Day</a>. Hope to see you there as well!</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>You'll want to have TensorFlow version 1.1 installed. TensorFlow 1.1 was only officially released on April 20, and the API really has changed.</p>
<p>The <em>core documented</em> API, mostly the low-level API, is frozen as of TensorFlow 1.0, but a lot of higher-level stuff is still changing. TensorFlow 1.1 brings in some really neat stuff, like Keras, which we'll use later.</p>
<hr>
<p><img src="img/hello_tensorflow.png" alt="Hello, TensorFlow! on O'Reilly" height="100%"></p>
<hr>
<p>As an example: I wrote <a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a> about a year ago. It used TensorFlow 0.8.</p>
<p>After TensorFlow 1.0 came out, I went and looked at the summary code block at the end of "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>". It had 15 lines of TensorFlow code. Of that, 5 lines no longer worked. I had to update 33% of the lines of my simple TensorFlow example code because of API changes.</p>
<hr>
<p><img alt="one third" src="img/one_third.jpg"></p>
<hr>
<p>I was surprised that it was one third.</p>
<p>I have an <a href="/20160620-hello_tensorflow_just_the_code/">updated code snippet</a>, but we're not doing "<a href="https://www.oreilly.com/learning/hello-tensorflow">Hello, TensorFlow!</a>" today.</p>
<hr>
<pre><code class="language-bash">$ pip install --upgrade tensorflow
$ pip freeze | grep tensorflow
## tensorflow==1.1.0</code></pre>

<hr>
<p>So please have TensorFlow 1.1 installed, is the point of that whole story.</p>
<p>The stable bits of TensorFlow really are stable, but there's a lot of exciting new stuff, and I don't want you to miss out today!</p>
<hr>
<p>THE BIG IDEA</p>
<hr>
<p>Okay! What is this workshop about?</p>
<hr>
<p>use what you need</p>
<hr>
<p>TensorFlow works for you! Use it where it does things that help you.</p>
<p>TensorFlow is a tool. It's a very general tool, on the one hand. It's also a tool with lots of pieces. You can use <a href="/20170312-use_only_what_you_need_from_tensorflow/">some</a> of the pieces. Or you can decide not to use TensorFlow altogether!</p>
<p>We'll look at several specific aspects of TensorFlow. Maybe you'll want to use them. Maybe you won't. The hope is that you'll be more comfortable with what's available and able to decide what to apply when.</p>
<p>If you like <a href="https://www.infoq.com/presentations/Simple-Made-Easy">Rich Hickey words</a>, maybe I'm trying to <em>decomplect</em> the strands within TensorFlow so they can be understood individually.</p>
<hr>
<p>THE PLAN</p>
<hr>
<p>Here's the plan for this workshop.</p>
<hr>
<ul>
<li>one short work</li>
<li>three longer works</li>
</ul>
<hr>
<p>I'll talk about some things, but the hope of the workshop is that you do some good work.</p>
<p>I hope that you don't finish all the things you think of, but want to keep working after the workshop is over.</p>
<p>Also, there's a break from 10:30 to 11:00 on the official schedule, but I don't really like that. If we happen to be working during the break, fine. Feel free to take a break whenever you need a break.</p>
<hr>
<p>Do what you want!</p>
<hr>
<p>TensorFlow is really big, and not every part will be interesting or important to you.</p>
<p>I'll have very specific things for you to work on for each of the works, but if you think of something better to do, you better do it!</p>
<hr>
<p>Intro by Logo</p>
<hr>
<p>To get creative juices flowing a little, let's explore some logo history.</p>
<hr>
<p><img alt="LOGO turle" src="img/logo_turtle.jpg" height="100%"></p>
<hr>
<p>Not that sort of <a href="https://en.wikipedia.org/wiki/Logo_(programming_language)">Logo</a>!</p>
<p>Fine to think about, though.</p>
<hr>
<p><img alt="G&#246;del, Escher, Bach" src="img/geb.jpg" height="100%"></p>
<hr>
<p>In the beginning, there was <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">G&#246;del, Escher, Bach: An Eternal Golden Braid</a> by Douglas Hofstadter.</p>
<p>This "metaphorical fugue on minds and machines in the spirit of Lewis Carroll" includes the <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> idea and related discussion of consciousness and formal systems. Hofstadter's influence can be seen, for example, <a href="https://cs.illinois.edu/news/strange-loop-conference">in the name</a> of the <a href="http://www.thestrangeloop.com/">Strange Loop</a> tech conference.</p>
<p>It seems likely that many people working in artificial intelligence and machine learning have encountered G&#246;del, Escher, Bach.</p>
<hr>
<p><img src="img/chernin.jpg" alt="Chernin Entertainment" height="100%"></p>
<hr>
<p>Of course, there's also <a href="https://en.wikipedia.org/wiki/Chernin_Entertainment">Chernin Entertainment</a>, the production company.</p>
<p>So we shouldn't rule out the possibility that Google engineers are fans of <a href="http://www.imdb.com/company/co0286257/">Chernin's work</a>. <a href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden Figures</a> is quite good. And I guess a lot of people like <a href="https://en.wikipedia.org/wiki/New_Girl">New Girl</a>?</p>
<hr>
<p><img alt="TensorFlow logo - old?" src="img/tf-old-big.png" height="100%"></p>
<hr>
<p>In any event, somehow we get to this TensorFlow logo:</p>
<p>If you look carefully, does it seem like the right side of the "T" view is too short?</p>
<hr>
<p><img alt="Issue 1922" src="img/issue_1922.png"></p>
<hr>
<p>This very serious concern appears as <a href="https://github.com/tensorflow/tensorflow/issues/1922">issue #1922</a> on the TensorFlow github, and <a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XhO1sqp4l4g">on the TensorFlow mailing list</a> complete with ASCII art illustration.</p>
<p>The consensus response seemed to be some variant of "won't fix" (it wouldn't look as cool, anyway) until...</p>
<hr>
<p><img alt="TensorFlow logo - new?" src="img/tf-new.jpg" height="100%"></p>
<hr>
<p>As of around the 1.0 release of TensorFlow, which was around the first <a href="https://events.withgoogle.com/tensorflow-dev-summit/">TensorFlow Dev Summit</a>, this logo variant seems to be in vogue. It removes the (possibly contentious) shadows, and adds additional imagery in the background.</p>
<p>I want to suggest that the image in the background can be a kind of Rorschach test for at least three ways you might be thinking about TensorFlow.</p>
<ul>
<li>Is it a diagram of connected neurons? Maybe you're interested in TensorFlow because you want to make neural networks.</li>
<li>Is it a diagram of a computation graph? Maybe you're interested in TensorFlow for general calculation, possibly using GPUs.</li>
<li>Is it a diagram of multiple computers in a distributed system? Maybe you're interested in TensorFlow for its distributed capabilities.</li>
</ul>
<p>There can be overlap among those interpretations as well, but I hope the point is not lost that TensorFlow can be different things to different people.</p>
<hr>
<p>short work</p>
<hr>
<p>I want to get you thinking about systems without further restriction. The idea is to imagine and start fleshing out a system that might involve TensorFlow.</p>
<p>Later on we'll follow the usual workshop pattern of showing you something you can do and having you do it. But it's much more realistic and interesting to start from something you want to do and then try to figure out how to do it. So let's start from the big picture.</p>
<hr>
<p>draw a system!</p>
<ul>
<li>block diagram</li>
<li>add detail</li>
<li>pseudocode?</li>
</ul>
<hr>
<p>You can draw a system you've already made, or something you're making, or something you'd like to make. It could be something you've heard about, or something totally unique.</p>
<p>You don't have to know how to build everything in the system. You don't need to know how TensorFlow fits in. Feel free to draw what you <em>wish</em> TensorFlow might let you do.</p>
<p>Keep adding more detail. If you get everything laid out, start to think about what functions or objects you might need to have, and start putting pseudocode together.</p>
<hr>
<p>short work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p>Let's do hard AI!</p>
<hr>
<p>I'm going to walk through an example and talk about how TensorFlow can fit in.</p>
<p>Doing hard AI, or Artificial General Intelligence (AGI), is intended as a joke, but it's increasingly less so, with Google DeepMind and OpenAI both explicitly working on getting to AGI.</p>
<hr>
<p>(build)</p>
<hr>
<p>I've got 35 slides of sketches, so they're in <a href="build.pdf">a separate PDF</a> to go through.</p>
<hr>
<p><img alt="system" src="img/system.jpg"></p>
<hr>
<p>Here's the final system that we built to.</p>
<p>We're not going to do everything in there today.</p>
<hr>
<ul>
<li>getting data in</li>
<li>distributed programs</li>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>These are the three focuses for today.</p>
<p>The <a href="https://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">original description</a> for this workshop also mentioned serving, but I'm not covering it today. Sorry. Not enough time.</p>
<hr>
<p>What even is TensorFlow?</p>
<hr>
<p>Here's one way to think about TensorFlow.</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>The beating heart of TensorFlow is the Distributed Execution Engine, or runtime.</p>
<p>One way to think of it is as a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">virtual machine</a> whose language is the TensorFlow graph.</p>
<p>That core is written in C++, but lots of TensorFlow functionality lives in the frontends. In particular, there's a lot in the Python frontend.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<p>Python is the richest frontend for TensorFlow. It's what we'll use today.</p>
<p>You should remember that not everything in the Python TensorFlow API touches the graph. Some parts are just Python.</p>
<hr>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>R has an unofficial <a href="https://rstudio.github.io/tensorflow/">TensorFlow API</a> which is kind of interesting in that it just wraps the Python API. It's really more like an R API to the Python API to TensorFlow. So when you use it, you write R, but Python runs. This is not how TensorFlow encourages languages to implement TensorFlow APIs, but it's out there.</p>
<hr>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<p>The way TensorFlow encourages API development is via TensorFlow's C bindings.</p>
<hr>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<p>You could use C++, of course.</p>
<hr>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<p>Also there's Java.</p>
<hr>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<p>And there's Go.</p>
<hr>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<p>And there's Rust.</p>
<hr>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<p>And there's even Haskell!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Currently, languages other than Python have TensorFlow support that is very close to the runtime. Basically make your ops and run them.</p>
<p>This is likely to be enough support to deploy a TensorFlow system in whatever language you like, but if you're developing and training systems you probably still want to use Python.</p>
<hr>
<p>graph or not graph?</p>
<hr>
<p>So this is a distinction to think about: Am I using the TensorFlow graph, or not?</p>
<hr>
<ul>
<li>getting data in</li>
</ul>
<hr>
<p>So here we are at the first focus area.</p>
<p>We'll do two sub-parts.</p>
<hr>
<ul>
<li>getting data in<ul>
<li>to the graph</li>
<li>TFRecords</li>
</ul>
</li>
</ul>
<hr>
<p>First, a quick review of the the graph and putting data into it.</p>
<p>Second, a bit about TensorFlow's TFRecords format.</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="data_start.ipynb">start state</a> / <a href="data_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/data_end.ipynb">end state</a></li>
</ul>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>work with data!</p>
<ul>
<li>your own system?</li>
<li>planspace.org: <code>mystery.tfrecords</code></li>
<li>move to graph</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Almost certainly there's some data input that needs to happen. How are you going to read that data, and possibly get it into TensorFlow?</p>
<p>If you want to stay really close to the TensorFlow stuff just demonstrated, here's a fun little puzzle for you: What's in <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a>?</p>
<p>That could be hard, or it could be easy. If you want to extend it, migrate the reading and parsing of the TFRecords file into the TensorFlow graph. The demonstration in the notebook worked with TFRecords/Examples without using the TensorFlow graph. The links on the <a href="/20170425-mystery.tfrecords/"><code>mystery.tfrecords</code></a> page can help with this.</p>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="data" src="img/data_data.jpg" width="100%"></p>
<hr>
<p>As a wrap-up: Working directly with data, trying to get it into the right shape, cleaning it, etc., may not be the most fun, but it's got to be done. Here's a horrible pie chart <a href="https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/">from some guy on Forbes</a>, the point of which is that people spend a good deal of time fighting with data.</p>
<hr>
<ul>
<li>distributed programs</li>
</ul>
<hr>
<p>We arrive at the second focus area. TensorFlow has some pretty wicked distributed computing capabilities.</p>
<hr>
<ul>
<li>distributed programs<ul>
<li>command-line arguments</li>
<li>MapReduce example</li>
</ul>
</li>
</ul>
<hr>
<p>I'm putting a bit about command-line arguments in here because I think it's interesting. It doesn't necessarily fit in with distributed computing, although you might well have a distributed program that takes command-line arguments. Google Cloud ML can use command-line arguments for hyperparameters, for example.</p>
<p>Then we'll get to a real distributed example, in which we implement a distributed MapReduce word count in 50 lines of Python TensorFlow code.</p>
<hr>
<p>command-line arguments</p>
<hr>
<p>So let's take a look at some ways to do command-line arguments.</p>
<p>This section comes from the post <a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a>.</p>
<hr>
<pre><code class="language-bash">$ python script.py --color red
a red flower</code></pre>

<hr>
<p>I'll show eight variants that all do the same thing. You provide a <code>--color</code> argument, and it outputs (in text) a flower of that color.</p>
<hr>
<pre><code class="language-python">import sys

def main():
    assert sys.argv[1] == '--color'
    print('a {} flower'.format(sys.argv[2]))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This is a bare-bones <code>sys.argv</code> method. Arguments become elements of the <code>sys.argv</code> list, and can be accessed as such. This has limitations when arguments get more complicated.</p>
<hr>
<pre><code class="language-python">import sys
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main():
    flags.FLAGS._parse_flags(args=sys.argv[1:])
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This <code>FLAGS</code> API is <a href="/20170313-tensorflow_use_of_google_technologies/">familiar to Googlers</a>, I think. It's interesting to me where some Google-internal things peak out from the corners of TensorFlow.</p>
<hr>
<pre><code class="language-python">import sys
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main():
    gflags.FLAGS(sys.argv)
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>You could also install the <code>gflags</code> module, which works much the same way.</p>
<hr>
<pre><code class="language-python">import sys
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args(sys.argv[1:])
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>Here's Python's own standard <a href="https://docs.python.org/3/library/argparse.html"><code>argparse</code></a>, set up to mimic the <code>gflags</code> example, still using <code>sys.argv</code> explicitly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

def main(args):
    assert args[1] == '--color'
    print('a {} flower'.format(args[2]))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>Using <code>tf.app.run()</code>, another Google-ism, frees us from accessing <code>sys.argv</code> directly.</p>
<hr>
<pre><code class="language-python">import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main(args):
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<hr>
<p>We can combine <code>tf.app.run()</code> with <code>tf.app.flags</code>.</p>
<hr>
<pre><code class="language-python">import google.apputils.app
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main(args):
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    google.apputils.app.run()</code></pre>

<hr>
<p>To see the equivalent outside the TensorFlow package, we can combine <code>gflags</code> and <code>google.apputils.app</code>.</p>
<hr>
<pre><code class="language-python">import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args()
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<hr>
<p>This has all been fun, but here's what you should really do. Just use <code>argparse</code>.</p>
<hr>
<p>Whew!</p>
<hr>
<p>That was a lot of arguing.</p>
<p>It may be worth showing all these because you'll encounter various combinations as you read code out there in the world. More recent examples are tending to move to <code>argparse</code>, but there are some of the other variants out there as well.</p>
<hr>
<p>MapReduce example</p>
<hr>
<p>Now to the MapReduce example!</p>
<hr>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<hr>
<p>Recall that the core of TensorFlow is a <em>distributed</em> runtime. What does that mean?</p>
<hr>
<p><code>tf.device()</code></p>
<hr>
<p>Behold, the power of <code>tf.device()</code>!</p>
<hr>
<p></p><pre><code>with tf.device('/cpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local CPU.</p>
<hr>
<p></p><pre><code>with tf.device('/gpu:0'):
    # Do something.</code></pre>
<hr>
<p>You can specify that work happen on a local GPU.</p>
<hr>
<p></p><pre><code>with tf.device('/job:ps/task:0'):
    # Do something.</code></pre>
<hr>
<p>In exactly the same way, you can specify that work happen on <em>a different computer</em>!</p>
<p>This is pretty amazing. I think of it as sort of <a href="/20170423-tensorflow_as_automatic_mpi/">declarative MPI</a>.</p>
<hr>
<p><img alt="distributing a TensorFlow graph" src="img/distributed_graph.png"></p>
<hr>
<p>TensorFlow automatically figures out when it needs to send information between devices, whether they're on the same machine or on different machines. So cool!</p>
<hr>
<p><img alt="oh map reduce..." src="img/oh_map_reduce.png"></p>
<hr>
<p>MapReduce is often associated with Hadoop. It's just divide and conquer.</p>
<p>So let's do it with TensorFlow!</p>
<hr>
<p>(demo)</p>
<hr>
<p>It's time to see how this looks in practice! (Well, or at least to see how it looks in a cute little demo.)</p>
<p>The demo uses the contents of the <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">mapreduce_with_tensorflow</a> repo on GitHub. For more explanation, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a> and <a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a>.</p>
<hr>
<p>(code)</p>
<hr>
<p>Walking through some details inside <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow/blob/master/count.py"><code>count.py</code></a>.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>different distributed functionality?</li>
<li>add command-line?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Maybe command-line args are relevant to you. Maybe running a system across multiple machines is relevant for you. Or maybe not.</p>
<p>If you want to exercise the things just demonstrated, you could add a command-line argument to the distributed word-count program. For example, you could make it count only a particular word, or optionally count characters, or something else. Or you could change the distributed functionality without any command-line fiddling. (You could make it a distributed neural net training program, for example.)</p>
<p>Here are some links that might be helpful:</p>
<ul>
<li><a href="/20170314-command_line_apps_and_tensorflow/">Command-Line Apps and TensorFlow</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
<li><a href="/20170423-tensorflow_as_automatic_mpi/">TensorFlow as Automatic MPI</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="oh kubernetes..." src="img/oh_kubernetes.jpg"></p>
<hr>
<p>I should probably say that you don't really want to start all your distributed TensorFlow programs by hand. <a href="https://www.docker.com/">Containers</a> and <a href="https://kubernetes.io/">Kubernetes</a> and all that.</p>
<hr>
<ul>
<li>high-level ML APIs</li>
</ul>
<hr>
<p>Let's to some machine learning!</p>
<hr>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<hr>
<p>New and exciting things are being added in TensorFlow Python land, building up the ladder of abstraction.</p>
<p>This material comes from <a href="/20170321-various_tensorflow_apis_for_python/">Various TensorFlow APIs for Python</a>.</p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<hr>
<p><img alt="slim... shady?" src="img/slim_shady.png" height="100%"></p>
<hr>
<p>The "layer" abstractions largely from TF-Slim are now appearing at <code>tf.layers</code>.</p>
<hr>
<p><img alt="scikit-learn logo" src="img/sklearn.png" height="100%"></p>
<hr>
<p>The Estimators API now at <code>tf.estimator</code> is drawn from <code>tf.contrib.learn</code> work, which is itself heavily inspired by scikit-learn.</p>
<hr>
<p><img alt="Keras" src="img/keras.jpg" height="100%"></p>
<hr>
<p>And Keras is entering TensorFlow first as <code>tf.contrib.keras</code> and soon just <code>tf.keras</code> with version 1.2.</p>
<hr>
<ul>
<li>high-level ML APIs<ul>
<li>training an Estimator</li>
<li>pre-trained Keras</li>
</ul>
</li>
</ul>
<hr>
<p>So let's try this out!</p>
<hr>
<p>(notebook)</p>
<hr>
<p>There's a Jupyter notebook to talk through at this point.</p>
<ul>
<li>Download: <a href="high_ml_start.ipynb">start state</a> / <a href="high_ml_end.ipynb">end state</a></li>
<li>View on GitHub: <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_start.ipynb">start state</a> / <a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20170509-building_tensorflow_systems_from_components/high_ml_end.ipynb">end state</a></li>
</ul>
<p>There's also a TensorBoard demo baked in there. A couple backup slides follow.</p>
<hr>
<p><img alt="graph" src="img/tensorboard_graph.png"></p>
<hr>
<p>This is what the graph should look like in TensorBoard.</p>
<hr>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<hr>
<p>Steps per second should look something like this.</p>
<hr>
<p><img alt="loss" src="img/loss.png"></p>
<hr>
<p>Loss should look like this.</p>
<hr>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg" height="100%"></p>
<hr>
<p>I just enjoy this image too much not to share it.</p>
<hr>
<p>long work</p>
<hr>
<p>It's time to do some work!</p>
<hr>
<p>make something happen!</p>
<ul>
<li>your own system?</li>
<li>flip regression to classification?</li>
<li>classify some images?</li>
</ul>
<hr>
<p>Option one is always to work on your own system. Do you need a model of some kind in your system? Maybe you can use TensorFlow's high-level machine learning APIs.</p>
<p>If you want to work with the stuff just shown some more, that's also totally cool! Instead of simple regression, maybe you want to flip the presidential GDP problem around to be logistic regression. TensorFlow has <code>tf.contrib.learn.LinearClassifier</code> for that. And many more variants!</p>
<p>Or maybe you want to classify your own images, or start to poke around the model some more. Also good! If you want more example images, there's <a href="https://github.com/ajschumacher/imagen">this set</a>.</p>
<p>Here are some links that could be helpful:</p>
<ul>
<li><a href="/20170506-simple_regression_with_a_tensorflow_estimator/">Simple Regression with a TensorFlow Estimator</a></li>
<li><a href="/20170502-canned_models_with_keras_in_tensorflow/">Canned Models with Keras in TensorFlow</a></li>
</ul>
<hr>
<p>long work over</p>
<hr>
<p>Moving right along...</p>
<hr>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<hr>
<p>Oh my! Are we out of time already?</p>
<hr>
<p>What else?</p>
<hr>
<p>There are a lot of things we haven't covered.</p>
<hr>
<p>debugging, optimizing (XLA, low-precision, etc.), serving, building custom network architectures, embeddings, recurrent, generative, bazel, protobuf, gRPC, queues, threading...</p>
<hr>
<p>Here's a list of the first things that came to mind.</p>
<p>I hope you continue to explore!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20170509-building_tensorflow_systems_from_components/</link>
<guid>http://planspace.org/20170509-building_tensorflow_systems_from_components/</guid>
<pubDate>Tue, 09 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Regression with a TensorFlow Estimator</title>
<description><![CDATA[

<p>With TensorFlow 1.1, the <a href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators">Estimator</a> API is now at <code>tf.estimator</code>. A number of "canned estimators" are <a href="https://www.tensorflow.org/extend/estimators">at</a> <code>tf.contrib.learn</code>. This higher-level API bakes in some best practices and makes it much easier to do a lot quickly with TensorFlow, similar to using APIs available in other languages.</p>
<hr>
<h2>Data</h2>
<p>This example will use the very simple <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/">US Presidential Party and GDP Growth dataset</a>: <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/president_gdp.csv">president_gdp.csv</a>.</p>
<p>The regression problem will be to predict annualized percentage GDP growth from presidential party.</p>
<hr>
<h2>R</h2>
<p><a href="https://www.r-project.org/">R</a> is made for problems such as this, with an API that makes it quite easy:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; model = lm(growth ~ party, data)
&gt; predict(model, data.frame(party=c('R', 'D')))
##        1        2
## 2.544444 4.332857</code></pre>

<p>The dataset is very small, and we won't introduce a train/test split. Linear regression is just a way of calculating means: we expect our model to predict the mean GDP growth conditional on party. Annual GDP growth during Republican presidents has been about 2.5%, and during Democratic presidents about 4.3%.</p>
<hr>
<h2>sklearn</h2>
<p>Moving into Python, let's first read in the data and get it ready, using <a href="http://www.numpy.org/">NumPy</a> and <a href="http://pandas.pydata.org/">Pandas</a>.</p>
<pre><code class="language-python">import numpy as np
import pandas as pd

data = pd.read_csv('president_gdp.csv')
party = data.party == 'D'
party = np.expand_dims(party, axis=1)
growth = data.growth</code></pre>

<p>With R, we relied on automatic handling of categorical variables. Here we explicitly change the strings 'R' and 'D' to be usable in a model: Boolean values will become zeros and ones. We also adjust the <code>party</code> data shape to be one row per observation.</p>
<p>Tracking <a href="/20170321-various_tensorflow_apis_for_python/">TensorFlow Python APIs</a>, the Estimator API comes from TF Learn, which is inspired by <a href="http://scikit-learn.org/">scikit-learn</a>. Here's the regression with scikit:</p>
<pre><code class="language-python">import sklearn.linear_model

model = sklearn.linear_model.LinearRegression()
model.fit(X=party, y=growth)
model.predict([[0], [1]])
## array([ 2.54444444,  4.33285714])</code></pre>

<hr>
<h2>TensorFlow</h2>
<p>This will abuse the API a little to maximize comparability to the examples above; you'll see warnings when you run the code, which will be addressed in the next section.</p>
<pre><code class="language-python">import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col])</code></pre>

<p>Unlike with scikit, we need to specify the structure of our regressors when we instantiate the model object. This is done with FeatureColumns. There are several <a href="https://www.tensorflow.org/tutorials/wide#selecting_and_engineering_features_for_the_model">options</a>; <code>real_valued_column</code> is probably the simplest but others are useful for general categorical data, etc.</p>
<p>We're providing that data as a simple matrix, so it's important that we use the empty string <code>''</code> for <code>column_name</code>. If there is a substantial <code>column_name</code>, we'll have to provide data in dictionaries with column names as keys.</p>
<pre><code class="language-python">model.fit(x=party, y=growth, steps=1000)
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>TensorFlow needs to be told how many steps of gradient descent to run, or it will keep going indefinitely, without additional configuration. A thousand iterations gets very close to the results achieved with R and with scikit.</p>
<p>There are a lot of things that <code>LinearRegressor</code> takes care of. In this code, we did not have to explicitly:</p>
<ul>
<li>Create any TensorFlow variables.</li>
<li>Create any Tensorflow ops.</li>
<li>Choose an optimizer or learning rate.</li>
<li>Create a TensorFlow session.</li>
<li>Run ops in a session.</li>
</ul>
<p>This API also does a lot more than the R or scikit examples above, and allows for even more extensions.</p>
<hr>
<h2>TensorFlow Extensions</h2>
<p>The Estimator API does a lot by default, and allows for a lot more optionally.</p>
<p>First, there is a <code>model_dir</code>. Above, TensorFlow automatically used a temporary directory. It's nicer to explicitly choose a <code>model_dir</code>.</p>
<pre><code class="language-python">model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')</code></pre>

<p>The <code>model_dir</code> is used for two main purposes:</p>
<ul>
<li>Saving TensorBoard summaries (log info)</li>
<li>Saving model checkpoints</li>
</ul>
<h3>Automatic TensorBoard</h3>
<p><a href="/20170430-tensorflows_queuerunner/">Like</a> an <code>input_producer</code>, an Estimator automatically writes information for TensorBoard. To check them out, point TensorBoard at the <code>model_dir</code> and browse to <code>localhost:6006</code>.</p>
<pre><code class="language-bash">$ tensorboard --logdir tflinreg</code></pre>

<p>For the example above, we get the model graph and two scalar summaries.</p>
<p>Here's what was was constructed in the TensorFlow graph for our <code>LinearRegressor</code>:</p>
<p><img alt="graph" src="img/graph.png"></p>
<p>In the scalar summaries, we get a measure of how fast the training process was running, in global steps per second:</p>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<p>The variation in speed shown here is not particularly meaningful.</p>
<p>And we get the training loss:</p>
<p><img alt="loss" src="img/loss.png"></p>
<p>We didn't really need to train for a full thousand steps.</p>
<p>By default, summaries are generated every 100 steps, but this can be set via <code>save_summary_steps</code> in a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>, along with several other settings.</p>
<p>Further customization, with support for additional metrics, validation on separate data, and even automatic early stopping, is available with <a href="https://www.tensorflow.org/get_started/monitors">ValidationMonitor</a>.</p>
<h3>Automatic Model Save/Restore</h3>
<p>After training for 1,000 steps above, TensorFlow saved the model to the <code>model_dir</code>. If we point to the same <code>model_dir</code> again in a new Python session, the model will be automatically restored from that checkpoint.</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>For more control over how often and when checkpoints are saved, see <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>.</p>
<h3>Using input functions</h3>
<p>Above, training data was provided via <code>x</code> and <code>y</code> arguments, which is like how scikit works, but not really what TensorFlow Estimators should use.</p>
<p>The appropriate mechanism is to make an <a href="https://www.tensorflow.org/get_started/input_fn">input function</a> that returns the equivalents to <code>x</code> and <code>y</code> when called. The function is passed as the <code>input_fn</code> argument to <code>model.fit()</code>, for example.</p>
<p>This approach is flexible and makes it easy to avoid, for example, keeping track of separate data structures for data and labels.</p>
<h3>Distributed Training</h3>
<p>Among the <code>tf.contrib.learn</code> <a href="https://www.tensorflow.org/api_guides/python/contrib.learn">goodies</a> is <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"><code>tf.contrib.learn.Experiment</code></a>, which works with an Estimator to help do distributed training. It looks like this one is still settling down, with a lot of deprecated bits at the moment. I'm interested to see more about this. For now, you could check out a Google Cloud ML <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/iris/trainer/task.py">example</a> that works with <code>learn_runner</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</link>
<guid>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</guid>
<pubDate>Sat, 06 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Dataset: US Presidential Party and GDP Growth</title>
<description><![CDATA[

<p>Here's a simple <a href="president_gdp.csv">dataset</a> for use in examples. It's taken from the <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">online appendix</a> to <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20140913">Presidents and the US Economy: An Econometric Exploration</a> by Blinder and Watson.</p>
<p><a href="president_gdp.csv"><code>president_gdp.csv</code></a></p>
<p>The fields are:</p>
<ul>
<li><code>term</code>: A short text description of the presidential term, like "Reagan 2".</li>
<li><code>party</code>: The political party of the presidency, either "D" for the <a href="https://en.wikipedia.org/wiki/Democratic_Party_(United_States)">Democratic Party</a> or "R" for the <a href="https://en.wikipedia.org/wiki/Republican_Party_(United_States)">Republican Party</a>.</li>
<li><code>growth</code>: The average annualized growth in US Gross Domestic Product (<a href="https://en.wikipedia.org/wiki/Gross_domestic_product">GDP</a>) for that presidential term, expressed as percentage points.</li>
</ul>
<p>For more details, please see the <a href="http://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.20140913">paper</a> and <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">appendix</a>.</p>
<p>The only changes I've made are to order the data chronologically and put it in the convenient CSV format.</p>
<p>For example, in <a href="https://www.r-project.org/">R</a>, you can do the following:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; lm(growth ~ party, data)
&gt; t.test(growth ~ party, data)</code></pre>

<p>Because the <a href="president_gdp.csv">dataset</a> is so small, I'll also display it as spaced-out text here:</p>
<pre><code>term,             party,  growth
Truman,               D,    6.57
Eisenhower 1,         R,    2.72
Eisenhower 2,         R,    2.26
Kennedy-Johnson,      D,    5.74
Johnson 2,            D,    4.95
Nixon 1,              R,    3.57
Nixon-Ford,           R,    1.97
Carter,               D,    3.56
Reagan 1,             R,    3.12
Reagan 2,             R,    3.89
G.H.W. Bush,          R,    2.05
Clinton 1,            D,    3.53
Clinton 2,            D,    4.00
G.W. Bush 1,          R,    2.78
G.W. Bush 2,          R,    0.54
Obama 1,              D,    1.98</code></pre>    
    ]]></description>
<link>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</link>
<guid>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</guid>
<pubDate>Fri, 05 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Canned Models with Keras in TensorFlow</title>
<description><![CDATA[

<p>With TensorFlow 1.1, <a href="https://github.com/fchollet/keras">Keras</a> is now at <code>tf.contrib.keras</code>. With TensorFlow 1.2, it'll be at <code>tf.keras</code>. This is great for making new models, but we also get canned models previously found <a href="https://github.com/fchollet/deep-learning-models">outside core Keras</a>. It's so easy to classify images!</p>
<pre><code class="language-python">import tensorflow as tf

model = tf.contrib.keras.applications.ResNet50()</code></pre>

<p>This will automatically download trained weights for a model based on <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>. The weights are cached below your home directory, in <code>~/.keras/models/</code>.</p>
<p>Convenient image tools are also included. Let's use an <a href="https://github.com/ajschumacher/imagen/blob/master/imagen/n01882714_4157_koala_bear.jpg">image</a> of a koala from the <a href="https://github.com/ajschumacher/imagen">imagen</a> ImageNet subset.</p>
<p><img alt="original koala" src="n01882714_4157_koala_bear.jpg"></p>
<pre><code class="language-python">filename = 'n01882714_4157_koala_bear.jpg'
image = tf.contrib.keras.preprocessing.image.load_img(
    filename, target_size=(224, 224))</code></pre>

<p>This model can take input images that are 224 pixels on a side, so we have to make our image that size. We're just doing it by squishing, in this case.</p>
<p><img alt="smaller koala" src="smaller_koala.jpg"></p>
<p>We'll make that into an array that the model can take as input.</p>
<pre><code class="language-python">import numpy as np

array = tf.contrib.keras.preprocessing.image.img_to_array(image)
array = np.expand_dims(array, axis=0)</code></pre>

<p>Now we can classify the image!</p>
<pre><code class="language-python">probabilities = model.predict(array)</code></pre>

<p>We have one thousand probabilities, one for each class the model knows about. To interpret the result, we can use another helpful function.</p>
<pre><code class="language-python">tf.contrib.keras.applications.resnet50.decode_predictions(probabilities)
## [[(u'n01882714', u'koala', 0.99466419),
##   (u'n02497673', u'Madagascar_cat', 0.0013330306),
##   (u'n01877812', u'wallaby', 0.00085774728),
##   (u'n02137549', u'mongoose', 0.00063530984),
##   (u'n02123045', u'tabby', 0.00056512095)]]</code></pre>

<p>Great success! The model is highly confident that it's looking at a koala. Not bad.</p>
<p>It's pretty fun that this kind of super-easy access to quite good pre-trained models is now available all within the TensorFlow package. Just <code>pip install</code> and go!</p>
<hr>
<p>The thousand ImageNet categories this model knows about include some things that are commonly associated with people, but not a "person" class. Still, just for fun, what will <code>ResNet50</code> say about me?</p>
<pre><code class="language-python">## [[(u'n02883205', u'bow_tie', 0.3144455),
##   (u'n03787032', u'mortarboard', 0.059674311),
##   (u'n02992529', u'cellular_telephone', 0.049916871),
##   (u'n04357314', u'sunscreen', 0.048197504),
##   (u'n04350905', u'suit', 0.03481029)]]</code></pre>

<p>I guess I'll take it?</p>
<p><img alt="Aaron" src="aaron.jpg"></p>
<hr>
<p><strong>Notes:</strong></p>
<p>The model may have been trained on the very koala picture we're testing it with. I'm okay with that. Feel free to test your own koala pictures!</p>
<p>There's also another function, <code>resnet50.preprocess_input</code>, which in theory should help the model work better, but my tests gave seemingly worse results when using that pre-processing. It would be used like this:</p>
<pre><code class="language-python">array = tf.contrib.keras.applications.resnet50.preprocess_input(array)</code></pre>

<p>Keras in TensorFlow also contains <code>vgg16</code>, <code>vgg19</code>, <code>inception_v3</code>, and <code>xception</code> models as well, along the same lines as <code>resnet50</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</link>
<guid>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</guid>
<pubDate>Tue, 02 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling ImageNet</title>
<description><![CDATA[

<p><a href="http://image-net.org/">ImageNet</a> is a standard image dataset. It's pretty big; just the IDs and URLs of the images take over a gigabyte of text. I collected a fun <a href="https://github.com/ajschumacher/imagen">sampling</a> for small-scale purposes.</p>
<hr>
<p>ImageNet is distributed primarily as a text file of <a href="http://image-net.org/download-imageurls">image URLs</a>. The compressed file is 334 megabytes. The unpacked file is 1.1 gigabytes.</p>
<pre><code class="language-bash">$ wget http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz
$ tar zxvf imagenet_fall11_urls.tgz
$ wc fall11_urls.txt
##  14197122 28414665 1134662781 fall11_urls.txt
$ head -3 fall11_urls.txt
## n00004475_6590   http://farm4.static.flickr.com/3175/2737866473_7958dc8760.jpg
## n00004475_15899  http://farm4.static.flickr.com/3276/2875184020_9944005d0d.jpg
## n00004475_32312  http://farm3.static.flickr.com/2531/4094333885_e8462a8338.jpg</code></pre>

<p>The first field is an image ID. The part before the underscore is a WordNet ID, so the first image is of <code>n00004475</code>. What's that?</p>
<p>The mapping from WordNet ID to a brief text label can be downloaded from a link on the ImageNet <a href="http://image-net.org/download-API">API page</a>.</p>
<pre><code class="language-bash">$ wget http://image-net.org/archive/words.txt
$ wc words.txt
##   82114  302059 2655750 words.txt
$ head -3 words.txt
## n00001740   entity
## n00001930   physical entity
## n00002137   abstraction, abstract entity</code></pre>

<p>There are 82,114 WordNet IDs. Now we can decode the one we're interested in.</p>
<pre><code class="language-bash">$ grep n00004475 words.txt
## n00004475    organism, being</code></pre>

<p>So the first picture in ImageNet is of an "organism, being". What does such a thing look like?</p>
<p><img alt="organism, being" src="img/n00004475_6590.jpg"></p>
<p>There are eight examples of "organism, being" and two of the others are cats.</p>
<p>I think 82,114 categories is too many to try to sample randomly from, for my purposes. I'll use the <a href="http://image-net.org/challenges/LSVRC/2017/browse-det-synsets">200 categories</a> specified for the <a href="http://image-net.org/challenges/LSVRC/2017/">ILSVRC2017</a> object detection <a href="http://image-net.org/challenges/LSVRC/2017/#det">challenge</a>.</p>
<pre><code class="language-bash">wget -O 200words.html http://image-net.org/challenges/LSVRC/2017/browse-det-synsets</code></pre>

<p>I used Emacs to pull out the 200 WordNet IDs and convenient extra-short descriptions, saved in <a href="200words.csv">200words.csv</a>. The script <a href="make_urls_subset.py">make_urls_subset.py</a> produces <a href="200words100urls.csv">200words100urls.csv</a> with 100 random URLs for each of the categories. Finally, <a href="get_fives.py">get_fives.py</a> downloads five working JPGs for each category. A couple came back with "missing" images, so I manually replaced those with others from the list.</p>
<p>The results are packaged up on <a href="https://github.com/">GitHub</a> at <a href="https://github.com/ajschumacher/imagen">ajschumacher/imagen</a> and feature such beauties as <a href="img/n02118333_27_fox.jpg">n02118333_27_fox.jpg</a>.</p>
<p><img alt="n02118333_27_fox.jpg" src="img/n02118333_27_fox.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20170430-sampling_imagenet/</link>
<guid>http://planspace.org/20170430-sampling_imagenet/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow's QueueRunner</title>
<description><![CDATA[

<p>A TensorFlow <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a> helps to feed a TensorFlow <a href="http://planspace.org/20170327-tensorflow_and_queues/">queue</a> using threads which are optionally managed with a TensorFlow <a href="http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/">Coordinator</a>. QueueRunner objects can be used directly, or via higher-level APIs, which also offer automatic TensorBoard summaries.</p>
<pre><code class="language-python">import tensorflow as tf
session = tf.Session()</code></pre>

<hr>
<h2>QueueRunner Directly</h2>
<p>To use a QueueRunner you need a TensorFlow queue and an op that puts a new thing in the queue every time that op is evaluated. Typically, such an op will itself involve a queue, which is a bit of a tease. To avoid that circularity, this example will use random numbers.</p>
<pre><code class="language-python">queue = tf.FIFOQueue(capacity=10, dtypes=[tf.float32])
random_value_to_enqueue = tf.random_normal([])  # shape=[] means a single value
enqueue_op = queue.enqueue(random_value_to_enqueue)
random_value_from_queue = queue.dequeue()</code></pre>

<p>At this point if you evaluate <code>random_value_from_queue</code> in the session it will block, because nothing has been put in the queue yet.</p>
<pre><code class="language-python">queue_runner = tf.train.QueueRunner(queue, [enqueue_op])</code></pre>

<p>Still nothing has been enqueued, but <code>queue_runner</code> stands ready to make threads that will do the enqueuing.</p>
<p>If you put more enqueue ops in the list, or the same one multiple times, more threads will be started when things get going.</p>
<pre><code class="language-python">coordinator = tf.train.Coordinator()
threads = queue_runner.create_threads(session, coord=coordinator, start=True)</code></pre>

<p>Using <code>start=True</code> means we won't have to call <code>.start()</code> for each thread ourselves.</p>
<pre><code class="language-python">&gt;&gt;&gt; len(threads)
## 2</code></pre>

<p>There are two threads running: one for handling coordinated shutdown, and one for the enqueue op.</p>
<p>Now at last we can get at random values from the queue!</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(random_value_from_queue)
## 0.69283932
&gt;&gt;&gt; session.run(random_value_from_queue)
## -0.53802371</code></pre>

<p>The feeding thread will try to keep the queue at capacity, which was set to 10, so there should always be more items available to dequeue.</p>
<p>Since we used a coordinator, we can shut the threads down nicely.</p>
<pre><code class="language-python">coordinator.request_stop()
coordinator.join(threads)</code></pre>

<hr>
<h2>QueueRunner with Higher-Level APIs</h2>
<p>It's possible to work with QueueRunner directly, as shown above, but it's easier to use higher-level TensorFlow APIs that themselves use QueueRunner.</p>
<p>It's common for TensorFlow queue-chains to start with a list of filenames (sometimes a list of just one filename) to read data from. The <code>string_input_producer</code> function makes a queue using provided strings.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'])
letter_from_queue = queue.dequeue()</code></pre>

<p>This is a <code>FIFOQueue</code>, just as before, but notice we don't have an enqueue op for it. Like many things in <code>tf.train</code>, here TensorFlow has already done some work for us. A QueueRunner has already been made, and it was added to the <code>queue_runners</code> collection.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.get_collection('queue_runners')
## [&lt;tensorflow.python.training.queue_runner_impl.QueueRunner object at 0x121ee2c10&gt;]</code></pre>

<p>You could access and run that QueueRunner directly, but <code>tf.train</code> makes things easier.</p>
<!--

Finally, here's a place where it would be useful to be
doing a session context manager!

-->

<pre><code class="language-python">coordinator.clear_stop()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p><code>tf.train.start_queue_runners</code> automatically starts the threads.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'</code></pre>

<p>By default, the queue will go through the original items multiple times, or multiple epochs, and shuffle the order of strings within an epoch.</p>
<p>Limiting the number of epochs uses a local variable, which must be initialized.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, num_epochs=1)
letter_from_queue = queue.dequeue()
initializer = tf.local_variables_initializer()
session.run(initializer)
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p>Now the QueueRunner will close the queue when there isn't anything more to put in it, so the dequeue op will eventually give an <code>OutOfRangeError</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## OutOfRangeError</code></pre>

<hr>
<h2>Automatic TensorBoard Summaries</h2>
<p>A single-epoch queue will be helpful for illustrating an interesting thing about <code>tf.train.string_input_producer</code>: it automatically adds a TensorBoard summary to the graph.</p>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<p>It's nice to have direct control over every detail of your program, but the conveniences of higher-level APIs are also pretty nice. The summary that gets added is a scalar summary representing the percent full that the queue is.</p>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg"></p>
<pre><code class="language-python">tf.reset_default_graph()  # Starting queue runners will fail if a queue is
session = tf.Session()    # closed, so we need to clear things out.
queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, capacity=4, num_epochs=1)
letter_from_queue = queue.dequeue()
summaries = tf.summary.merge_all()
summary_writer = tf.summary.FileWriter('logs')
initializer = tf.local_variables_initializer()
session.run(initializer)
coordinator = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)
for i in range(4):
    summary_writer.add_summary(session.run(summaries), i)
    session.run(letter_from_queue)
summary_writer.add_summary(session.run(summaries), 4)
summary_writer.flush()  # Ensure everything is written out.</code></pre>

<p>After opening up TensorBoard with <code>tensorboard --logdir=logs</code> and going to <code>http://localhost:6006/</code> and and turning off plot smoothing, you'll see this:</p>
<p><img alt="log" src="img/fraction_full.png"></p>
<p>This shows that the queue, with capacity four, started 100% full and then every time something was dequeued from it it became 25 percentage points less full until it was empty.</p>
<p>For this example, the queue wasn't being refilled, so we knew it would become less and less full. But if you're reading data into an input queue that you expect to keep full, it's a great diagnostic to be able to check how full it actually is while things are running, to find find out if loading data is a bottleneck.</p>
<p>The automatic TensorBoard logging here is also a nice first taste of all the things that happen with even higher-level TensorFlow APIs.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170430-tensorflows_queuerunner/</link>
<guid>http://planspace.org/20170430-tensorflows_queuerunner/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>From Behaviorist to Constructivist AI</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/B._F._Skinner">B. F. Skinner</a> might be satisfied that neural networks achieve intelligence when they perform tasks well. This behaviorist perspective leads to misunderstandings of current technology and limits development toward systems that think. Pervasive epistemological confusion about categories is one example. In general, a constructivist approach will become necessary for advanced machine learning and artificial intelligence.</p>
<p><img alt='bird in operant conditioning chamber or "Skinner box"' src="img/skinner_box.jpg"></p>
<p>Training a neural network by <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagating</a> from a loss function is a lot like <a href="https://en.wikipedia.org/wiki/Operant_conditioning">operant conditioning</a>. Error becomes punishment. The objective and result of eliciting particular behavior is the same whether you're doing object detection <a href="https://motherboard.vice.com/en_us/article/america-secretly-tried-to-destroy-totalitarianism-with-pigeons">with a pigeon</a> in a <a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber">Skinner box</a> or with a convolutional neural network.</p>
<p>Little could be less like real intelligence than blurting out a name for every object you see. Little could better epitomize behaviorist stimulus-response. This is the intelligence of the <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> classifiers that popularized deep learning.</p>
<p>These classifiers really shouldn't be anthropomorphized. A cat/dog classifier is not thinking about cats and dogs. An engineer designed the network with a cat neuron and a dog neuron, and got them to light them up as desired.</p>
<p>An image classifier has continuous input and categorical output. The categories are specified by design. This is clearly a limitation on the output side, and a very different limitation from any constraint on the input side in image resolution or color space. A cat/dog classifier cannot say anything other than cat and dog.</p>
<p>One could argue that this output is not strictly categorical because it might be read, for example, as 95% cat and 5% dog, but this does not undo the designed categories, and this sort of non-<a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac</a> enhancement is not generally provided when systems take categorical input.</p>
<p>Language models are frequently categorically constrained in both input and output. At the word level, this means a model can't deal with words it's never seen before. This leads to approaches like <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">Google's neural machine translation</a> falling back to <a href="https://arxiv.org/abs/1508.07909">subword units</a> for rare words. But even if a language model goes to the character level, this is still a categorical constraint, and a system trained on "e" could be perfectly blind to "&#233;."</p>
<p>Whether categories are imposed on the input or output side, they make it obvious that the system is limited. The system cannot handle categories not specified in the design.</p>
<p>Categorical input is also foreign to humans. It would be like having a separate sense for every category. Instead of feeling "warm" or "not-warm", for instance, you could feel "cat" or "not-cat" and "dog" or "not-dog," and however many more. But if you didn't have a sense for "aardvark," you would be congenitally blind to "aardvark."</p>
<p>Continuous sensor data, like images and audio, is much more interesting and analogous to the human experience. There are still limitations - for example, you don't really see the edges of your field of view, and you can't really imagine what sensing magnetism would be like - but unstructured input provides a starting point for forming gestalt perceptions.</p>
<p><img height="240" alt="ouija board" src="img/ouija_board.jpeg"></p>
<p><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a>, or word vectors, might seem to handle the categorical problem, but this is largely misdirection. Word embeddings are representations of words that have fixed dimensionality, so that a language system no longer needs a separate input for every possible word, but only a separate input for every dimension of the word embeddings. There might be 50,000 words, but only 200 or 300 dimensions for an embedding.</p>
<p>Using word embeddings doesn't solve the categorical problem; it just pushes the problem to the embeddings. A system can still only handle words that embeddings have been generated for.</p>
<p>Word embeddings are useful in that they are representations of words that tend to give good results when used as input to various algorithms. In the same way, convolutional classifiers learn representations of images that tend to give good results when used as input to other algorithms. In both cases, having a good representation is useful. In both cases, people may or may not find the representations interpretable.</p>
<p>There tends to be <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/3/yoshua-bengio-machines-that-dream">excitement</a> about the meaningfulness of word embeddings when people amuse themselves with arithmetic like <em>king - man + woman = queen</em>, or make visualizations with reasonable-seeming clusters. But the understanding that's happening here is happening in the people; having nice representations does not mean that a system has achieved understanding.</p>
<p>It seems important that an intelligent system should be able to develop internal concepts without those concepts being built into the system's design, so there was interest when Google <a href="https://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html">found</a> a cat neuron in a 2011 system.</p>
<p><img height="240" src="img/google_cat.jpg" alt="Google cat neuron"></p>
<p>The system was trained to take an image as input and generate the same image as output. This is easy for a computer to do just by copying, so to make it interesting you have to put restrictions on the flow of information from input to output. The restricted system learns good representations for the images. Then, by testing lots of images with and without cats, Google found one point in the system that tended to respond positively to cats and negatively to other things.</p>
<p>The temptation is to declare that the system formed an idea of cats. You could just as well say that a mold formed an idea of its cast.</p>
<p>Visual systems learn many useful internal representations. For example, they learn oriented edge detectors. Humans have these too. It should be clear that in neither case is there an idea of an oriented edge. The Google researchers were correct in their <a href="https://arxiv.org/abs/1112.6209">paper</a>'s title when they said that their system learned high-level features rather than concepts.</p>
<p>More recently, <a href="https://openai.com/">OpenAI</a>'s <a href="https://blog.openai.com/unsupervised-sentiment-neuron/">unsupervised sentiment neuron</a> is another case of humans interpreting neurons. The model is categorical at input and output on the character level, learning to predict the next character in text. OpenAI used all 4,096 dimensions of their learned representation, but their title comes from noticing that just one of those dimensions captured a lot of sentiment-related information.</p>
<p>Word embeddings are patterns of activation over perhaps 300 neurons. They are <a href="http://www.bcp.psych.ualberta.ca/~mike/Pearl_Street/Dictionary/contents/D/distributed.html">distributed representations</a>. A cat neuron or sentiment neuron, on the other hand, is in line with the implausible "<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">unit/value principle</a>" that a single neuron represents a single concept.</p>
<p>It is easy to think that words <em>are</em> categories, and so a distributed representation of a word is a distributed representation of a category. With images, it's clear that one picture of a cat is different from another picture of a cat. But words are not categories. The difference between words and images is principally one of cardinality (there are many more images than words) and composibility (images more easily contain many things). But just as a word vector for <em>cat</em> is close to a word vector for <em>kitty</em>, distributed representations for pictures of cats should also be close to one another, and the Google system could just as well have been mined for distributed representations. Perhaps it is the human desire to categorize that makes us comfortable with multi-dimensional representations when we've provided categories in advance, but look for single-dimensional representations when we haven't. (Or maybe it's just easier.)</p>
<p>Regardless of whether distributed or unit representations are better, having a representation does not imply thought. These representations flash through their networks, coming before the result like Pavlovian slobber. This is not to say these representations couldn't be used in a system that thinks, but that current usage is too limited. One thing that's missing is state that develops over time.</p>
<p>Sequence models (such as the sentiment neuron example) introduce a limited kind of time-awareness, and a kind of memory. There could be something here (<a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/9/ilya-sutskever-unsupervised-learning-attention-and-other-mysteries">attention</a> in particular is interesting) but most <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/10/oriol-vinyals-sequence-to-sequence-machine-learning">usage</a> still seems to be learning representations and doing encoding to and decoding from these representations.</p>
<p>To be clear: Good representations are useful, whether or not they are utilized for anything like higher-level thought. But it seems unlikely that conventional models used in supervised or unsupervised learning will spontaneously invent higher-level thought, no matter how good their representations are.</p>
<p>By its name, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> seems purely behavioristic, but it also recognizes the idea of internal models, as illustrated in <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>'s taxonomy of reinforcement learning agents. These models are generally something like an agent's internal conception of the world around it.</p>
<p><img alt="taxonomy of reinforcement learning agents" src="img/rl_taxonomy.png"></p>
<p>Lots of reinforcement learning is model-free, and where there are internal models they are often heavily specified in advance or quite distant from what we think of as mental models. The idea, if not current implementations, is key. The behaviorist perspective is one of only inputs and outputs. Model-based reinforcement learning suggests, at least in spirit, a missing piece. Humans certainly aren't model-free, for example.</p>
<p>What the constructivist perspective adds is that it isn't enough to simply have an internal model. Intelligence entails building and working with new models as a part of problem-solving.</p>
<p>One view of building different models for different situations might be selectively enlisting parts of a large system for a given task. <a href="https://arxiv.org/abs/1701.08734">PathNet</a> tries to "discover which parts of the network to re-use for new tasks." This is interesting, but the focus on selecting a subset of wiring seems distant from the imaginative process of building a mental model, likely drawing on representations which may be distributed.</p>
<p>It becomes important to understand how a representation behaves. Say a system does have a cat neuron; can it reason about cats? The cat neuron can be excited, but this seems more like experiencing the <a href="https://en.wikipedia.org/wiki/Qualia">quale</a> of cat-ness than like imagining a cat. Experiencing an emotion is even more clearly not an abstraction, so the case of a sentiment neuron makes this distinction even clearer.</p>
<p>Imagining a cat might be an algebraic operation, in the sense that it posits an entity, a variable, which has cat properties or is a cat. In <a href="https://mitpress.mit.edu/books/algebraic-mind">The Algebraic Mind</a>, Gary Marcus argues that connectionist (neural network) models lack this kind of kind of ability.</p>
<p><img alt="The Algebraic Mind" src="img/algebraic_mind_cover.jpg"></p>
<p>An intelligent system should be able not only to represent things but to build and manipulate models composed from these representations.</p>
<p>For example, whether you like Chomsky or not, understanding a sentence seems like an algebraic procedure in the sense of apprehending values for variables like subject, verb, and object. The plug and play composability of noun phrases and the like also suggests a constructive mental process.</p>
<p>Or take the example of number: can a system perceive that a picture has three cats, as opposed to two cats? There's some depth here, as a system could represent two or three entities all of which are cats individually, or it could represent number concepts explicitly. It's hard to find current models that do either in a meaningful way.</p>
<p>The Algebraic Mind was published in 2001. There have been many advances in machine learning since 2001, but they largely haven't been advances toward reasoning. In this sense, Marcus thinks artificial intelligence is <a href="https://www.technologyreview.com/s/603945/is-artificial-intelligence-stuck-in-a-rut/">in a rut</a>.</p>
<p>If you take the view that machine learning is a type of statistics, then you may not care about any of this. But for machine learning as artificial intelligence, it may be that algebraic (or symbolic) considerations will be necessary.</p>
<p>A fair criticism is that if it isn't clear how to make direct progress in the constructivist direction, time is better spent advancing what have become traditional techniques. There is certainly value in this kind of advancement.</p>
<p>There may also be some reason for hope in <a href="https://deepmind.com/blog/differentiable-neural-computers/">differentiable neural computers</a> (DNCs) and related work. In some ways, the linking of DNC memory locations is like Marcus's proposal for representing structured data, and the DNC memory itself is something like Marcus's idea of registers. But it looks like DNCs work with categorical input and output, relying on it for seemingly algebraic task performance.</p>
<p>It seems likely that artificial general intelligence will use a composite approach. It may process visual input with convolutions. It may use distributed representations for internal concepts. It may access memory along the lines of a differentiable neural computer. It may have an attention mechanism that allows it to focus on the state of the external world one moment and its internal world the next. Combining all these ideas into a system that can come up with its own ideas is an intriguing project.</p>
<blockquote>
<p>&#8220;There is no reason, as yet, to be confident that an intermediate symbolic representation will not be required for modeling higher cognitive processes.&#8221; (<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">Feldman and Ballard, 1982</a>)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</link>
<guid>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</guid>
<pubDate>Sat, 29 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Everything in the Graph? Even Glob?</title>
<description><![CDATA[

<p>Programming with TensorFlow is largely building TensorFlow graphs. It can be like using Python to write a program for another computer: the TensorFlow runtime. If you want all your computation to be in the graph, you need to be able to express everything with TensorFlow ops.</p>
<p>It's not necessarily bad to do work outside the TensorFlow graph. Maybe there's a package that does exactly what you need for one part of your program. It could make sense to use TensorFlow for some things, and then pull off the graph and use the other package.</p>
<p>Advantages of staying inside the graph might include better performance, as you aren't moving data into and out of the TensorFlow runtime's internal formats. And if you're developing in Python but want to take your graph and ship it using TensorFlow serving, or perhaps use another language, anything outside the graph will need special attention.</p>
<p>The desire to stay in-graph leads to lots of ops existing that you might not think are essential for a machine learning library. This includes functionality for file formats like PNG and JPEG, queues and queue management, and even the lowly <code>glob</code>.</p>
<p><a href="https://en.wikipedia.org/wiki/Glob_(programming)"><code>glob</code></a> refers to patterns used for matching filenames. For example, in a shell, you might use a <code>glob</code> to list all the text files in a directory.</p>
<pre><code class="language-bash">$ ls *.txt
## one.txt        two.txt</code></pre>

<p>In Python, the standard library includes <a href="https://docs.python.org/3/library/glob.html"><code>glob</code></a> functionality.</p>
<pre><code class="language-python">&gt;&gt;&gt; import glob
&gt;&gt;&gt; glob.glob('*.txt')
## ['one.txt', 'two.txt']</code></pre>

<p>TensorFlow has a similar operation: <a href="https://www.tensorflow.org/api_docs/python/tf/matching_files"><code>tf.matching_files</code></a>.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; glob = tf.matching_files('*.txt')
&gt;&gt;&gt; session.run(glob)
## array(['./one.txt', './two.txt'], dtype=object)</code></pre>

<p>TensorFlow's globbing is limited in that it doesn't support wildcards in directory names, and it doesn't support the recursive globbing found in the <code>glob</code> of Python 3.5+.</p>
<p>Every time a <code>tf.matching_files</code> op is evaluated, it goes and reads filenames from disk. What's often seen instead is <a href="https://www.tensorflow.org/api_docs/python/tf/train/match_filenames_once"><code>tf.train.match_filenames_once</code></a>, which introduces an extra level of lazy evaluation in that it will execute <code>tf.matching_files</code> the first time it's evaluated, and then it caches the results, returning the same list of files when evaluated again even if the files on disk become different. Because this relies on a local variable for the cache (<a href="https://github.com/tensorflow/tensorflow/blob/a5b1fb8e56ceda0ee2794ee05f5a7642157875c5/tensorflow/python/training/input.py#L55">source</a>) you'll have to initialize.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; glob = tf.train.match_filenames_once('*.txt')
&gt;&gt;&gt; initializer = tf.local_variables_initializer()
&gt;&gt;&gt; session.run(initializer)
&gt;&gt;&gt; session.run(glob)
## array(['./one.txt', './two.txt'], dtype=object)</code></pre>

<p>With functionality like this, TensorFlow really subsumes a lot of things that could at least in theory be handled separately. The choice of whether to put any particular piece of computation into the TensorFlow graph or not remains up to developer.</p>
<p><img alt="glob" src="glob.png"></p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170428-everything_in_the_graph_even_glob/</link>
<guid>http://planspace.org/20170428-everything_in_the_graph_even_glob/</guid>
<pubDate>Fri, 28 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Tensors and TFRecords</title>
<description><![CDATA[

<p>When a matrix, array, or tensor has lots of values that are zero, it can be called <em>sparse</em>. You might want to represent the zeros implicitly with a <em>sparse representation</em>. TensorFlow has support for this, and the support extends to its TFRecords <code>Example</code> format.</p>
<p>Here is a sparse one-dimensional tensor:</p>
<pre><code class="language-python">[0, 7, 0, 0, 8, 0, 0, 0, 0]</code></pre>

<p>The tensor is sparse, in that it has a lot of zeros, but the representation is dense, in that all those zeros are represented explicitly.</p>
<p>A sparse representation of the same tensor will focus only on the non-zero values.</p>
<pre><code class="language-python">values = [7, 8]</code></pre>

<p>We have to also remember where those values occur, by their indices:</p>
<pre><code class="language-python">indices = [1, 5]</code></pre>

<p>The one-dimensional <code>indices</code> form will work with some methods, for this one-dimensional example, but in general indices have multiple dimensions, so it will be more consistent (and work everywhere) to represent <code>indices</code> like this:</p>
<pre><code class="language-python">indices = [[1], [5]]</code></pre>

<p>With <code>values</code> and <code>indices</code>, we don't have quite enough information yet. How many zeros are there to the right of the last value? We have to represent the dense shape of the tensor.</p>
<pre><code class="language-python">dense_shape = [9]</code></pre>

<p>These three things together, <code>values</code>, <code>indices</code>, and <code>dense_shape</code>, are a sparse representation of the tensor.</p>
<p>TensorFlow accepts lists of values and NumPy arrays to define dense tensors, and it returns NumPy arrays when dense tensors are evaluated. But what to do with sparse tensors? SciPy has <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html">several</a> sparse matrix representations, but not a good match for TensorFlow's general sparse tensor form. So for sparse tensors, instead of reusing an existing Python class, TensorFlow provides <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensorValue"><code>tf.SparseTensorValue</code></a>. These are values that exist outside the TensorFlow graph, so they can be made without a <code>tf.Session</code>, for example.</p>
<pre><code class="language-python">tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[1], [5]], values=[7, 8], dense_shape=[9])</code></pre>

<p>Using <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensor"><code>tf.SparseTensor</code></a> puts that in the TensorFlow graph.</p>
<pre><code class="language-python">tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
## &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x11a4e0c10&gt;</code></pre>

<p>That <code>tf.SparseTensor</code> will be constant, since we specified all the pieces of it, and if you run it in a session, you'll get back the equivalent <code>tf.SparseTensorValue</code>.</p>
<p>TensorFlow has <a href="https://www.tensorflow.org/api_guides/python/sparse_ops">operations</a> specifically for working with sparse tensors, such as <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_matmul"><code>tf.sparse_matmul</code></a>. And you can change a sparse matrix to a dense one with <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_to_dense"><code>tf.sparse_tensor_to_dense</code></a>. These operations live in the graph, so they have to be run to see a result.</p>
<pre><code class="language-python">sparse = tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
dense = tf.sparse_tensor_to_dense(sparse)
session.run(dense)
## array([0, 7, 0, 0, 0, 8, 0, 0, 0], dtype=int32)</code></pre>

<p>Going from dense to sparse seems a little less <a href="http://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow">straightforward</a> at the moment, so let's continue assuming we already have the components of our sparse representation.</p>
<p>Going to more dimensions is quite natural. Here's a two-dimensional tensor with three non-zero values:</p>
<pre><code class="language-python">[[0, 0, 0, 0, 0, 7],
 [0, 5, 0, 0, 0, 0],
 [0, 0, 0, 0, 9, 0],
 [0, 0, 0, 0, 0, 0]]</code></pre>

<p>This can be represented in sparse form as:</p>
<pre><code class="language-python">indices = [[0, 5],
           [1, 1],
           [2, 4]]

values = [7, 5, 9]

dense_shape = [4, 6]

tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[0, 5], [1, 1], [2, 4]], values=[7, 5, 9], dense_shape=[4, 6])</code></pre>

<p>Now, to represent this in a TFRecords <code>Example</code> requires a little bit of transformation. <a href="/20170323-tfrecords_for_humans/">TFRecords</a> only support lists of integers, floats, and bytestrings. The values are easily represented in one <code>Feature</code>, but to represent the <code>indices</code>, each dimension will need its own <code>Feature</code> in the <code>Example</code>. The <code>dense_shape</code> isn't represented at all; that's left to be specified at parsing.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'index_0': tf.train.Feature(int64_list=tf.train.Int64List(value=[0, 1, 2])),
    'index_1': tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 1, 4])),
    'values': tf.train.Feature(int64_list=tf.train.Int64List(value=[7, 5, 9]))
}))
my_example_str = my_example.SerializeToString()</code></pre>

<p>This TFRecord sparse representation can then be <a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">parsed inside the graph</a> as a <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a>.</p>
<pre><code class="language-python">my_example_features = {'sparse': tf.SparseFeature(index_key=['index_0', 'index_1'],
                                                  value_key='values',
                                                  dtype=tf.int64,
                                                  size=[4, 6])}
serialized = tf.placeholder(tf.string)
parsed = tf.parse_single_example(serialized, features=my_example_features)
session.run(parsed, feed_dict={serialized: my_example_str})
## {'sparse': SparseTensorValue(indices=array([[0, 5], [1, 1], [2, 4]]),
##                              values=array([7, 5, 9]),
##                              dense_shape=array([4, 6]))}</code></pre>

<p>Support for multi-dimensional sparse features seems to be new in TensorFlow 1.1, and TensorFlow gives this warning when you use <code>SparseFeature</code>:</p>
<pre><code>WARNING:tensorflow:SparseFeature is a complicated feature config
                   and should only be used after careful consideration
                   of VarLenFeature.</code></pre>

<p><code>VarLenFeature</code> doesn't support real sparsity or multi-dimensionality though; it only supports "ragged edges" as in the case when one example has three elements and the next has seven, for example.</p>
<p>It is a little awkward to put together a sparse representation for TFRecords, but it does give you a lot of flexibility. To put a point on it, I don't know what you can do with a <code>SequenceExample</code> that you can't do with a regular <code>Example</code> using all of <code>FixedLenFeature</code>, <code>VarLenFeature</code>, and <code>SparseFeature</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170427-sparse_tensors_and_tfrecords/</link>
<guid>http://planspace.org/20170427-sparse_tensors_and_tfrecords/</guid>
<pubDate>Thu, 27 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Parsing TFRecords inside the TensorFlow Graph</title>
<description><![CDATA[

<p>You can parse TFRecords using the standard protocol buffer <code>.FromString</code> method, but you can also parse them inside the TensorFlow graph.</p>
<p>The examples here assume you have in memory the serialized Example <code>my_example_str</code> and SequenceExample <code>my_seq_ex_str</code> from <a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a>. You could create them, or read them from <a href="/20170323-tfrecords_for_humans/my_example.tfrecords">my_example.tfrecords</a> and <a href="/20170323-tfrecords_for_humans/my_seq_ex.tfrecords">my_seq_ex.tfrecords</a>. That loading could be via <code>tf.python_io.tf_record_iterator</code> or via <code>tf.TFRecordReader</code> following the pattern shown in <a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a>.</p>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/parse_single_example"><code>tf.parse_single_example</code></a> decoder works like <code>tf.decode_csv</code>: it takes a string of raw data and turns it into structured data, based on the options it's created with. The structured data it turns it into is <em>not</em> a protocol buffer message object, but a dictionary that is hopefully easier to work with.</p>
<pre><code class="language-python">import tensorflow as tf

serialized = tf.placeholder(tf.string)

my_example_features = {'my_ints': tf.FixedLenFeature(shape=[2], dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session = tf.Session()

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': array([5, 6]),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>The <code>shape</code> parameter is part of the schema we're defining. A <code>shape</code> of <code>[]</code> means a single element, so the result returned won't be in an array, as for <code>my_float</code>. The <code>shape</code> of <code>[1]</code> means an array containing one element, like for <code>my_bytes</code>. Within a Feature, things are always listed, so the choice of how to get a single element back out is decided by the choice of <code>shape</code> argument. A <code>shape</code> of <code>[2]</code> means a list of two elements, naturally enough, and there's no alternative.</p>
<p>The <code>dtype=object</code> is how NumPy works with strings.</p>
<p>When some feature might have differing numbers of values across records, they can all be read with <code>tf.VarLenFeature</code>. This distinction is made only when parsing. Records are made with however many values you put in; you don't specify <code>FixedLen</code> or <code>VarLen</code> when you're making an <code>Example</code>. So the <code>my_ints</code> feature just parsed as <code>FixedLen</code> can also be parsed as <code>VarLen</code>.</p>
<pre><code class="language-python">my_example_features = {'my_ints': tf.VarLenFeature(dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': SparseTensorValue(indices=array([[0], [1]]),
##                               values=array([5, 6]),
##                               dense_shape=array([2])),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>When parsing as a <code>VarLenFeature</code>, the result is a sparse representation. This can seem a little silly, because features here will always be dense from left to right. Early versions of TensorFlow <a href="https://github.com/tensorflow/tensorflow/issues/976">didn't</a> have the current behavior. But this sparseness is a mechanism by which TensorFlow can support non-rectangular data, for example when forming batches from multiple variable length features, or as seen next with a <code>SequenceExample</code>:</p>
<pre><code class="language-python">my_context_features = {'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_sequence_features = {'my_ints': tf.VarLenFeature(shape=[2], dtype=tf.int64)}
my_seq_ex = tf.parse_single_sequence_example(
                serialized,
                context_features=my_context_features,
                sequence_features=my_sequence_features)

result = session.run(my_seq_ex, feed_dict={serialized: my_seq_ex_str})
result
## ({'my_bytes': array(['data'], dtype=object)},
##  {'my_ints': SparseTensorValue(
##                  indices=array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 2]]),
##                  values=array([5, 6, 7, 8, 9]),
##                  dense_shape=array([2, 3]))})</code></pre>

<p>The result is a tuple of two dicts: the context data and the sequence data.</p>
<p>Since the <code>my_ints</code> sequence feature is parsed as a <code>VarLenFeature</code>, it's returned as a sparse tensor. This example has to be parsed as a <code>VarLenFeature</code>, because the two entries in <code>my_ints</code> are of different lengths (<code>[5, 6]</code> and <code>[7, 8, 9]</code>).</p>
<p>The way the <code>my_ints</code> values get combined into one sparse tensor is the same as the way it would be done when making a batch from multiple records each containing a <code>VarLenFeature</code>.</p>
<p>To make it clearer what's going on, we can look at the sparse tensor in dense form:</p>
<pre><code class="language-python">session.run(tf.sparse_tensor_to_dense(result[1]['my_ints']))
## array([[5, 6, 0],
##        [7, 8, 9]])</code></pre>

<p>The other option for parsing sequence features is <code>tf.FixedLenSequenceFeature</code>, which will work if each entry of the sequence feature is the same length. The result then is a dense tensor.</p>
<p>To parse multiple <code>Example</code> records in one op, there's <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/parse_example"><code>tf.parse_example</code></a>. This returns a dict with the same keys you'd get from parsing a single <code>Example</code>, with the values combining the values from all the parsed examples, in a batch-like fashion. There isn't a corresponding op for <code>SequenceExample</code> records.</p>
<p>More could be said about sparse tensors and TFRecords. The <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_merge">tf.sparse_merge</a> op is one way to combine sparse tensors, similar to the combination that happened for <code>my_ints</code> in the <code>SequenceExample</code> above. And there's <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a> for parsing out general sparse features directly from TFRecords (better documentation in <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parsing_ops.py">source</a>).</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 26 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>mystery.tfrecords</title>
<description><![CDATA[

<p>Here's a practical puzzle: what's in the file <a href="mystery.tfrecords"><code>mystery.tfrecords</code></a>?</p>
<hr>
<p>You don't need any more extra information specific to that file. Here are a few posts about the format and related issues, some of which might be helpful:</p>
<ul>
<li><a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a></li>
<li><a href="/20170330-tfrecords_via_proto/">TFRecords via Protocol Buffer Definitions</a></li>
<li><a href="/20170403-images_and_tfrecords/">Images and TFRecords</a></li>
<li><a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a></li>
<li><a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">Parsing TFRecords inside the TensorFlow Graph</a></li>
</ul>
<!--

Welcome to hidden additional notes! Finding this is a way of solving
the puzzle, I suppose...

Here's how `mystery.tfrecords` was made:

<pre><code class="language-python">import tensorflow as tf

with open('success.jpg') as f:
    success = f.read()

example = tf.train.Example(features=tf.train.Features(feature={
    'jpg': tf.train.Feature(bytes_list=tf.train.BytesList(value=[success]))
}))

example_str = example.SerializeToString()

with tf.python_io.TFRecordWriter('mystery.tfrecords') as writer:
    writer.write(example_str)</code></pre>

Here's one way to get the contents back out:

<pre><code class="language-python">reader = tf.python_io.tf_record_iterator('mystery.tfrecords')

examples = [tf.train.Example().FromString(example_str)
            for example_str in reader]
# Using `SequenceExample` rather than `Example` also works.

len(examples)  # 1
# So we know there's just one example in there.

example = examples[0]

example.features.feature.keys()  # 'jpg'
# If parsed as a SequenceExample, this would instead be:
# `example.context.feature.keys()`

# It should be clear from the 'jpg' key, but you can also check:
len(example.features.feature['jpg'].int64_list.value)  # 0
len(example.features.feature['jpg'].float_list.value)  # 0

len(example.features.feature['jpg'].bytes_list.value)  # 1

jpg = example.features.feature['jpg'].bytes_list.value[0]

# If you don't trust the key, you can check the magic number:
jpg[:2]  # '\xff\xd8'
# That's the JPG magic number, FFD8.

with open('success.jpg', 'wb') as f:
    f.write(jpg)</code></pre>

Success!

-->

<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170425-mystery.tfrecords/</link>
<guid>http://planspace.org/20170425-mystery.tfrecords/</guid>
<pubDate>Tue, 25 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow as Automatic MPI</title>
<description><![CDATA[

<p>TensorFlow raises the level of abstraction in distributed programs from message-passing to data structures and operations directly on those data structures. The difference is analogous to the difference between programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a> and programming in a high-level language. TensorFlow may not have every possible high-performance feature for cluster computing, but what it offers is compelling.</p>
<hr>
<h2>Message-Passing</h2>
<p>Leaving aside shared filesystems or directly accessed shared memory, systems that run across multiple computers have to communicate by sending messages to one another. A low-level approach might use <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> sockets directly. Some convenience could be obtained by using Remote Procedure Calls (<a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a>).</p>
<p>The Message-Passing Interface (<a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>) was created in the early 1990s to make distributed programming easier within the High Performance Computing (HPC) community. It standardized interfaces like <code>MPI_Send</code> and <code>MPI_Recv</code> to facilitate exchanging data between processes of a distributed system.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Actor_model">actor model</a> also focuses on message-passing, as for example in <a href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">Erlang</a> or with <a href="http://akka.io/">Akka</a> in Java or Scala.</p>
<p>Message-passing gives you fine-grained control over how nodes communicate. You can use message-passing to build up a variety of algorithms and new systems. For example, <a href="http://spark.apache.org/">Spark</a> was built in part with Akka agents for some time, before switching to an RPC-based implementation. TensorFlow also builds its own abstractions using message-passing.</p>
<hr>
<h2>Automatic Message-Passing with TensorFlow</h2>
<p>TensorFlow uses message-passing, but it's largely invisible to TensorFlow users. The TensorFlow API lets you say where data and operations should live, and TensorFlow automatically handles any necessary messaging.</p>
<p>Programming with explicit messages is like the low-level memory shifting necessary when programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a>. For example, here is some assembly pseudo-code:</p>
<pre><code>copy value from position `a` to register 1
copy value from position `b` to register 2
add register 1 and 2
copy result to position `c`</code></pre>

<p>In a high-level language, this could be:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>Similarly, with TensorFlow, you focus on the computation you want performed, and don't worry about how values may need to be moved around to make it happen. A distributed analog to the assembly above might look like this:</p>
<pre><code>send value of `a` from machine A to machine D
send value of `b` from machine B to machine D
add values on machine D
send result to machine C</code></pre>

<p>And with TensorFlow:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>At least, that's the ideal. TensorFlow may still need explicit direction on where things should be placed (<a href="https://www.tensorflow.org/api_docs/python/tf/device">with tf.device</a>) especially when multiple machines are involved. But that direction is more succinct and declarative than the full imperative detail of message-passing.</p>
<hr>
<h2>Faster Messages and Collective Communications</h2>
<p>TensorFlow's user API is essentially message-less, but TensorFlow still uses messages under the hood. The contents of the messages are serialized protocol buffers sent via <a href="http://www.grpc.io/">gRPC</a>, which uses <a href="https://en.wikipedia.org/wiki/HTTP/2">HTTP/2</a>.</p>
<p>A message-based approach can be made faster by using a faster transport, and by using faster algorithms. TensorFlow has some early support for both, but other frameworks may offer more. You may or may not care, as the simplicity of TensorFlow's approach may or may not outweigh possible performance boosts.</p>
<p>One easy transport speedup could come in hardware with NVIDIA's <a href="http://www.nvidia.com/object/nvlink.html">NVLink</a> interconnect, which is faster than <a href="https://en.wikipedia.org/wiki/PCI_Express">PCIe</a>. As far as I can tell NVLink won't require code changes, but it looks like it will be pretty rare: it may be that only some supercomputers (<a href="https://www.olcf.ornl.gov/summit/">Summit</a>, <a href="https://asc.llnl.gov/coral-info">Sierra</a>) will use NVLink, with IBM's <a href="https://en.wikipedia.org/wiki/IBM_POWER_microprocessors">POWER</a> processors.</p>
<p>Some clusters connect machines with <a href="https://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a> rather than <a href="https://en.wikipedia.org/wiki/Ethernet">ethernet</a>. InfiniBand Remote Direct Memory Access (<a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">RDMA</a>) is supposed to be pretty fast.</p>
<p>Jun Shi at Yahoo contributed an <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md">implementation</a>, <a href="https://github.com/tensorflow/tensorflow/pull/8943">merged</a> four days ago, that begins to let TensorFlow use RDMA for transport via IB verbs, in addition to gRPC.</p>
<p>MPI implementations often support InfiniBand, and an advantage of using MPI is that you can let that implementation worry about InfiniBand support without writing your own RDMA code. There's an <a href="https://github.com/tensorflow/tensorflow/pull/7710">open pull request</a> to give TensorFlow this kind of MPI integration.</p>
<p>On the algorithm side, there are <a href="https://computing.llnl.gov/tutorials/mpi/#Collective_Communication_Routines">collective communication</a> operations that can be optimized, and MPI implementations tend to be good at these. For example, <a href="http://research.baidu.com/bringing-hpc-techniques-deep-learning/">ring allreduce</a> is more efficient for syncing up model parameters than sending them all to a central parameter server and then sending them all back out. Baidu has a <a href="https://github.com/baidu-research/tensorflow-allreduce">fork</a> of TensorFlow that adds their <a href="https://github.com/baidu-research/baidu-allreduce">implementation</a> of ring allreduce using MPI at <a href="https://github.com/baidu-research/tensorflow-allreduce/tree/master/tensorflow/contrib/mpi"><code>tf.contrib.mpi</code></a>.</p>
<p>Outside the MPI world, the NVIDIA Collective Communications Library (<a href="https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/">NCCL</a>) works with multiple GPUs on the same machine. TensorFlow has some support for this via <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/nccl"><code>tf.contrib.nccl</code></a>.</p>
<p>Quite distinct from TensorFlow, the Facebook Incubator project <a href="https://github.com/facebookincubator/gloo">Gloo</a> focuses on collective communication algorithms and supports transport via TCP/IP and InfiniBand without MPI. Gloo is used by <a href="https://caffe2.ai/">Caffe2</a>; the Caffe2 example <a href="https://github.com/caffe2/caffe2/blob/master/caffe2/python/examples/resnet50_trainer.py">resnet50 trainer</a> uses Gloo's ring allreduce.</p>
<!--

Just for fun, here's an Akka implementation of allreduce that I found:
https://github.com/brianmartin/akka-allreduce
I believe that's more of a tree allreduce than a ring allreduce.

-->

<hr>
<h2>Single-Machine Parallelism</h2>
<p>The HPC community has an approach called "MPI everywhere" which means running one single-threaded MPI processes per CPU core. The simplicity of writing single-threaded programs with all parallelism via MPI is attractive, but it's not necessarily the best way to use a machine with a lot of cores and possibly GPUs.</p>
<p>The hybrid approach is to have parallelism both across multiple machines and within each individual machine, usually via threads. HPC folks seem to like to use <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> for multithreading.</p>
<p>TensorFlow supports this kind of within-process parallelism. TensorFlow uses threads pretty freely internally, and you can add your own additional parallelism with Python's <a href="https://docs.python.org/3/library/threading.html">threading</a> or <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a> libraries, among many possible options. Just as with MPI though, when programs are run by a cluster manager, resources may be restricted.</p>
<hr>
<h2>Code organization</h2>
<p>It isn't strictly required for either, but it seems most common to write MPI programs and distributed TensorFlow programs with one entry point that quickly specializes based on the particular role in the cluster for that node. As this implies, every invocation of the program has to know something about the cluster and its place in it.</p>
<hr>
<h2>Cluster Topology</h2>
<p>MPI implementations provide a mechanism in starting a group of processes by which every process knows how many there are in its group and its position or <em>rank</em> in the group. The rank 0 process, called the <em>root</em>, is generally special.</p>
<p>TensorFlow leaves the starting of distributed processes to a cluster manager like Kubernetes. It's common to have several task groups like <code>ps</code> and <code>worker</code>, with a given process having a task type and a number within that group.</p>
<p>Gloo is interestingly different in that individual processes may not start with knowledge of the cluster topology, but only with a reference to a central broker which could be a file on a shared filesystem or a Redis server. Each process checks in with and gets information about other members of the cluster via the central broker. Gloo calls this process <em>rendezvous</em>. As with MPI, processes find out about just one group of processes, and in fact Gloo can use MPI for its initial rendezvous setup.</p>
<hr>
<h2>See also</h2>
<ul>
<li><a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">TensorFlow as a Distributed Virtual Machine</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
</ul>
<hr>
<p>Thanks to <a href="https://twitter.com/danfujita123">Dan Fujita</a> for suggesting more connections between TensorFlow and MPI, and sharing some MPI code that was helpful. Thanks also to <a href="https://twitter.com/ljdursi">Jonathan Dursi</a>, whose <a href="https://www.dursi.ca/">blog</a> I read with interest. And thanks to <a href="https://github.com/cliffwoolley">Cliff Woolley</a> of NVIDIA for <a href="https://github.com/NVIDIA/nccl/issues/86">clarifying</a> the meaning of NCCL.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</link>
<guid>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</guid>
<pubDate>Sun, 23 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Reading from Disk inside the TensorFlow Graph</title>
<description><![CDATA[

<p>As I've <a href="/20170312-use_only_what_you_need_from_tensorflow/">noted</a>, the TensorFlow input pipeline misleadingly <a href="https://www.tensorflow.org/programmers_guide/reading_data">described</a> as "reading from files" is far more complicated than many people need or want to deal with. Indeed, TensorFlow developers realize this and are <a href="https://github.com/tensorflow/tensorflow/issues/7951">thinking about</a> adding alternative interfaces for getting data in to TensorFlow programs.</p>
<p>You can still use any method you like for loading data in Python and then put it in your TensorFlow graph via the <code>feed_dict</code> method. But say you do want to do your file reading with ops that live inside the TensorFlow graph. How does it work?</p>
<p>TensorFlow does have <a href="https://www.tensorflow.org/api_docs/python/tf/read_file"><code>tf.read_file</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/write_file"><code>tf.write_file</code></a>, which let you read and write whole files at once, something like regular Python <code>file.read()</code> and <code>file.write()</code>. But you likely want something a little more helpful.</p>
<p>In Python, you can read lines from a file like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; reader = open('filename.txt'):
&gt;&gt;&gt; for line in reader:
&gt;&gt;&gt;    print(line)</code></pre>

<p>TensorFlow has readers that generalize this idea of getting multiple things, like lines, from files. The immediate example is the <a href="https://www.tensorflow.org/api_docs/python/tf/TextLineReader"><code>tf.TextLineReader</code></a>. There are also <a href="https://www.tensorflow.org/api_guides/python/io_ops#Readers">readers</a> that read records from TFRecords files, or just fixed number of bytes at a time from arbitrary files. Reading a whole file at a time is sort of degenerate case.</p>
<p>Instead of taking a filename, however, TensorFlow readers take a queue of filenames. This can be useful:</p>
<ul>
<li>You may have lots of files that you want to read from, possibly using multiple machines.</li>
<li>You may want to read through one or more files multiple times, for multiple epochs of training.</li>
</ul>
<p>It's a little awkward to use a queue when you just want to read from one file, but that's how TensorFlow works.</p>
<p>TensorFlow's file reading gets tangled up with its threading <code>QueueRunner</code> because they're often shown together, but this is not necessary. We <a href="/20170327-tensorflow_and_queues/">can</a> set up a quick and dirty one-item queue like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; filename_queue = tf.FIFOQueue(capacity=1, dtypes=[tf.string])
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(filename_queue.enqueue('limerick.txt'))
&gt;&gt;&gt; session.run(filename_queue.close())</code></pre>

<p>We make a reader and tell it to read from files in the queue. There are two outputs that update every time you evaluate either of them:</p>
<ul>
<li>A <code>key</code>, based on the current filename, which you may not need and don't need to explicitly evaluate.</li>
<li>A <code>value</code>, which is the bit of data we're probably interested in.</li>
</ul>
<pre><code class="language-python">&gt;&gt;&gt; reader = tf.TextLineReader()
&gt;&gt;&gt; key, value = reader.read(filename_queue)
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:1', 'This limerick goes in reverse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:2', "Unless I'm remiss"]
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:3', 'The neat thing is this:']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:4', 'If you start from the bottom-most verse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:5', "This limerick's not any worse"]
&gt;&gt;&gt; session.run([key, value])
# raises `OutOfRangeError`</code></pre>

<p>The <a href="limerick.txt">limerick</a> is <a href="http://www.smbc-comics.com/?id=3201">due to</a> Zach Weinersmith.</p>
<p>If we hadn't closed the filename queue, that last <code>session.run</code> would block, waiting for somebody to add another filename to the filename queue.</p>
<p>If we had added more filenames to the filename queue, the reader would continue happily reading from the next, and the next.</p>
<p>The records here are lines of simple ASCII text, so we can immediately see them clearly in the REPL. But for many types of data you'll need a decoder.</p>
<p>To read CSV files, you would read text lines just as above, and then use the <a href="https://www.tensorflow.org/api_docs/python/tf/decode_csv"><code>tf.decode_csv</code></a> decoder on  <code>value</code>. The <a href="https://www.tensorflow.org/api_docs/python/tf/decode_raw"><code>tf.decode_raw</code></a> decoder turns raw bytes into standard TensorFlow datatypes. And you can parse TFRecords examples.</p>
<p>Boom! Reading files inside the graph!</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 12 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed MapReduce with TensorFlow</title>
<description><![CDATA[

<p>Using many computers to count words is a tired Hadoop example, but might be unexpected with TensorFlow. In 50 lines, a TensorFlow program can implement not only map and reduce steps, but a whole MapReduce system.</p>
<hr>
<h2>Set up the cluster</h2>
<p>The design will have three roles, or jobs. There will be one task in the <code>files</code> job, distributing units of work. There will be one task for the <code>reduce</code> role, keeping track of results. There could be arbitrarily many tasks doing the <code>map</code> job, but for this example there will be two.</p>
<p>TensorFlow programs often use <code>ps</code> (parameter server) and <code>worker</code> tasks, but this is largely a convention. The program here won't follow the convention.</p>
<p>The four tasks can run on four computers, or on fewer. To stay local, here's a cluster definition that runs them all on your local machine.</p>
<pre><code class="language-python">cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}</code></pre>

<p>I have a <a href="make_configs.py"><code>make_configs.py</code></a> script that produces four shell scripts (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>). Each should be sourced (like <code>source config_map_0.sh</code>) in a separate shell. This setting of environment variables is work that could be handled by a cluster manager like <a href="https://kubernetes.io/">Kubernetes</a>, but these scripts will get it done.</p>
<p>Every task will run <a href="count.py"><code>count.py</code></a>. The first few lines establish the cluster.</p>
<pre><code class="language-python">import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)</code></pre>

<p>For more on TensorFlow clusters, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a>.</p>
<p>To avoid lots of indentation, I'm not using <code>tf.Session</code> as a context manager, which is fine for this example. I'll similarly avoid <code>with</code> blocks when reading files later.</p>
<hr>
<h2>Set up the data</h2>
<p>For distributed data processing to make sense, you likely want a distributed file system like HDFS providing a way for workers to grab chunks of data to work on. You might have hundred-megabyte <a href="/20170323-tfrecords_for_humans/">TFRecords files</a> prepared, for example.</p>
<p>For this example, we'll use <a href="/20170331-on_tyranny/">a dataset of 22 small text files</a>. We'll generate a <a href="filenames.txt">list of filenames</a>. Then, assuming every member of the cluster can access the file system, we can give a worker a filename and have it read the file.</p>
<pre><code class="language-bash">$ wget http://planspace.org/20170331-on_tyranny/on_tyranny.tar.gz
$ tar zxvf on_tyranny.tar.gz
$ find on_tyranny -type f &gt; filenames.txt</code></pre>

<hr>
<h2>Make a filename distributor</h2>
<p>The <code>files</code> task will host a <a href="/20170327-tensorflow_and_queues/">queue</a> of filenames.</p>
<pre><code class="language-python">with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')</code></pre>

<p>This code will be executed by every member of the cluster, but it won't make multiple queues. The <code>tf.device</code> context specifies that the queue lives on the <code>files</code> task machine, and the <code>shared_name</code> uniquely identifies this queue. So just one queue gets made, but every member of the cluster can refer to it with the Python variable name <code>filename_queue</code>.</p>
<p>The next part of the code only runs on the <code>files</code> task:</p>
<pre><code class="language-python">if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()</code></pre>

<p>Device assignment here is left up to TensorFlow, which is fine.</p>
<p>The <code>file</code> task uses normal Python file reading to get filenames from <a href="filenames.txt"><code>filenames.txt</code></a> and put them in the queue.</p>
<p>I'm loading the queue explicitly to avoid getting into <code>Coordinator</code> and <code>QueueRunner</code>; you could also use a <code>string_input_producer</code>, for example.</p>
<p>Then the <code>files</code> task runs <code>server.join()</code>, which keeps it running so that the queue doesn't disappear. We'll have to kill the process eventually because it won't know when to stop. This is another thing a cluster manager could be responsible for.</p>
<hr>
<h2>Make a reduce node</h2>
<p>There's just going to be one variable storing the total word count.</p>
<pre><code class="language-python">with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')</code></pre>

<p>Like the code defining the queue, every task in the cluster will run this code, so every task in the cluster can refer to this variable. Variables in specific places are "de-duplicated" using <code>name</code> instead of <code>shared_name</code>.</p>
<p>There's very little code that only the <code>reduce</code> task runs.</p>
<pre><code class="language-python">if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)</code></pre>

<p>The <code>reduce</code> task initializes and then displays the current value of <code>total_word_count</code> every two seconds.</p>
<p>It would be a bit more like Hadoop MapReduce to have the reducer explicitly receive data emitted from mappers, perhaps via another queue. Then the reducer would have to run some code to reduce down data from that queue.</p>
<p>The absence of any reducing code in the <code>reduce</code> task demonstrates the way distribution works in TensorFlow. The <code>reduce</code> task owns a variable, but we can add to that variable from another machine.</p>
<p>Like the <code>files</code> task, the <code>reduce</code> task doesn't have any way of knowing when the counting process is done and then shutting down, which I think is okay for this example.</p>
<hr>
<h2>Make map nodes</h2>
<p>The <code>map</code> task has already run code establishing the filename queue and the total word count variable. Here's what each <code>map</code> task does:</p>
<pre><code class="language-python">if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign_add(total_word_count,
                                 word_count_to_add,
                                 use_locking=True)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>Each <code>map</code> task pulls a filename from the queue, reads the file, counts its words, and then adds its count to the total.</p>
<p>A <code>map</code> task will run until the queue is empty, and then it will die with an <code>OutOfRangeError</code>. This would be a little more careful:</p>
<pre><code class="language-python">        try:
            filename = session.run(filename_from_queue)
        except tf.errors.OutOfRangeError:
            break</code></pre>

<p>Inside the <code>map</code> task, the actual work that's done is not part of the TensorFlow graph. We can execute arbitrary Python here.</p>
<p>There's a five second pause in the loop so that things don't happen too fast to watch.</p>
<p>The total word count is stored off in the <code>reduce</code> task, possibly on a different computer, but that doesn't matter. This is part of what's cool about TensorFlow.</p>
<hr>
<h2>Run</h2>
<p>To execute, we run <a href="count.py"><code>count.py</code></a> in four places with the appropriate environment variables set. That's it. We've counted words with many computers.</p>
<hr>
<h2>So what?</h2>
<p>Programming a cluster with TensorFlow is just like programming a single computer with TensorFlow. This is pretty neat, and it makes a lot of things possible beyond just distributed stochastic gradient descent.</p>
<p>The example above demonstrates a distributed queue. If you can do that, do you need a separate queueing system like <a href="https://www.rabbitmq.com/">RabbitMQ</a>? Maybe not in every situation.</p>
<p>The example above sends filenames to workers, which is a pretty general model. What if you feel comfortable sending executable filenames, or some representation of code? You might implement something like <a href="http://www.celeryproject.org/">Celery</a> pretty quickly.</p>
<p>The example I've shown can probably fail in more ways than I even realize. It would be more work to make it robust. It would be again more work to make it more general. But it's pretty exciting to be able to write something like this at all.</p>
<p>And while even the typical TensorFlow distributed training is itself a kind of MapReduce process, TensorFlow is general enough that it could be used for wildly different architectures. TensorFlow is an amazing tool.</p>
<hr>
<h2>Code</h2>
<p>Here's <a href="make_configs.py"><code>make_configs.py</code></a>:</p>
<pre><code class="language-python">import json

cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}
for task_type, addresses in cluster.items():
    for index in range(len(addresses)):
        tf_config = {'cluster': cluster,
                     'task': {'type': task_type,
                              'index': index}}
        tf_config_string = json.dumps(tf_config, indent=2)
        with open('config_{}_{}.sh'.format(task_type, index), 'w') as f:
            f.write("export TF_CONFIG='{}'\n".format(tf_config_string))
            # GPUs won't be needed, so prevent accessing GPU memory.
            f.write('export CUDA_VISIBLE_DEVICES=-1\n')</code></pre>

<p>The files produced by <a href="make_configs.py"><code>make_configs.py</code></a> (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>) look like this:</p>
<pre><code class="language-json">export TF_CONFIG='{
  "cluster": {
    "files": [
      "localhost:2222"
    ], 
    "map": [
      "localhost:2224", 
      "localhost:2225"
    ], 
    "reduce": [
      "localhost:2223"
    ]
  }, 
  "task": {
    "index": 0, 
    "type": "map"
  }
}'
export CUDA_VISIBLE_DEVICES=-1</code></pre>

<p>And here's <a href="count.py"><code>count.py</code></a> all together:</p>
<pre><code class="language-python">from __future__ import print_function

import time
import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)

with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')

if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()

with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')

if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)

if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign(total_word_count,
                             total_word_count + word_count_to_add)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>All the files needed to demo this are together in a <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">repo on GitHub</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</link>
<guid>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</guid>
<pubDate>Tue, 11 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow Clusters: Questions and Code</title>
<description><![CDATA[

<p>One way to think about TensorFlow is as a framework for <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed computing</a>. I've suggested that TensorFlow is a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">distributed virtual machine</a>. As such, it offers a lot of flexibility. TensorFlow also suggests some conventions that make writing programs for distributed computation tractable.</p>
<h2>When is there a cluster?</h2>
<p>A <a href="http://hadoop.apache.org/">Hadoop</a> or <a href="http://spark.apache.org/">Spark</a> cluster is generally long-lived. The cluster runs, processing jobs as they come to it. A company might have a thousand-node Spark cluster, for example, used by everyone in a division.</p>
<p>In contrast, TensorFlow clusters generally spring into being for the purpose of running a particular TensorFlow program. There are computers on the network, and they become members of TensorFlow clusters based on what they're running.</p>
<p>To make this process manageable, you might use a system like <a href="https://kubernetes.io/">Kubernetes</a> or <a href="https://cloud.google.com/ml-engine/">Google Cloud ML</a> to intelligently run TensorFlow programs on specific machines in a larger pool.</p>
<h2>What does the cluster do for me?</h2>
<p>In systems like Hadoop MapReduce and Spark, there's usually considerable distance between the programming interface and how the computation actually gets distributed. If you're writing your own map and reduce steps for Hadoop, you're close to that mechanism, but you're still plugging in to the pre-built larger architecture. Most users prefer higher-level APIs like <a href="https://pig.apache.org/">Pig</a> on Hadoop or the standard Spark APIs. And both Hadoop and Spark support at least one SQL-style interface, even farther from the underlying implementations. As a user of Hadoop or Spark, you don't think about putting computation on particular machines, or worry about the different roles that different machines might play.</p>
<p>With TensorFlow, the abstraction for distributed computing is the same as the abstraction for local computing: the computation graph. Distributing TensorFlow programs means having graphs that span multiple computers. You're responsible for what parts of the graph go where, and what every computer in the cluster does.</p>
<h2>How many programs do I write?</h2>
<p>One familiar model of distributed computing is client-server, like the web. Web servers and browsers are quite different programs.</p>
<p>Closer to TensorFlow applications, a central server could do some computation on request, or it could offer data to be processed on the client side, like <a href="https://setiathome.berkeley.edu/">SETI@home</a> or <a href="https://folding.stanford.edu/">Folding@home</a>. Again, server and client code are distinct.</p>
<p>Nothing prevents you from writing separate "server" and "client" TensorFlow programs, but it isn't necessary, natural, or recommended. One of TensorFlow's design goals was to get away from the client-server divide in DistBelief.</p>
<p>The TensorFlow distributed runtime is peer-to-peer: every machine can accept graph nodes from any other machine, and every machine can put graph nodes on any other machine. Generally, every machine will run the same program. Whether the system behaves like a client-server system or something else entirely is up to you.</p>
<h2>Which computer does what?</h2>
<p>Distributed TensorFlow works by running the same program on multiple machines, but that doesn't mean that every machine does exactly the same thing.</p>
<p>If a system with separate client and server programs is like a system of two symbiotic species, your TensorFlow program is like the DNA of an organism whose cells specialize based on their environment.</p>
<p>The dominant convention is to have two main roles: <em>parameter servers</em> (<code>ps</code>) and <em>workers</em>. You might also have a <em>master</em> role, and one of your workers can be the <em>chief</em> worker, but <code>ps</code> and <code>worker</code> is usually the main division.</p>
<p>Usually a machine running your TensorFlow program will learn what its role should be based on the <code>TF_CONFIG</code> environment variable, which should be set by your cluster manager.</p>
<h2>Who knows what about the cluster topology?</h2>
<p>In Hadoop and Spark, the system is keeping track of all the machines in the cluster. On the web, servers generally only know about client addresses long enough to provide a response.</p>
<p>Usually every machine in a TensorFlow cluster will be given a complete listing of machines in the cluster.</p>
<p>If some machines really don't need to know about others in the same cluster, for example if workers never communicate with one another, it's also possible to provide less complete information.</p>
<p>The cluster topology is also most often provided via the <code>TF_CONFIG</code> environment variable.</p>
<h2>Code?</h2>
<p>Let's say you have a network which includes the following machines:</p>
<ul>
<li><code>192.168.0.10</code></li>
<li><code>192.168.0.11</code></li>
<li><code>192.168.0.12</code></li>
</ul>
<p>The machines are all running the same version of TensorFlow. Let's see how we could get them set up to run a distributed TensorFlow program.</p>
<p>We'll have one parameter server (<code>ps</code>) and two workers. Each machine will know about the cluster's topology.</p>
<pre><code class="language-python">import tensorflow as tf

cluster = {'ps': ['192.168.0.10:2222'],
           'worker': ['192.168.0.11:2222', '192.168.0.12:2222']}
cluster_spec = tf.train.ClusterSpec(cluster)
server = tf.train.Server(cluster_spec)</code></pre>

<p>In the strings like <code>'192.168.0.10:2222'</code>, the protocol (gRPC) is omitted, and the port (<code>2222</code>) is the default for TensorFlow communication.</p>
<p>The <code>server</code> that every machine starts is what enables the communication of the TensorFlow distributed runtime; its behavior is largely at a lower level than the code we'll write.</p>
<p>This code gets the network topology going, but it doesn't tell an individual machine which role it should have. Working out which IP address or hostname refers to the current machine is not necessarily straightforward, but hard-coding different values into different copies of the code would be an even worse idea than hard-coding the topology once. This is where the environment variable <code>TF_CONFIG</code> becomes very useful.</p>
<p>We'll put cluster and task information into the <code>TF_CONFIG</code> environment variable as a JSON serialization. On the machine that will be our parameter server, the data will look like this as a Python dictionary:</p>
<pre><code class="language-python">tf_config = {'cluster': {'ps': ['192.168.0.10:2222'],
                         'worker': ['192.168.0.11:2222', '192.168.0.12:2222']},
             'task': {'type': 'ps',
                      'index': 0}}</code></pre>

<p>You can set the environment variable in the usual way in a shell, making small changes to achieve JSON syntax.</p>
<pre><code class="language-bash">$ export TF_CONFIG='{"cluster": {"ps": ["192.168.0.10:2222"], "worker": ["192.168.0.11:2222", "192.168.0.12:2222"]}, "task": {"type": "ps", "index": 0}}'</code></pre>

<p>To avoid fussing with that directly, you could do it in Python.</p>
<pre><code class="language-python">import os
import json

os.environ['TF_CONFIG'] = json.dumps(tf_config)</code></pre>

<p>Really, it'll be best if your cluster manager sets <code>TF_CONFIG</code> for you.</p>
<p>Once <code>TF_CONFIG</code> is set, you can read it and get to work.</p>
<pre><code class="language-python">tf_config = json.loads(os.environ['TF_CONFIG'])
cluster_spec = tf.train.ClusterSpec(tf_config['cluster'])
task_type = tf_config['task']['type']
task_id = tf_config['task']['index']
server = tf.train.Server(cluster_spec,
                         job_name=task_type,
                         task_index=task_id)</code></pre>

<p>Another way to do that is with <code>tf.contrib.learn.RunConfig</code>, which automatically checks the <code>TF_CONFIG</code> environment variable.</p>
<pre><code class="language-python">config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)</code></pre>

<p>At this point, you are ready to begin writing a distributed TensorFlow program.</p>
<!-- helpful references:

https://github.com/tensorflow/tensorflow/blob/17c47804b86e340203d451125a721310033710f1/tensorflow/contrib/learn/python/learn/estimators/run_config.py

https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/tensorflowcore/trainer/task.py

-->

<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</link>
<guid>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</guid>
<pubDate>Mon, 10 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>How Not To Program the TensorFlow Graph</title>
<description><![CDATA[

<p>Using TensorFlow from Python is like using Python to program <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">another computer</a>. Some Python statements build your TensorFlow program, some Python statements execute that program, and of course some Python statements aren't involved with TensorFlow at all.</p>
<p>Being thoughtful about the graphs you construct can help you avoid confusion and performance pitfalls. Here are a few considerations.</p>
<h2>Avoid having many identical ops</h2>
<!-- Better re-write; maybe separate out a section about Python names for TF ops, losing references to your ops, etc. -->

<p>Lots of methods in TensorFlow create ops in the computation graph, but do not execute them. You may want to execute multiple times, but that doesn't mean you should create lots of copies of the same ops.</p>
<p>A clear example is <code>tf.global_variables_initializer()</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
# Create some variables...
&gt;&gt;&gt; initializer = tf.global_variables_initializer()
# Variables are not yet initialized.
&gt;&gt;&gt; session.run(initializer)
# Now variables are initialized.
# Do some more work...
&gt;&gt;&gt; session.run(initializer)
# Now variables are re-initialized.</code></pre>

<p>If the call to <code>tf.global_variables_initializer()</code> is repeated, for example directly as the argument to <code>session.run()</code>, there are downsides.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.global_variables_initializer())
&gt;&gt;&gt; session.run(tf.global_variables_initializer())</code></pre>

<p>A new initializer op is created every time the argument to <code>session.run()</code> here is evaluated. This creates multiple initializer ops in the graph. Having multiple copies isn't a big deal for small ops in an interactive session, and you might even want to do it in the case of the initializer if you've created more variables that need to be included in initialization. But think about whether you need lots of duplicate ops.</p>
<p>Creating ops inside <code>session.run()</code>, you also don't have a Python variable referring to those ops, so you can't easily reuse them.</p>
<p>In Python, if you create an object that nothing refers to, it can be garbage collected. The abandoned object will be deleted and and memory it used will be freed. That doesn't happen to things in the TensorFlow graph; everything you put in the graph stays there.</p>
<p>It's pretty clear that <code>tf.global_variables_initializer()</code> returns an op. But ops are also created in less obvious ways.</p>
<p>Let's compare to how NumPy works.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>At this point there are two arrays in memory, <code>x</code> and <code>y</code>. The <code>y</code> has the value 2.0, but there's no record of <em>how</em> it came to have that value. The addition has left no record of itself.</p>
<p>TensorFlow is different.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Now only <code>x</code> is a TensorFlow variable; <code>y</code> is an <code>add</code> op, which can return the result of that addition if we ever run it.</p>
<p>One more comparison.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Here <code>y</code> is assigned to refer to one result array <code>x + 1.0</code>, and then reassigned to point to a different one. The first one will be garbage collected and disappear.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>In this case, <code>y</code> refers to one <code>add</code> op in the TensorFlow graph, and then <code>y</code> is reassigned to point to a different <code>add</code> op in the graph. Since <code>y</code> only points to the second <code>add</code> now, we don't have a convenient way to work with the first one. But both the <code>add</code> ops are still around, in the graph, and will stay there.</p>
<p>(As an aside, Python's mechanism for defining class-specific addition <a href="http://www.python-course.eu/python3_magic_methods.php">and so on</a>, which is how <code>+</code> is made to create TensorFlow ops, is pretty neat.)</p>
<p>Especially if you're just working with the default graph and running interactively in a regular REPL or a notebook, you can end up with a lot of abandoned ops in your graph. Every time you re-run a notebook cell that defines any graph ops, you aren't just redefining ops&#8212;you're creating new ones.</p>
<p>Often it's okay to have a few extra ops floating around when you're experimenting. But things can get out of hand.</p>
<pre><code class="language-python">for _ in range(1e6):
    x = x + 1</code></pre>

<p>If <code>x</code> is a NumPy array, or just a regular Python number, this will run in constant memory and finish with one value for x.</p>
<p>But if <code>x</code> is a TensorFlow variable, there will be over a million ops in your TensorFlow graph, just defining a computation and not even <em>doing</em> it.</p>
<p>One immediate fix for TensorFlow is to use a <code>tf.assign</code> op, which gives behavior more like what you might expect.</p>
<pre><code class="language-python">increment_x = tf.assign(x, x + 1)
for _ in range(1e6):
    session.run(increment_x)</code></pre>

<p>This revised version does not create any ops inside the loop, which is generally good advice. TensorFlow does have <a href="https://www.tensorflow.org/api_guides/python/control_flow_ops">control flow constructs</a> including <a href="https://www.tensorflow.org/api_docs/python/tf/while_loop">while loops</a>. But only use these when really needed.</p>
<p>Be conscious of when you're creating ops, and only create the ones you need. Try to keep op creation distinct from op execution. And after interactive experimentation, eventually get to a state, probably in a script, where you're only creating the ops that you need.</p>
<h2>Avoid constants in the graph</h2>
<p>A particularly unfortunate op to needlessly add to a graph is accidental constant ops, especially large ones.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones = np.ones((1000, 1000))</code></pre>

<p>There are a million ones in the NumPy array <code>many_ones</code>. We can add them up.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones.sum()
## 1000000.0</code></pre>

<p>What if we add them up with TensorFlow?</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.reduce_sum(many_ones))
## 1000000.0</code></pre>

<p>The result is the same, but the mechanism is quite different. This not only added some ops to the graph&#8212;it put a copy of the entire million-element array into the graph as a constant.</p>
<p>Variations on this pattern can result in accidentally loading an entire data set into the graph as constants. A program might still run, for small data sets. Or your system might fail.</p>
<p>One simple way to avoid storing data in the graph is to use the <code>feed_dict</code> mechanism.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_things = tf.placeholder(tf.float64)
&gt;&gt;&gt; adder = tf.reduce_sum(many_things)
&gt;&gt;&gt; session.run(adder, feed_dict={many_things: many_ones})
## 1000000.0</code></pre>

<p>As before, be clear about what you're adding to the graph and when. Concrete data usually only enters the graph at moments of evaluation.</p>
<h2>TensorFlow as Functional Programming</h2>
<p>TensorFlow ops are like functions. This is especially clear when an op has one or more placeholder inputs; evaluating the op in a session is like calling a function with those arguments. So Python functions that return TensorFlow ops are like <a href="https://en.wikipedia.org/wiki/Higher-order_function">higher-order functions</a>.</p>
<p>You might decide that it's worth putting a constant into the graph. For example, if you have to reshape a lot of tensors to 28x28, you might make an op that does that.</p>
<pre><code class="language-python">&gt;&gt;&gt; input_tensor = tf.placeholder(tf.float32)
&gt;&gt;&gt; reshape_to_28 = tf.reshape(input_tensor, shape=[28, 28])</code></pre>

<p>This is like <a href="https://en.wikipedia.org/wiki/Currying">currying</a> in that the <code>shape</code> argument has now been set. The <code>[28, 28]</code> has become a constant in the graph and specified that argument. Now to evaluate <code>reshape_to_28</code> we only have to provide <code>input_tensor</code>.</p>
<p>Broader connections have been suggested between <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">neural networks, types, and functional programming</a>. It's interesting to think of TensorFlow as a system that supports this kind of construction.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</guid>
<pubDate>Tue, 04 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Images and TFRecords</title>
<description><![CDATA[

<p>There are a number of ways to work with images in TensorFlow and, if you wish, with TFRecords. These methods aren't so mysterious if you <a href="/20170323-tfrecords_for_humans/">understand TFRecords</a> and a little bit about how digital images work.</p>
<hr>
<h2>Representations for Images</h2>
<!--

(I kind of like this paragraph, at least in that it gives credit for the image source, but it's not really key to the development here.)

Wikipedia [tells me](https://en.wikipedia.org/wiki/Keep_Calm_and_Carry_On) that this [poster](https://commons.wikimedia.org/wiki/File:Freedom_Is_In_Peril_Defend_It_With_All_Your_Might.svg), and the more widely known "Keep Calm and Carry On," were not very well received [in 1939](img/freedom_in_tube.jpg). Perhaps its time has come.

-->

<p>For example, this image is 600 pixels tall and 400 pixels wide. Every pixel has some intensity in red, green, and blue: three values, or channels, for every pixel. This image has shape [600, 400, 3]. (The order of the dimensions doesn't matter as long as everybody agrees on the convention.) The display on your screen is like a dense matrix; it is a <a href="https://en.wikipedia.org/wiki/Raster_graphics">raster graphic</a>.</p>
<p><img alt="FREEDOM IS IN PERIL / DEFEND IT WITH ALL YOUR MIGHT" src="img/freedom.png"></p>
<p>Neural networks that work with images typically work with this kind of dense matrix representation. For this image, the matrix will have 600 * 400 * 3 = 720,000 values. If each value is an unsigned 8-bit integer, that's 720,000 bytes, or about three quarters of a megabyte.</p>
<p>Images sometimes also have an alpha transparency channel, which is a fourth channel in a color image, but not necessary if there's nothing else "underneath" the image. And not all images are color; greyscale (black and white) images can have just one channel.</p>
<p>Using unsigned 8-bit integers (256 possible values) for each value in the image array is enough for displaying images to humans. But when working with image data, it isn't uncommon to switch to 32-bit floats, for example. This quadruples the size of the data in memory. As always, remain aware of your data types.</p>
<h3>Dense Array</h3>
<p>One way to store complete raster image data is by serializing a NumPy array to disk with <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html"><code>numpy.save</code></a>.</p>
<pre>720,080 bytes  <a href="img/freedom.npy">freedom.npy</a></pre>

<p>The file <code>freedom.npy</code> has 80 more bytes than the ones required to store the array values. Those extra bytes specify things like the dimensions of the array. If we save raw array values in TFRecords, we'll also have to keep track of this additional information.</p>
<p>Because storing one or more value for every pixel takes so many bytes, file formats for images typically do something cleverer.</p>
<h3>JPEG</h3>
<p><a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a> is lossy. When you save an array as JPG and then read from the JPG, it will generally look about the same, but you won't necessarily get back exactly the same values for your array. JPEG is like <a href="https://en.wikipedia.org/wiki/MP3">MP3</a> for images. JPEG is good for photographs.</p>
<pre> 25,136 bytes  <a href="img/freedom.jpg">freedom.jpg</a></pre>

<p><code>freedom.jpg</code> is less than 4% the size of the NumPy array, and it still looks pretty good. It does have some ringing artifacts around the letter edges. JPEG file size depends on compression parameters when saving, and on the encoder used. For example, Google <a href="https://research.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html">released</a> their <a href="https://github.com/google/guetzli/">Guetzli</a> JPEG encoder, which can produce smaller files but takes more computation to do so.</p>
<pre> 46,567 bytes  <a href="img/freedom2.jpg">freedom2.jpg</a></pre>

<p><code>freedom2.jpg</code> is another JPEG file, saved with higher quality. Ringing artifacts are pretty much gone. The file is still under 7% the size of the NumPy arrray.</p>
<h3>PNG</h3>
<p><a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics">PNG</a> is lossless. If you save as PNG and then read from the PNG, you can get back exactly what you had originally. PNG is like <a href="https://en.wikipedia.org/wiki/Zip_(file_format)">ZIP</a> for images, but image viewers do the "un-zipping" automatically so they can put a raster image on the screen. PNG is like <a href="https://en.wikipedia.org/wiki/GIF">GIF</a> without animation.</p>
<pre> 33,192 bytes  <a href="img/freedom.png">freedom.png</a></pre>

<p><code>freedom.png</code> is under 5% the size of the NumPy array, and it preserves the image perfectly. It's less often used, but compression parameters can also affect PNG size, and the encoder can make a difference too. PNG uses deflate (zlip) compression, so Google's <a href="https://github.com/google/zopfli">Zopfli</a> algorithm can be used, for example, while <a href="https://github.com/google/brotli">Brotli</a> cannot.</p>
<h3>SVG</h3>
<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> doesn't represent pixels directly at all, but represents in a text format the geometry of shapes in the image. SVG images can look good at any zoom level; they don't suffer from pixelization. They can also be edited with different tools than raster images. And for simple images, an SVG can be quite small. SVG is like <a href="https://en.wikipedia.org/wiki/HTML">HTML</a>; you can look at it as text, but to see it as intended requires a program like a web browser.</p>
<pre> 42,721 bytes  <a href="img/freedom.svg">freedom.svg</a></pre>

<p><code>freedom.svg</code> turns out not be a super efficient encoding of the image; it represents all the letter outlines instead of using plain text in some font, for example. But it represents the image fundamentally differently from just recreating pixels, and it's still under 6% the size of the NumPy array file.</p>
<h3>TFRecords?</h3>
<p>TFRecords don't know anything about image formats. You just put bytes in them. So you have your choice of whether that means you store dense arrays of values or a well-known image format. TensorFlow provides <a href="https://www.tensorflow.org/api_guides/python/image">image format support</a> for JPEG, PNG, and GIF in the computation graph.</p>
<hr>
<h2>Images without TensorFlow</h2>
<p>As always, <a href="/20170312-use_only_what_you_need_from_tensorflow/">you have a choice about whether you need to do everything with TensorFlow</a>. If you're loading data batches with a <code>feed_dict</code>, in particular, feel free to use whatever Python functionality you're comfortable with. Even if you're not, you'll probably want to do some work with your data outside of TensorFlow before you move all your computation into the graph.</p>
<p>The Matplotlib <a href="http://matplotlib.org/users/image_tutorial.html">image tutorial</a> recommends using <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imread"><code>matplotlib.image.imread</code></a> to read image formats from disk. This function will automatically change image array values to floats between zero and one, and it doesn't give you options about how to read the image.</p>
<p>The <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imread.html">scipy.misc.imread</a> function uses the Python Imaging Library (PIL) to support many image formats, including PNG and JPEG. This function also has some useful options. The original <code>freedom.png</code> has an alpha channel which we don't need. Passing <code>mode='RGB'</code> tells the reader to give us just three color channels.</p>
<pre><code class="language-python">&gt;&gt;&gt; import scipy.misc
&gt;&gt;&gt; image = scipy.misc.imread('freedom.png', mode='RGB')
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>Matplotlib's <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.imshow">imshow</a> is good for checking out what image arrays look like. (Also specify <code>%matplotlib inline</code> if you're in a notebook.)</p>
<pre><code class="language-python">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.imshow(image)</code></pre>

<p>NumPy gives us a way to save and load its arrays.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.save('freedom.npy', image)
&gt;&gt;&gt; same_image = np.load('freedom.npy')</code></pre>

<p>As long as we have <code>scipy.misc</code> imported, we can use <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imsave.html">scipy.misc.imsave</a> to write various image formats as well. For saving, <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imsave">matplotlib.image.imsave</a> actually has more options. Neither of these offer control over compression level, for example.</p>
<pre><code class="language-python">&gt;&gt;&gt; scipy.misc.imsave('freedom.jpg', image)</code></pre>

<hr>
<h2>PNG and JPEG with TensorFlow</h2>
<p>TensorFlow has ops for decoding and encoding PNGs and JPEGs, so we can put these operations into the computation graph.</p>
<p>Above, <code>imread</code> both read a file from disk and decoded it, and <code>imsave</code> both encoded an image and wrote it to disk. The TensorFlow functionality decouples the decoding and encoding from file reading and writing. This example will avoid using TensorFlow's file reading and writing, to focus just on the encoding and decoding.</p>
<p>The <code>channels=3</code> passed to <code>tf.image.decode_image()</code> is the equivalent of <code>mode='RGB'</code> above. We don't want to get the alpha channel from the PNG file.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; with open('freedom.png', 'rb') as f:
...     png_bytes = f.read()
&gt;&gt;&gt; bytes = tf.placeholder(tf.string)
&gt;&gt;&gt; decode_png = tf.image.decode_image(bytes, channels=3)
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; image = session.run(decode_png, feed_dict={bytes: png_bytes})
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_image"><code>tf.image.decode_image</code></a> here intelligently uses <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_png"><code>tf.image.decode_png</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_jpeg"><code>tf.image.decode_jpeg</code></a>, or <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_gif"><code>tf.image.decode_gif</code></a>, similar to how above <code>imread</code> can handle a variety of formats. You can also use the appropriate function directly.</p>
<p>Above, <code>imsave</code> supported writing various formats, choosing the one appropriate for the filename specified. In TensorFlow, you have to use <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_jpeg"><code>tf.image.encode_jpeg</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_png"><code>tf.image.encode_png</code></a> directly, and both provide extra arguments controlling compression and more.</p>
<pre><code class="language-python">&gt;&gt;&gt; tensor = tf.placeholder(tf.uint8)
&gt;&gt;&gt; encode_jpeg = tf.image.encode_jpeg(tensor)
&gt;&gt;&gt; jpeg_bytes = session.run(encode_jpeg, feed_dict={tensor: image})
&gt;&gt;&gt; with open('freedom.jpg', 'wb') as f:
...     f.write(jpeg_bytes)</code></pre>

<p>With the default settings, TensorFlow encodes a larger JPEG than <code>imsave</code>, coming in at 46,567 bytes. It looks pretty good.</p>
<hr>
<h2>PNG and JPEG in TFRecords</h2>
<p>We can put plain bytes into <code>Example</code> TFRecords <a href="/20170323-tfrecords_for_humans/">without too much trouble</a>. So PNG or JPEG images are easily handled.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'png_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=[png_bytes]))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_png_bytes = same_example.features.feature['png_bytes'].bytes_list.value[0]</code></pre>

<p>When the <code>same_png_bytes</code> is decoded by <code>tf.image.decode_image</code>, as above, or <code>tf.image.decode_png</code> directly, you'll get back a tensor with the correct dimensions, because PNG (and JPEG) include that information in their encodings.</p>
<hr>
<h2>Image Arrays in TFRecords</h2>
<p>If you want to save dense matrix representations in TFRecords, there's a little bit of bookkeeping to do, but it isn't too bad.</p>
<pre><code>image_bytes = image.tostring()
image_shape = image.shape

my_example = tf.train.Example(features=tf.train.Features(feature={
    'image_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes])),
    'image_shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image_shape))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_image_bytes = same_example.features.feature['image_bytes'].bytes_list.value[0]
same_image_shape = list(
    same_example.features.feature['image_shape'].int64_list.value)</code></pre>

<p>With the information recovered from TFRecord form, it's easy to use NumPy to put the image back together.</p>
<pre><code class="language-python">same_image = np.fromstring(same_image_bytes, dtype=np.uint8)
same_image.shape = same_image_shape</code></pre>

<p>You can do the same using TensorFlow.</p>
<pre><code class="language-python">shape = tf.placeholder(tf.int32)
new_image = tf.reshape(tf.decode_raw(bytes, tf.uint8), shape)
same_image = session.run(encode_jpeg, feed_dict={bytes: same_image_bytes,
                                                 shape: same_image_shape})</code></pre>

<p>In this example, however, the parsing of the <code>Example</code> was already done outside the TensorFlow graph, so there isn't a strong reason to stay inside the graph here.</p>
<hr>
<h3>Comparison to Caffe and LMDB</h3>
<p><a href="http://caffe.berkeleyvision.org/">Caffe</a> is an older deep learning framework that can work with data stored in <a href="https://symas.com/offerings/lightning-memory-mapped-database/">LMDB</a> on-disk databases, similar to how TensorFlow can work with data stored in TFRecords files.</p>
<p>Like TensorFlow, Caffe defines a protocol buffer message for training examples. In Caffe, it's called <code>Datum</code>. These are saved just like in TensorFlow, by serializing them and putting them in a file on disk, but instead of a TFRecords file, which just puts records in a row and reads them out in that order, here Caffe will work with LMDB, which has the semantics of a key-value store.</p>
<p>TensorFlow's <code>Example</code> format is super flexible, but the trade-off is that it doesn't automatically do things for you. Caffe's <code>Datum</code>, on the other hand, expects you to put in a dense image array, and an integer class label. So here there isn't any fiddling with array size, for example, but the trade-off is that it won't work easily for arbitrary data structures that we might eventually want to store.</p>
<p>In the TFRecords examples above, we stored only image data, and said nothing about a class label or anything else. This is because TFRecords lets you decide what you want to save, rather than defining a format in advance. You could save an integer label, or a float regression label, or a string of text, or an image mask, and on and on. Here, we're just using the built-in Caffe integer label.</p>
<pre><code class="language-python">import caffe
import lmdb

# `image` was read above
label = 9  # for a classification problem

datum = caffe.io.array_to_datum(arr=image, label=label)
datum_str = datum.SerializeToString()

env = lmdb.open('lmdb_data')
txn = env.begin(write=True)
txn.put(key='my datum', value=datum_str)

cur = txn.cursor()
same_datum_str = cur.get('my datum')

same_datum = caffe.proto.caffe_pb2.Datum().FromString(same_datum_str)
same_image = caffe.io.datum_to_array(same_datum)
same_label = datum.label</code></pre>

<p>To actually work with the Caffe training process, there are some other conditions for the data to satisfy. Caffe expects [channels, height, width] instead of [height, width, channels], for example.</p>
<hr>
<h2>What should you use?</h2>
<p>Prefer doing fewer separate manipulation steps to whatever your original data is, if you can help it: try not to have lots of different versions of your data in different places on disk. This will make your life easier.</p>
<p>If you are choosing a format, JPEG is good for photos.</p>
<p>Think about whether you need to put everything into the TensorFlow computation graph. Think about whether you need to use TFRecords. Try to spend your time on things that help solve your problems.</p>
<p>For two more complete <em>in situ</em> examples of converting images to TFRecords, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py">code for MNIST images</a> and <a href="https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py">code for ImageNet images</a>. The ImageNet code can be run on the command-line.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170403-images_and_tfrecords/</link>
<guid>http://planspace.org/20170403-images_and_tfrecords/</guid>
<pubDate>Mon, 03 Apr 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
