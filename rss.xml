<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>TensorFlow Clusters: Questions and Code</title>
<description><![CDATA[

<p>One way to think about TensorFlow is as a framework for <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed computing</a>. I've suggested that TensorFlow is a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">distributed virtual machine</a>. As such, it offers a lot of flexibility. TensorFlow also suggests some conventions that make writing programs for distributed computation tractable.</p>
<h2>When is there a cluster?</h2>
<p>A <a href="http://hadoop.apache.org/">Hadoop</a> or <a href="http://spark.apache.org/">Spark</a> cluster is generally long-lived. The cluster runs, processing jobs as they come to it. A company might have a thousand-node Spark cluster, for example, used by everyone in a division.</p>
<p>In contrast, TensorFlow clusters generally spring into being for the purpose of running a particular TensorFlow program. There are computers on the network, and they become members of TensorFlow clusters based on what they're running.</p>
<p>To make this process manageable, you might use a system like <a href="https://kubernetes.io/">Kubernetes</a> or <a href="https://cloud.google.com/ml-engine/">Google Cloud ML</a> to intelligently run TensorFlow programs on specific machines in a larger pool.</p>
<h2>What does the cluster do for me?</h2>
<p>In systems like Hadoop MapReduce and Spark, there's usually considerable distance between the programming interface and how the computation actually gets distributed. If you're writing your own map and reduce steps for Hadoop, you're close to that mechanism, but you're still plugging in to the pre-built larger architecture. Most users prefer higher-level APIs like <a href="https://pig.apache.org/">Pig</a> on Hadoop or the standard Spark APIs. And both Hadoop and Spark support at least one SQL-style interface, even farther from the underlying implementations. As a user of Hadoop or Spark, you don't think about putting computation on particular machines, or worry about the different roles that different machines might play.</p>
<p>With TensorFlow, the abstraction for distributed computing is the same as the abstraction for local computing: the computation graph. Distributing TensorFlow programs means having graphs that span multiple computers. You're responsible for what parts of the graph go where, and what every computer in the cluster does.</p>
<h2>How many programs do I write?</h2>
<p>One familiar model of distributed computing is client-server, like the web. Web servers and browsers are quite different programs.</p>
<p>Closer to TensorFlow applications, a central server could do some computation on request, or it could offer data to be processed on the client side, like <a href="https://setiathome.berkeley.edu/">SETI@home</a> or <a href="https://folding.stanford.edu/">Folding@home</a>. Again, server and client code are distinct.</p>
<p>Nothing prevents you from writing separate "server" and "client" TensorFlow programs, but it isn't necessary, natural, or recommended. One of TensorFlow's design goals was to get away from the client-server divide in DistBelief.</p>
<p>The TensorFlow distributed runtime is peer-to-peer: every machine can accept graph nodes from any other machine, and every machine can put graph nodes on any other machine. Generally, every machine will run the same program. Whether the system behaves like a client-server system or something else entirely is up to you.</p>
<h2>Which computer does what?</h2>
<p>Distributed TensorFlow works by running the same program on multiple machines, but that doesn't mean that every machine does exactly the same thing.</p>
<p>If a system with separate client and server programs is like a system of two symbiotic species, your TensorFlow program is like the DNA of an organism whose cells specialize based on their environment.</p>
<p>The dominant convention is to have two main roles: <em>parameter servers</em> (<code>ps</code>) and <em>workers</em>. You might also have a <em>master</em> role, and one of your workers can be the <em>chief</em> worker, but <code>ps</code> and <code>worker</code> is usually the main division.</p>
<p>Usually a machine running your TensorFlow program will learn what its role should be based on the <code>TF_CONFIG</code> environment variable, which should be set by your cluster manager.</p>
<h2>Who knows what about the cluster topology?</h2>
<p>In Hadoop and Spark, the system is keeping track of all the machines in the cluster. On the web, servers generally only know about client addresses long enough to provide a response.</p>
<p>Usually every machine in a TensorFlow cluster will be given a complete listing of machines in the cluster.</p>
<p>If some machines really don't need to know about others in the same cluster, for example if workers never communicate with one another, it's also possible to provide less complete information.</p>
<p>The cluster topology is also most often provided via the <code>TF_CONFIG</code> environment variable.</p>
<h2>Code?</h2>
<p>Let's say you have a network which includes the following machines:</p>
<ul>
<li><code>192.168.0.10</code></li>
<li><code>192.168.0.11</code></li>
<li><code>192.168.0.12</code></li>
</ul>
<p>The machines are all running the same version of TensorFlow. Let's see how we could get them set up to run a distributed TensorFlow program.</p>
<p>We'll have one parameter server (<code>ps</code>) and two workers. Each machine will know about the cluster's topology.</p>
<pre><code class="language-python">import tensorflow as tf

cluster = {'ps': ['192.168.0.10:2222'],
           'worker': ['192.168.0.11:2222', '192.168.0.12:2222']}
cluster_spec = tf.train.ClusterSpec(cluster)
server = tf.train.Server(cluster_spec)</code></pre>

<p>In the strings like <code>'192.168.0.10:2222'</code>, the protocol (gRPC) is omitted, and the port (<code>2222</code>) is the default for TensorFlow communication.</p>
<p>The <code>server</code> that every machine starts is what enables the communication of the TensorFlow distributed runtime; its behavior is largely at a lower level than the code we'll write.</p>
<p>This code gets the network topology going, but it doesn't tell an individual machine which role it should have. Working out which IP address or hostname refers to the current machine is not necessarily straightforward, but hard-coding different values into different copies of the code would be an even worse idea than hard-coding the topology once. This is where the environment variable <code>TF_CONFIG</code> becomes very useful.</p>
<p>We'll put cluster and task information into the <code>TF_CONFIG</code> environment variable as a JSON serialization. On the machine that will be our parameter server, the data will look like this as a Python dictionary:</p>
<pre><code class="language-python">tf_config = {'cluster': {'ps': ['192.168.0.10:2222'],
                         'worker': ['192.168.0.11:2222', '192.168.0.12:2222']},
             'task': {'type': 'ps',
                      'index': 0}}</code></pre>

<p>You can set the environment variable in the usual way in a shell, making small changes to achieve JSON syntax.</p>
<pre><code class="language-bash">$ export TF_CONFIG='{"cluster": {"ps": ["192.168.0.10:2222"], "worker": ["192.168.0.11:2222", "192.168.0.12:2222"]}, "task": {"type": "ps", "index": 0}}'</code></pre>

<p>To avoid fussing with that directly, you could do it in Python.</p>
<pre><code class="language-python">import os
import json

os.environ['TF_CONFIG'] = json.dumps(tf_config)</code></pre>

<p>Really, it'll be best if your cluster manager sets <code>TF_CONFIG</code> for you.</p>
<p>Once <code>TF_CONFIG</code> is set, you can read it and get to work.</p>
<pre><code class="language-python">tf_config = json.loads(os.environ['TF_CONFIG'])
cluster_spec = tf.train.ClusterSpec(tf_config['cluster'])
task_type = tf_config['task']['type']
task_id = tf_config['task']['index']
server = tf.train.Server(cluster_spec,
                         job_name=task_type,
                         task_index=task_id)</code></pre>

<p>Another way to do that is with <code>tf.contrib.learn.RunConfig</code>, which automatically checks the <code>TF_CONFIG</code> environment variable.</p>
<pre><code class="language-python">config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)</code></pre>

<p>At this point, you are ready to begin writing a distributed TensorFlow program.</p>    
    ]]></description>
<link>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</link>
<guid>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</guid>
<pubDate>Mon, 10 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>How Not To Program the TensorFlow Graph</title>
<description><![CDATA[

<p>Using TensorFlow from Python is like using Python to program <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">another computer</a>. Some Python statements build your TensorFlow program, some Python statements execute that program, and of course some Python statements aren't involved with TensorFlow at all.</p>
<p>Being thoughtful about the graphs you construct can help you avoid confusion and performance pitfalls. Here are a few considerations.</p>
<h2>Avoid having many identical ops</h2>
<!-- Better re-write; maybe separate out a section about Python names for TF ops, losing references to your ops, etc. -->

<p>Lots of methods in TensorFlow create ops in the computation graph, but do not execute them. You may want to execute multiple times, but that doesn't mean you should create lots of copies of the same ops.</p>
<p>A clear example is <code>tf.global_variables_initializer()</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
# Create some variables...
&gt;&gt;&gt; initializer = tf.global_variables_initializer()
# Variables are not yet initialized.
&gt;&gt;&gt; session.run(initializer)
# Now variables are initialized.
# Do some more work...
&gt;&gt;&gt; session.run(initializer)
# Now variables are re-initialized.</code></pre>

<p>If the call to <code>tf.global_variables_initializer()</code> is repeated, for example directly as the argument to <code>session.run()</code>, there are downsides.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.global_variables_initializer())
&gt;&gt;&gt; session.run(tf.global_variables_initializer())</code></pre>

<p>A new initializer op is created every time the argument to <code>session.run()</code> here is evaluated. This creates multiple initializer ops in the graph. Having multiple copies a big deal for small ops in an interactive session, and you might even want to do it in the case of the initializer if you've created more variables that need to be included in initialization. But think about whether you need lots of duplicate ops.</p>
<p>Creating ops inside <code>session.run()</code>, you also don't have a Python variable referring to those ops, so you can't easily reuse them.</p>
<p>In Python, if you create an object that nothing refers to, it can be garbage collected. The abandoned object will be deleted and and memory it used will be freed. That doesn't happen to things in the TensorFlow graph; everything you put in the graph stays there.</p>
<p>It's pretty clear that <code>tf.global_variables_initializer()</code> returns an op. But ops are also created in less obvious ways.</p>
<p>Let's compare to how NumPy works.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>At this point there are two arrays in memory, <code>x</code> and <code>y</code>. The <code>y</code> has the value 2.0, but there's no record of <em>how</em> it came to have that value. The addition has left no record of itself.</p>
<p>TensorFlow is different.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Now only <code>x</code> is a TensorFlow variable; <code>y</code> is an <code>add</code> op, which can return the result of that addition if we ever run it.</p>
<p>One more comparison.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Here <code>y</code> is assigned to refer to one result array <code>x + 1.0</code>, and then reassigned to point to a different one. The first one will be garbage collected and disappear.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>In this case, <code>y</code> refers to one <code>add</code> op in the TensorFlow graph, and then <code>y</code> is reassigned to point to a different <code>add</code> op in the graph. Since <code>y</code> only points to the second <code>add</code> now, we don't have a convenient way to work with the first one. But both the <code>add</code> ops are still around, in the graph, and will stay there.</p>
<p>(As an aside, Python's mechanism for defining class-specific addition <a href="http://www.python-course.eu/python3_magic_methods.php">and so on</a>, which is how <code>+</code> is made to create TensorFlow ops, is pretty neat.)</p>
<p>Especially if you're just working with the default graph and running interactively in a regular REPL or a notebook, you can end up with a lot of abandoned ops in your graph. Every time you re-run a notebook cell that defines any graph ops, you aren't just redefining ops&#8212;you're creating new ones.</p>
<p>Often it's okay to have a few extra ops floating around when you're experimenting. But things can get out of hand.</p>
<pre><code class="language-python">for _ in range(1e6):
    x = x + 1</code></pre>

<p>If <code>x</code> is a NumPy array, or just a regular Python number, this will run in constant memory and finish with one value for x.</p>
<p>But if <code>x</code> is a TensorFlow variable, there will be over a million ops in your TensorFlow graph, just defining a computation and not even <em>doing</em> it.</p>
<p>One immediate fix for TensorFlow is to use a <code>tf.assign</code> op, which gives behavior more like what you might expect.</p>
<pre><code class="language-python">increment_x = tf.assign(x, x + 1)
for _ in range(1e6):
    session.run(increment_x)</code></pre>

<p>This revised version does not create any ops inside the loop, which is generally good advice. TensorFlow does have <a href="https://www.tensorflow.org/api_guides/python/control_flow_ops">control flow constructs</a> including <a href="https://www.tensorflow.org/api_docs/python/tf/while_loop">while loops</a>. But only use these when really needed.</p>
<p>Be conscious of when you're creating ops, and only create the ones you need. Try to keep op creation distinct from op execution. And after interactive experimentation, eventually get to a state, probably in a script, where you're only creating the ops that you need.</p>
<h2>Avoid constants in the graph</h2>
<p>A particularly unfortunate op to needlessly add to a graph is accidental constant ops, especially large ones.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones = np.ones((1000, 1000))</code></pre>

<p>There are a million ones in the NumPy array <code>many_ones</code>. We can add them up.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones.sum()
## 1000000.0</code></pre>

<p>What if we add them up with TensorFlow?</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.reduce_sum(many_ones))
## 1000000.0</code></pre>

<p>The result is the same, but the mechanism is quite different. This not only added some ops to the graph&#8212;it put a copy of the entire million-element array into the graph as a constant.</p>
<p>Variations on this pattern can result in accidentally loading an entire data set into the graph as constants. A program might still run, for small data sets. Or your system might fail.</p>
<p>One simple way to avoid storing data in the graph is to use the <code>feed_dict</code> mechanism.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_things = tf.placeholder(tf.float64)
&gt;&gt;&gt; adder = tf.reduce_sum(many_things)
&gt;&gt;&gt; session.run(adder, feed_dict={many_things: many_ones})
## 1000000.0</code></pre>

<p>As before, be clear about what you're adding to the graph and when. Concrete data usually only enters the graph at moments of evaluation.</p>
<h2>TensorFlow as Functional Programming</h2>
<p>TensorFlow ops are like functions. This is especially clear when an op has one or more placeholder inputs; evaluating the op in a session is like calling a function with those arguments. So Python functions that return TensorFlow ops are like <a href="https://en.wikipedia.org/wiki/Higher-order_function">higher-order functions</a>.</p>
<p>You might decide that it's worth putting a constant into the graph. For example, if you have to reshape a lot of tensors to 28x28, you might make an op that does that.</p>
<pre><code class="language-python">&gt;&gt;&gt; input_tensor = tf.placeholder(tf.float32)
&gt;&gt;&gt; reshape_to_28 = tf.reshape(input_tensor, shape=[28, 28])</code></pre>

<p>This is like <a href="https://en.wikipedia.org/wiki/Currying">currying</a> in that the <code>shape</code> argument has now been set. The <code>[28, 28]</code> has become a constant in the graph and specified that argument. Now to evaluate <code>reshape_to_28</code> we only have to provide <code>input_tensor</code>.</p>
<p>Broader connections have been suggested between <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">neural networks, types, and functional programming</a>. It's interesting to think of TensorFlow as a system that supports this kind of construction.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</guid>
<pubDate>Tue, 04 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Images and TFRecords</title>
<description><![CDATA[

<p>There are a number of ways to work with images in TensorFlow and, if you wish, with TFRecords. These methods aren't so mysterious if you <a href="/20170323-tfrecords_for_humans/">understand TFRecords</a> and a little bit about how digital images work.</p>
<hr>
<h2>Representations for Images</h2>
<!--

(I kind of like this paragraph, at least in that it gives credit for the image source, but it's not really key to the development here.)

Wikipedia [tells me](https://en.wikipedia.org/wiki/Keep_Calm_and_Carry_On) that this [poster](https://commons.wikimedia.org/wiki/File:Freedom_Is_In_Peril_Defend_It_With_All_Your_Might.svg), and the more widely known "Keep Calm and Carry On," were not very well received [in 1939](img/freedom_in_tube.jpg). Perhaps its time has come.

-->

<p>For example, this image is 600 pixels tall and 400 pixels wide. Every pixel has some intensity in red, green, and blue: three values, or channels, for every pixel. This image has shape [600, 400, 3]. (The order of the dimensions doesn't matter as long as everybody agrees on the convention.) The display on your screen is like a dense matrix; it is a <a href="https://en.wikipedia.org/wiki/Raster_graphics">raster graphic</a>.</p>
<p><img alt="FREEDOM IS IN PERIL / DEFEND IT WITH ALL YOUR MIGHT" src="img/freedom.png"></p>
<p>Neural networks that work with images typically work with this kind of dense matrix representation. For this image, the matrix will have 600 * 400 * 3 = 720,000 values. If each value is an unsigned 8-bit integer, that's 720,000 bytes, or about three quarters of a megabyte.</p>
<p>Images sometimes also have an alpha transparency channel, which is a fourth channel in a color image, but not necessary if there's nothing else "underneath" the image. And not all images are color; greyscale (black and white) images can have just one channel.</p>
<p>Using unsigned 8-bit integers (256 possible values) for each value in the image array is enough for displaying images to humans. But when working with image data, it isn't uncommon to switch to 32-bit floats, for example. This quadruples the size of the data in memory. As always, remain aware of your data types.</p>
<h3>Dense Array</h3>
<p>One way to store complete raster image data is by serializing a NumPy array to disk with <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html"><code>numpy.save</code></a>.</p>
<pre>720,080 bytes  <a href="img/freedom.npy">freedom.npy</a></pre>

<p>The file <code>freedom.npy</code> has 80 more bytes than the ones required to store the array values. Those extra bytes specify things like the dimensions of the array. If we save raw array values in TFRecords, we'll also have to keep track of this additional information.</p>
<p>Because storing one or more value for every pixel takes so many bytes, file formats for images typically do something cleverer.</p>
<h3>JPEG</h3>
<p><a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a> is lossy. When you save an array as JPG and then read from the JPG, it will generally look about the same, but you won't necessarily get back exactly the same values for your array. JPEG is like <a href="https://en.wikipedia.org/wiki/MP3">MP3</a> for images. JPEG is good for photographs.</p>
<pre> 25,136 bytes  <a href="img/freedom.jpg">freedom.jpg</a></pre>

<p><code>freedom.jpg</code> is less than 4% the size of the NumPy array, and it still looks pretty good. It does have some ringing artifacts around the letter edges. JPEG file size depends on compression parameters when saving, and on the encoder used. For example, Google <a href="https://research.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html">released</a> their <a href="https://github.com/google/guetzli/">Guetzli</a> JPEG encoder, which can produce smaller files but takes more computation to do so.</p>
<pre> 46,567 bytes  <a href="img/freedom2.jpg">freedom2.jpg</a></pre>

<p><code>freedom2.jpg</code> is another JPEG file, saved with higher quality. Ringing artifacts are pretty much gone. The file is still under 7% the size of the NumPy arrray.</p>
<h3>PNG</h3>
<p><a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics">PNG</a> is lossless. If you save as PNG and then read from the PNG, you can get back exactly what you had originally. PNG is like <a href="https://en.wikipedia.org/wiki/Zip_(file_format)">ZIP</a> for images, but image viewers do the "un-zipping" automatically so they can put a raster image on the screen. PNG is like <a href="https://en.wikipedia.org/wiki/GIF">GIF</a> without animation.</p>
<pre> 33,192 bytes  <a href="img/freedom.png">freedom.png</a></pre>

<p><code>freedom.png</code> is under 5% the size of the NumPy array, and it preserves the image perfectly. It's less often used, but compression parameters can also affect PNG size, and the encoder can make a difference too. PNG uses deflate (zlip) compression, so Google's <a href="https://github.com/google/zopfli">Zopfli</a> algorithm can be used, for example, while <a href="https://github.com/google/brotli">Brotli</a> cannot.</p>
<h3>SVG</h3>
<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> doesn't represent pixels directly at all, but represents in a text format the geometry of shapes in the image. SVG images can look good at any zoom level; they don't suffer from pixelization. They can also be edited with different tools than raster images. And for simple images, an SVG can be quite small. SVG is like <a href="https://en.wikipedia.org/wiki/HTML">HTML</a>; you can look at it as text, but to see it as intended requires a program like a web browser.</p>
<pre> 42,721 bytes  <a href="img/freedom.svg">freedom.svg</a></pre>

<p><code>freedom.svg</code> turns out not be a super efficient encoding of the image; it represents all the letter outlines instead of using plain text in some font, for example. But it represents the image fundamentally differently from just recreating pixels, and it's still under 6% the size of the NumPy array file.</p>
<h3>TFRecords?</h3>
<p>TFRecords don't know anything about image formats. You just put bytes in them. So you have your choice of whether that means you store dense arrays of values or a well-known image format. TensorFlow provides <a href="https://www.tensorflow.org/api_guides/python/image">image format support</a> for JPEG, PNG, and GIF in the computation graph.</p>
<hr>
<h2>Images without TensorFlow</h2>
<p>As always, <a href="/20170312-use_only_what_you_need_from_tensorflow/">you have a choice about whether you need to do everything with TensorFlow</a>. If you're loading data batches with a <code>feed_dict</code>, in particular, feel free to use whatever Python functionality you're comfortable with. Even if you're not, you'll probably want to do some work with your data outside of TensorFlow before you move all your computation into the graph.</p>
<p>The Matplotlib <a href="http://matplotlib.org/users/image_tutorial.html">image tutorial</a> recommends using <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imread"><code>matplotlib.image.imread</code></a> to read image formats from disk. This function will automatically change image array values to floats between zero and one, and it doesn't give you options about how to read the image.</p>
<p>The <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imread.html">scipy.misc.imread</a> function uses the Python Imaging Library (PIL) to support many image formats, including PNG and JPEG. This function also has some useful options. The original <code>freedom.png</code> has an alpha channel which we don't need. Passing <code>mode='RGB'</code> tells the reader to give us just three color channels.</p>
<pre><code class="language-python">&gt;&gt;&gt; import scipy.misc
&gt;&gt;&gt; image = scipy.misc.imread('freedom.png', mode='RGB')
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>Matplotlib's <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.imshow">imshow</a> is good for checking out what image arrays look like. (Also specify <code>%matplotlib inline</code> if you're in a notebook.)</p>
<pre><code class="language-python">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.imshow(image)</code></pre>

<p>NumPy gives us a way to save and load its arrays.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.save('freedom.npy', image)
&gt;&gt;&gt; same_image = np.load('freedom.npy')</code></pre>

<p>As long as we have <code>scipy.misc</code> imported, we can use <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imsave.html">scipy.misc.imsave</a> to write various image formats as well. For saving, <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imsave">matplotlib.image.imsave</a> actually has more options. Neither of these offer control over compression level, for example.</p>
<pre><code class="language-python">&gt;&gt;&gt; scipy.misc.imsave('freedom.jpg', image)</code></pre>

<hr>
<h2>PNG and JPEG with TensorFlow</h2>
<p>TensorFlow has ops for decoding and encoding PNGs and JPEGs, so we can put these operations into the computation graph.</p>
<p>Above, <code>imread</code> both read a file from disk and decoded it, and <code>imsave</code> both encoded an image and wrote it to disk. The TensorFlow functionality decouples the decoding and encoding from file reading and writing. This example will avoid using TensorFlow's file reading and writing, to focus just on the encoding and decoding.</p>
<p>The <code>channels=3</code> passed to <code>tf.image.decode_image()</code> is the equivalent of <code>mode='RGB'</code> above. We don't want to get the alpha channel from the PNG file.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; with open('freedom.png', 'rb') as f:
...     png_bytes = f.read()
&gt;&gt;&gt; bytes = tf.placeholder(tf.string)
&gt;&gt;&gt; decode_png = tf.image.decode_image(bytes, channels=3)
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; image = session.run(decode_png, feed_dict={bytes: png_bytes})
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_image"><code>tf.image.decode_image</code></a> here intelligently uses <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_png"><code>tf.image.decode_png</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_jpeg"><code>tf.image.decode_jpeg</code></a>, or <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_gif"><code>tf.image.decode_gif</code></a>, similar to how above <code>imread</code> can handle a variety of formats. You can also use the appropriate function directly.</p>
<p>Above, <code>imsave</code> supported writing various formats, choosing the one appropriate for the filename specified. In TensorFlow, you have to use <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_jpeg"><code>tf.image.encode_jpeg</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_png"><code>tf.image.encode_png</code></a> directly, and both provide extra arguments controlling compression and more.</p>
<pre><code class="language-python">&gt;&gt;&gt; tensor = tf.placeholder(tf.uint8)
&gt;&gt;&gt; encode_jpeg = tf.image.encode_jpeg(tensor)
&gt;&gt;&gt; jpeg_bytes = session.run(encode_jpeg, feed_dict={tensor: image})
&gt;&gt;&gt; with open('freedom.jpg', 'wb') as f:
...     f.write(jpeg_bytes)</code></pre>

<p>With the default settings, TensorFlow encodes a larger JPEG than <code>imsave</code>, coming in at 46,567 bytes. It looks pretty good.</p>
<hr>
<h2>PNG and JPEG in TFRecords</h2>
<p>We can put plain bytes into <code>Example</code> TFRecords <a href="/20170323-tfrecords_for_humans/">without too much trouble</a>. So PNG or JPEG images are easily handled.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'png_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=png_bytes))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_png_bytes = ''.join(
    same_example.features.feature['png_bytes'].bytes_list.value)</code></pre>

<p>(The <code>''.join()</code> relies on Python 2 string behavior.)</p>
<p>When the <code>same_png_bytes</code> is decoded by <code>tf.image.decode_image</code>, as above, or <code>tf.image.decode_png</code> directly, you'll get back a tensor with the correct dimensions, because PNG (and JPEG) include that information in their encodings.</p>
<hr>
<h2>Image Arrays in TFRecords</h2>
<p>If you want to save dense matrix representations in TFRecords, there's a little bit of bookkeeping to do, but it isn't too bad.</p>
<pre><code>image_bytes = image.tostring()
image_shape = image.shape

my_example = tf.train.Example(features=tf.train.Features(feature={
    'image_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=image_bytes)),
    'image_shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image_shape))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_image_bytes = ''.join(
    same_example.features.feature['image_bytes'].bytes_list.value)
same_image_shape = list(
    same_example.features.feature['image_shape'].int64_list.value)</code></pre>

<p>With the information recovered from TFRecord form, it's easy to use NumPy to put the image back together.</p>
<pre><code class="language-python">same_image = np.fromstring(same_image_bytes, dtype=np.uint8)
same_image.shape = same_image_shape</code></pre>

<p>You can do the same using TensorFlow.</p>
<pre><code class="language-python">shape = tf.placeholder(tf.int32)
new_image = tf.reshape(tf.decode_raw(bytes, tf.uint8), shape)
same_image = session.run(encode_jpeg, feed_dict={bytes: same_image_bytes,
                                                 shape: same_image_shape})</code></pre>

<p>In this example, however, the parsing of the <code>Example</code> was already done outside the TensorFlow graph, so there isn't a strong reason to stay inside the graph here.</p>
<hr>
<h3>Comparison to Caffe and LMDB</h3>
<p><a href="http://caffe.berkeleyvision.org/">Caffe</a> is an older deep learning framework that can work with data stored in <a href="https://symas.com/offerings/lightning-memory-mapped-database/">LMDB</a> on-disk databases, similar to how TensorFlow can work with data stored in TFRecords files.</p>
<p>Like TensorFlow, Caffe defines a protocol buffer message for training examples. In Caffe, it's called <code>Datum</code>. These are saved just like in TensorFlow, by serializing them and putting them in a file on disk, but instead of a TFRecords file, which just puts records in a row and reads them out in that order, here Caffe will work with LMDB, which has the semantics of a key-value store.</p>
<p>TensorFlow's <code>Example</code> format is super flexible, but the trade-off is that it doesn't automatically do things for you. Caffe's <code>Datum</code>, on the other hand, expects you to put in a dense image array, and an integer class label. So here there isn't any fiddling with array size, for example, but the trade-off is that it won't work easily for arbitrary data structures that we might eventually want to store.</p>
<p>In the TFRecords examples above, we stored only image data, and said nothing about a class label or anything else. This is because TFRecords lets you decide what you want to save, rather than defining a format in advance. You could save an integer label, or a float regression label, or a string of text, or an image mask, and on and on. Here, we're just using the built-in Caffe integer label.</p>
<pre><code class="language-python">import caffe
import lmdb

# `image` was read above
label = 9  # for a classification problem

datum = caffe.io.array_to_datum(arr=image, label=label)
datum_str = datum.SerializeToString()

env = lmdb.open('lmdb_data')
txn = env.begin(write=True)
txn.put(key='my datum', value=datum_str)

cur = txn.cursor()
same_datum_str = cur.get('my datum')

same_datum = caffe.proto.caffe_pb2.Datum().FromString(same_datum_str)
same_image = caffe.io.datum_to_array(same_datum)
same_label = datum.label</code></pre>

<p>To actually work with the Caffe training process, there are some other conditions for the data to satisfy. Caffe expects [channels, height, width] instead of [height, width, channels], for example.</p>
<hr>
<h2>What should you use?</h2>
<p>Prefer doing fewer separate manipulation steps to whatever your original data is, if you can help it: try not to have lots of different versions of your data in different places on disk. This will make your life easier.</p>
<p>If you are choosing a format, JPEG is good for photos.</p>
<p>Think about whether you need to put everything into the TensorFlow computation graph. Think about whether you need to use TFRecords. Try to spend your time on things that help solve your problems.</p>
<p>For two more complete <em>in situ</em> examples of converting images to TFRecords, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py">code for MNIST images</a> and <a href="https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py">code for ImageNet images</a>. The ImageNet code can be run on the command-line.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170403-images_and_tfrecords/</link>
<guid>http://planspace.org/20170403-images_and_tfrecords/</guid>
<pubDate>Mon, 03 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>On Tyranny: Twenty Lessons from the Twentieth Century</title>
<description><![CDATA[

<p>Shortly after the U.S. presidential election of 2016, <a href="http://history.yale.edu/people/timothy-snyder">Timothy Snyder</a> wrote a popular post that was widely <a href="https://qz.com/846940/a-yale-history-professors-20-point-guide-to-defending-democracy-under-a-trump-presidency/">re-published</a> and then expanded to become the book <a href="https://www.amazon.com/Tyranny-Twenty-Lessons-Twentieth-Century/dp/0804190119">On Tyranny: Twenty Lessons from the Twentieth Century</a>.</p>
<p>I've put together text versions of the post, in one file and in 22 files, because I think the ideas are important and so that I can use these versions in text data processing examples.</p>
<ul>
<li>Whole post in a single file:<ul>
<li><a href="on_tyranny.txt"><code>on_tyranny.txt</code></a></li>
</ul>
</li>
<li>Compressed directory of 22 files:<ul>
<li><a href="on_tyranny.tar.gz"><code>on_tyranny.tar.gz</code></a></li>
<li><a href="on_tyranny.zip"><code>on_tyranny.zip</code></a></li>
</ul>
</li>
<li>Each section individually:<ul>
<li><a href="on_tyranny/00_introduction.txt"><code>00_introduction.txt</code></a></li>
<li><a href="on_tyranny/01_do_not_obey_in_advance.txt"><code>01_do_not_obey_in_advance.txt</code></a></li>
<li><a href="on_tyranny/02_defend_an_institution.txt"><code>02_defend_an_institution.txt</code></a></li>
<li><a href="on_tyranny/03_recall_professional_ethics.txt"><code>03_recall_professional_ethics.txt</code></a></li>
<li><a href="on_tyranny/04_distinguish_certain_words.txt"><code>04_distinguish_certain_words.txt</code></a></li>
<li><a href="on_tyranny/05_be_calm_when_the_unthinkable_arrives.txt"><code>05_be_calm_when_the_unthinkable_arrives.txt</code></a></li>
<li><a href="on_tyranny/06_be_kind_to_our_language.txt"><code>06_be_kind_to_our_language.txt</code></a></li>
<li><a href="on_tyranny/07_stand_out.txt"><code>07_stand_out.txt</code></a></li>
<li><a href="on_tyranny/08_believe_in_truth.txt"><code>08_believe_in_truth.txt</code></a></li>
<li><a href="on_tyranny/09_investigate.txt"><code>09_investigate.txt</code></a></li>
<li><a href="on_tyranny/10_practice_corporeal_politics.txt"><code>10_practice_corporeal_politics.txt</code></a></li>
<li><a href="on_tyranny/11_make_eye_contact_and_small_talk.txt"><code>11_make_eye_contact_and_small_talk.txt</code></a></li>
<li><a href="on_tyranny/12_take_responsibility_for_the_face_of_the_world.txt"><code>12_take_responsibility_for_the_face_of_the_world.txt</code></a></li>
<li><a href="on_tyranny/13_hinder_the_one-party_state.txt"><code>13_hinder_the_one-party_state.txt</code></a></li>
<li><a href="on_tyranny/14_give_regularly_to_good_causes.txt"><code>14_give_regularly_to_good_causes.txt</code></a></li>
<li><a href="on_tyranny/15_establish_a_private_life.txt"><code>15_establish_a_private_life.txt</code></a></li>
<li><a href="on_tyranny/16_learn_from_others_in_other_countries.txt"><code>16_learn_from_others_in_other_countries.txt</code></a></li>
<li><a href="on_tyranny/17_watch_out_for_the_paramilitaries.txt"><code>17_watch_out_for_the_paramilitaries.txt</code></a></li>
<li><a href="on_tyranny/18_be_reflective_if_you_must_be_armed.txt"><code>18_be_reflective_if_you_must_be_armed.txt</code></a></li>
<li><a href="on_tyranny/19_be_as_courageous_as_you_can.txt"><code>19_be_as_courageous_as_you_can.txt</code></a></li>
<li><a href="on_tyranny/20_be_a_patriot.txt"><code>20_be_a_patriot.txt</code></a></li>
<li><a href="on_tyranny/21_credits.txt"><code>21_credits.txt</code></a></li>
</ul>
</li>
</ul>
<p>The contents of <a href="on_tyranny.txt"><code>on_tyranny.txt</code></a> inline:</p>
<pre>
Americans are no wiser than the Europeans who saw democracy yield to
fascism, Nazism, or communism. Our one advantage is that we might
learn from their experience. Now is a good time to do so. Here are
twenty lessons from the twentieth century, adapted to the
circumstances of today.

Lesson 1. Do not obey in advance.

Much of the power of authoritarianism is freely given. In times like
these, individuals think ahead about what a more repressive government
will want, and then start to do it without being asked. You&#8217;ve already
done this, haven&#8217;t you? Stop. Anticipatory obedience teaches
authorities what is possible and accelerates unfreedom.


Lesson 2. Defend an institution.

Defend an institution. Follow the courts or the media, or a court or a
newspaper. Do not speak of &#8220;our institutions&#8221; unless you are making
them yours by acting on their behalf. Institutions don&#8217;t protect
themselves. They go down like dominoes unless each is defended from
the beginning.


Lesson 3. Recall professional ethics.

When the leaders of state set a negative example, professional
commitments to just practice become much more important. It is hard to
break a rule-of-law state without lawyers, and it is hard to have show
trials without judges.


Lesson 4. When listening to politicians, distinguish certain words.

Look out for the expansive use of &#8220;terrorism&#8221; and &#8220;extremism.&#8221; Be
alive to the fatal notions of &#8220;exception&#8221; and &#8220;emergency.&#8221; Be angry
about the treacherous use of patriotic vocabulary.


Lesson 5: Be calm when the unthinkable arrives.

When the terrorist attack comes, remember that all authoritarians at
all times either await or plan such events in order to consolidate
power. Think of the Reichstag fire. The sudden disaster that requires
the end of the balance of power, the end of opposition parties, and so
on, is the oldest trick in the Hitlerian book. Don&#8217;t fall for it.


Lesson 6: Be kind to our language.

Avoid pronouncing the phrases everyone else does. Think up your own
way of speaking, even if only to convey that thing you think everyone
is saying. (Don&#8217;t use the internet before bed. Charge your gadgets
away from your bedroom, and read.) What to read? Perhaps The Power of
the Powerless by V&#225;clav Havel, 1984 by George Orwell, The Captive Mind
by Czes&#322;aw Milosz, The Rebel by Albert Camus, The Origins of
Totalitarianism by Hannah Arendt, or Nothing is True and Everything is
Possible by Peter Pomerantsev.


Lesson 7: Stand out.

Someone has to. It is easy, in words and deeds, to follow along. It
can feel strange to do or say something different. But without that
unease, there is no freedom. And the moment you set an example, the
spell of the status quo is broken, and others will follow.


Lesson 8: Believe in truth.

To abandon facts is to abandon freedom. If nothing is true, then no
one can criticize power, because there is no basis upon which to do
so. If nothing is true, then all is spectacle. The biggest wallet pays
for the most blinding lights.


Lesson 9: Investigate.

Figure things out for yourself. Spend more time with long articles.
Subsidize investigative journalism by subscribing to print media.
Realize that some of what is on your screen is there to harm you.
Learn about sites that investigate foreign propaganda pushes.


Lesson 10: Practice corporeal politics.

Power wants your body softening in your chair and your emotions
dissipating on the screen. Get outside. Put your body in unfamiliar
places with unfamiliar people. Make new friends and march with them.


Lesson 11: Make eye contact and small talk.

This is not just polite. It is a way to stay in touch with your
surroundings, break down unnecessary social barriers, and come to
understand whom you should and should not trust. If we enter a culture
of denunciation, you will want to know the psychological landscape of
your daily life.


Lesson 12: Take responsibility for the face of the world.

Notice the swastikas and the other signs of hate. Do not look away and
do not get used to them. Remove them yourself and set an example for
others to do so.


Lesson 13: Hinder the one-party state.

The parties that took over states were once something else. They
exploited a historical moment to make political life impossible for
their rivals. Vote in local and state elections while you can.


Lesson 14: Give regularly to good causes, if you can.

Pick a charity and set up autopay. Then you will know that you have
made a free choice that is supporting civil society helping others
doing something good.


Lesson 15: Establish a private life.

Nastier rulers will use what they know about you to push you around.
Scrub your computer of malware. Remember that email is skywriting.
Consider using alternative forms of the internet, or simply using it
less. Have personal exchanges in person. For the same reason, resolve
any legal trouble. Authoritarianism works as a blackmail state,
looking for the hook on which to hang you. Try not to have too many
hooks.


Lesson 16: Learn from others in other countries.

Keep up your friendships abroad, or make new friends abroad. The
present difficulties here are an element of a general trend. And no
country is going to find a solution by itself. Make sure you and your
family have passports.


Lesson 17: Watch out for the paramilitaries.

When the men with guns who have always claimed to be against the
system start wearing uniforms and marching around with torches and
pictures of a Leader, the end is nigh. When the pro-Leader
paramilitary and the official police and military intermingle, the
game is over.


Lesson 18: Be reflective if you must be armed.

If you carry a weapon in public service, God bless you and keep you.
But know that evils of the past involved policemen and soldiers
finding themselves, one day, doing irregular things. Be ready to say
no. (If you do not know what this means, contact the United States
Holocaust Memorial Museum and ask about training in professional
ethics.)


Lesson 19: Be as courageous as you can.

If none of us is prepared to die for freedom, then all of us will die
in unfreedom.


Lesson 20: Be a patriot.

The incoming president is not. Set a good example of what America
means for the generations to come. They will need it.


This was written by Timothy Snyder:
http://history.yale.edu/people/timothy-snyder

This text is from the version published here:
https://qz.com/846940/a-yale-history-professors-20-point-guide-to-defending-democracy-under-a-trump-presidency/

The short post version was expanded to a book:
https://www.amazon.com/Tyranny-Twenty-Lessons-Twentieth-Century/dp/0804190119
</pre>    
    ]]></description>
<link>http://planspace.org/20170331-on_tyranny/</link>
<guid>http://planspace.org/20170331-on_tyranny/</guid>
<pubDate>Fri, 31 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TFRecords via Protocol Buffer Definitions</title>
<description><![CDATA[

<p>I've written about <a href="/20170323-tfrecords_for_humans/">how to use TFRecords</a>, and I've written about <a href="/20170329-protocol_buffers_in_python/">how protocol buffer messages are defined</a>. Since the data structures in TFRecords are defined as protocol buffer messages, we can use their definitions to understand them.</p>
<p>Protocol buffer definitions remind me a little bit of <a href="https://en.wikipedia.org/wiki/Syntax_diagram">railroad diagrams</a> like the ones in Douglas Crockford's <a href="http://shop.oreilly.com/product/9780596517748.do">JavaScript: The Good Parts</a>. Here's how Crockford builds up number literals in JavaScript, starting from digits:</p>
<p><img alt="integer railroad diagram" src="img/railroad_integer.png"></p>
<p><img alt="fraction railroad diagram" src="img/railroad_fraction.png"></p>
<p><img alt="exponent railroad diagram" src="img/railroad_exponent.png"></p>
<p><img alt="number railroad diagram" src="img/railroad_number.png"></p>
<p>Small components are fully defined, which can then be built into more complex constructs. (<a href="http://archive.oreilly.com/pub/a/javascript/excerpts/javascript-good-parts/syntax-diagrams.html">More examples</a>.) Protocol buffer definitions are built in much the same way.</p>
<p>With comments, the TensorFlow source files <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto">feature.proto</a> and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto">example.proto</a> total 400 lines. There are only about 30 lines are not comments. These lines (rearranged slightly) do all the defining of the <code>Example</code> and <code>SequenceExample</code> formats that are used for TFRecords.</p>
<pre><code>message BytesList {
  repeated bytes value = 1;
}
message FloatList {
  repeated float value = 1 [packed = true];
}
message Int64List {
  repeated int64 value = 1 [packed = true];
}
message Feature {
  oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
  }
}
message Features {
  map&lt;string, Feature&gt; feature = 1;
}
message Example {
  Features features = 1;
}
message FeatureList {
  repeated Feature feature = 1;
}
message FeatureLists {
  map&lt;string, FeatureList&gt; feature_list = 1;
}
message SequenceExample {
  Features context = 1;
  FeatureLists feature_lists = 2;
}</code></pre>

<p>These brief definitions give rise to most of the functionality <a href="/20170329-protocol_buffers_in_python/">described</a> for creating, writing, and reading the TFRecords formats.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170330-tfrecords_via_proto/</link>
<guid>http://planspace.org/20170330-tfrecords_via_proto/</guid>
<pubDate>Thu, 30 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Protocol Buffers in Python</title>
<description><![CDATA[

<p>Google's data interchange format, <a href="https://en.wikipedia.org/wiki/Protocol_Buffers">Protocol Buffers</a>, is pretty straightforward.</p>
<hr>
<h3>Installing Protobuf</h3>
<p>Python support for protocol buffers can be installed with <code>pip</code>:</p>
<pre><code class="language-bash">pip install protobuf</code></pre>

<p>The package is imported as <code>google.protobuf</code>, but you likely won't need to import it.</p>
<p>To define new protocol buffer formats, you'll also want the <code>protoc</code> tool. One way to get it is to find and install a system-specific package, but you can also get it by installing another Google Python package, <code>grpcio-tools</code>:</p>
<pre><code class="language-bash">pip install grpcio-tools</code></pre>

<p>This won't put a <code>protoc</code> executable in your <code>PATH</code>, but it will let you run <code>protoc</code> via Python, as <code>python -m grpc_tools.protoc</code>. For convenience, you can add an alias in your shell:</p>
<pre><code class="language-bash">alias protoc='python -m grpc_tools.protoc'</code></pre>

<hr>
<h3>Defining Protobuf Messages</h3>
<p>The details of protocol buffer messages types are defined in <code>.proto</code> files like <a href="my_example.proto"><code>my_example.proto</code></a>.</p>
<pre><code>syntax = "proto3";

message Bottle {
  string note = 1;
}</code></pre>

<p>Syntax version 3 has to be specified, as the default is still version 2.</p>
<p>We're defining a fairly dull message. A <code>Bottle</code> can contain one <code>note</code>, which is a string.</p>
<p>The number one there is not setting a default value, but specifying a numbering that's used internally when reading and writing binary representations of our messages.</p>
<hr>
<h3>Generating Code for our Protobuf</h3>
<p>Assuming we're in the same directory as <code>my_example.proto</code>, we can use <code>protoc</code> to generate some Python code corresponding to the message type we defined:</p>
<pre><code class="language-bash">protoc --proto_path=./ --python_out=./ my_example.proto</code></pre>

<p>This will produce a new file <code>my_example_pb2.py</code>. (Syntax version 3 does not affect the filename here.)</p>
<hr>
<h3>Using Protobuf Messages in Python</h3>
<p>With the generated <code>my_example_pb2.py</code> code, we can use our message type in Python, and take advantage of features like serialization and deserialization.</p>
<pre><code class="language-python">import my_example_pb2

my_bottle = my_example_pb2.Bottle(note='Ahoy!')

with open('my_bottle.pb', 'wb') as f:
    f.write(my_bottle.SerializeToString())

with open('my_bottle.pb', 'rb') as f:
    new_bottle = my_example_pb2.Bottle().FromString(f.read())</code></pre>

<p>This looks a lot like <a href="/20170323-tfrecords_for_humans/">examples with TFRecords</a>, because they use the same mechanism.</p>
<hr>
<h3>More Information</h3>
<p>The example here is deliberately minimal. For more detail, the <a href="https://developers.google.com/protocol-buffers/">Google protocol buffers site</a> is quite good, with a very nice <a href="https://developers.google.com/protocol-buffers/docs/pythontutorial">Python tutorial</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170329-protocol_buffers_in_python/</link>
<guid>http://planspace.org/20170329-protocol_buffers_in_python/</guid>
<pubDate>Wed, 29 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow as a Distributed Virtual Machine</title>
<description><![CDATA[

<p>TensorFlow has a flexible API, and it has automatic differentiation, and it can run on GPUs. But the thing that's really neat about TensorFlow is that it gives you a fairly general way to easily program across multiple computers.</p>
<p>TensorFlow's distributed runtime, the big bottom box in this figure from the <a href="https://research.google.com/pubs/pub45381.html">2016 paper</a> "TensorFlow: A system for large-scale machine learning", is the part of TensorFlow that runs the computation graph.</p>
<p><img alt="layered TensorFlow architecture" src="img/tensorflow_layers.png"></p>
<p>The computation graph, specified with <a href="https://en.wikipedia.org/wiki/Protocol_Buffers">protocol buffers</a>, is much higher level than <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java virtual machine</a> (JVM) bytecode. But I think it's interesting to think of the TensorFlow distributed runtime as a sort of <a href="https://en.wikipedia.org/wiki/Virtual_machine">virtual machine</a>. This is not a whole system virtual machine, but a process virtual machine, like the JVM, to "execute computer programs in a platform-independent environment." The Python API can be like a <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific language</a> for programming the TensorFlow graph.</p>
<p>Unlike the JVM, and unlike any other system I know, TensorFlow lets you directly put computation on multiple machines, pretty much however you want, and then it quietly handles all the details for you. Wherever it needs to, TensorFlow adds send and receive nodes to allow the graph to be executed as specified. This is shown in this figure from the <a href="https://research.google.com/pubs/pub45166.html">2015 paper</a> "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems" (also <a href="https://arxiv.org/abs/1603.04467">available on arXiv</a> and commonly linked as the <a href="http://download.tensorflow.org/paper/whitepaper2015.pdf">PDF on tensorflow.org</a>).</p>
<p><img alt="distributing a TensorFlow graph" src="img/distributed_graph.png"></p>
<p>The box labels in the figure are for devices, but with TensorFlow they could be on the same machine or on different machines, and it hardly matters.</p>
<p>The flexibility and ease that result are incredible. It isn't terribly hard to think of implementing your own map-reduce system using TensorFlow. It might even be reasonably performant, if you have a distributed file system! The common TensorFlow distributed model training techniques are a kind of map-reduce, after all. But you could do pretty near anything! This is pretty cool.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/</link>
<guid>http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/</guid>
<pubDate>Tue, 28 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow and Queues</title>
<description><![CDATA[

<p>There are many ways to implement <a href="https://en.wikipedia.org/wiki/Queue_(abstract_data_type)">queue data structures</a>, and TensorFlow has some of its own.</p>
<p><img alt="queue" src="img/queue.png"></p>
<hr>
<h3>FIFO Queue with a list</h3>
<p>In Python, a <a href="https://docs.python.org/3/library/stdtypes.html?highlight=list#list">list</a> can implement a first-in first-out (FIFO) queue, with slightly awkward syntax:</p>
<pre><code class="language-python">&gt;&gt;&gt; my_list = []
&gt;&gt;&gt; my_list.insert(0, 'a')
&gt;&gt;&gt; my_list.insert(0, 'b')
&gt;&gt;&gt; my_list.insert(0, 'c')
&gt;&gt;&gt; my_list.pop()
'a'
&gt;&gt;&gt; my_list.pop()
'b'
&gt;&gt;&gt; my_list.pop()
'c'</code></pre>

<p>A Python list is not a very efficient implementation for a first-in first-out queue, and it doesn't offer mechanisms for limiting the total length of the queue at any given time, for example. But also, queues are a common way to implement communication between threads, and not everything you might do with a list is perfectly thread-safe (though it is surprisingly close; thanks <a href="https://en.wikipedia.org/wiki/Global_interpreter_lock">GIL</a>).</p>
<hr>
<h3>FIFO Queue with the Python standard library</h3>
<p>The Python standard library's <a href="https://docs.python.org/3/library/queue.html">queue</a> (<code>Queue</code> in Python 2) provides several queue options. The <code>queue.Queue</code> class implements a thread-safe FIFO queue:</p>
<pre><code class="language-python">&gt;&gt;&gt; import queue
&gt;&gt;&gt; my_queue = queue.Queue()
&gt;&gt;&gt; my_queue.put('a')
&gt;&gt;&gt; my_queue.put('b')
&gt;&gt;&gt; my_queue.put('c')
&gt;&gt;&gt; my_queue.get()
'a'
&gt;&gt;&gt; my_queue.get()
'b'
&gt;&gt;&gt; my_queue.get()
'c'</code></pre>

<p>When making a <code>queue.Queue</code>, you can specify an integer <code>maxsize</code> argument to set a bound on how many things can be in the queue at any given time. The default is zero, which makes a queue that can store (theoretically) any number of things. And there are no restrictions on what you can put in; anything in Python can be added to one of these queues.</p>
<hr>
<h3>Why use TensorFlow Queues?</h3>
<p>TensorFlow also offers a number of queue options. There are a couple reasons to use TensorFlow queues over standard Python queues:</p>
<ul>
<li>TensorFlow queues live in TensorFlow computation graphs, with the attendant benefits of unifying things there and allowing distributed graph computation.</li>
<li>TensorFlow queues offer a few more methods than standard Python queues, like <code>dequeue_many</code>, which is good for getting training batches.</li>
<li>TensorFlow queues work with additional TensorFlow constructs, like the <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a>.</li>
<li>TensorFlow offers queue variants not in the Python standard library: the <a href="https://www.tensorflow.org/api_docs/python/tf/PaddingFIFOQueue">PaddingFIFOQueue</a> and <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue">RandomShuffleQueue</a>.</li>
</ul>
<hr>
<h3>FIFO Queue with TensorFlow</h3>
<p>Here's a standard TensorFlow <a href="https://www.tensorflow.org/api_docs/python/tf/FIFOQueue">FIFOQueue</a>:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; letter = tf.placeholder(tf.string)
&gt;&gt;&gt; queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string])
&gt;&gt;&gt; enqueue = queue.enqueue(letter)
&gt;&gt;&gt; dequeue = queue.dequeue()
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'a'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'b'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'c'})
&gt;&gt;&gt; session.run(dequeue)
'a'
&gt;&gt;&gt; session.run(dequeue)
'b'
&gt;&gt;&gt; session.run(dequeue)
'c'</code></pre>

<p>There's a little extra code to deal with the computation graph, and the TensorFlow queue requires a <code>capacity</code> argument and a <code>dtypes</code> argument.</p>
<p>The <code>capacity</code> is like the <code>maxsize</code> of a regular Python queue.</p>
<p>The <code>dtypes</code> argument is a list of <a href="https://www.tensorflow.org/programmers_guide/dims_types#data_types">Tensorflow data types</a>. The elements added to the queue will always be lists of tensors with the specified data types. Here, we add a single string tensor at a time to the queue.</p>
<hr>
<h3>Comparing Python standard library and TensorFlow queue offerings</h3>
<p>Here's a listing of queue types in the Python queue library and in TensorFlow.</p>
<ul>
<li>Python standard: <a href="https://docs.python.org/3/library/queue.html#queue.Queue">queue.Queue</a> / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/FIFOQueue">FIFOQueue</a></li>
<li>Python standard: <a href="https://docs.python.org/3/library/queue.html#queue.LifoQueue">queue.LifoQueue</a> / TensorFlow: no close equivalent</li>
<li>Python standard: <a href="https://docs.python.org/3/library/queue.html#queue.PriorityQueue">queue.PriorityQueue</a> / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/PriorityQueue">PriorityQueue</a></li>
<li>Python standard: no close equivalent / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/PaddingFIFOQueue">PaddingFIFOQueue</a></li>
<li>Python standard: no close equivalent / TensorFlow: <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue">RandomShuffleQueue</a></li>
</ul>
<hr>
<h3>Priority Queue with the Python standard library and TensorFlow</h3>
<p>A priority queue lets you assign a priority to each item as it is added, and the item that comes out when you next de-queue is the item with the highest priority currently in the queue. Higher priority is represented by lower numbers.</p>
<p>Both the standard Python queue library and TensorFlow have priority queues.</p>
<pre><code class="language-python">&gt;&gt;&gt; import queue
&gt;&gt;&gt; my_queue = queue.PriorityQueue()
&gt;&gt;&gt; my_queue.put([4, 'a'])
&gt;&gt;&gt; my_queue.put([1, 'b'])
&gt;&gt;&gt; my_queue.put([2, 'c'])
&gt;&gt;&gt; my_queue.get()
[1, 'b']
&gt;&gt;&gt; my_queue.get()
[2, 'c']
&gt;&gt;&gt; my_queue.get()
[4, 'a']</code></pre>

<p>The TensorFlow priority queue is just like the standard Python one, but with a little more strictness on types. When adding to the queue, the first tensor provided must be <code>tf.int64</code>, the priority.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; priority = tf.placeholder(tf.int64)
&gt;&gt;&gt; letter = tf.placeholder(tf.string)
&gt;&gt;&gt; queue = tf.PriorityQueue(capacity=10, types=[tf.string], shapes=[[]])
&gt;&gt;&gt; enqueue = queue.enqueue([priority, letter])
&gt;&gt;&gt; dequeue = queue.dequeue()
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(enqueue, feed_dict={priority: 4, letter: 'a'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={priority: 1, letter: 'b'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={priority: 2, letter: 'c'})
&gt;&gt;&gt; session.run(dequeue)
[1, 'b']
&gt;&gt;&gt; session.run(dequeue)
[2, 'c']
&gt;&gt;&gt; session.run(dequeue)
[4, 'a']</code></pre>

<p>As of TensorFlow 1.0.1 at least, <code>tf.PriorityQueue</code> calls its argument <code>types</code> rather than <code>dtypes</code>, and it seems to be requiring the <code>shapes</code> argument in spite of the documentation.</p>
<hr>
<h3>TensorFlow's <code>PaddingFIFOQueue</code></h3>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/PaddingFIFOQueue">PaddingFIFOQueue</a> has mini-batch model training in mind. It allows you to put in tensors of variable size, and when using <code>dequeue_many</code>, shorter ones get zero-padded to the maximum size in the batch.</p>
<pre><code class="language-python">import tensorflow as tf
numbers = tf.placeholder(tf.int64)
queue = tf.PaddingFIFOQueue(capacity=10, dtypes=[tf.int64], shapes=[[]])
enqueue = queue.enqueue(numbers)
dequeue_many = queue.dequeue_many(n=3)
session = tf.Session()
session.run(enqueue, feed_dict={numbers: [1]})
session.run(enqueue, feed_dict={numbers: [2, 3]})
session.run(enqueue, feed_dict={numbers: [3, 4, 5]})
session.run(dequeue_many)
array([[1, 0, 0],
       [2, 3, 0],
       [3, 4, 5]])</code></pre>

<hr>
<h3>TensorFlow's <code>RandomShuffleQueue</code></h3>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/RandomShuffleQueue">RandomShuffleQueue</a> also has batching in mind. It draws randomly from items currently in the queue.</p>
<p>To prevent pulling out items as they go in (not at all randomly) the <code>RandomShuffleQueue</code> has an argument <code>min_after_dequeue</code> which ensures that there are at least that many (plus the number being drawn) to randomly draw from. This requirement is dropped after the queue eventually has <code>.close()</code> called on it, so that all items can be drawn.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; letter = tf.placeholder(tf.string)
&gt;&gt;&gt; queue = tf.RandomShuffleQueue(capacity=10, dtypes=[tf.string],
...                               min_after_dequeue=2)
&gt;&gt;&gt; enqueue = queue.enqueue(letter)
&gt;&gt;&gt; dequeue = queue.dequeue()
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'a'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'b'})
&gt;&gt;&gt; session.run(enqueue, feed_dict={letter: 'c'})
&gt;&gt;&gt; session.run(dequeue)
'b'  # or 'a', or 'c'</code></pre>

<hr>
<h3>Queues inside TensorFlow</h3>
<p>Some parts of TensorFlow automatically create and use queues. For example, <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/train/string_input_producer">tf.train.string_input_producer</a> (among others in <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/input.py">input.py</a>) makes a queue, and a <a href="https://www.tensorflow.org/api_docs/python/tf/train/QueueRunner">QueueRunner</a>, and sets up a TensorBoard summary op.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170327-tensorflow_and_queues/</link>
<guid>http://planspace.org/20170327-tensorflow_and_queues/</guid>
<pubDate>Mon, 27 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>The TensorFlow Coordinator for Python Threading</title>
<description><![CDATA[

<p>A lot of Python code doesn't bother with <a href="https://en.wikipedia.org/wiki/Thread_(computing)">threads</a> at all, but TensorFlow <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues">encourages</a> using threads, especially for loading data.</p>
<p>Threading as TensorFlow recommends still uses the standard Python <a href="https://docs.python.org/3/library/threading.html">threading</a> library, but TensorFlow adds a <a href="https://www.tensorflow.org/api_docs/python/tf/train/Coordinator">Coordinator</a> class that makes some good practices easier and fits with the rest of TensorFlow.</p>
<p>This <code>Coordinator</code> is for within-process thread coordination on one machine. It doesn't touch the TensorFlow computation graph or "coordinate" across multiple machines.</p>
<p>The three scripts below show how TensorFlow's <code>Coordinator</code> functionality relates to standard Python functionality.</p>
<hr>
<h3>Script <a href="code/base.py">1</a>: Python standard library</h3>
<p>Four threads are going to run the function <code>sleep_politely</code>. The main script will sleep for five seconds and then ask that all the threads stop, by setting <code>SHOULD_STOP</code> to <code>True</code>. The script then waits for the threads to stop, with <code>thread.join()</code>, before finishing.</p>
<pre><code class="language-python">def sleep_politely(should_stop):
    while not should_stop():
        time.sleep(2)

SHOULD_STOP = False
should_stop = lambda: SHOULD_STOP
threads = [threading.Thread(target=sleep_politely, args=(should_stop,))
           for _ in range(4)]

for thread in threads:
    thread.start()
time.sleep(5)
SHOULD_STOP = True
for thread in threads:
    thread.join()</code></pre>

<p>This could be set up to allow requesting stoppage inside <code>sleep_politely</code>, but the setup here parallels the TensorFlow example that comes next.</p>
<hr>
<h3>Script <a href="code/tf.py">2</a>: with TensorFlow Coordinator</h3>
<p>This script behaves just as the above, but uses <code>tf.train.Coordinator</code>.</p>
<pre><code class="language-python">def sleep_politely(coord):
    while not coord.should_stop():
        time.sleep(2)

coord = tf.train.Coordinator()
threads = [threading.Thread(target=sleep_politely, args=(coord,))
           for _ in range(4)]

for thread in threads:
    thread.start()
time.sleep(5)
coord.request_stop()
coord.join(threads)</code></pre>

<p>If appropriate, <code>coord.request_stop()</code> could also be called inside <code>sleep_politely</code> here.</p>
<hr>
<p>Coordinating threads is not always easy, and there are some problems with both scripts above.</p>
<ul>
<li>If there's an exception in the main script, the threads won't get stopped, and the program will effectively hang forever.</li>
<li>If there's an exception in any of the threads, everything else will carry on, whether it should or not.</li>
</ul>
<hr>
<h3>Script <a href="code/tf_full.py">3</a>: more complete TensorFlow</h3>
<pre><code class="language-python">def sleep_politely(coord):
    with coord.stop_on_exception():
        while not coord.should_stop():
            time.sleep(2)

coord = tf.train.Coordinator()
threads = [threading.Thread(target=sleep_politely, args=(coord,))
           for _ in range(4)]

try:
    for thread in threads:
        thread.start()
    time.sleep(5)
except Exception as exception:
    coord.request_stop(exception)
finally:
    coord.request_stop()
    coord.join(threads)</code></pre>

<p>The <code>try</code>/<code>except</code>/<code>finally</code> here means that even if there's an exception in the main script, the threads should get shut down. We could also do the same without TensorFlow.</p>
<p>The addition of the <code>coord.stop_on_exception()</code> context manager in <code>sleep_politely</code> means that if there's an exception in one of the threads, this will also shut everything down appropriately and pass along the exception. This would be a little more work to implement without TensorFlow, but it could be done.</p>
<hr>
<p>There are two reasons to use TensorFlow's <code>Coordinator</code>:</p>
<ul>
<li><code>Coordinator</code> makes some nice things particularly convenient.</li>
<li><code>Coordinator</code> fits with more pieces of TensorFlow, like the <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a>.</li>
</ul>
<p>The <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/training/coordinator.py">Python <code>Coordinator</code> code</a> is in one Python file and not super entangled with the rest of TensorFlow. For the threading convenience it provides, one could imagine extracting it for use elsewhere. The TensorFlow project seems to like the functionality well enough that it's one of the few components of <code>tf.training</code> that appears in both a Python and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/cc/training/coordinator.cc">C++ version</a>.</p>
<hr>
<p>Even with the full <code>Coordinator</code> setup, the SIGINT signal (as from pressing <code>ctrl-c</code>) and the SIGTERM signal (as from <code>kill</code>) will interrupt the main script as exceptions. An additional step would be to set up <a href="https://docs.python.org/3/library/signal.html">signal</a> handlers to orchestrate a more orderly shutdown for these cases, if the main script is doing anything very involved.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/</link>
<guid>http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/</guid>
<pubDate>Fri, 24 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TFRecords for Humans</title>
<description><![CDATA[

<p>TensorFlow <a href="https://www.tensorflow.org/programmers_guide/reading_data#standard_tensorflow_format">recommends</a> its TFRecords format as the standard TensorFlow format for data on disk.</p>
<p>You <a href="/20170312-use_only_what_you_need_from_tensorflow/">don't have to</a> use TFRecords with TensorFlow. But if you need to read data inside your TensorFlow graph, and a reader op doesn't exist for your data, it might be easier to transform your data to TFRecords than to write a <a href="https://www.tensorflow.org/extend/new_data_formats">custom data reader op</a>.</p>
<p>Before using TFRecords in a distributed setting, you probably want to understand and work with them locally.</p>
<hr>
<h3>The TFRecords Format</h3>
<p>TFRecords is a <a href="https://www.tensorflow.org/api_guides/python/python_io#TFRecords_Format_Details">simple binary file format</a>. It lets you put one or more strings of bytes into a file. You could put any bytes you like in a TFRecords file, but it'll be more useful to use the formats provided in TensorFlow.</p>
<p>TensorFlow defines two <a href="https://developers.google.com/protocol-buffers/">protocol buffer</a> message types for use with TFRecords: the <a href="https://www.tensorflow.org/api_docs/python/tf/train/Example">Example</a> message type and the <a href="https://www.tensorflow.org/api_docs/python/tf/train/SequenceExample">SequenceExample</a> message type.</p>
<p>The pre-defined protocol buffer message types offer flexibility by letting you arrange your data as a map from string keys to values that are lists of integers, lists of 32-bit floats, or lists of bytes.</p>
<hr>
<h3>Writing and Reading in the style of <code>Example</code> Records without TensorFlow</h3>
<p>An equivalent representation of an <code>Example</code> TFRecord using Python dictionaries might look like this:</p>
<pre><code class="language-python">my_dict = {'features' : {
    'my_ints': [5, 6],
    'my_float': [2.7],
    'my_bytes': 'data'
}}</code></pre>

<p>In Python 2, the string literal <code>'data'</code> is bytes. The equivalent in Python 3 is <code>bytes('data', 'utf-8')</code>. And Python uses 64-bit floats rather than the 32-bit floats that TFRecords uses, so we have more precision in Python.</p>
<p>The values in this <code>dict</code> are accessed like this:</p>
<ul>
<li><code>my_dict['features']['my_ints']</code></li>
<li><code>my_dict['features']['my_float']</code></li>
<li><code>my_dict['features']['my_bytes']</code></li>
</ul>
<p>Ordinarily, to save this data (serialize and write to disk) and then read it again (read from disk and deserialize) in Python you might use the <a href="https://docs.python.org/3/library/pickle.html">pickle</a> module. For example:</p>
<pre><code class="language-python">import pickle

my_dict_str = pickle.dumps(my_dict)
with open('my_dict.pkl', 'w') as f:
    f.write(my_dict_str)

with open('my_dict.pkl', 'r') as f:
    that_dict_str = f.read()
that_dict = pickle.loads(that_dict_str)</code></pre>

<hr>
<h3>Writing and Reading <code>Example</code> Records with TensorFlow</h3>
<p>The TFRecords <code>Example</code> format defines things in detail: An <code>Example</code> contains one <code>Features</code>, which is a map from strings to <code>Feature</code> elements, which can each be <code>Int64List</code>, <code>FloatList</code>, or <code>BytesList</code>. (See also: <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto">example.proto</a> and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto">feature.proto</a>)</p>
<pre><code class="language-python">import tensorflow as tf

my_example = tf.train.Example(features=tf.train.Features(feature={
    'my_ints': tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 6])),
    'my_float': tf.train.Feature(float_list=tf.train.FloatList(value=[2.7])),
    'my_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value='data'))
}))</code></pre>

<p>The values in the <code>Example</code> can be accessed then like this:</p>
<ul>
<li><code>my_example.features.feature['my_ints'].int64_list.value</code></li>
<li><code>my_example.features.feature['my_float'].float_list.value</code></li>
<li><code>my_example.features.feature['my_bytes'].bytes_list.value</code></li>
</ul>
<p>Writing to and reading from disk are much like with <code>pickle</code>, except that the reader here provides all the records from a TFRecords file. In this example, there's only one record in the file.</p>
<pre><code class="language-python">my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]</code></pre>

<hr>
<h3>Images in <code>Example</code> Records</h3>
<p>The <code>Example</code> format lets you store pretty much any kind of data, including images. But the mechanism for arranging the data into serialized bytes, and then reconstructing the original format again, is left up to you. For more on this, see my post on <a href="/20170403-images_and_tfrecords/">Images and TFRecords</a>.</p>
<p>For two more complete <em>in situ</em> examples of converting images to TFRecords, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py">code for MNIST images</a> and <a href="https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py">code for ImageNet images</a>. The ImageNet code can be run on the command-line.</p>
<hr>
<h3>The <code>SequenceExample</code> Record</h3>
<p>The <code>SequenceExample</code> message type essentially extends <code>Example</code> for sequence data. (You could imagine achieving the same effect with just <code>Example</code>, but it would be awkward.)</p>
<p>A <code>SequenceExample</code> keeps the same kind of map as <code>Example</code>, but calls it <code>context</code>, because it's thought of as the static context for the dynamic sequence data. And it adds another map, called <code>feature_lists</code>, that maps from string keys to lists of lists.</p>
<p>In Python dictionaries, a <code>SequenceExample</code> is like this:</p>
<pre><code class="language-python">my_seq_dict = {
    'context' : {
        'my_bytes':
            'data'},
    'feature_lists' : {
        'my_ints': [
            [5, 6],
            [7, 8, 9]]}}</code></pre>

<p>A corresponding full <code>SequenceExample</code> is a bit more verbose:</p>
<pre><code class="language-python">my_seq_ex = tf.train.SequenceExample(
    context=tf.train.Features(feature={
        'my_bytes':
            tf.train.Feature(bytes_list=tf.train.BytesList(value='data'))}),
    feature_lists=tf.train.FeatureLists(feature_list={
        'my_ints': tf.train.FeatureList(feature=[
            tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 6])),
            tf.train.Feature(int64_list=tf.train.Int64List(value=[7, 8, 9]))])}))</code></pre>

<p>You probably don't want to mix <code>Example</code> and <code>SequenceExample</code> records in the same TFRecords file.</p>
<hr>
<h3>Reading TFRecords in a TensorFlow Graph</h3>
<p>You may eventually want to read TFRecords files with ops in a TensorFlow graph, using <a href="https://www.tensorflow.org/api_docs/python/tf/TFRecordReader">tf.TFRecordReader</a>. This will involve a filename queue; for an example, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/fully_connected_reader.py">some MNIST tutorial code</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170323-tfrecords_for_humans/</link>
<guid>http://planspace.org/20170323-tfrecords_for_humans/</guid>
<pubDate>Thu, 23 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow Logging</title>
<description><![CDATA[

<p>TensorFlow wraps the standard Python <a href="https://docs.python.org/3/library/logging.html">logging library</a>, exposing functionality at <code>tf.logging</code>.</p>
<p>This means that you can start logging things immediately after importing TensorFlow:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; tf.logging.info('Things are good!')
INFO:tensorflow:Things are good!</code></pre>

<p>TensorFlow also automatically logs things using this functionality. For example, estimators will log out information about the many things they're up to.</p>
<p>The TensorFlow <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/tf_logging.py">logging code</a> also adds some non-standard functionality. A comment notes that some of the code "is taken from pyglib/logging". I assume pyglib is a Google-internal Python library. I think it's interesting to see these Google tools <a href="/20170315-tensorflow_and_a_googleverse_like_the_hadleyverse/">appearing here and there</a>.</p>
<p>Here's an example of TensorFlow logging's added functionality: <code>log_every_n</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
INFO:tensorflow:Common error!
&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
&gt;&gt;&gt; tf.logging.log_every_n(tf.logging.INFO, 'Common error!', 2)
INFO:tensorflow:Common error!</code></pre>

<p>(This example won't work in an <a href="https://ipython.org/">IPython</a> shell because every new REPL "read" there looks like a new file to Python.)</p>
<hr>
<p>If you want to use regular Python logging techniques in addition to TensorFlow's mechanism, everything should generally work, but it is possible to get into a situation that seems surprising in a REPL.</p>
<pre><code class="language-python">&gt;&gt;&gt; import logging
&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; logging.warn('Something interesting!')
WARNING:root:Something interesting!
&gt;&gt;&gt; tf.logging.info('1+1=2')
INFO:tensorflow:1+1=2
INFO:tensorflow:1+1=2</code></pre>

<p>This doubling is because TensorFlow's log messages are bubbling up to the root logger as intended, but both the root logger and TensorFlow's logger are getting handled in the same place, so we see the message twice.</p>
<p>Shortly after the initial release of TensorFlow this kind of thing <a href="http://stackoverflow.com/questions/33662648/tensorflow-causes-logging-messages-to-double">was happening more than it should</a>, but the current behavior is correct.</p>
<p>The situation shown above happens because both the base logging package and TensorFlow's logging are trying to automatically be helpful by writing things out to the interactive session.</p>
<p>In practice you should set up log handlers yourself, <a href="http://docs.python-guide.org/en/latest/writing/logging/">as appropriate</a>. And of course if you only use one or the other of <code>logging</code> or <code>tf.logging</code> you'll be particularly unlikely to have any conflicts.</p>
<p>A poor solution to the doubling above would be to turn off log message propagation from the TensorFlow logger.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.logging._logger.propagate = False
&gt;&gt;&gt; tf.logging.info('1+1=2')
INFO:tensorflow:1+1=2</code></pre>

<p>But I can't recommend using that as anything but a quick fix in a REPL if double messages happen to be bothering you.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170322-tensorflow_logging/</link>
<guid>http://planspace.org/20170322-tensorflow_logging/</guid>
<pubDate>Wed, 22 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Various TensorFlow APIs for Python</title>
<description><![CDATA[

<p>An early <a href="http://fastml.com/what-you-wanted-to-know-about-tensorflow/">perception</a> of TensorFlow was that the API was quite low-level.</p>
<blockquote>
<p>"meaning you&#8217;ll be multiplying matrices and vectors."</p>
</blockquote>
<p>TensorFlow offers ops for low-level operations, and from the beginning programmers used those low-level ops to build higher-level APIs.</p>
<p>To paraphrase <a href="http://www.paulgraham.com/avg.html">Paul Graham</a>:</p>
<blockquote>
<p>"if you have a choice of several [APIs], it is, all other things being equal, a mistake to program in anything but the [highest-level] one."</p>
</blockquote>
<p>You don't need to be a Lisp hacker to make it easier to use TensorFlow the way you want by writing new functions and classes, and you probably should.</p>
<p>But because TensorFlow had already done the obviously hard work of making low-level things work, it was relatively easy for lots of people to attempt the subtly hard work of designing higher-level APIs.</p>
<p>This led to a proliferation of Python APIs, which can sometimes be confusing to track.</p>
<p>Things are still moving around and settling a bit, but Google is communicating a pretty clear plan for official Python APIs for TensorFlow.</p>
<p><img alt="TensorFlow six-tier diagram" src="img/tf_six_tiers.png"></p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<p>This plan changes some names and rearranges things slightly, but it's an incremental development of things that have existed for a while. We can look at how things have developed over time both for what's becoming official TensorFlow API and some of the projects that still exist outside official TensorFlow.</p>
<hr>
<h3>Python Frontend (op level)</h3>
<p>You can continue to use TensorFlow's low-level APIs.</p>
<p>There are ops, like <code>tf.matmul</code> and <code>tf.nn.relu</code>, which you might use to build a neural network architecture in full detail. To do really novel things, you may want this level of control. But you may also prefer to work with larger building blocks. The other other APIs below will mostly specialize in this kind of application.</p>
<p>There are also ops like <code>tf.image.decode_jpeg</code> (and many others) which may be necessary but don't necessarily relate to what is usually considered the architecture of neural networks. Some higher-level APIs wrap some of this functionality, but they usually stay close to the building of network architectures and the training of such networks once defined.</p>
<hr>
<h3>Layers (as from TF-Slim)</h3>
<p><a href="https://research.googleblog.com/2016/08/tf-slim-high-level-library-to-define.html">TF-Slim</a> has a number of <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/slim">components</a> but it looks like essentially we're seeing the following: <code>tf.contrib.slim.layers</code> became <code>tf.contrib.layers</code> becomes <code>tf.layers</code>.</p>
<p>This <code>layers</code> API provides a first higher level of abstraction over writing things out by individual ops. For example, <code>tf.layers.conv2d</code> implements a convolution layer that involves multiple individual ops.</p>
<p>Other parts of TF-Slim are likely still worth using, and there is a collection of <a href="https://github.com/tensorflow/models/tree/master/slim">models that use TF-Slim</a> in the <a href="https://github.com/tensorflow/models">TensorFlow models repository</a>.</p>
<p>Historical note: It looks like before calling them layers, TF-Slim overloaded the word "op" for their layer concept (see <a href="https://github.com/tensorflow/models/tree/master/inception/inception/slim">earlier documentation</a>).</p>
<p>TF-Slim is in the TensorFlow codebase as <code>tf.contrib.slim</code>.</p>
<p><img alt="slim... shady?" src="img/slim_shady.png"></p>
<hr>
<h3>Estimator (as from TF Learn)</h3>
<p>Distinct from TensorFlow, the <a href="http://scikit-learn.org/">scikit-learn</a> project makes a lot of machine learning models conveniently available in Python.</p>
<p><img alt="scikit-learn logo" src="img/sklearn.png"></p>
<p>A key design element of scikit is that many different models offer the same simple API: they are all implemented as <em>estimators</em>. So regardless of whether the model is linear regression or a GBM, if your instantiated scikit-learn model object is called <code>estimator</code>, you can do all of these:</p>
<pre><code class="language-python">estimator.fit(data, labels)       # train
estimator.score(data, labels)     # test
estimator.predict(data)           # run on new data</code></pre>

<p>The estimator API for TensorFlow is directly inspired by scikit-learn, though implemented quite independently. Here's nearly equivalent TensorFlow code, assuming you have a TensorFlow estimator model instantiated as <code>estimator</code>:</p>
<pre><code class="language-python">estimator.fit(data, labels)       # train
estimator.evaluate(data, labels)  # test
estimator.predict(data)           # run on new data</code></pre>

<p>The only thing that changed in the code is scikit-learn's <code>score</code> method becomes <code>evaluate</code>, which I think is going to do a lot of good for the self esteem of TensorFlow models.</p>
<p>There are other differences too. In scikit-learn, usually models have an idea of training "until done"&#8212;but in TensorFlow, if you don't specify a number of steps to train for, the model will go on training forever. TensorFlow estimators also do a lot of things by default, like adding TensorBoard summaries and saving model checkpoints.</p>
<p>The estimators API started as <a href="https://github.com/tensorflow/skflow">skflow</a> ("scikit-flow") before moving into the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn">tf.contrib.learn</a> ("TF Learn") and now the base estimator code is getting situated in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/estimator">tf.estimator</a> and <a href="https://www.tensorflow.org/extend/estimators">documentation</a> is accumulating.</p>
<hr>
<h3>Keras Model (absorbing Keras)</h3>
<p><a href="https://keras.io/">Keras</a> was around before TensorFlow. It was always a high-level API for neural nets, originally running with <a href="http://www.deeplearning.net/software/theano/">Theano</a> as its backend.</p>
<p>After the release of TensorFlow, Keras moved to also work with TensorFlow as a backend. And now TensorFlow is absorbing at least some aspects of the Keras project into the TensorFlow codebase, though <a href="https://twitter.com/fchollet">Fran&#231;ois Chollet</a> seems likely to continue championing Keras as a very fine project in its own right. (See also his response to a <a href="https://www.quora.com/What-will-Keras-do-with-TensorFlow-Slim">Quora question</a> about any possible relationship between TF-Slim and Keras.)</p>
<p>Once specified, Keras models offer basically the same API (<code>fit</code>/<code>evaluate</code>/<code>predict</code>) as estimators (above).</p>
<p>Keras is appearing in the TensorFlow codebase as <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/keras">tf.contrib.keras</a> and should move to <code>tf.keras</code> at TensorFlow 1.2.</p>
<p><img alt="Keras" src="img/keras.jpg"></p>
<hr>
<h3>Canned Estimators</h3>
<p>Canned estimators are concrete pre-defined models that follow the estimator conventions. Currently there are a bunch right in <code>tf.contrib.learn</code>, such as <code>LinearRegressor</code> and <code>DNNClassifier</code>. There are some elsewhere. For example, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/factorization/g3doc/kmeans.md">kmeans</a> is in <code>tf.contrib.factorization</code>. It isn't clear to me exactly where all the canned estimators will eventually settle down in the API.</p>
<hr>
<h3>Other APIs</h3>
<p>All the APIs that follow are not part of the official TensorFlow project. I guess you could use them.</p>
<p><img alt="Bucksstar Coffee" src="img/bucksstar.png"></p>
<hr>
<h3>Pretty Tensor</h3>
<p><a href="https://github.com/google/prettytensor">Pretty Tensor</a> is still a Google project, so it might be worth checking out if you really like <a href="https://en.wikipedia.org/wiki/Fluent_interface">fluent interfaces</a> with lots of chaining, like <a href="https://d3js.org/">d3</a>.</p>
<hr>
<h3>TFLearn</h3>
<p>The confusingly named <a href="https://github.com/tflearn/tflearn">TFLearn</a> (no space; perhaps more clearly identified as <a href="(http://tflearn.org/)">TFLearn.org</a>) is not at all the same thing as the TF Learn (with a space) that appears in TensorFlow at <code>tf.contrib.learn</code>. TFLearn is a <a href="http://stackoverflow.com/questions/38859354/what-is-the-difference-between-tf-learn-aka-scikit-flow-and-tflearn-aka-tflea">separate</a> Python package that uses TensorFlow. It seems like it aspires to be like Keras. Here is the TFLearn logo:</p>
<p><img alt="TFLearn logo" src="img/tflearn.png"></p>
<hr>
<h3>TensorLayer</h3>
<p>Another one with a confusing name is <a href="https://github.com/zsdonghao/tensorlayer/">TensorLayer</a>. This is a separate package from TensorFlow and it's different from TensorFlow's layers API.</p>
<p><img alt="TensorLayer logo" src="img/tensorlayer.png"></p>
<hr>
<p>My recommendation is to use the rich APIs available inside TensorFlow. You shouldn't have to import anything else, with the possible exception of Keras if you like it and are working before the Keras integration into TensorFlow is complete.</p>
<hr>
<p>Thanks to <a href="https://twitter.com/amuellerml">Andreas Mueller</a> of scikit-learn for his <a href="https://twitter.com/amuellerml/status/844300337666240514">summary</a> of the relation between scikit-learn and TensorFlow.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170321-various_tensorflow_apis_for_python/</link>
<guid>http://planspace.org/20170321-various_tensorflow_apis_for_python/</guid>
<pubDate>Tue, 21 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow APIs for Various Languages</title>
<description><![CDATA[

<p>You couldn't be blamed for thinking that <a href="https://www.tensorflow.org/">TensorFlow</a> is a Python package. And a lot of TensorFlow functionality is unique to Python. But the core of TensorFlow&#8212;the part of TensorFlow that most truly <em>is</em> TensorFlow&#8212;is the distributed runtime ("TensorFlow Distributed Execution Engine") and Python is just one way to talk to it.</p>
<p><img alt="TensorFlow three-tier diagram" src="img/tf_three_tiers.png"></p>
<p>(Image from the TensorFlow Dev Summit 2017 <a href="https://www.youtube.com/watch?v=4n1AHvDvVvw">keynote</a>.)</p>
<p>The runtime itself is written in C++, but it has APIs ("frontends") in many languages.</p>
<hr>
<h3>C</h3>
<p>The only TensorFlow APIs with any <a href="https://www.tensorflow.org/programmers_guide/version_semantics">official stability</a> are the <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/c/c_api.h">C API</a> and (parts of) the Python API.</p>
<p>TensorFlow <a href="https://www.tensorflow.org/extend/language_bindings">suggests</a> that you use the C API if you are making a TensorFlow API for some other language. Lots of programming languages have mechanisms for connecting with <a href="https://en.wikipedia.org/wiki/C_(programming_language)">C</a>. In a very real sense the TensorFlow C API exists <em>so that</em> other APIs can be built.</p>
<p>There are a number of non-Python <a href="https://www.tensorflow.org/api_docs/">languages with TensorFlow APIs</a> but for the most part I will consider them "deploy only" ("inference only") and largely ignore them.</p>
<p><img alt="C programming language" src="img/clang.png"></p>
<hr>
<h3>C++</h3>
<p>The <a href="https://www.tensorflow.org/api_docs/cc/">C++ API</a> is "exposed through header files in <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/cc">tensorflow/cc</a>" and <a href="https://en.wikipedia.org/wiki/C%2B%2B">C++</a> is the language that the TensorFlow runtime is written in, but the C++ API is still marked as experimental and has fewer features than are accessible via the Python API. It may be where you want to be for some deploy scenarios.</p>
<p>As a possibly interesting historical note, <a href="https://research.google.com/archive/mapreduce.html">the MapReduce paper</a> was another place where the world saw Google (and Jeff Dean) making something cool with C++. Yahoo! (and others) eventually implemented those concepts in Java, as <a href="http://hadoop.apache.org/">Hadoop</a>. Google seems to continue to do a lot of work in C++.</p>
<p><img alt="C++ programming language" src="img/cplusplus.png"></p>
<hr>
<h3>Java</h3>
<p>Possibly recognizing <a href="https://en.wikipedia.org/wiki/Java_(programming_language)">Java's</a> presence in industry, TensorFlow now has a <a href="https://www.tensorflow.org/api_docs/java/reference/org/tensorflow/package-summary">Java API</a>.</p>
<p><img alt="Java programming language" src="img/javalang.png"></p>
<hr>
<h3>Go</h3>
<p><a href="https://en.wikipedia.org/wiki/Go_(programming_language)">Go</a> is a Google programming language, and so it seems appropriate that there is a <a href="https://godoc.org/github.com/tensorflow/tensorflow/tensorflow/go">Go API</a> for TensorFlow.</p>
<p><img alt="Go programming language" src="img/golang.png"></p>
<hr>
<h3>Rust</h3>
<p><a href="https://en.wikipedia.org/wiki/Rust_(programming_language)">Rust</a> is a neat language, and it has a TensorFlow <a href="https://github.com/tensorflow/rust">api</a> too (<a href="https://tensorflow.github.io/rust/tensorflow/">docs</a>).</p>
<p><img alt="Rust programming language" src="img/rust.png"></p>
<hr>
<h3>Haskell</h3>
<p><a href="https://en.wikipedia.org/wiki/Haskell_(programming_language)">Haskell</a> is not a language I would have guessed would have a TensorFlow API, but <a href="https://github.com/tensorflow/haskell">it does</a> (<a href="https://tensorflow.github.io/haskell/haddock/">docs</a>).</p>
<p><img alt="Haskell programming language" src="img/haskell.png"></p>
<hr>
<h3>Python</h3>
<p><a href="https://en.wikipedia.org/wiki/Python_(programming_language)">Python</a> is where the action is. TensorFlow's <a href="https://www.tensorflow.org/api_docs/python/">Python API documentation</a> is headed simply "All symbols in TensorFlow" and it really does <a href="https://www.tensorflow.org/extend/language_bindings">seem to be</a> that TensorFlow functionality is prototyped in Python and then moved into the C++ core:</p>
<blockquote>
<p>Python was the first client language supported by TensorFlow and currently supports the most features. More and more of that functionality is being moved into the core of TensorFlow (implemented in C++) and exposed via a C API.</p>
</blockquote>
<p>The Python API is so rich, you'll have to choose which levels of Python API you'll want to use.</p>
<p><img alt="Python programming language" src="img/python.png"></p>
<hr>
<h3>R</h3>
<p>The <a href="https://en.wikipedia.org/wiki/R_(programming_language)">R</a> TensorFlow API made by <a href="https://www.rstudio.com/">RStudio</a> takes a different approach than the APIs that use TensorFlow's C API to connect with TensorFlow. The <a href="https://rstudio.github.io/tensorflow/">R API</a> (<a href="https://github.com/rstudio/tensorflow">on github</a>) wraps the entire Python API. This is not exactly what the TensorFlow project recommends, but it gives R users access to all the features in the Python API, which would be quite a lot of work to replicate using only the C API.</p>
<p><img alt="R programming language" src="img/rlang.png"></p>
<hr>
<p>Thanks to <a href="https://blog.rstudio.org/author/kevinusheyrstudio/">Kevin Ushey</a> for <a href="https://support.rstudio.com/hc/en-us/community/posts/115005611307">directing</a> me to the right venue, and to <a href="https://github.com/jjallaire">@jjallaire</a> and <a href="https://github.com/pourzanj">@pourzanj</a> for an illuminating <a href="https://github.com/rstudio/tensorflow/issues/105">discussion</a> about the R TensorFlow API.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170320-tensorflow_apis_for_various_languages/</link>
<guid>http://planspace.org/20170320-tensorflow_apis_for_various_languages/</guid>
<pubDate>Mon, 20 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Thank You for Reaching Out</title>
<description><![CDATA[

<p>I make myself pretty <a href="/aaron/">easy to contact</a>, and sometimes people contact me.</p>
<h3>1. Will I hire you?</h3>
<p>I'm not in a position to hire you (or give you an internship). Please don't send me your r&#233;sum&#233;. If you want to work where I work, or anywhere I've worked before, please go through official channels.</p>
<h3>2. Will I recommend you for a job?</h3>
<p>If I already know your work and I know it to be good, then yes.</p>
<h3>3. How can you find a job?</h3>
<p>I've found jobs mostly through people I know, and once through a recruiter.</p>
<p>Don't meet people for the sake of finding a job. Meet people by being active in your community and doing good work. Go to a <a href="https://www.meetup.com/">meetup</a>. Talk to people. Help where you can. Give a talk about something you know or are learning about. Help out on a <a href="http://brigade.codeforamerica.org/brigade/">community project</a>. Keep developing new skills.</p>
<p>The majority of the cold contacts I get are from recruiters. I don't have an answer for them on this page because they don't <em>care</em>; it is their <em>job</em> to contact people all the time&#8212;even people who don't seem to be looking for work&#8212;on the off chance that they make a contact at exactly the right time. Can they find you? They will send you jobs.</p>
<h3>4. How can you develop skills?</h3>
<p>I have a presentation about my career path and things I think are very generally important. It's called <a href="/20151206-how_to_eat_computers/">How to Eat Computers</a> and it was made for children, but even such a very mature person as you might enjoy it.</p>
<p>I have two short lists of recommended books:</p>
<ul>
<li><a href="/20160322-books_for_programmers/">Books for Programmers</a></li>
<li><a href="/20160320-books_for_professionals/">Books for Professionals</a></li>
</ul>
<p>You should read and write a lot. Develop interests and pursue them.</p>
<h3>5. What educational program should you do?</h3>
<p>You should go for the best program(s) you can, but remember that individual (student) variability is much larger than variability between programs.</p>
<h3>6. Should you do online courses?</h3>
<p>This is like asking "Should I read books?" Put them in the same category. Choose what will best help you learn what you want to learn.</p>
<h3>7. Should you do a coding bootcamp?</h3>
<p>Don't think of a bootcamp as a credential. They don't play in the same world as graduate degrees. If you go for a bootcamp, you are saying to the world that you are <em>scrappy</em>; you are going to <em>work hard</em> and you are going to <em>make something happen</em>. The structures of the program may help you to do more than you would on your own, but no program can do everything for you.</p>
<p>At the end of the program you are going to have your work to show and talk about. Could you have done that work without the program? Yes. Would you have?</p>
<h3>8. What's the best coding bootcamp?</h3>
<p>I've taught data science evening classes for <a href="https://generalassemb.ly/">General Assembly</a> and the full-time program for <a href="https://www.thisismetis.com/">Metis</a>. I developed some materials and I hope I helped both programs, but things vary a lot with locations, staff, and time: I don't know which programs are better than others.</p>
<h3>9. Will I collaborate with you? Will I be your mentor?</h3>
<p>If you have a project or question that you think I'm uniquely positioned to help with, let's talk about that project or question.</p>
<p>If you're looking for people in general but don't yet have a project or question to talk about, thank you for thinking of me, and please contact me if I am still the right person to talk to after you do. I keep some project ideas in my blog's <a href="https://github.com/ajschumacher/ajschumacher.github.io/issues">github issues</a>; you could look there to see if anything interests you.</p>    
    ]]></description>
<link>http://planspace.org/20170319-thank_you_for_reaching_out/</link>
<guid>http://planspace.org/20170319-thank_you_for_reaching_out/</guid>
<pubDate>Sun, 19 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Much Ado about the TensorFlow Logo</title>
<description><![CDATA[

<p>In the beginning, there was <a href="https://en.wikipedia.org/wiki/G%C3%B6del,_Escher,_Bach">G&#246;del, Escher, Bach: An Eternal Golden Braid</a> by Douglas Hofstadter.</p>
<p><img alt="G&#246;del, Escher, Bach" src="img/geb.jpg"></p>
<p>This "metaphorical fugue on minds and machines in the spirit of Lewis Carroll" includes the <a href="https://en.wikipedia.org/wiki/Strange_loop">strange loop</a> idea and related discussion of consciousness and formal systems. Hofstadter's influence can be seen, for example, <a href="https://cs.illinois.edu/news/strange-loop-conference">in the name</a> of the <a href="http://www.thestrangeloop.com/">Strange Loop</a> tech conference.</p>
<p>It seems likely that many people working in artificial intelligence and machine learning have encountered G&#246;del, Escher, Bach.</p>
<p>Of course, there's also <a href="https://en.wikipedia.org/wiki/Chernin_Entertainment">Chernin Entertainment</a>, the production company.</p>
<p><img alt="Chernin Entertainment" src="img/chernin.jpg"></p>
<p>So we shouldn't rule out the possibility that Google engineers are fans of <a href="http://www.imdb.com/company/co0286257/">Chernin's work</a>. I hear <a href="https://en.wikipedia.org/wiki/Hidden_Figures">Hidden Figures</a> is quite good. And I guess a lot of people like <a href="https://en.wikipedia.org/wiki/New_Girl">New Girl</a>?</p>
<p>In any event, somehow we get to this TensorFlow logo:</p>
<p><img alt="TensorFlow logo - old?" src="img/tf-old.png"></p>
<p>If you look carefully, does it seem like the right side of the "T" view is too short?</p>
<p>This very serious concern appears as <a href="https://github.com/tensorflow/tensorflow/issues/1922">issue #1922</a> on the TensorFlow github, and <a href="https://groups.google.com/a/tensorflow.org/forum/#!topic/discuss/XhO1sqp4l4g">on the TensorFlow mailing list</a> complete with ASCII art illustration.</p>
<p>The consensus response seemed to be some variant of "won't fix" (it wouldn't look as cool, anyway) until...</p>
<p><img alt="TensorFlow logo - new?" src="img/tf-new.jpg"></p>
<p>As of around the 1.0 release of TensorFlow, which was around the first <a href="https://events.withgoogle.com/tensorflow-dev-summit/">TensorFlow Dev Summit</a>, this logo variant seems to be in vogue. It removes the (possibly contentious) shadows, and adds a picture of a computation graph, in case you were about to forget that TensorFlow is about computation graphs. (If you want to think of it as a neural network, you're free to do so.)</p>
<p>Logos are fun!</p>
<hr>
<p>Thanks to <a href="https://twitter.com/divergentdave">David Cook</a> and <a href="https://twitter.com/philipashlock">Philip Ashlock</a> for helping me find Chernin again based on a very murky memory.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170318-much_ado_about_the_tensorflow_logo/</link>
<guid>http://planspace.org/20170318-much_ado_about_the_tensorflow_logo/</guid>
<pubDate>Sat, 18 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow and a Googleverse like the Hadleyverse</title>
<description><![CDATA[

<p>I've been having a good time looking at <a href="/20170313-tensorflow_use_of_google_technologies/">how TensorFlow incorporates Google technologies</a>, and how some things (like the <a href="/20170314-command_line_apps_and_tensorflow/">command-line helpers</a>) seem to reflect Google's internal conventions for development. Taken together, the view of Google software practices via TensorFlow reminds me of the Hadleyverse.</p>
<p><img alt="Hadley Wickham" src="img/hadley.png"></p>
<p>"The Hadleyverse" refers to the many packages that <a href="http://blog.revolutionanalytics.com/2015/03/hadleyverse.html">Hadley Wickham</a> has created and contributed to, which largely replace, for proficient users, the standard functionality available in the <a href="https://www.r-project.org/">R</a> programming environment. Wickham more humbly allows the name "<a href="http://tidyverse.org/">tidyverse</a>" and indeed, tidiness is part of the philosophy of his packages.</p>
<p>In addition to just providing useful packages with better functionality and better APIs than the ones that come standard with R, the Hadleyverse includes a lot of "how-to" as well. Among Wickham's freely available books are <a href="http://adv-r.had.co.nz/">Advanced R</a>, which contains a <a href="http://adv-r.had.co.nz/Style.html">style guide</a>, and the <a href="http://r-pkgs.had.co.nz/">R Packages</a> book for package developers.</p>
<p>Wickham is exceptionally productive and has had a large influence in the R community. He's also very open, sharing and communicating freely with tons of people.</p>
<p>Google might be even more productive than Hadley, but Google doesn't always seem to share as much. So the TensorFlow project can seem like a window into that Googleverse. And like the Hadleyverse, there is code, but there is also philosophy and practices.</p>
<p><img alt="google" src="img/google.png"></p>
<p>For one thing, the TensorFlow <a href="https://github.com/tensorflow/tensorflow">code</a> is an example of a Python code-base that follows the <a href="https://google.github.io/styleguide/pyguide.html">Google Python style guide</a>. (I'm primarily interested in the Python parts.)</p>
<p>Of course Python has the venerable <a href="https://www.python.org/dev/peps/pep-0008/">PEP 8</a> style guide, and I tend to like the <a href="https://github.com/numpy/numpy/blob/master/doc/HOWTO_DOCUMENT.rst.txt">NumPy/SciPy documentation guide</a> for docstrings. But looking at the TensorFlow code has nearly convinced me that their more concise argument documentation could be the way to go, for example.</p>
<p>I'm also impressed with the religious use of <a href="https://www.pylint.org/">pylint</a>, and even just the organization of unit tests.</p>
<p>In places you can see TensorFlow moving from Google-internal things to more standard Python things: <a href="https://github.com/google/python-gflags">gflags</a> and <a href="https://github.com/google/google-apputils">apputils</a> to <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/googletest.py">testing.pybase.googletest</a> to <a href="https://docs.python.org/3/library/unittest.html">unittest</a>.</p>
<p>Google also doesn't just use <a href="https://github.com/">GitHub</a>; they've got a bunch of cool extra automation and review processes set up. When a pull request is submitted, <a href="https://github.com/googlebot">googlebot</a> and <a href="https://github.com/tensorflow-jenkins">tensorflow-jenkins</a> spring into action. Contributors have to fill out a Contributor License Agreement (CLA) online, and an admin is pulled in for review and to have tests run for the pull request. After all approvals are in, the new code can go in to TensorFlow.</p>
<p>Oh and there's also <a href="https://github.com/tensorflower-gardener">tensorflower-gardener</a>, which I speculate is some connection between GitHub and work that happens inside Google proper.</p>
<p>It's interesting to think about what aspects of the Googleverse should (and/or could) be adopted by developers outside Google.</p>    
    ]]></description>
<link>http://planspace.org/20170315-tensorflow_and_a_googleverse_like_the_hadleyverse/</link>
<guid>http://planspace.org/20170315-tensorflow_and_a_googleverse_like_the_hadleyverse/</guid>
<pubDate>Wed, 15 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Command-Line Apps and TensorFlow</title>
<description><![CDATA[

<p>You can do pretty much with TensorFlow interactively in a Python <a href="https://en.wikipedia.org/wiki/Read%E2%80%93eval%E2%80%93print_loop">REPL</a> or <a href="http://jupyter.org/">notebook</a>, but there are a lot of reasons why you might want to turn your code into something you can run at the command line.</p>
<p>Running model training may take a lot of time, so you may want to run it "by itself" as an independent process. You may prefer to share <code>.py</code> files rather than notebooks, for the universality of text editors over notebook viewers. You may want to make it easier to re-run or eventually productionize your code, and especially to easily run your code on other computers.</p>
<p>You may also want to test the behavior of a system with a variety of values for certain parameters. It becomes easier and more repeatable to keep your code the same and run it with different command-line arguments, rather than editing values in the code directly and re-running.</p>
<p>If you're using a cloud service like Google's <a href="https://cloud.google.com/products/machine-learning/">Cloud ML</a>, command-line arguments may be particularly important. As they <a href="https://cloud.google.com/ml-engine/docs/concepts/trainer-considerations">say</a>, "Command-line arguments are the primary mechanism for communicating with your [TensorFlow program] at the time of execution." Writing your program to support command-line arguments like <code>--argument_name argument_value</code> is required, if you're using <a href="https://cloud.google.com/ml-engine/docs/concepts/hyperparameter-tuning-overview">Cloud ML's hyperparameter tuning features</a>.</p>
<p>There are multiple options for how to set up your command-line app. I'll show eight of them below. (Not all of them are good options!) All the scripts below have the same behavior when called like this:</p>
<pre><code class="language-bash">$ python script.py --color red
a red flower</code></pre>

<hr>
<h3>Script <a href="code/script1_argv.py">1</a>: <code>sys.argv</code></h3>
<pre><code class="language-python">import sys

def main():
    assert sys.argv[1] == '--color'
    print('a {} flower'.format(sys.argv[2]))

if __name__ == '__main__':
    main()</code></pre>

<p>You can access <code>sys.argv</code> to get a list of arguments, split on spaces in the command line, starting with the script name.</p>
<p>This direct approach is concise, for this simple example, but brittle (what if there are multiple arguments, in different order?) and opaque (what was argument 7 again?).</p>
<p>The approaches below will improve on this. All of them will make it clearer what the arguments are, both in the code and by automatically providing interactive help information on the command line, with <code>python script.py --help</code>, for example. And they'll all handle the parsing of command lines, taking them from lists of strings to data structures that are more easily used in programs.</p>
<hr>
<h3>Script <a href="code/script2_argv_tfappflags.py">2</a>: <code>tf.app.flags</code></h3>
<pre><code class="language-python">import sys
import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main():
    flags.FLAGS._parse_flags(args=sys.argv[1:])
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<p>You shouldn't do this, for multiple reasons, but it does give our first example of using something that manages your command-line interface more than manipulating <code>sys.argv</code> directly.</p>
<p>Like all the following examples, we set here a default value and some documentation for the command-line argument. We won't demonstrate other possible features, like specifying constraints on arguments and the like.</p>
<p>The first <a href="http://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/33938519#33938519">reason</a> not to use this particular technique is that while TensorFlow currently includes some argument-parsing functionality, it's there mostly so that it's easy to make cute demos, and it could change at any time.</p>
<p>The second reason not to do this is that we're still providing <code>sys.argv</code> ourselves, which is awkward, especially with TensorFlow's mechanism here. The next two examples will continue specifying <code>sys.argv</code> like this, before turning to nicer methods.</p>
<hr>
<h3>Script <a href="code/script3_argv_gflags.py">3</a>: gflags</h3>
<pre><code class="language-python">import sys
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main():
    gflags.FLAGS(sys.argv)
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    main()</code></pre>

<p>The TensorFlow <code>tf.app.flags</code> mechanism is a sort of partial re-implementation of Google's <a href="https://github.com/google/python-gflags">gflags</a> system for command-line arguments. It's an interesting case of <a href="/20170313-tensorflow_use_of_google_technologies/">Google conventions influencing TensorFlow</a>, and the comparison between the TensorFlow code in Script 2 and the gflags code in Script 3 illustrates this.</p>
<p>The Google gflags for Python <a href="https://pypi.python.org/pypi/python-gflags">package</a> can be installed with <code>pip install python-gflags</code>.</p>
<hr>
<h3>Script <a href="code/script4_argv_argparse.py">4</a>: argparse with <code>sys.argv</code></h3>
<pre><code class="language-python">import sys
import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args(sys.argv[1:])
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<p>The usual way to write command-line programs in Python is with the Python standard library's <a href="https://docs.python.org/3/library/argparse.html">argparse</a>. Interestingly, while the API of TensorFlow's <code>tf.app.flags</code> is very close to gflags, it is itself implemented with argparse.</p>
<hr>
<h3>Script <a href="code/script5_tfapprun.py">5</a>: <code>tf.app.run()</code></h3>
<pre><code class="language-python">import tensorflow as tf

def main(args):
    assert args[1] == '--color'
    print('a {} flower'.format(args[2]))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<p>Rather than access <code>sys.argv</code> directly, it's nice to use something that will automatically process it.</p>
<p>The <code>tf.app.run()</code> functionality does a minimal transformation in this case, passing <code>sys.argv</code> as an argument to <code>main()</code>.</p>
<p>The choice of <code>main()</code> as the function to run is by convention, and can be overridden, for example like <code>tf.app.run(main=my_cool_function)</code>.</p>
<p>Just as TensorFlow has built-in functionality that is not gflags but mimics part of gflags, <code>tf.app.run()</code> looks a lot like functionality from Google's <a href="https://github.com/google/google-apputils">apputils</a>. That functionality can't be shown quite as independently as in the example here, because it would automatically involve gflags too. That kind of combination between an argument parser and an app runner will be shown in the next examples.</p>
<hr>
<h3>Script <a href="code/script6_tfapprun_tfappflags.py">6</a>: <code>tf.app.run()</code> with <code>tf.app.flags</code></h3>
<pre><code class="language-python">import tensorflow as tf

flags = tf.app.flags
flags.DEFINE_string(flag_name='color',
                    default_value='green',
                    docstring='the color to make a flower')

def main(args):
    print('a {} flower'.format(flags.FLAGS.color))

if __name__ == '__main__':
    tf.app.run()</code></pre>

<p>Now only un-parsed arguments are passed to <code>main()</code>, and this is starting to look like something we could use.</p>
<p>Reasons not to use this built-in TensorFlow functionality still include that it is not officially supported and could change. It's worth noting also though that the TensorFlow argument-parsing and app-running functionality are stripped-down versions, which is enough for simple demos like what I'm showing here, but may not be enough for a real project. And of course you may just feel that some separation of concerns is in order, and TensorFlow shouldn't be doing your machine learning and also managing your command-line arguments.</p>
<hr>
<h3>Script <a href="code/script7_apputils_gflags.py">7</a>: apputils and gflags</h3>
<pre><code class="language-python">import google.apputils.app
import gflags

gflags.DEFINE_string(name='color',
                     default='green',
                     help='the color to make a flower')

def main(args):
    print('a {} flower'.format(gflags.FLAGS.color))

if __name__ == '__main__':
    google.apputils.app.run()</code></pre>

<p>The combination of Google's <a href="https://github.com/google/google-apputils">apputils</a> with gflags looks just like the TensorFlow code in Script 6, but is independent of TensorFlow and offers a lot more features. For example, using <code>google.apputils.app.run()</code> automatically gives your app an extra debugging mode.</p>
<p>The Google apputils <a href="https://pypi.python.org/pypi/google-apputils">package</a> can be installed with <code>pip install google-apputils</code>. Possible reasons not to use it include that it has not been updated (in the <a href="https://github.com/google/google-apputils">open source</a> version) since March 2015.</p>
<hr>
<h3>Script <a href="code/script8_argparse.py">8</a>: argparse</h3>
<pre><code class="language-python">import argparse

parser = argparse.ArgumentParser()
parser.add_argument('--color',
                    default='green',
                    help='the color to make a flower')

def main():
    args = parser.parse_args()
    print('a {} flower'.format(args.color))

if __name__ == '__main__':
    main()</code></pre>

<p>This is a pretty conventional way to use argparse; just omitting any argument to <code>parse_args()</code> brings in <code>sys.argv</code> automatically.</p>
<p>A method like this is what you should probably use. For examples of using it in full TensorFlow systems, one place to look is in some of the <a href="https://github.com/GoogleCloudPlatform/cloudml-samples">Google Cloud ML examples</a>.</p>
<hr>
<p>Thanks to <a href="https://twitter.com/mrry">Derek Murray</a> at <a href="https://research.google.com/teams/brain/">Google Brain</a> for pointing me in the right direction in understanding the relationship between TensorFlow and gflags.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170314-command_line_apps_and_tensorflow/</link>
<guid>http://planspace.org/20170314-command_line_apps_and_tensorflow/</guid>
<pubDate>Tue, 14 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow use of Google Technologies</title>
<description><![CDATA[

<p>TensorFlow is a large project. It has a lot of unique features, and as a Google project, it also connects with other Google projects: gflags, apputils, Bazel, protobuf, gRPC, gemmlowp, StreamExecutor, GFile, and even XLA. To better understand and use TensorFlow, it helps to know what these pieces are and how they fit together.</p>
<hr>
<h3>gflags</h3>
<p>In TensorFlow code, you may see, for <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/mnist/trainable/trainer/task.py">example</a>, <code>tf.app.flags.FLAGS</code>. This is part of a TensorFlow <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/flags.py">implementation</a> for <a href="https://en.wikipedia.org/wiki/Command-line_interface">command-line</a> argument parsing. It happens automatically when using <code>tf.app.run()</code>. Internally it uses Python's standard <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, but the API is inspired by Google's gflags.</p>
<p>Formerly Google Commandline Flags, gflags is a Google system for building standard command-line interfaces. There's a <a href="https://github.com/gflags/gflags">C++ implementation</a>, which has <a href="https://gflags.github.io/gflags/">a documentation page</a> reminiscent of a command-line interface. The <a href="https://github.com/google/python-gflags">Python gflags</a> is documented primarily via a collection of <a href="https://github.com/google/python-gflags/tree/master/examples">examples</a>.</p>
<p>Python users may be more familiar with the standard <a href="https://docs.python.org/3/library/argparse.html">argparse</a> API, or perhaps even the older <a href="https://docs.python.org/2/library/optparse.html">optparse</a>, which both achieve functionality similar to that of gflags.</p>
<p>While TensorFlow doesn't include a full gflags implementation, the TensorFlow version appears in pretty many code examples, where it imparts a little bit of extra Google flavor to argument handling for TensorFlow scripts. You can use it (though there are <a href="http://stackoverflow.com/questions/33932901/whats-the-purpose-of-tf-app-flags-in-tensorflow/33938519#33938519">reasons not to</a>) or opt for a separate and possibly more full-featured package, whether it be the full <a href="https://github.com/google/python-gflags">gflags</a>, <a href="https://docs.python.org/3/library/argparse.html">argparse</a>, or <a href="https://pythonhosted.org/horetu/">something stranger</a>.</p>
<hr>
<h3>apputils</h3>
<p>The TensorFlow <code>tf.app.run()</code> invocation doesn't actually use Google's Python apputils, but like with gflags, TensorFlow is mimicking some of the behavior of Google tooling.</p>
<p>The <a href="https://github.com/google/google-apputils">open source google-apputils for Python</a> hasn't been updated since March 2015, but it's still pretty interesting, with a lot of functionality that could still be relevant.</p>
<hr>
<h3>Bazel</h3>
<p><a href="https://bazel.build/">Bazel</a> is a build tool like <a href="https://www.gnu.org/software/make/">make</a>. Bazel is <a href="https://github.com/bazelbuild/bazel">open source</a>; <a href="https://en.wikipedia.org/wiki/Bazel_(software)">originally</a> there was Google's internal "Blaze" tool.</p>
<p>You might encounter Bazel if you're <a href="https://www.tensorflow.org/install/install_sources">building TensorFlow from source</a>, for example.</p>
<p><img alt="bazel" src="img/bazel.png"></p>
<hr>
<h3>protobuf</h3>
<p><a href="https://developers.google.com/protocol-buffers/">Protocol buffers</a> (often protobuf, or just proto) "are Google's [<a href="https://github.com/google/protobuf">open source</a>] language-neutral, platform-neutral, extensible mechanism for serializing structured data &#8211; think XML, but smaller, faster, and simpler."</p>
<p>When you look at protocol buffers in their text representations, they look something like <a href="http://www.json.org/">JSON</a>.</p>
<p>You likely won't encounter protocol buffers directly when using TensorFlow. They're used under the hood all over the place: for example, as the format for <a href="https://www.tensorflow.org/get_started/summaries_and_tensorboard">TensorBoard</a> summaries, and to help make data transfer efficient in distributed settings.</p>
<hr>
<h3>gRPC</h3>
<p><a href="http://www.grpc.io/">gRPC</a> is Google's <a href="https://github.com/grpc/grpc">open source</a> framework for <a href="https://en.wikipedia.org/wiki/Remote_procedure_call">remote procedure call (RPC)</a> systems. It uses protocol buffers.</p>
<p>If RPC is unfamiliar, there is some comparison to <a href="https://en.wikipedia.org/wiki/Representational_state_transfer">RESTful</a> communications for web services.</p>
<p>You might encounter gRPC if you build a client for a <a href="https://tensorflow.github.io/serving/">TensorFlow serving</a> server, for example.</p>
<p><img alt="gRPC" src="img/grpc.svg"></p>
<hr>
<h3>gemmlowp</h3>
<p><a href="https://github.com/google/gemmlowp">gemmlowp</a> is Google's open source low-precision matrix multiplication library.</p>
<p>You won't likely encounter gemmlowp directly, but you'll benefit from it if you're using TensorFlow for <a href="https://github.com/google/gemmlowp/blob/master/doc/low-precision.md">low-precision</a> implementations.</p>
<hr>
<h3>StreamExecutor</h3>
<p><a href="http://www.nvidia.com/">NVIDIA</a> currently dominates the <a href="https://en.wikipedia.org/wiki/Graphics_processing_unit">GPU</a> market, and so their <a href="https://en.wikipedia.org/wiki/CUDA">CUDA</a> language for programming GPUs is the dominant GPU language. But other vendors would like to sell GPUs too, and they would like everybody to standardize on <a href="https://en.wikipedia.org/wiki/OpenCL">OpenCL</a> for programming them. Programmers would rather just program once for whatever platform; they would like to have <a href="https://en.wikipedia.org/wiki/General-purpose_computing_on_graphics_processing_units">general-purpose computing on graphics processing units (GPGPU)</a>.</p>
<p>StreamExecutor is Google's GPGPU system. They've <a href="http://lists.llvm.org/pipermail/llvm-dev/2016-March/096576.html">talked about</a> open-sourcing it directly as part of the <a href="http://llvm.org/">LLVM project</a>, but as far as I know it still exists in open source only <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/stream_executor">inside TensorFlow</a>.</p>
<p>OpenCL support is not necessarily perfect, but you may be able to try it out if you <a href="https://www.tensorflow.org/install/install_sources">build from source</a>. It's <a href="https://github.com/tensorflow/tensorflow/issues/22">issue #22</a> for the <a href="https://github.com/tensorflow/tensorflow/issues/22">TensorFlow GitHub project</a>.</p>
<p>StreamExecutor is another implementation technology that you aren't likely to encounter directly as a user of TensorFlow.</p>
<hr>
<h3>GFile</h3>
<p>Google has C++ file I/O code that avoids thread locking, and <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/lib/io">this</a> is <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/lib/io/file_io.py">included</a> as <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/platform/gfile.py">part</a> TensorFlow.</p>
<p>This is another implementation technology that should benefit TensorFlow users silently within the system.</p>
<hr>
<h3>XLA</h3>
<p>The <a href="https://www.tensorflow.org/versions/master/experimental/xla/">Accelerated Linear Algebra (XLA)</a> system is described as <a href="https://haosdent.gitbooks.io/tensorflow-document/content/resources/xla_prerelease.html">the TensorFlow compiler framework</a>, but there's more to it than that.</p>
<p>For one thing, the aspects of XLA that appear in open source TensorFlow aren't all the XLA that exists; Google uses their own hardware, <a href="https://en.wikipedia.org/wiki/Tensor_processing_unit">TPUs</a>, and both that hardware and the XLA components to target it are internal to Google.</p>
<p>For another, XLA has been described as <a href="https://autodiff-workshop.github.io/slides/JeffDean.pdf">"designed for reuse"</a>, and other projects could incorporate the technology as well.</p>
<p>XLA is still experimental, and largely deep inside TensorFlow's implementation. Users might <a href="https://gist.github.com/yaroslavvb/53052184e50cdfec35f0a127dd6df843">try</a> turning on XLA to see whether it helps speed up their computations.</p>
<hr>
<p>Thanks to Googlers <a href="https://twitter.com/juliaferraioli">Julia Ferraioli</a>, <a href="https://twitter.com/petewarden">Pete Warden</a>, and <a href="https://twitter.com/martin_wicke">Martin Wicke</a>, for a brief <a href="https://twitter.com/petewarden/status/841062427202527232">twitter exchange</a> that informed this list, and to <a href="https://twitter.com/mrry">Derek Murray</a> for <a href="https://twitter.com/mrry/status/841315298221342720">correcting</a> my misconception about gflags in TensorFlow.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170313-tensorflow_use_of_google_technologies/</link>
<guid>http://planspace.org/20170313-tensorflow_use_of_google_technologies/</guid>
<pubDate>Mon, 13 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Use only what you need from TensorFlow</title>
<description><![CDATA[

<p>There isn't just one decision to use <a href="https://www.tensorflow.org/">TensorFlow</a> or not use TensorFlow; you have to make decisions about which pieces of TensorFlow you're going to use.</p>
<p>I've thought about <a href="/20170311-does_tensorflow_suffer_from_the_second_system_effect/">whether Tensorflow suffers from the second-system effect</a>, and my conclusion is that while TensorFlow has a huge abundance of features, it can't really be said to "suffer" from this. The engineering is solid and it supports very interesting applications.</p>
<p>For the individual user of TensorFlow, however, <a href="https://en.wikipedia.org/wiki/Overengineering">over-engineering</a> is a relative term, and it's relative to the problem at hand.</p>
<p>One relevant dimension to consider is where you'll be running your code, which is related to the size of your data.</p>
<p><img alt="mac mini" src="img/mac_mini.jpg"></p>
<p>If your data fits in memory and you're running on one local machine, a lot of simple approaches will work for you.</p>
<p><img alt="data center" src="img/data_center.jpg"></p>
<p>If you're using massive quantities of data on multitudes of machines in a data center, you have a lot of concerns that you wouldn't have on one machine.</p>
<p>TensorFlow can run well in large distributed settings. But if you're not going to run that way, you may not need to use all that functionality.</p>
<p><img alt="TensorFlow data pipeline" src="img/tf_data_pipeline.gif"></p>
<p>For example, the bulk of the TensorFlow <a href="https://www.tensorflow.org/programmers_guide/reading_data">documentation on reading data</a> focuses on a data pipeline that involves multiple processes and multiple coordinating queues. For reading large datasets from network storage (in <a href="https://wiki.apache.org/hadoop/HDFS">HDFS</a>, say) this might make a lot of sense.</p>
<p><img alt="the Titanic" src="img/titanic.jpg"></p>
<p>On the other hand, you may be trying out the <a href="https://www.kaggle.com/c/titanic">Titanic machine learning challenge</a> on <a href="https://www.kaggle.com/">Kaggle</a>, where the entire training set is a single CSV file with 892 lines.</p>
<p><img alt="the Titanic sinking" src="img/titanic_sinking.jpg"></p>
<p>If you decide you need to use the full TensorFlow data pipeline instead of something like <a href="http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html">pandas.read_csv</a>, you're doing a lot of unnecessary work, before you've even started working on the substance of your problem.</p>
<p>There's something to be said for trying out functionality you don't really need, just for the sake of learning it for possible future application. And if you're working on a toy problem like <a href="https://www.kaggle.com/c/titanic">Titanic</a>, maybe that is exactly what you want to do.</p>
<p>But for most work, if <a href="https://en.wikipedia.org/wiki/You_aren't_gonna_need_it">you aren't gonna need it</a>, don't use it&#8212;even if it's available in TensorFlow.</p>    
    ]]></description>
<link>http://planspace.org/20170312-use_only_what_you_need_from_tensorflow/</link>
<guid>http://planspace.org/20170312-use_only_what_you_need_from_tensorflow/</guid>
<pubDate>Sun, 12 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Does TensorFlow Suffer from the Second-System Effect?</title>
<description><![CDATA[

<p>In <a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month">The Mythical Man Month</a>, Fred Brooks includes an essay called <a href="http://wiki.c2.com/?SecondSystemEffect">The Second-System Effect</a>.</p>
<p><a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month"><img alt="The Mythical Man Month (book cover)" src="img/mythical_man_month.jpg"></a></p>
<p>The second-system effect describes two problems likely when building a new project like one you've done before:</p>
<ul>
<li>too many features: "frill after frill and embellishment after embellishment"</li>
<li>focus on the wrong features: "a tendency to refine [obsolete] techniques"</li>
</ul>
<p>The "too many features" problem is the one most commonly associated with the second-system effect.</p>
<hr>
<p><a href="https://www.tensorflow.org/">TensorFlow</a> is a second system, in that Google had a system called <a href="https://static.googleusercontent.com/media/research.google.com/en//archive/large_deep_networks_nips2012.pdf">DistBelief</a>, and TensorFlow is <a href="http://download.tensorflow.org/paper/whitepaper2015.pdf">explicitly</a> the second-generation system based on experience with DistBelief.</p>
<p><a href="https://www.tensorflow.org/"><img alt="TensorFlow" src="img/tensorflow.jpg"></a></p>
<p>TensorFlow has a <em>lot</em> of features. It aims to support research as well as production, running on multiple processing targets (CPU, GPU, etc.) on single devices and distributed, on mobile devices, workstations, and data centers. It includes a visualization system. It includes a serving system. It includes a compiler. It includes multiple high-level APIs. The tagline it uses is <a href="https://www.youtube.com/watch?v=mWl45NkFBOc">Machine Learning for Everyone</a>.</p>
<p>TensorFlow may not focus on obsolete features, but it also doesn't necessarily focus on features that matter to every user. The fundamental design priority of the computation graph, which then supports device-specific optimization and distributed execution, also introduces conceptual and computational overhead that may do more harm than good for users who won't use these features.</p>
<hr>
<p>While TensorFlow could be seen as a second system, and has some possible symptoms of the second-system effect, criticism of TensorFlow is rare.</p>
<p>Do people love TensorFlow as much as it seems they do? Or are some people quietly griping about (or just ignoring) TensorFlow, with issues like <a href="https://en.wikipedia.org/wiki/Fast_Fourier_transform">FFT</a> being supported on GPU but not CPU?</p>
<p>Is the second-system effect no longer relevant? The Mythical Man-Month was published in 1975. Using 26 bytes to handle leap years was an example of second-system excess for Brooks. And this was a <a href="https://en.wikipedia.org/wiki/Waterfall_model">waterfall</a> closed-source world.</p>
<p>Brooks also focused on individuals: the second-system effect was specifically about second systems <em>for individual developers</em> (or architects). TensorFlow is certainly not the second system <a href="https://www.quora.com/What-are-all-the-Jeff-Dean-facts">Jeff Dean</a> has been involved with. Maybe Google's engineering culture is so good, the second-system effect is no match for it.</p>
<p>Or was the second-system effect never really a useful idea? Second systems have always had the benefit of experience, as much or more than the temptation of feature creep.</p>    
    ]]></description>
<link>http://planspace.org/20170311-does_tensorflow_suffer_from_the_second_system_effect/</link>
<guid>http://planspace.org/20170311-does_tensorflow_suffer_from_the_second_system_effect/</guid>
<pubDate>Sat, 11 Mar 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
