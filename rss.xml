<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Swimming with Sharks, by Joris Luyendijk</title>
<description><![CDATA[

<p>"Inside the world of the bankers,"</p>
<blockquote>
<p>"You see a cluster of islands in the fog, staffed by mercenaries."
(page 145)</p>
</blockquote>
<p><a href="https://twitter.com/JORISLUIJENDIJK">Luyendijk</a> wrote
<a href="https://www.theguardian.com/commentisfree/joris-luyendijk-banking-blog">a 2011-2013 banking blog</a>
for the Guardian based on interviews with bankers in London. It became
this 2015 book. It's quite late to the "What happened in 2008?"
party, and even makes the observation that</p>
<blockquote>
<p>"the sector has become immune to exposure." (page 252)</p>
</blockquote>
<p>It does seem like some things these days are so bad they can hardly be
made to look worse, and yet far from being stopped, some people really
like them.</p>
<p>So Luyendijk writes about how things in finance are complicated, and
bad, but it's hard to single out any bad guys by name. His Dutch
perspective is interesting: he identifies lack of job security as a
major cause of problems in finance, for example.</p>
<p>On page 254 he has one paragraph of recommendations for law to change
the system:</p>
<ol>
<li>Break up banks so nothing is too big or too complex to fail</li>
<li>Don't let units of the same company have conflicts of interest
    with one another</li>
<li>Don't allow the building/selling/owning of overly complex
    financial products</li>
<li>Only allow bonuses that have symmetric risk</li>
</ol>
<p>No problem, right?</p>
<p>The observation and recommendation that resonated the most for me was
reconnecting reward to risk. With big public companies, a trader has
essentially only upside: good performance means a big bonus, but bad
performance means at worst finding another high-paying job at a
different company.</p>
<blockquote>
<p>"nobody should have more reason to lie awake at night worrying over
the risks to the bank's capital or reputation than the bankers
taking those risks." (page 254)</p>
</blockquote>
<p>I can get behind the sentiment, at least.</p>
<p><img alt="Swimming with Sharks cover" src="sharks_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190204-swimming_with_sharks/</link>
<guid>http://planspace.org/20190204-swimming_with_sharks/</guid>
<pubDate>Mon, 04 Feb 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Unreal and The Real, Volume Two</title>
<description><![CDATA[

<p>I bought
<a href="https://smile.amazon.com/Unreal-Real-Selected-Stories-Outer/dp/1618730355/">this</a>
collection of
<a href="https://en.wikipedia.org/wiki/Ursula_K._Le_Guin">Le Guin</a>'s short
stories and then she died before I read it. Before that, I once bought
a Kurt Vonnegut print and he died before it was delivered. I'm cursed!</p>
<p>I got the book because it has The Ones Who Walk Away From Omelas,
which I heard about in
<a href="https://www.amazon.com/How-Think-Survival-Guide-World/dp/0451499603">How to Think</a>
(which by the way is better for reacting to than learning from, if
there's a difference). Le Guin notes that Omelas "has had a long and
happy career of being used by teachers to upset students and make them
argue fiercely about morality." I missed that class, so it was nice to
catch up. File with:
<a href="https://en.wikipedia.org/wiki/Trolley_problem">trolley problems</a>?</p>
<p>To guess how Le Guin chose the stories for this volume, I'd say she
was trying to span the largest possible space of worlds. There are
lots of different universes. For example:</p>
<p>What if:</p>
<ul>
<li>the ratio of boy babies to girl babies was 1:16 instead of close to
   1:1?</li>
<li>one in a thousand people grew working, flying wings around age 20?</li>
<li>wolves were occasionally werepeople?</li>
<li>a peasant broke into Sleeping Beauty's castle and lived there while
   she slept?</li>
<li>an all-woman party of explorers reached the South Pole a couple
   years before
   <a href="https://en.wikipedia.org/wiki/Amundsen%27s_South_Pole_expedition">Amundsen</a>,
   but left no trace?</li>
</ul>
<p>Spoiler alert! A lot of the fun of many of these stories is figuring
out the premise, because the perspective doesn't always make it
obvious from the beginning.</p>
<p>In The Author of the Acacia Seeds, either people actually figure out
how to study the writing of ants and penguins and so on, or it's sort
of a joke about the modern academy inventing the study of everything,
whether there's really anything there to study or not.</p>
<p>My favorite story is probably Solitude.</p>
<p>Recommended reading order: As printed, but then re-read the
introduction.</p>
<p><img alt="cover of The Unreal and The Real, Volume Two" src="cover_volume_two.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190129-unreal_and_the_real_volume_two/</link>
<guid>http://planspace.org/20190129-unreal_and_the_real_volume_two/</guid>
<pubDate>Tue, 29 Jan 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Bird by Bird, by Anne Lamott</title>
<description><![CDATA[

<p><a href="https://smile.amazon.com/Bird-Some-Instructions-Writing-Life/dp/0385480016/">This</a>
is a charming short book containing "Some instructions on writing and
life."</p>
<p>It's very personal. Anne Lamott is cartoonishly neurotic and suggests
everyone else is too, which I've decided is nice.</p>
<p>She recommends index cards. I concur.</p>
<p>She mentions somehow that the population of the world is four billion.
I had to check the copyright date. It was 1994. Now in 2019 there are
7.7 billion people. Time flies!</p>
<p>Her writing glistens especially over short time scales. She is a
master of evocative two-item lists.</p>
<p>She has something to say (by proxy?) about not holding back:</p>
<blockquote>
<p>"Annie Dillard has said that day by day you have to give the work
before you all the best stuff you have, not saving up for later
projects. If you give freely, there will always be more."</p>
</blockquote>
<p>This is a phenomenon I've thought about in the context of breakdancing
competitions, and it is interesting to think about it also in writing,
and elsewhere.</p>
<p>Also she is very character-first. Plot comes from character, she says.</p>
<p><img alt="Bird by Bird (cover)" src="bird_by_bird_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190128-bird_by_bird_by_anne_lamott/</link>
<guid>http://planspace.org/20190128-bird_by_bird_by_anne_lamott/</guid>
<pubDate>Mon, 28 Jan 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Expectant Father and so on</title>
<description><![CDATA[

<p>I read some baby books in 2018. I got a copy of <a href="https://smile.amazon.com/Expectant-Father-Ultimate-Dads-Be/dp/0789212137/">The Expectant Father</a>, and then I got <a href="https://smile.amazon.com/New-Father-Dads-Guide-First/dp/0789211777/">The New Father</a>, by the same author. I also got the <a href="https://smile.amazon.com/FAQ-Expectant-Fathers-Armin-Brott/dp/0789212692/">FAQ</a> <a href="https://smile.amazon.com/FAQ-New-Fathers-Armin-Brott/dp/0789212706/">versions</a>, thinking (mostly wrongly) that it would be fun to see the material in multiple-choice-question form. And Erica read <a href="https://smile.amazon.com/What-Expect-When-Youre-Expecting/dp/0761187480/">What to Expect When You're Expecting</a>, so I saw bits of that as well.</p>
<p>I have mixed feelings on these baby books. Some things you can't prepare for. I think a lot of baby books are sold just to make people feel better. It's like reading about dancing: you might learn something, but you won't become a great dancer with books alone.</p>
<p>As far as facts, there are lots that you'll care about with some lowish probability. Baby books can be like encyclopedias that way. And for nearly everything, if you don't know in advance, you'll find out when you need to know. You can learn in advance about <a href="https://en.wikipedia.org/wiki/Meconium">meconium</a>, or later as needed, or not at all. Regardless, the baby poops.</p>
<p>For example, information about induction turned out to be relevant for our family. I don't recall reading much about induction, maybe because I didn't think it would be so relevant. As it happened, the best preparation I had came not from a book but from an in-person session we did at our hospital. Regardless, we relied on the medical professionals at the hospital, and I think it would have been a big mistake to favor an opinion from elsewhere over our nurses and doctors, who did a great job.</p>
<p>There is good general advice, but the parts most people agree on are pretty well known: don't smoke, eat healthy, etc. I think I prefer <a href="/20181209-brain_rules_for_baby/">Brain Rules for Baby</a> over more conventional baby books for summarizing such advice.</p>
<p>There are two things in particular that I would have liked to be more prepared for:</p>
<ul>
<li>During delivery, pushing changes at the last moment, when it's most painful, just as the baby's head is about to pop out. All of a sudden you're supposed to basically stop pushing so they can try to ease the head out without tearing anything. I don't recall hearing about this in advance. Maybe not everybody does it this way. We got these surprise directions in the middle of what was already a very intense process, and everything worked out reasonably well, but I feel like we wouldn't have minded learning about this in advance.</li>
<li>Babies cry sometimes for no apparent reason. Sometimes for considerable time (even if less than the pseudoscientific definitions associated with "colic"). I knew that babies cry, of course, but nothing prepared me for the feeling of my own daughter crying inconsolably in my arms. I don't know how much it would have helped, but I think I would have liked to hear more advice like, "Sometimes your baby will cry no matter what you do. It will be painful, but here are some things you can try. It will be okay."</li>
</ul>
<p>I'm new to all this. I'm trying to learn as much as I can. You can always learn some things from books, and there are always some things you can't. For having a child, I am strongly impressed with how valuable social learning and direct experience are when stacked against the information for sale in books.</p>
<p><img alt="cover of The Expectant Father" src="expectant_father_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181231-expectant_father/</link>
<guid>http://planspace.org/20181231-expectant_father/</guid>
<pubDate>Mon, 31 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Equation of a Plane</title>
<description><![CDATA[

<p>I wasn't a great undergraduate student, but I had at least one great
professor. She connected me with my post-graduate career path, and she
taught some of my calculus classes. One day she turned briefly from
the chalkboard and said with a smile:</p>
<blockquote>
<p>"If you don't know equation of a plane, I kill you!"</p>
</blockquote>
<p>I thought it was funny, and it wasn't what we were really working on
in class, so I didn't think much more about it. But because I never
thought it through then, doubt remained for some fifteen years: Was my
life in danger?</p>
<p>That teacher wasn't even an associate professor back then, by official
title. Recently, I learned that she's become an associate dean of the
college. I was inspired to finally think about the equation of a
plane.</p>
<p>Maybe she just meant that the equation of a plane is linear in
Cartesian coordinates. That could have been it. But I think she was
also getting at something about using the axis intercepts.</p>
<p>I learned \( y=mx+b \) like a kind of incantation, and I don't think
it was only because I went to Catholic schools. The "standard form" is
<a href="http://jwilson.coe.uga.edu/emt668/emat6680.2002/jackson/chapter%205%20lesson%20plan/day6.html">treated</a>
like a purposeless afterthought, more about following rules than
understanding anything.</p>
<p>But if the \( x \)-intercept is \( a \) and the \( y
\)-intercept is \( b \), then \( bx + ay = ab \) is a very nice
equation of a line. Maybe call it both-intercepts form.</p>
<p>Adding a \( z \)-intercept of \( c \), a plane then has a similar
equation: \( bcx + acy + abz = abc \).</p>
<p>I don't know for sure whether that's the equation that should have
spared my life back then, but it's a charming one. It's fun to think
about how it's related to other forms, and other dimensions&#8212;and how
silly a rule like "\( A \), \( B \), and \( C \) must be
integers" is.</p>
<p>I wish I had been a better student back then. I had no idea how useful
multivariate calculus and linear algebra would turn out to be, in
particular. But I've been very lucky, and I'm honored to have had
teachers who give me the pleasure of thinking things through, even
little things like this, and even many years later than I should have.
And I am deeply pleased to see that people who were so helpful to me
have themselves found greater success, and positions from which to
help many more people.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20181228-the_equation_of_a_plane/</link>
<guid>http://planspace.org/20181228-the_equation_of_a_plane/</guid>
<pubDate>Fri, 28 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Whole-Brain Child</title>
<description><![CDATA[

<p>The advice of this book overlaps substantially with that in
<a href="/20181209-brain_rules_for_baby/">Brain Rules for Baby</a>. In
comparison, the science here feels less rigorous, but the advice still
seems reasonable. There's more focus on memory, which I mostly like.</p>
<blockquote>
<p>"... children whose parents talk with them about their experiences
tend to have better access to the memories of those experiences."
(page 8)</p>
</blockquote>
<p>If there's an overall theme, it is a kind of Western presentation of a
<a href="https://en.wikipedia.org/wiki/Middle_Way">middle way</a> between logic
and emotion, then between reactive thought and higher executive
function (shades of
<a href="/2011/12/17/selections-from-and-thoughts-on/">Thinking, Fast and Slow</a>).
There's a kind of meditative concept they call
<a href="https://www.drdansiegel.com/about/mindsight/">mindsight</a>.</p>
<p>The way they think about human development reminds me a little bit of
Montessori's "spontaneous discipline" - people become better.</p>
<blockquote>
<p>"As you create a whole-brain family, you also join a broader vision
of creating an entire society full of rich, relational communities
where emotional well-being is nurtured for this and future
generations." (page 148)</p>
</blockquote>
<p>Here are their "12 revolutionary strategies to nurture your child's
developing mind":</p>
<ol>
<li>Connect and redirect: Surfing emotional waves</li>
<li>Name it to tame it: Telling stories to calm big emotions</li>
<li>Engage, don't enrage: Appealing to the upstairs brain</li>
<li>Use it or lose it: Exercising the upstairs brain</li>
<li>Move it or lose it: Moving the body to avoid losing the mind</li>
<li>Use the remote of the mind: Replaying memories</li>
<li>Remember to remember: Making recollection a part of your family's daily life</li>
<li>Let the clouds of emotion roll by: Teaching that feelings come and go</li>
<li>SIFT (Sensations, Images, Feelings, Thoughts): Paying attention to what's going on inside</li>
<li>Exercise mindsight: Getting back to the hub</li>
<li>Increase the family fun factor: Making a point to enjoy each other</li>
<li>Connect through conflict: Teach kids to argue with a "we" in mind</li>
</ol>
<p>There's also a one-page (front and back) "refrigerator sheet" at the
back of the book, which is cute.</p>
<p><img alt="cover of The Whole-Brain Child" src="whole_brain_child_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181227-the_whole_brain_child/</link>
<guid>http://planspace.org/20181227-the_whole_brain_child/</guid>
<pubDate>Thu, 27 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Processes are Not So Fancy</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian Processes</a>
have a mystique related to the dense probabilistic terminology that's
already evident in their name. But Gaussian Processes are just models,
and they're much more like k-nearest neighbors and linear regression
than may at first be apparent.</p>
<p><img alt="Predictive mean and range" src="img/predictive_mean_and_range.png"></p>
<p>Gaussian Processes have
<a href="https://en.wikipedia.org/wiki/Kriging#Applications">applications</a>
ranging from finding gold to
<a href="https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization">optimizing hyperparameters</a>
of other models. The focus here is on how Gaussian Processes work,
using an example that's simple enough to show completely from
beginning to end.</p>
<hr>
<h3>A simple training data set</h3>
<p>A model is trained with predictors \( X \) and known labels \( y
\). Here's some data in Python:</p>
<pre><code class="language-python">train_X = [[0.8], [1.2], [3.8], [4.2]]
train_y = [   3,     4,    -2,    -2 ]</code></pre>

<p>Since elements of \( X \) are one-dimensional, all the data can be
shown in a simple figure:</p>
<p><img alt="Training data" src="img/training_data.png"></p>
<p>The task of a model is to predict \( y \) values for test points \(
x \).</p>
<hr>
<h3>Applying a kernel function</h3>
<p>Like <a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVMs</a>,
Gaussian Processes use kernel functions. A kernel gives a closeness,
or similarity, between two points. This is related to distances, and a
kernel may involve distance. Here's the matrix of Euclidean distances
between points in our training data \( X \):</p>
<pre><code class="language-python">dist_XX = sklearn.metrics.pairwise_distances(train_X)
## array([[0. , 0.4, 3. , 3.4],
##        [0.4, 0. , 2.6, 3. ],
##        [3. , 2.6, 0. , 0.4],
##        [3.4, 3. , 0.4, 0. ]])
# Recall:
# train_X = [[0.8], [1.2], [3.8], [4.2]]</code></pre>

<p>The Gaussian radial basis function (RBF) kernel is commonly used. In
<a href="http://gaussianprocess.org/gpml/chapters/">Gaussian Processes for Machine Learning</a>,
Rasmussen and Williams call it the <em>squared exponential</em> kernel,
probably to avoid confusion with other things that are Gaussian. For
distance \( d \), it's \( e^{-\frac{1}{2}d^2}\):</p>
<pre><code class="language-python">def squared_exponential(distance):
    return np.exp(distance**2 / -2)</code></pre>

<p>It's one when distance is zero, and it goes to zero when distance is
big. This is evident for our data when we make the matrix of kernel
values for the training data \( X \):</p>
<pre><code class="language-python">kern_XX = squared_exponential(dist_XX)
## array([[1.  , 0.92, 0.01, 0.  ],
##        [0.92, 1.  , 0.03, 0.01],
##        [0.01, 0.03, 1.  , 0.92],
##        [0.  , 0.01, 0.92, 1.  ]])
# Recall:
# train_X = [[0.8], [1.2], [3.8], [4.2]]</code></pre>

<p>This matrix \( K(X, X) \) is a core component of Gaussian Processes,
and as the example shows, it reflects a core concern with nearness as
represented via a kernel function.</p>
<hr>
<h3>Using the Gaussian Process prediction equation</h3>
<p>This is Rasmussen and Williams' Equation 2.19 for the predictive
posterior distribution, which I promise isn't as bad as it looks:</p>
<p>\[ \mathbf{f_{\ast}} | X_{\ast}, X, \mathbf{f} \sim \mathcal{N}( K(X_{\ast}, X) K(X,X)^{-1} \mathbf{f}, \\ K(X_{\ast}, X_{\ast}) - K(X_{\ast}, X) K(X,X)^{-1} K(X, X_{\ast}) ) \]</p>
<p>Here \( \mathbf{f_{\ast}} \) is the predicted \( y \) for a test
point in \( X_{\ast} \) based on the training data \( X \) and \(
y \) labels \( \mathbf{f} \). It's predicted to be normally
distributed with mean \( K(X_{\ast}, X) K(X,X)^{-1} \mathbf{f} \)
and the given covariance. The mean can be considered "the" prediction,
though you can also sample.</p>
<hr>
<h3>Behaves like nearest neighbors</h3>
<p>Let's use Equation 2.19 to make a prediction for this \( X_{\ast} \):</p>
<pre><code class="language-python">test_X = [[1]]</code></pre>

<p>We find the kernel similarities between the test point and each point
of training data:</p>
<pre><code class="language-python">kern_xX = squared_exponential(
              sklearn.metrics.pairwise_distances(test_X, train_X))
## array([[0.98019867, 0.98019867, 0.01984109, 0.00597602]])</code></pre>

<p>The test point is close to the first two training points, and far from
the second two.</p>
<p>Let's evaluate just the first two terms of the predictive mean given
in Equation 2.19:</p>
<pre><code class="language-python">kern_xX.dot(np.linalg.inv(kern_XX))
## array([[ 0.50835358,  0.51127124, -0.01378216,  0.01144869]])</code></pre>

<p>It isn't exactly, but this looks a lot like a weighting for a weighted
average. And the prediction roughly fits that interpretation:</p>
<pre><code class="language-python">test_y = kern_xX.dot(np.linalg.inv(kern_XX)).dot(train_y)[0]
## 3.57</code></pre>

<p><img alt="Predictive mean at one point" src="img/predictive_mean_at_one_point.png"></p>
<p>This behavior is similar to k-nearest neighbors. Nearby points matter;
points that are far away don't. Nearest neighbors can even include a
weighting based on a kernel function.</p>
<p>Also like k-nearest neighbors, you have to use the whole training set
for every Gaussian Process prediction, comparing the test point to
every training point.</p>
<p>With Gaussian Processes, however, you don't have to specify a number
of neighbors \( k \). Every point that is near enough contributes to
the prediction.</p>
<p>The comparison here to a kind of weighted average nearest neighbors is
much more directly valid for
<a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel regression</a>,
which is another technique that also uses kernels. With Gaussian
Processes, there's really a further step of curve-fitting going on.</p>
<hr>
<h3>It's linear regression</h3>
<p>The mean prediction of a Gaussian Process is the same as a linear
regression with a particular choice of coordinates. Let's talk about
how.</p>
<p>Why is \( K(X,X)^{-1} \) involved in the predictive mean?</p>
<p>Consider the kernel matrix as a transformation of the original
training data \( X \) into new variables, where each of the new
variables is kernel-nearness to one of the points of training data.
This is very much like transforming raw data to include interactions
or polynomial terms, as is common in regression. Say the transformed
data is \( Z \).</p>
<p>Linear regression coefficients
<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation">can be solved for</a>
with \( (Z^T Z)^{-1} Z^T \). But with a nice symmetric square matrix
like \( K(X,X) \), this is the same as just taking \( Z^{-1} \)
directly.</p>
<p>So the mean prediction of a Gaussian Process is the same as linear
regression in the coordinates defined by kernel similarities with each
training point. It <em>is</em> linear regression.</p>
<hr>
<h3>With covariance</h3>
<p>With Gaussian Processes, kernel functions are also called covariance
functions. Things that are kernel-near vary together&#8212;they have similar
values&#8212;and this enforces smoothness.</p>
<p>Here's the covariance part of Equation 2.19 again. It has two terms:</p>
<p>\[ K(X_{\ast}, X_{\ast}) - K(X_{\ast}, X) K(X,X)^{-1} K(X, X_{\ast}) \]</p>
<p>The positive first term shows that test point(s) vary together when
they're close to one another. The negative second term (which is also
a linear regression solution, like the mean) captures how much
variance is eliminated due to being close to observed points.</p>
<!-- How to read the negative term as linear regression: The data set
X is, for each training point, kernel nearness to each other training
point. The training labels y are kernel nearness from test data to
training data. Then the test point has its nearness to the training
data, and we get out a consolidated estimate of nearness of this test
point to the training data. It can be thought of like a local weighted
average, just like for the mean. -->

<hr>
<h3>Predicting for multiple points</h3>
<p>If you get the predictive mean for many points, you can draw out a
curve:</p>
<p><img alt="Predictive mean at many points" src="img/predictive_mean_at_many_points.png"></p>
<p>You may instead want to use the predictive mean and variance at some
test point to <em>sample</em> an outcome \( y \). If you then take that
sampled value as a new point of training data (adding it to your
training set) future predictions will be consistent with that first
one, and so on.</p>
<p>You can equivalently make multiple consistent predictions
simultaneously by putting multiple test points in \( X_{\ast} \) and
sampling from a multivariate Gaussian.</p>
<p>Just like the mean alone, values sampled this way will draw out a
smooth curve. But unlike the mean alone, each random draw will be a
different curve: Gaussian Processes are random over a space of
functions. An approachable
<a href="http://katbailey.github.io/post/gaussian-processes-for-dummies/">post by Bailey</a>
focuses on this.</p>
<p>It can be fun to sample curves, but often the mean and variance alone
are useful.</p>
<p><img alt="Predictive mean and range" src="img/predictive_mean_and_range.png"></p>
<hr>
<h3>The Bayesian prior away from data</h3>
<p>Gaussian Processes have Bayesian priors. For the example here, the
assumption is that the function is always zero with covariance one,
until we see training data showing otherwise.</p>
<p>In Equation 2.19, you can think of the mean as zero plus contributions
from the training data; that's where the zero comes from. Covariance
comes from the kernel function, which gets as big as one. To change
the prior on the covariance, change the kernel function.</p>
<p><img alt="Predictive mean and range (wider view)" src="img/predictive_mean_and_range_wider_view.png"></p>
<p>The prior is visible when the bounds of the plot are expanded, which
illustrates that Gaussian Processes often focus on local interpolation
more than extrapolation.</p>
<hr>
<h3>Length scale matters</h3>
<p>The kernel specifies the scale of the variance, and in the case of the
squared exponential kernel, there's also a length scale parameter that
has significant effects.</p>
<pre><code class="language-python">def squared_exponential(distance, length_scale=1):
    return np.exp((distance / length_scale)**2 / -2)</code></pre>

<p>For our example, a length scale of one works reasonably well. For a
smaller length scale, the function is allowed to change faster, and
the prior asserts itself more quickly:</p>
<p><img alt="Predictive mean with length scale = 0.05" src="img/predictive_mean_with_small_length_scale.png"></p>
<p>For a Gaussian Process to capture a trend, its kernel needs to support
it. In the case of the squared exponential kernel, this means a long
enough length scale.</p>
<hr>
<h3>When hyperparameters are your parameters</h3>
<p>As presented here, a Gaussian Process will always exactly fit its
training data. This is often considered a sign of overfitting, and
regardless it's clearly unsustainable if training data ever has two
different \( y \) values for identical \( x \) values.</p>
<p>Noise can be added to address these issues, and the scale of the noise
is another hyperparameter, joining the overall scale and length scale
of the covariance function.</p>
<p>To get really interesting behavior may require composing kernel
functions, altering what "near" means for the model and adding even
more hyperparameters, as in
<a href="https://scikit-learn.org/stable/modules/gaussian_process.html#gpr-on-mauna-loa-co2-data">this example</a>.</p>
<p>Gaussian Process implementations like
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html">sci-kit's</a>
try to automatically fit these hyperparameters, which may remove some
of the need to know what they should be in advance. But optimization
can't do all the work of designing an appropriate kernel for a
problem, or eliminate the difficulty of distances in high-dimensional
spaces.</p>
<p>There are also some approaches for improving computational efficiency.</p>
<hr>
<h3>Not useless, but not magical</h3>
<p>Even with enhancements, the fundamental nature of Gaussian Processes
is as presented here: local smooth curve fitting built on linear
regression.</p>
<p>A Gaussian Process might be useful for you. But please don't assume
that it is sophisticated just because the language around it often is,
or that its results are automatically true just because they have
error bars.</p>
<hr>
<p>The code and plots from this post are all in
<a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20181226-gaussian_processes_are_not_so_fancy/gaussian_processes.ipynb">a Jupyter notebook</a>.</p>
<p>Thanks to Erica Blom, Marco Pariguana, Sylvia Blom, Travis Hoppe, and Ajay Deonarine for reading drafts of this post and providing feedback. Thanks also to everybody <a href="https://www.reddit.com/r/MachineLearning/comments/ac6er3/d_gaussian_processes_are_not_so_fancy/">on Reddit</a>, <a href="https://news.ycombinator.com/item?id=18814776">on HN</a>, <a href="https://www.datatau.com/item?id=28391">on DataTau</a>, <a href="https://twitter.com/search?f=tweets&amp;vertical=default&amp;q=https%3A%2F%2Fplanspace.org%2F20181226-gaussian_processes_are_not_so_fancy%2F&amp;src=typd">on Twitter</a>, <a href="https://pinboard.in/url:077f95c750f2b064e882bafb0d7899b7a3eaf48a/">on Pinboard</a>, and elsewhere!</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20181226-gaussian_processes_are_not_so_fancy/</link>
<guid>http://planspace.org/20181226-gaussian_processes_are_not_so_fancy/</guid>
<pubDate>Wed, 26 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Brain Rules for Baby</title>
<description><![CDATA[

<p>To be a good parent, be a good person. The core recommendations on "How to raise a smart and happy child from zero to five" are empathy and understanding emotion.</p>
<blockquote>
<p>"Human learning ... is primarily a relational exercise." (page 111)</p>
</blockquote>
<p>Medina aims for objective reasonableness, and it's usually easy for me to agree.</p>
<p>For the abbreviated book (and a few extra nuggets) check out the 15 pages of "practical tips" starting on page 287. Here's my summary of recommendations:</p>
<p>When someone is in an emotional state, use "the empathy reflex" (page 83):</p>
<ol>
<li>Describe the emotional changes you think you see.</li>
<li>Make a guess as to where those emotional changes come from.</li>
</ol>
<p>This is like a flip of "<a href="https://en.wikipedia.org/wiki/I-message">I statements</a>."</p>
<p>After empathy, the second major focus is on understanding emotion: talking about emotions, accepting them as natural, and (eventually) not being ruled by them.</p>
<p>There are lots of concrete recommendations:</p>
<ul>
<li>(During pregnancy) Gain appropriate weight, eat a balanced diet, exercise moderately, reduce stress.</li>
<li>Breast-feed for a year.</li>
<li>Talk to your baby a lot.</li>
<li>Praise effort, not IQ (encourage a growth mindset).</li>
<li>Use "authoritative parenting" which is "demanding and warm" and involves:<ul>
<li>Clear, consistent rules and rewards</li>
<li>Swift punishment</li>
<li>Rules that are explained</li>
</ul>
</li>
<li>(Your child should) Watch no TV before age two.</li>
<li>(Your child should) Exercise frequently.</li>
<li>(Your child should) Play a lot in open-ended ways, such as the "mature dramatic play" of <a href="https://toolsofthemind.org/">Tools of the Mind</a>, which may involve making a "play plan."</li>
<li>(Your child should) Learn sign language.</li>
<li>(Your child should) Study a musical instrument for ten years.</li>
</ul>
<p>Medina gives a very reasonable presentation of intelligence and IQ, including this historical tidbit on how we got the term "Intelligence Quotient" in the first place:</p>
<blockquote>
<p>"The score was the ratio of a child's mental age to his or her chronological age, multiplied by 100. So a 10-year-old who could solve problems normally solved only by 15-year-olds had an IQ of 150: (15/10) X 100." (page 95)</p>
</blockquote>
<p>On page 100 we get five ingredients of human intelligence stew, which I like:</p>
<ul>
<li>The desire to explore</li>
<li>Self-control</li>
<li>Creativity</li>
<li>Verbal communication</li>
<li>Interpreting nonverbal communication</li>
</ul>
<p>The associating, questioning, observing, experimenting, and networking from an old Harvard Business Review article on "<a href="https://hbr.org/2009/12/the-innovators-dna">Innovators DNA</a>" get woven in, and it's not awful.</p>
<p>Three last details I enjoyed:</p>
<ul>
<li>Episodic memory is a distinct kind of memory, and may be particularly useful for creative thinking. (page 106) Maybe consciously draw on it when brainstorming?</li>
<li>There's a test of creativity called the <a href="https://en.wikipedia.org/wiki/Torrance_Tests_of_Creative_Thinking">Torrance Tests of Creative Thinking</a>. (page 107) It's mostly things like "how many uses can you come up with for X?"</li>
<li>There's a Harvard-developed test of morality, now at <a href="http://www.moralsensetest.com/">moralsensetest.com</a>. (page 222) It's mostly variations on <a href="https://en.wikipedia.org/wiki/Trolley_problem">the trolley problem</a>.</li>
</ul>
<p><img alt="Brain Rules for Baby cover" src="cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181209-brain_rules_for_baby/</link>
<guid>http://planspace.org/20181209-brain_rules_for_baby/</guid>
<pubDate>Sun, 09 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Is it worth doing even if it fails?</title>
<description><![CDATA[

<p>Many things can "succeed" or "fail." Avoid anything that isn't worth
doing even if it fails.</p>
<p>If two tasks each have a probability of success and payout, but one is
worth doing regardless of success and one is only about payout, it
almost doesn't matter what the probabilities and payouts are. One
guarantees a good use of time.</p>
<p>This filter also averts unhealthy fixation on "winning" and excessive
disappointment with "losing." It lets you focus on the task itself,
which is the right focus.</p>
<p>You can't always choose what you have to do. But you can try to find
or create value even in endeavors you wouldn't otherwise choose.</p>
<p>Asking "Is it worth doing even if it fails?" encourages healthy things
like sports, discourages antisocial things like fraud, and provides a
positive direction to move everything in between. I think it's a
useful question.</p>    
    ]]></description>
<link>http://planspace.org/20181204-worth_doing_even_if_it_fails/</link>
<guid>http://planspace.org/20181204-worth_doing_even_if_it_fails/</guid>
<pubDate>Tue, 04 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>How To Invent Everything</title>
<description><![CDATA[

<p><a href="http://www.howtoinventeverything.com/">How To Invent Everything</a> is like <a href="https://www.billnye.com/">Bill Nye</a> writing how-tos for <a href="https://www.youtube.com/channel/UCAL3JXZSzSm8AlZyD3nQdBA">Primitive Technology</a>, in the distinctive voice of <a href="https://www.qwantz.com/">Dinosaur Comics</a> writer and actual author <a href="https://en.wikipedia.org/wiki/Ryan_North">Ryan North</a>. It's fun!</p>
<blockquote>
<p>"Scientists are often seen as turbonerds, but the philosophical foundations of science are actually those of pure punk-rock anarchy: never respect authority, never take anyone's word on anything, and test all the things you <em>think</em> you know to confirm or deny them for yourself." (page 34)</p>
</blockquote>
<p>The book is a compendium of "<a href="http://blog.sigfpe.com/2006/08/you-could-have-invented-monads-and.html">You</a> <a href="http://www.ams.org/notices/200601/fea-chow.pdf">could</a> <a href="https://reprog.wordpress.com/2010/05/13/you-could-have-invented-git-and-maybe-you-already-have/">have</a> <a href="http://luis.impa.br/aulas/lectures/inventingTopology.pdf">invented</a>" <a href="http://blog.ezyang.com/2012/02/anatomy-of-you-could-have-invented/">articles</a> for everything from language to selective breeding to steel to the <a href="https://en.wikipedia.org/wiki/Pelton_wheel">pelton wheel</a> to computers. A lot of fun (and some dangerous) science fair projects could be adopted from these pages. Do you know how to make your own charcoal? Or that clover is a <a href="https://en.wikipedia.org/wiki/Legume">legume</a> useful in crop rotation? This book is chockablock with <a href="https://minecraft.gamepedia.com/Recipe">Minecraft recipes</a> for the real world.</p>
<p>The "survival guide for the stranded time traveler" framing is amusing, but it's historical details (like the story of the <a href="https://en.wikipedia.org/wiki/Semmelweis_reflex">Semmelweis reflex</a>) that really add interest. There are a few <a href="http://www.howtoinventeverything.com/errata">errata</a> but overall this is the most easily recommendable book I've read in some time.</p>
<p><img alt="How To Invent Everything cover" src="cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181125-how_to_invent_everything/</link>
<guid>http://planspace.org/20181125-how_to_invent_everything/</guid>
<pubDate>Sun, 25 Nov 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Machine learning / Deep learning</title>
<description><![CDATA[

<p>I participated in a
<a href="https://www.meetup.com/TechinMotionDC/events/252910659/">panel</a> on
deep learning yesterday, so I thought about some of the introductory
questions and wrote out my responses.</p>
<hr>
<h3>How would you define machine learning?</h3>
<p>Machine learning is programming with examples instead of instructions.</p>
<p>It's a way to get computers to do something: in that sense it's
"programming." It's been called
<a href="https://medium.com/@karpathy/software-2-0-a64152b37c35">Software 2.0</a>.</p>
<p>Usually we program by writing explicit instructions, step by step.
With machine learning, we program using examples, by which I mean
data.</p>
<p>Say that programming is writing functions, which is basically right. A
function takes some input and returns some output.</p>
<p><a href="https://en.wikipedia.org/wiki/Higher-order_programming">Higher-order programming</a>
involves functions as the inputs and outputs of other functions, and
machine learning is a specific kind of higher-order programming.</p>
<p>Machine learning is done by a function that takes data (examples) as
input and produces a function as output. Running that higher-order
function is when machine learning takes place. We say that we learn a
function.</p>
<p>Sometimes learned functions are called models, and learning them is
sometimes called training or fitting.</p>
<p>There can also be functions that learn functions that learn functions,
as in <a href="https://arxiv.org/abs/1611.01578">Neural Architecture Search</a>,
which involves machine learning at multiple levels.</p>
<h3>What is deep learning? What is the difference between deep learning and machine learning?</h3>
<p>Deep learning is machine learning where the function you're learning
has a particular style of implementation.</p>
<p>In deep learning, the learned function has an internal implementation
involving one or more intermediate representations.</p>
<p>Aspects of these intermediate stages are sometimes called layers or
activations.</p>
<p>The number of layers is how we measure the depth of the model.</p>
<p>I don't care to argue about how many layers you "need" to be "deep."
As far as I'm concerned if there's more than zero intermediate
representations, you can call it deep if you want to. But it's common
to have five, ten, twenty, or more layers.</p>
<p>Usually, but not always, when people talk about deep learning they're
also talking about neural nets, and usually using
<a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagation</a>.</p>
<p>The reason people care about deep learning is that it performs better
than many machine learning alternatives on a lot of hard problems
typically involving high-dimensional inputs, like images, audio, and
text.</p>
<hr>
<p>Cassie Kozyrkov's <a href="https://www.youtube.com/watch?v=iLu9XyZ55oI">talk</a>
has a number of useful metaphors for machine learning in practice and
I found it helpful for thinking about how to explain things to a
general audience.</p>    
    ]]></description>
<link>http://planspace.org/20180823-machine_learning_deep_learning/</link>
<guid>http://planspace.org/20180823-machine_learning_deep_learning/</guid>
<pubDate>Thu, 23 Aug 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Land of Lisp</title>
<description><![CDATA[

<p><a href="http://landoflisp.com/">Land of Lisp</a> is an irresistibly fun <a href="https://common-lisp.net/">Common Lisp</a> book. The cartoon illustrations and "Learn to program in Lisp, one game at a time" subtitle don't fully suggest the depth of the content. Recursion is in Chapter 2, you write both <a href="https://www.graphviz.org/">Graphviz</a> and raw <a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> for graphics, you code up a web server from scratch, and you write your own lazy evaluation and multiple <a href="https://en.wikipedia.org/wiki/Domain-specific_language">DSL</a>s with macros before the end of the book. With that much material, some glitches sneak in, but it's still a very nice book.</p>
<p><img alt="Land of Lisp cover" src="land_of_lisp-cover.jpg"></p>
<p>I've done a little bit with <a href="https://en.wikipedia.org/wiki/Emacs_Lisp">Emacs Lisp</a> and <a href="https://clojure.org/">Clojure</a>, but this was my first time learning anything much about Common Lisp. Thinking about languages through contrast (<a href="https://planspace.org/20141129-ruby_symbols_are_not_lisp_symbols/">one old example of mine</a>) is fun, and there's a good deal of that in Land of Lisp.</p>
<p><img alt="Haskell/Lisp map" src="map.png"></p>
<p>I really liked some of the organization/presentation of material. The Periodic Table of the Loop Macro on pages 200 and 201 was one striking example.</p>
<p><img alt="Periodic Table of the Loop Macro" src="loop_table.png"></p>
<p>The way the author <a href="https://en.wikipedia.org/wiki/Memoization">memoized</a> functions, by first defining the un-memoized version and then defining the memoized version with the same name, capturing the original function by <a href="https://en.wikipedia.org/wiki/Closure_(computer_programming)">closure</a>, really tickled me.</p>
<p>There was also reference to a Haskel functional-style "<a href="http://happs.org/">database</a>" that I thought was interesting. Maybe sort of like <a href="https://www.datomic.com/">Datomic</a>? And there were occasional references to other neat things; I may now read <a href="https://cs.brown.edu/~sk/Publications/Papers/Published/khmgpf-impl-use-plt-web-server-journal/">Implementation and Use of the PLT Scheme Web Server</a>, for example.</p>
<p>I've thought about good ways to learn to program a little bit, even <a href="https://planspace.org/20150101-learn_to_code_with_emacs/">suggesting</a> that learning Emacs/Lisp might be good. Then at least you can use Emacs Lisp to customize your editor. I don't think I know of any current Common Lisp projects.</p>
<p>It's nice when learning (and working with) a language to have a handy documentation system. Emacs Lisp <a href="https://planspace.org/20141230-use_info_in_emacs/">has this</a> in spades. In Python, I think beginners should know about, for example, <code>help()</code> and <code>dir()</code> very early. There's nothing like this covered in Land of Lisp. The only documentation referenced, as I recall, was the <a href="http://www.lispworks.com/documentation/HyperSpec/Front/">Common Lisp HyperSpec</a>, which doesn't seem very beginner-friendly.</p>
<p>The most extremely anti-didactic bit of the book, however, has to be on page 382: "The only way to understand [these functions] is to stare at them for a long time." I think non-explanations like this are not likely to be helpful to anyone.</p>
<p>Then there are little glitches in the book. I suspect the book kept expanding and editors were overwhelmed:</p>
<ul>
<li>The intro to Chapter 7 claims to include things that don't start until Chapter 8.</li>
<li><code>sexp</code> is used in code but never <a href="https://en.wikipedia.org/wiki/S-expression">explained</a>, which I don't think is a good idea.</li>
<li><code>pushnew</code> is used on page 149 but not explained until page 368.</li>
<li>On page 445 it says "Restarts are discussed in Chapter 14," but they aren't.</li>
<li>On page 450 a reference to Chapter 16 should be to Chapter 17.</li>
</ul>
<p>In the end the book can't cover everything. For example, the <a href="https://en.wikipedia.org/wiki/Common_Lisp_Object_System">Common Lisp Object System</a> is praised but not explained in enough detail for me to know why it's so great. That was particularly frustrating because I'd still love to see more about managing large projects in languages like Common Lisp or Clojure (with OO concepts or not).</p>
<p>I liked the book a lot; it's great recreational reading for language breadth.</p>    
    ]]></description>
<link>http://planspace.org/20180812-land_of_lisp/</link>
<guid>http://planspace.org/20180812-land_of_lisp/</guid>
<pubDate>Sun, 12 Aug 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Broken Earth trilogy</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/N._K._Jemisin">N. K. Jemisin</a>'s three-book tale is both enjoyable and sufficiently deep to support a lot of conversation. The first <a href="https://en.wikipedia.org/wiki/Hugo_Award">Hugo</a>-winning book, <a href="https://en.wikipedia.org/wiki/The_Fifth_Season_(novel)">The Fifth Season</a>, was the first book I read for a book club. I couldn't stop, and finished <a href="https://en.wikipedia.org/wiki/The_Obelisk_Gate">The Obelisk Gate</a> and <a href="https://en.wikipedia.org/wiki/The_Stone_Sky">The Stone Sky</a> as well.</p>
<p><img alt="Broken Earth covers" src="BrokenEarth.jpg"></p>
<p>Here some aspects of the trilogy that I found interesting:</p>
<ul>
<li>Revolution vs. gradual change or "working within the system": What
   is possible, and when is revolutionary change justified/necessary?</li>
<li>Science fiction vs. fantasy (possibly philosophy of science?):
   Arthur C. Clarke's "Any sufficiently advanced technology is
   indistinguishable from magic" is relevant, but also, what if some
   sufficiently advanced science turns out to be very different from
   what the current popular scientific view thinks is likely?</li>
<li>Gift vs. curse: Having special powers isn't always as great as in
   Harry Potter; this is a little more like X-Men, but more extreme.</li>
<li>Race and slavery: There's a lot to think about; one aspect is an
   R-word that maps pretty directly to the real-world N-word.</li>
</ul>
<p>I noted some quotes from each book, though I started saving more after
the first volume:</p>
<h3>The Fifth Season</h3>
<blockquote>
<p>"It's a gift if it makes us better. It's a curse if we let it
destroy us." (p. 418)</p>
</blockquote>
<h3>The Obelisk Gate</h3>
<blockquote>
<p>"He's never hurt you, though. The world has, but not him. Maybe the
world deserved to be destroyed." (p. 31)</p>
<p>"Something else, neither flesh nor stone. Something immaterial, and
yet it is there for you to perceive. It glimmers in threads strung
between the bits of him, crossing itself in lattices, shifting
constantly. A... tension? An energy, shining and streaming.
Potential. Intention." (p. 101)</p>
<p>"He showed you-again and again, unrelentingly, he would not let you
pretend otherwise-that if obedience did not make one feel safe from
the Guardians or the nodes or the lynchings or the breeding or the
disrespect, then what was the point? The game was too rigged to
bother playing." (p. 159)</p>
<p>"Like those weird cults that crop up from time to time. I heard of one that asks an old man in the sky to keep them alive every time they go to sleep. People need to believe there's more to the world than there is." (p. 166)</p>
<p>"It isn't fair. You just want your life to matter."</p>
</blockquote>
<h3>The Stone Sky</h3>
<blockquote>
<p>'When a slave rebels, it is nothing much to the people who read
about it later. Just thin words on thinner paper worn finer by the
friction of history. ("So you were slaves, so what?" they whisper.
Like it's nothing.)' (p. 7)</p>
<p>"People who are not tuners can perceive magic only in rudimentary
ways; they use machines and instruments to do what is natural for
us." (p. 107)</p>
<p>"But breathing doesn't always mean living, and maybe... maybe
genocide doesn't always leave bodies." (p. 179)</p>
<p>'"Because it must be his choice, first." Harder voice here. A
reprimand. You flinch. "More importantly, because we are fragile at
the beginning, like all new creatures. It takes centuries for us,
the who of us, to...cool. Even the slightest of pressures-like you,
demanding that he fit himself to your needs rather than his own-can
damage the final shape of his personality."' (p. 282)</p>
<p>'"Orogeny," I say, sharply so she will pay attention, "was never the
only way to change the world."' (p. 396)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20180723-broken_earth_trilogy/</link>
<guid>http://planspace.org/20180723-broken_earth_trilogy/</guid>
<pubDate>Mon, 23 Jul 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Geolocation precision by digit</title>
<description><![CDATA[

<p>Latitude and longitude define increasingly fine grids over the Earth
as you add digits after the decimal points, but no matter how fine the
grid, most things still aren't at points of intersection. How far you
can get from a closest point of intersection as the grid gets finer is
a way to describe precision. This precision can be compared to
measurement accuracy and the sizes of objects of interest.</p>
<p>The Earth is not perfectly spherical, and the size of one degree can
vary, especially for longitude. To get an upper bound on error, we'll
use the circumference of the Earth at the equator, around 40,075 km,
to start calculations. With correct rounding, maximum error in one
direction is half a unit. That maximum error could happen in two
directions. Using the usual Pythagorean theorem on a plane for the
worst-case diagonal distance is easy and will be slightly higher than
the actual distance along the Earth's surface, assuming no change in
elevation, so the bound won't even be as tight as it could be. Also
I'll round up.</p>
<hr>
<h3>0.&#176; lat, 0.&#176; lon: 80 km</h3>
<p>Every point that rounds, to the nearest degree, to a particular
latitude and longitude, is within 80 kilometers of the exact
intersection of those lines of latitude and longitude.</p>
<h3>0.1&#176; lat, 0.1&#176; lon: 8 km</h3>
<p>With one decimal place, you're within 8 km. This might already be
precise enough to describe a large city.</p>
<h3>0&#176; 59' lat, 0&#176; 59' lon: 1.4 km</h3>
<p>Any point that is the same to the nearest minute in latitude and
longitude is within 1.4 km of that position exactly.</p>
<p>I recommend sticking with the decimal system.</p>
<h3>0.12&#176; lat, 0.12&#176; lon: 800 m</h3>
<p>Two decimal places of precision gets you within 800 meters.</p>
<h3>0.123&#176; lat, 0.123&#176; lon: 80 m</h3>
<p>Three decimal places will always be within 80 m of the point
described. This is specific enough already for the smallest towns and
even large buildings.</p>
<h3>0&#176; 59' 59" lat, 0&#176; 59&#176; 59" lon: 22 m</h3>
<p>When you have a location to the nearest second, it's within 22 m of
that exact location.</p>
<p>This is the annoying format that you get, for example, from the Apple
Compass app. If your phone's location accuracy
<a href="https://www.gps.gov/systems/gps/performance/accuracy/">is within 5 m</a>,
then you likely have your location correct to the nearest second,
though you may not. (Your phone reports more precise location values
internally, as you can see on your map.)</p>
<h3>0.1234&#176; lat, 0.1234&#176; lon: 8 m</h3>
<p>Four decimal places is enough to specify pretty much any landmark.</p>
<h3>0.12345&#176; lat, 0.12345&#176; lon: 80 cm</h3>
<p>You probably consider everything within your five-digit
latitude/longitude location your personal space. But also, at five
decimal places of precision, my best guess is that error in the
accuracy of your GPS is around the same size or greater than the error
from using this many digits.</p>
<h3>0.123456&#176; lat, 0.123456&#176; lon: 8 cm</h3>
<p>You don't need six digits of precision for the locations of buildings;
you need six digits of precision to describe where each plate is set
on the dinner table.</p>
<p>If you can measure your location as accurately as this, you have
specialized equipment and probably know more about these kinds of
error issues than I do.</p>
<h3>0.1234567&#176; lat, 0.1234567&#176; lon: &lt; 1 cm</h3>
<p>With seven digits of precision you can differentiate one marble from
the marble next to it.</p>
<h3>0.12345678&#176; lat, 0.12345678&#176; lon: &lt; 1 mm</h3>
<p>Is latitude and longitude really your best choice of coordinate
system?</p>
<hr>
<p>You can check out my calculations in <a href="latlon.ipynb">latlon.ipynb</a>.</p>    
    ]]></description>
<link>http://planspace.org/20180719-geolocation_precision_by_digit/</link>
<guid>http://planspace.org/20180719-geolocation_precision_by_digit/</guid>
<pubDate>Thu, 19 Jul 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Portfolios of the Poor</title>
<description><![CDATA[

<blockquote>
<p>"One of the biggest challenges of living on two dollars a day is that it doesn't always come." (page 181)</p>
</blockquote>
<p><a href="http://www.portfoliosofthepoor.com/"><img alt="Portfolios of the Poor (cover)" src="cover.jpg"></a></p>
<blockquote>
<p>"Convenience, flexibility, and reliability are at the heart of building workable financial tools for the poor, and are key to understanding the economic lives of poor households more broadly. Just as we found no households truly living hand to mouth&#8211;even among the very poor&#8211;we found no households so absolutely limited in their resources that price was the overriding determinant of financial choices." (page 153)</p>
</blockquote>
<p><a href="http://www.portfoliosofthepoor.com/">Portfolios of the Poor</a> is a 2009 book referenced in <a href="/20180218-scarcity_by_sendhil_mullainathan_and_eldar_shafir/">Scarcity</a>. While <em>Scarcity</em> positions itself as explaining why poor people make bad financial decisions, <em>Portfolios</em> explores without judgment how people who are poor manage their money: <em>Portfolios</em> doesn't necessarily think a high-interest loan is "bad" if it helps someone solve a pressing problem.</p>
<p>The source data are <a href="http://financialdiaries.com/">financial diaries</a>, collected in a kind of ethnographic process in Bangladesh, India, and <a href="https://www.datafirst.uct.ac.za/dataportal/index.php/catalog/2">South Africa</a>. They follow around 250 families for a year or more each, collecting detailed information that frequently surfaces as personal anecdotes.</p>
<p>You can get a quick summary by reading Chapter Seven, where they present their recommendations (based on observations) in just eleven pages, with this structure:</p>
<ul>
<li>Opportunities: Cash-flow management, building savings, loans for all uses</li>
<li>Principles: Reliability, convenience, flexibility, structure</li>
</ul>
<p>Chapter One is a more complete overview of the book and its methods. Chapter two focuses on cash flow, and the "triple whammy" of low income, irregular income, and lack of really helpful financial instruments. Chapter three is on dealing with risk, chapter four is on building up usefully large sums of money, and chapter five is on the sometimes strange world of pricing financial instruments for the poor.</p>
<p>One interesting service involves having money collected and kept over time for a fee, which seems like a negative interest rate. But in a repeated setting, the difference between saving and borrowing can blur.</p>
<blockquote>
<p>"In Vijaywada, there are customers who simply didn't distinguish between deposit collectors and moneylenders, so similar is the service provided. Both offered repeated money-accumulation cycles for a fee." (page 151)</p>
</blockquote>
<p>The authors explore this and other cases in which the rich-world focus on interest rates may not be the most appropriate, as costs often behave more like fees. Interest is rarely compounded, for example (page 136), and terms are not always as strict as they may seem, though this doesn't necessarily benefit everyone in the same ways.</p>
<blockquote>
<p>"...repayment delays are factored into the nominal price, with the effect that the customer who repays on time pays the highest price. This inverted pattern of incentives can be seen as one of the more unsatisfactory aspects of informal loan finance." (page 141)</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Microfinance">Microfinance</a> is not the largest money-mover in the diaries, but the authors are interested in it, and in improving it. Especially in Bangladesh many diary households were working with microfinance providers like <a href="https://en.wikipedia.org/wiki/Grameen_Bank">Grameen Bank</a>. Author Stuart Rutherford founded <a href="http://www.safesave.org/">SafeSave</a> which is now part of <a href="https://en.wikipedia.org/wiki/BRAC_(organization)">BRAC</a>, and they also mention <a href="https://en.wikipedia.org/wiki/Association_for_Social_Advancement">ASA</a>. The main microfinance recommendation in the book is probably that loans shouldn't be only for micro-entrepreneurial purposes: people have a range of needs that loans can help with.</p>
<p><em>Portfolios</em> was talking about <a href="https://en.wikipedia.org/wiki/Behavioral_economics">behavioral economics</a> before it was cool. It was interesting hearing about rural microfinance administered through weekly group meetings, sometimes with the risks of group liability. They also describe some people's preferences for borrowing on interest rather than spending from savings because of the attendant motivation to pay back quickly, and ideas for commitment savings devices.</p>
<p>There were a lot of things I wasn't familiar with, and the authors provide a helpful glossary in a table footnote on pages 207-208.</p>
<ul>
<li><em>Chit funds</em> are a government regulated form of RoSCA (see below) found only in India.</li>
<li><em>Pro-poor insurers</em> are found only in Bangladesh: they adapt the methods of NGO microcredit banks to offer endowments (savings plans linked to life insurance) to the poor, and to recycle the premiums as loans to the poor.</li>
<li><em>Saving-up clubs</em> are clubs where participants save together toward a particular event, such as a religious festival: they do not recycle the fund as loans.</li>
<li><em>RoSCAs</em>, or rotating savings and credit associations, are a form of savings-and-loan club in which a fixed number of members pay a fixed sum into a pool at a fixed interval, and on each occasion one of the members takes the whole of the pool (there are many variants on this theme).</li>
<li><em>ASCAs</em>, or accumulating savings and credit associations differ from RoSCAs in that regularly depositing members accumulate their fund and lend it out when required to one or more of their members.</li>
<li><em>Burial societies</em>, as found in South Africa, are informal clubs where members insure each other against funeral costs.</li>
<li><em>Stokvels</em> and <em>umgalelos</em> are different names for South African RoSCAs and ASCAs.</li>
<li><em>Salary timing</em> is an agreement with others to share salaries as they arrive.</li>
<li><em>Reciprocal interest-free lending and borrowing</em> are loans between friends, neighbors or family members that are interest-free but bear the implied obligation to reciprocate at some time in the future.</li>
<li><em>Mahajan</em> and <em>mashonisa</em> are South Asian and South African terms for local money-lenders who lend for profit.</li>
<li><em>Moneyguarding</em> is having someone look after your money for you, often a relative, neighbor, employer or shopkeeper.</li>
<li><em>Remitting cash to the village</em> is often practiced by town dwellers as a way of saving and of building up assets in the home village. Note that in the South African study, we treated remitting cash to the village as an expense rather than a financial instrument because we knew that the households receiving the remittances were using it for their own needs rather than saving it. In Bangladesh and India, it was more often (but not always) the case that the money was invested in some way: in land or housing or lent out, for example, and we have treated remittances as savings.</li>
</ul>
<p>One of the interesting RoSCA variants is the auction RoSCA, whereby members effectively achieve different interest rates depending on how quickly they want it to be their turn for the lump payment.</p>
<p>There were also terms that aren't so obscure but were still not familiar to me.</p>
<ul>
<li><a href="https://www.investopedia.com/terms/c/credit_life_insurance.asp">Credit Life Insurance</a><ul>
<li>Pays off your debt if you die.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Endowment_policy">Life-Endowment Savings</a><ul>
<li>Pays a lump sum after a term, or if you die.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Debenture">Debenture</a><ul>
<li>Basically a bond; maybe (even) less asset-secured.</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Net_present_value">Net Present Value</a> (NPV)<ul>
<li>For a fixed period, positive if we beat a fixed compounding interest rate, negative if not: "Am I better off over the next three years buying this investment, or putting that money in the bank at 1.5% interest? (And by how much?)"</li>
</ul>
</li>
<li><a href="https://en.wikipedia.org/wiki/Internal_rate_of_return">Internal Rate of Return</a> (IRR)<ul>
<li>Compounding interest rate equivalent of an investment: "If this factory were a bank account, what would it's APR be?"</li>
</ul>
</li>
</ul>
<p>It's pretty far from the purpose of the book, but I did appreciate the excuse to finally understand IRR a little better, along with everything else.</p>
<!-- The book has only a few typographic problems. For example, incorrect parentheses break the equation in the sixth endnote to chapter five, on page 257. -->

<blockquote>
<p>"In human affairs, incremental improvements can provide the basis for broader changes." (page 64)</p>
</blockquote>
<p><em>Thanks to the <a href="https://www.dclibrary.org/">DC Public Library</a> for supporting my access to this book.</em></p>    
    ]]></description>
<link>http://planspace.org/20180429-portfolios_of_the_poor/</link>
<guid>http://planspace.org/20180429-portfolios_of_the_poor/</guid>
<pubDate>Sun, 29 Apr 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Consider Phlebas by Iain M. Banks</title>
<description><![CDATA[

<p>Amazon <a href="http://variety.com/2018/tv/news/consider-phlebas-iain-m-banks-amazon-novel-dennis-kelly-1202706327/">is</a> making a sci-fi TV show based on the 1987 <a href="https://en.wikipedia.org/wiki/Consider_Phlebas">Consider Phlebas</a>. Focusing on certain cultural aspects would be interesting, or it could be just another space opera.</p>
<p>There's a war between <a href="https://en.wikipedia.org/wiki/The_Culture">The Culture</a> and the <a href="https://en.wikipedia.org/wiki/List_of_civilisations_in_the_Culture_series#Idirans">Idirans</a>. They could stand in for modern American society: The Culture is liberal, the Idirans are conservative. In one paragraph Banks uses the word "contempt" eight times, which reminded me of <a href="https://www.amazon.com/Politics-Resentment-Consciousness-Wisconsin-American/dp/022634911X">The Politics of Resentment</a>. The comparison isn't perfect, but it could be explored.</p>
<p>Unfortunately, the bulk of the 514 pages is given to the details of who punched who, and how. Some ideas infiltrate a chapter about a cannibalistic cult, or poker for human lives. The chapters, especially early ones, are very episodic, and I could imagine Amazon following the material closely if they wanted to.</p>
<p>The Culture has sentient artificial Minds that participate in and even guide their society. This is the kind of thing I was hoping to read about, but it's largely swamped by swashbuckling. It would be neat to see Amazon produce a slightly more conceptual adventure based on the source material.</p>
<p>Why the weird title? It's from <a href="https://en.wikipedia.org/wiki/T._S._Eliot">T. S. Eliot</a>'s poem, <a href="https://en.wikipedia.org/wiki/The_Waste_Land">The Waste Land</a>. The relevant section:</p>
<hr>
<pre>
IV. Death by Water

Phlebas the Phoenician, a fortnight dead,
Forgot the cry of gulls, and the deep sea swell
And the profit and loss.
                                   A current under sea
Picked his bones in whispers. As he rose and fell
He passed the stages of his age and youth
Entering the whirlpool.
                                  Gentile or Jew
O you who turn the wheel and look to windward,
Consider Phlebas, who was once handsome and tall as you.
</pre>

<hr>
<p><a href="https://www.poetryfoundation.org/poems/47311/the-waste-land">The Waste Land</a> overflows with allusion; just to read the words as written requires six languages or so. <a href="https://en.wikipedia.org/wiki/Consider_Phlebas">Consider Phlebas</a>, in comparison, is much less dense.</p>
<p><img alt="Consider Phlebas cover" src="consider_phlebas.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180312-consider_phlebas_by_iain_m_banks/</link>
<guid>http://planspace.org/20180312-consider_phlebas_by_iain_m_banks/</guid>
<pubDate>Mon, 12 Mar 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Scarcity by Sendhil Mullainathan and Eldar Shafir</title>
<description><![CDATA[

<p>The promise of <a href="https://scholar.harvard.edu/sendhil/scarcity">Scarcity</a> is an uplifting unified <a href="/20170825-characteristics_of_good_theories/">theory</a> that "accounts for a diverse set of phenomena" (page 162) and has social policy implications. It's a research-based rationale for compassion rather than blame, encapsulated on page 144:</p>
<blockquote>
<p>'Scarcity creates a mindset that perpetuates scarcity. If all this seems bleak, consider the alternative viewpoint: the poor are poor because they lack skills. The lonely are lonely because they are unlikable; dieters lack willpower; and the busy are busy because they lack the capacity to organize their lives. In this alternative view, scarcity is the consequence of deep personal problems, very difficult to change.</p>
<p>'The scarcity mindset, in contrast, is a contextual outcome, more open to remedies. Rather than a personal trait, it is the outcome of environmental conditions brought on by scarcity itself, conditions that can often be managed.'</p>
</blockquote>
<p>The book expands and applies some ideas that were present in <a href="/2011/12/17/selections-from-and-thoughts-on/">Thinking, Fast and Slow</a>:</p>
<blockquote>
<p>'Imagine that you are asked to retain a list of seven digits for a minute or two. While your attention is focused on the digits, you are offered a choice between two desserts: a sinful chocolate cake and a virtuous fruit salad. The evidence suggests that you would be more likely to select the tempting chocolate cake when your mind is loaded with digits.'</p>
</blockquote>
<p>The book is about how scarcity, like (or as) a distracting mental task, makes it harder to make good decisions, which makes it harder to escape scarcity.</p>
<p>The biggest new contributions are probably from the authors' <a href="http://www.sciencemag.org/">Science</a> paper, <a href="http://science.sciencemag.org/content/341/6149/976.long">Poverty Impedes Cognitive Function</a>. They found, for example, that farmers did better on an IQ test shortly after being paid for their yearly harvest, compared to before.</p>
<p>The authors grant that some scarcity, like shortness of time before a deadline, can improve performance by increasing focus. They call this the "focus dividend." But focusing can mean other things are neglected; they speak of "tunneling" and a "bandwidth tax."</p>
<p>They praise the "slack" time and money that some people enjoy. If you're operating without slack, a surprise can do a lot of damage, for example starting a downward spiral of payday loans. If you don't have slack you also need to really trade off in making decisions, which might prevent you from making certain cognitive errors of scale, but requires bandwidth.</p>
<p>Some of the organizational applications remind me of <a href="https://en.wikipedia.org/wiki/The_Mythical_Man-Month">The Mythical Man-Month</a>. For example, they describe a NASA project that was behind, so they skipped integration tests, and eventually failed catastrophically.</p>
<blockquote>
<p>'The truly efficient laborer will be found not to crowd his day with work, but will saunter to his task surrounded by a wide halo of ease and leisure.' (Henry David Thoreau, quoted page 194)</p>
</blockquote>
<p>I particularly appreciated that the authors made a noticeable effort to anticipate and address readers' possible objections, but I was already largely in agreement, so it may have been hard for me to judge their success in this regard. They also got me curious about <a href="http://www.portfoliosofthepoor.com/">Portfolios of the Poor</a>, which seems like it could be a good window into how the global poor live. Mullainathan and Shafir inspire one to understand more.</p>
<p><img alt="Scarcity cover" src="scarcity_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180218-scarcity_by_sendhil_mullainathan_and_eldar_shafir/</link>
<guid>http://planspace.org/20180218-scarcity_by_sendhil_mullainathan_and_eldar_shafir/</guid>
<pubDate>Sun, 18 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Failure to replicate Schwartz-Ziv and Tishby</title>
<description><![CDATA[

<p><a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a> didn't appear at any conferences, to my knowledge, but it still built up some <a href="https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/">buzz</a>. It has been difficult to replicate, for both <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/">bloggers</a> and <a href="https://openreview.net/forum?id=ry_WPG-A-">academics</a>. I attempted to replicate some aspects, and emailed the authors with the message below in an attempt to resolve difficulties. As there has been no response after many weeks, I present the below as an open letter. I hope it can serve as a reference for others who might try to explore these ideas.</p>
<hr>
<p>Hi Ravid! I was intrigued by your paper, <a href="https://arxiv.org/abs/1703.00810">Opening the Black Box of Deep Neural Networks via Information</a>, so I wanted to replicate your results. I was particularly interested in your Figure 4; I thought that if I could see these kinds of patterns, maybe they could be used to identify when a network is well-trained, for example.</p>
<p>Here is the figure I mean:</p>
<p><img alt="figure 4" src="img/figure_4.png"></p>
<p>I started by trying to work with a simple ReLU network with two hidden layers, on MNIST, and what I found was that the sizes of gradients decrease through training, and so do differences or variances of those gradients. I couldn't see two distinct phases, or indeed any interesting crossing of two metrics.</p>
<p>At first, I wasn't sure about how to interpret the "mean" and "standard deviation" of your Figure 4. So I was happy to find your <a href="https://github.com/ravidziv/IDNNs">code</a>, which I've been using at the 2017-11-02 <a href="https://github.com/ravidziv/IDNNs/tree/c4abb1dad4fbb262315eb8b96eb85dc5c3e98e5c">c4abb1d commit</a>.</p>
<p>Unfortunately, using the definitions of "mean" and "standard deviation" as found in your <code>idnns/plots/plot_gradients.py</code> file, I didn't get results that matched those of your paper any better.</p>
<p>I started to use your code directly, running <code>main.py</code> with default arguments, adding only <code>-save_grads true</code> so I could then use <code>plot_gradients.py</code>. Here is the result:</p>
<p><img alt="default mean std" src="img/default_mean_std.png"></p>
<p>As you can see, there are differences from your published Figure 4.</p>
<p>Can you help me to replicate your results? What should I be doing differently? I would appreciate your feedback. I also posted comments about my process of trying to understand your work on a <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/">"severely theoretical" blog post</a>; if you would like to add public context there as well it would also be appreciated.</p>
<p>One thing that I noticed is that in your published Figure 4, one of the solid "mean" lines crosses the others. With the current calculation of "mean" in your code, in which the value for a layer is the sum of an always-positive calculation on each layer up to and including itself, it doesn't seem possible for lines to ever cross. Does this shine any light on what's going on?</p>
<p>Another thing I noticed is that the caption for your published Figure 4 says "values are normalized by the L2 norms of the weights for each layer" but in the current code, I don't see this happening. Is this perhaps part of what's going on?</p>
<p>I also noticed that the y axis labels in the plot don't seem to correspond to the values being plotted. For example, the first (leftmost) value for the last layer's "standard deviation" is calculated by your code as 0.49 (I check by inserting <code>print</code> messages) but it appears in the plot as less than 0.1. I think this issue doesn't affect the shapes of the curves but only the labeling, as far as I can tell.</p>
<p>More generally, is the cumulative sum approach to calculating statistics over layers appropriate? It strikes me that the pattern of the first layer dominates the plotted results, and it's then difficult to see what's happening uniquely in the higher layers.</p>
<p>I also tried running your <code>main.py</code> with the activation changed to ReLU, using <code>-activation_function 1</code>.</p>
<p><img alt="relu mean std" src="img/relu_mean_std.png"></p>
<p>The result here is mostly the same as the previous one, qualitatively, except that the gradient means seem to increase overall through the training process. Naftali Tishby has <a href="https://severelytheoretical.wordpress.com/2017/09/28/no-information-bottleneck-probably-doesnt-open-the-black-box-of-deep-neural-networks/#comment-183">said</a> that "results [showing non-decreasing mean gradients] are clearly incorrect as the gradients must decease eventually when the training error saturates." How should this be interpreted?</p>
<p>Let me lay out my understanding of the "mean" and "standard deviation" of your Figure 4, as based on reading your code. Both metrics are calculated for every layer and epoch, so I'll take just one as an example.</p>
<p>Let's say a network has a first layer with two weights, and there are two batches per epoch. Let's say the gradients for the first layer in the first epoch are <code>[2, 3]</code> and <code>[4, 5]</code>.</p>
<p>To get the "mean" we calculate the element-wise mean (<code>[3, 4]</code>) and take the norm, so the "mean" is <code>5</code>.</p>
<p>To get the "standard deviation" we calculate the element-wise variance (<code>[1, 1]</code>) and take the square root of its sum (<code>1.414</code>; call this the "variance") and then take the square root, so the "standard deviation" is <code>1.189</code>.</p>
<p>To show the cumulative sum aspect of the calculation, say there is a second layer with two weights and identical first-epoch gradients. Then for the second layer, first epoch, the "mean" is <code>5+5=10</code> and the "standard deviation" is the square root of the sum of the "variances" <code>sqrt(1.414+1.414)=1.682</code>.</p>
<p>This understanding is based on reading and experimenting with your <code>plot_gradients.py</code>. Does it seem right to you? Am I misinterpreting something? Is the intent other than this?</p>
<p>I would love to replicate the core result of your Figure 4, which I summarize as being "at first, gradients are large but don't vary much, and then later, gradients are small but vary a lot." What's the best way to find this result? I have seen that usually <em>both</em> gradients and the amount they vary decrease through training, but I have been unable to observe the two phases described in your paper.</p>
<p>While my main focus has been on finding the two phases by statistics on the gradients, I am also interested in the mutual information parts of your paper as well. Here are four plots generated by running your <code>main.py</code> (all default arguments) four times:</p>
<p><img alt="default info a" src="img/default_info_a.png"></p>
<p><img alt="default info b" src="img/default_info_b.png"></p>
<p><img alt="default info c" src="img/default_info_c.png"></p>
<p><img alt="default info d" src="img/default_info_d.png"></p>
<p>As you can see, I have not been able to consistently replicate the beautiful plots you have in your paper as Figure 3. In fact, the higher layers consistently fail to show the patterns described in your paper, and the lower layers behave somewhat irregularly. Am I doing something wrong?</p>
<p>One thing I noticed is that in the caption for Figure 3, the architecture is described as "input=12-10-8-6-4-2-1=output". This is somewhat different from the current default in your code, and in particular it doesn't seem possible to have a single output unit with your code in its current version, because for binary classification it uses a softmax over two output nodes. Is this an important difference? Could it be causing the difficulty in seeing compression phases?</p>
<p>I also kept the information plots from two runs of <code>main.py</code> with <code>-activation_function 1</code> for ReLU activations. Here they are:</p>
<p><img alt="relu info a" src="img/relu_info_a.png"></p>
<p><img alt="relu info b" src="img/relu_info_b.png"></p>
<p>I'm not sure if anything can be interpreted from these; maybe the code as I found it doesn't support ReLU at present? I know I've read that there are some different issues concerning binning when calculating mutual information involving ReLUs.</p>
<p>I would appreciate your insight on all these specific issues, but also more broadly: if the phenomena of your paper are general to neural networks, how should I understand the apparent difficulty in seeing similar results with other networks, code, and datasets? How should I understand the apparent difficulty even when using the same dataset, and your code?</p>
<p>Thank you very much for your help with all this,</p>
<p>~ Aaron</p>
<hr>
<p>I should add for completeness that I am aware of Saxe et al.'s Figure 1A, which seems to replicate Schwartz-Ziv and Tishby's information plane result better than Schwartz-Ziv's code does for me. I would like to know how they achieve that, but the point Saxe et al. make is that the information plane result doesn't hold for most neural nets that are used in practice, so it's kind of beside the point. I haven't seen anyone replicate the result on mean and variance of gradients during training, which is what I am most interested in.</p>    
    ]]></description>
<link>http://planspace.org/20180213-failure_to_replicate_schwartz-ziv_and_tishby/</link>
<guid>http://planspace.org/20180213-failure_to_replicate_schwartz-ziv_and_tishby/</guid>
<pubDate>Tue, 13 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Coming Jobs War by Jim Clifton</title>
<description><![CDATA[

<p>I read this book because current populist, protectionist urges seem like nationalistic competition for globally dwindling jobs, which might be called a "jobs war," and I wondered if <a href="https://en.wikipedia.org/wiki/Jim_Clifton">Jim Clifton</a>, the leader of <a href="https://en.wikipedia.org/wiki/Gallup_(company)">Gallup</a>, had unique insight on this phenomenon through the polling and research of his company. Unfortunately, the CEO's <a href="http://www.gallup.com/press/178343/gallup-press.aspx">self-published</a> book is surprisingly light on evidence, heavy on political proselytizing and advertising for Gallup. Jim Clifton is a slightly more sophisticated <a href="/20180122-why_the_rich_are_getting_richer/">Robert Kiyosaki</a>.</p>
<p>I'm surprised nobody, for the sake of Gallup's image, prevented Clifton from publishing his solution to high healthcare costs in the US, which has two prongs: (a) have people die sooner, and (b) fat-shaming. I'm not exaggerating:</p>
<blockquote>
<p>'Somebody has to tell Uncle Louie it's time to cross to the other side and go join his friends, not run doctor to doctor, accepting one low-probability procedure after another.' (pages 155-156)</p>
<p>'Unfit should mean something worse than it currently does. Unfit should mean "intervention required." Unfit should mean less employable because unfit is a cause of lower energy.' (page 160)</p>
</blockquote>
<p>Even if there are germs of reasonable ideas buried in what Clifton espouses, it's hard to get past his caustic victim-blaming, which is most extreme on healthcare, but present throughout.</p>
<p>Clifton sings the praises of entrepreneurs, with a kind of <a href="https://en.wikipedia.org/wiki/Great_man_theory">great man</a> delusion blended with borderline eugenic fatalism.</p>
<blockquote>
<p>'Some leaders even believe that anyone can be trained to be an entrepreneur. This is a mistaken assumption. Entrepreneurs have a rare gift. My estimate is that for every 1,000 people, there are only about three with the potential to develop an organization with $50 million or more in annual revenue.' (page 96)</p>
<p>'Nothing fixes bad managers, not coaching, competency training, incentives, or warnings &#8211; nothing works. A bad manager never gets better.' (page 114)</p>
</blockquote>
<p>Jim Clifton became CEO of Gallup when and because <a href="http://journalstar.com/gallup-s-clifton-dies-at-age-this-story-ran-in/article_cb499250-04a5-5852-b48f-282c047ff505.html">his father, Don Clifton</a>, bought Gallup and installed him as CEO. It's a little hard to take him seriously about individuals' abilities to be totally self-made, or when, with a perfect lack of self-awareness, he goes on to talk about the importance of fixing schools.</p>
<p>Even the core premise of the book, that "What the whole world wants is a good job," isn't well supported with evidence. I had assumed there would be some detailed analysis of polls, maybe some psychological research, but the closest thing to this didn't come very close at all:</p>
<blockquote>
<p>'Of the 7 billion people on Earth, there are 5 billion adults aged 15 and older. Of these 5 billion, 3 billion tell Gallup they work or want to work.' (page 2)</p>
</blockquote>
<p>So the strongest evidence provided that everyone wants to work is that 60% of adults say they <em>do</em> work, <em>or</em> want to work. I have no way of knowing whether "wanting to work" is separated from "wanting to have a decent standard of living." For example, how many people would want to work if they were <a href="/20180125-no_more_work_by_james_livingston/">guaranteed</a> a basic income and social services?</p>
<p>I also expected Clifton would address <a href="/20180121-janesville_an_american_story/">job loss</a> trends due to automation and so on, but there's scarcely a mention. He maintains that people just need to keep making companies, and that will make jobs. He is singularly focused on keeping US GDP higher than China's, never mind per-capita GDP or the strength of the connection between jobs and GDP.</p>
<p>Here are some questions listed in a section called "Defective employees." The questions are supposed to both help evaluate managers and "neatly factor all workers into the three categories of engaged, not engaged, and actively disengaged" (page 104):</p>
<ol>
<li>I know what is expected of me at work.</li>
<li>I have the materials and equipment I need to do my work right.</li>
<li>At work, I have the opportunity to do what I do best every day.</li>
<li>In the last seven days, I have received recognition or praise for doing good work.</li>
<li>My supervisor, or someone at work, seems to care about me as a person.</li>
<li>There is someone at work who encourages my development.</li>
<li>At work, my opinions seem to count.</li>
<li>The mission or purpose of my organization makes me feel my job is important.</li>
<li>My associates or fellow employees are committed to doing quality work.</li>
<li>I have a best friend at work.</li>
<li>In the last six months, someone at work has talked to me about my progress.</li>
<li>This last year, I have had opportunities at work to learn and grow.</li>
</ol>
<p>Most of these items map onto good advice for companies, but number ten's appearance on the list I think is symptomatic of a "<a href="https://www.wired.com/2008/06/pb-theory/">death of science</a>" problem in analysis. Why is "I have a best friend at work" on this list? I suspect the answer is that having a best friend at work is associated with being less likely to quit, and largely independent of other factors, so the item looks good statistically, despite being mostly useless to employees or managers.</p>
<p>Sometimes it's possible to see in Clifton's positions what could be genuine caring for people: a desire that everyone reach their full potential. Supporting people's strengths sounds good, and some of the ideas in the book are not all bad. But there are so many bad ideas, it's like picking through a dumpster.</p>
<p>I had thought that a book from <a href="https://en.wikipedia.org/wiki/Gallup_(company)">Gallup</a> might have interesting, relevant data and analysis. I really like the idea of using poll data to better understand the world. I had only known of Gallup's polling operation, not their management consulting side. Unfortunately, based on this book, it seems management consulting dominates their attention.</p>
<p>The only people who should read this book are those considering working with Gallup, the better to understand what they would be getting into.</p>
<p><img alt="The Coming Jobs War cover" src="coming_jobs_war_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180211-coming_jobs_war/</link>
<guid>http://planspace.org/20180211-coming_jobs_war/</guid>
<pubDate>Sun, 11 Feb 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Worm by Mark Bowden</title>
<description><![CDATA[

<p><a href="https://www.amazon.com/Worm-First-Digital-World-War/dp/0802119832">This</a> was the only book on computer security that I could find at my local library, which is a little surprising for DC. It's about the <a href="https://en.wikipedia.org/wiki/Conficker">Conficker worm</a>. It's <a href="http://www.nytimes.com/2011/10/04/books/mark-bowdens-worm-about-conficker-review.html">not</a> a great book.</p>
<p>I had hoped for more expertise. Even I know that <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> packets are not "packets of code" and that <a href="https://en.wikipedia.org/wiki/Denial-of-service_attack#Distributed_attack">DDoS</a> is not "Dedicated Denial of Service." That second error appears once, with later expansions being correct, but it is symptomatic of generally weak quality control.</p>
<p>A scan through the Wikipedia page for <a href="https://en.wikipedia.org/wiki/Conficker">Conficker</a> shows that Worm leaves out a number of interesting technical details: dictionary attacks are not mentioned at all, for example.</p>
<p>I did enjoy learning about the involvement of <a href="https://www.sri.com/">SRI</a> in connection with Conficker and with the development of the internet in general.</p>
<p>Bowden focuses on social aspects, and these mostly ring true: Lots of computers aren't updated, and hackers take advantage of them. Coordination, especially with government, can be difficult. Mischel Kwon in particular is not portrayed glowingly, but many of the cast of characters appear infantile in their own words.</p>
<p>The book succeeds in making security seem pretty inane. Microsoft makes a crummy product, so people get hacked. To stop the hacked machines from doing anything, the good guys try to buy a lot of domain names, and eventually fail. All our technology is both amazing and pitiful.</p>
<p><img alt="Worm cover" src="worm_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20180128-worm_by_mark_bowden/</link>
<guid>http://planspace.org/20180128-worm_by_mark_bowden/</guid>
<pubDate>Sun, 28 Jan 2018 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
