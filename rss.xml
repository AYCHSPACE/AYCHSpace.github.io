<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Chains</title>
<description><![CDATA[

<blockquote>
<p>&#8220;We were arguing energetically about whether the world is actually evolving, headed in a particular direction, or whether the entire universe is just a returning rhythm's game, a renewal of eternity.&#8221;</p>
</blockquote>
<p>There&#8217;s this little <a href="https://djjr-courses.wdfiles.com/local--files/soc180%3Akarinthy-chain-links/Karinthy-Chain-Links_1929.pdf">story</a>, published in 1929, by <a href="http://en.wikipedia.org/wiki/Frigyes_Karinthy">Frigyes Karinthy</a>. <a href="http://www.qwantz.com/">Dinosaur Comics</a> put me on to this:</p>
<p><a href="http://www.qwantz.com/index.php?comic=2646"><img alt="dinosaur comics #2646" src="http://www.qwantz.com/comics/comic2-2656.png"></a></p>
<blockquote>
<p>&#8220;Everything returns and renews itself. The difference now is that the rate of these returns has increased, in both space and time, in an unheard-of fashion. Now my thoughts can circle the globe in minutes. Entire passages of world history are played out in a couple of years.&#8221;</p>
</blockquote>
<p>Published in 1929.</p>    
    ]]></description>
<link>http://planspace.org/20150522-chains/</link>
<guid>http://planspace.org/20150522-chains/</guid>
<pubDate>Fri, 22 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Practical Mergic</title>
<description><![CDATA[

<p><em>A talk for the <a href="http://www.meetup.com/nyhackr/">New York Open Statistical Programming Meetup</a> on <a href="http://www.meetup.com/nyhackr/events/222328498/">Wednesday May 20, 2015</a>. Including some material originally given in a <a href="/20150514-mergic/">lightning talk</a> at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>.</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img width="1000%" title="Tweed Courthouse" src="img/tweed.jpg"></p>
<hr>
<p><em><a href="http://commons.wikimedia.org/wiki/File:Tweed_Courthouse_north_main_facade_118443pv.jpg">Image from Wikimedia Commons</a>.</em></p>
<p>The first time I wrote software to address this problem, I was working for the New York City Department of Education, in Tweed Courthouse.</p>
<p>The NYC DOE had unique IDs for every student, and for every teacher, but did not have unique IDs for principals. This was the case at least for some of the data system at that time. And this made sense, because there are only about sixteen hundred New York City public schools.</p>
<p>Those of you who have experience with humans will know that names are not unique IDs. People change their names, or add titles like &#8220;PhD&#8221;, or have their names entered differently at different times for no good reason. In the case of principals, sometimes they switch schools and change their names <em>at the same time</em>.</p>
<p>The DOE makes some decisions based on data, God bless them. The data associated with a principal might determine whether they get a bonus or an unpleasant phone call. In a situation like this, an approximate matching solution is not acceptable.</p>
<p>I had to get a perfect matching of all these principals' names, so I wrote some code to help speed the process. I had to verify the match by pairs, and it was awful, and I wished there was a better way to do it.</p>
<p>My primary concerns were, and are, to speed up the de-duplication process while allowing corrections by hand&#8212;still being reproducible and easily auditable by humans.</p>
<p>I've developed a process, or workflow, that I think is pretty good, and I've written a new tool to help with this process.</p>
<p>I've also learned about some of the broader ecosystem of techniques and tools available, and I'll talk about these as well.</p>
<p>I'll finish by suggesting that we probably need to do something entirely different.</p>
<p>I'd like to encourage a discussion that could lead to more and better work with data.</p>
<hr>
<blockquote>
<p>&#8220;There are only two hard things in computer science: cache invalidation and naming things.&#8221;</p>
</blockquote>
<hr>
<p><a href="http://www.meerkat.com/karlton/">Phil Karlton</a> <a href="http://martinfowler.com/bliki/TwoHardThings.html">said</a> that &#8220;There are only two hard things in computer science: cache invalidation and naming things.&#8221;</p>
<p>When working with data (let's call it &#8220;data science&#8221; then, instead of &#8220;computer science&#8221;) you have problems not only with your own names, but also with everybody else's names.</p>
<p>It's just semantics, I suppose.</p>
<hr>
<p>What are we talking about?</p>
<hr>
<p>&#51060;&#47492;&#51060;&#46976; &#44163;&#51008; &#51221;&#47568; &#51473;&#50836;&#54633;&#45768;&#45796;. &#50696;&#47484; &#46308;&#47732;, &#48120;&#44397;&#50640;&#49436; &#54620;&#44397;&#47568;&#47196;...</p>
<p>My Korean isn't that good. What I'm trying to say here is that agreeing on names is important, and the issue is a big one.</p>
<p>Maintaining a practical focus, let's focus on just two classes of problems:</p>
<hr>
<p>when names are the same</p>
<hr>
<p>Lots of things can go wrong when names are re-used when they shouldn't be.</p>
<hr>
<p>when names aren't the same</p>
<hr>
<p>It can be even worse when names for the same thing are <em>not</em> the same.</p>
<p>Both these problems are closely related to merging, and we'll think a lot about that context.</p>
<p>But first, an example illustrating the importance of checking that your names are unique (not the same).</p>
<hr>
<p>demo: DOE data</p>
<hr>
<p>The <a href="doe/SchoolMathResults20062012Public.xlsx">Excel file here</a> was <a href="http://schools.nyc.gov/NR/rdonlyres/A77DF9C5-BD62-4171-9995-4EB41E7E4067/0/SchoolMathResults20062012Public.xlsx">downloaded</a> from the <a href="http://schools.nyc.gov/NR/exeres/05289E74-2D81-4CC0-81F6-E1143E28F4C4,frameless.htm">NYC DOE site</a>. It contains standardized test results for New York City schools for individual grades and other sub-groups. We'll use two sheets that have been saved as CSV (originally for my <a href="http://planspace.org/2014/01/07/clean-data-with-r/">Clean Data with R</a> talk), <code>gender.csv</code> and <code>all.csv</code>. The R script itself is in <a href="doe/check_unique.R">check_unique.R</a>.</p>
<p>There's a little setup to make reading the data easier.</p>
<pre><code class="language-r">library("dplyr")

read.doe &lt;- function(filename) {
  data &lt;- read.csv(filename, as.is=TRUE,
                   skip=6, check.names=FALSE,
                   na.strings="s")
  stopifnot(names(data) == c("DBN", "Grade", "Year", "Category",
                             "Number Tested","Mean Scale Score",
                             "#","%","#","%","#","%","#","%","#","%"))
  names(data) &lt;- c("dbn", "grade", "year", "category",
                   "num_tested", "mean_score",
                   "num1", "per1", "num2", "per2", "num3", "per3",
                   "num4", "per4", "num34", "per34")
 return(tbl_df(data))
}</code></pre>

<p>Now we can start looking at the data, using <a href="https://github.com/hadley/dplyr">dplyr</a>:</p>
<pre><code class="language-r">gender &lt;- read.doe("gender.csv")

gender
## Source: local data frame [68,028 x 16]
##
##       dbn grade year category num_tested mean_score num1 per1 num2 per2
## 1  01M015     3 2006   Female         23        675    0  0.0    7 30.4
## 2  01M015     3 2006     Male         16        657    2 12.5    4 25.0
## 3  01M015     3 2007   Female         11        679    2 18.2    0  0.0
## 4  01M015     3 2007     Male         20        668    0  0.0    3 15.0
## 5  01M015     3 2008   Female         17        661    0  0.0    5 29.4
## 6  01M015     3 2008     Male         20        674    0  0.0    1  5.0
## 7  01M015     3 2009   Female         13        667    0  0.0    1  7.7
## 8  01M015     3 2009     Male         20        668    0  0.0    3 15.0
## 9  01M015     3 2010   Female         13        681    2 15.4    7 53.8
## 10 01M015     3 2010     Male         13        673    4 30.8    5 38.5
## ..    ...   ...  ...      ...        ...        ...  ...  ...  ...  ...
## Variables not shown: num3 (int), per3 (dbl), num4 (int), per4 (dbl), num34
##   (int), per34 (dbl)</code></pre>

<p>What's our unique key for this data set? It looks like it should be <code>dbn</code> (a unique identifier for a school), <code>grade</code>, <code>year</code>, and <code>category</code>. Let's check. The following should be zero if there are no duplicates.</p>
<pre><code class="language-r">gender %&gt;%
  select(dbn, grade, year, category) %&gt;%
  duplicated %&gt;%
  sum
## [1] 1421</code></pre>

<p>Shocking! Seeing that there are duplicates doesn't yet tell us how the duplicates are distributed; is it 1,422 copies of the same combination, or something else?</p>
<pre><code class="language-r">gender %&gt;%
  group_by(dbn, grade, year, category) %&gt;%
  summarize(n=n()) %&gt;%
  group_by(n) %&gt;%
  summarize(count=n())
## Source: local data frame [2 x 2]
##
##   n count
## 1 1 65186
## 2 2  1421</code></pre>

<p>Much like using <code>table</code>, now we can see that most key combinations appear just once, but 1,421 appear twice. Interesting! Let's look at them.</p>
<pre><code class="language-r">gender %&gt;%
  group_by(dbn, grade, year, category) %&gt;%
  filter(1 &lt; n())
## Source: local data frame [2,842 x 16]
## Groups: dbn, grade, year, category
##
##       dbn      grade year category num_tested mean_score num1 per1 num2
## 1  01M019          3 2010     Male         20        677    3   15    7
## 2  01M019          3 2010     Male          2         NA   NA   NA   NA
## 3  01M019          4 2010     Male         20        674    1    5    9
## 4  01M019          4 2010     Male          1         NA   NA   NA   NA
## 5  01M019          5 2010     Male          1         NA   NA   NA   NA
## 6  01M019          5 2010     Male         17        688    0    0    3
## 7  01M019 All Grades 2010     Male          4         NA   NA   NA   NA
## 8  01M019 All Grades 2010     Male         57         NA    4    7   19
## 9  01M020          3 2010     Male         50        686    8   16   15
## 10 01M020          3 2010     Male          1         NA   NA   NA   NA
## ..    ...        ...  ...      ...        ...        ...  ...  ...  ...
## Variables not shown: per2 (dbl), num3 (int), per3 (dbl), num4 (int), per4
##   (dbl), num34 (int), per34 (dbl)</code></pre>

<p>Looks like there are two different kinds of males! How strange! Can we see what's going on by looking at the rest of the file?</p>
<pre><code class="language-r">gender %&gt;%
  filter(dbn=='01M019', year==2010, grade==3)
## Source: local data frame [3 x 16]
##
##      dbn grade year category num_tested mean_score num1 per1 num2 per2
## 1 01M019     3 2010   Female         16        687    0    0    9 56.3
## 2 01M019     3 2010     Male         20        677    3   15    7 35.0
## 3 01M019     3 2010     Male          2         NA   NA   NA   NA   NA
## Variables not shown: num3 (int), per3 (dbl), num4 (int), per4 (dbl), num34
##   (int), per34 (dbl)</code></pre>

<p>Unfortunately not; we'll have to look at additional data to try to determine what's going on. (This is typical.)</p>
<pre><code class="language-r">all_students &lt;- read.doe("all.csv")
data &lt;- bind_rows(all_students, gender)

data %&gt;%
  filter(dbn=='01M019', year==2010, grade==3)
## Source: local data frame [4 x 16]
##
##      dbn grade year     category num_tested mean_score num1 per1 num2 per2
## 1 01M019     3 2010 All Students         36        682    3  8.3   16 44.4
## 2 01M019     3 2010       Female         16        687    0  0.0    9 56.3
## 3 01M019     3 2010         Male         20        677    3 15.0    7 35.0
## 4 01M019     3 2010         Male          2         NA   NA   NA   NA   NA
## Variables not shown: num3 (int), per3 (dbl), num4 (int), per4 (dbl), num34
##   (int), per34 (dbl)</code></pre>

<p>It looks like these extra males aren't being counted in the total for &#8220;All Students&#8221;, so maybe we can drop them. Or maybe the &#8220;All Students&#8221; total is wrong.</p>
<hr>
<p><img width="1000%" title="This happens." src="img/this_happens.png"></p>
<hr>
<p><em>Original image from <a href="http://en.wikipedia.org/wiki/Magnolia_%28film%29">Magnolia</a> via <a href="http://indie-outlook.com/2012/09/19/jeremy-blackman-on-magnolia-pta-0s-1s-and-pink-drink/">Indie Outlook</a>.</em></p>
<blockquote>
<p>&#8220;This happens. This is a thing that happens.&#8221;</p>
</blockquote>
<p>This is a scene from a movie called Magnolia when it's raining frogs. One of the things they say in that movie is that strange things happen, and if you've worked with any variety of data sets, you've probably encountered very strange things. You need to check everything&#8212;including things that you shouldn&#8217;t have to check.</p>
<p>(One good way to check is to use <a href="https://twitter.com/tonyfischetti">Tony</a>'s <a href="http://www.onthelambda.com/wp-content/uploads/2015/03/assertr.html">Assertive R</a> package!)</p>
<hr>
<p>merge</p>
<hr>
<p>One big reason to want nice unique IDs is that you would like to merge two data sets, and you need something to match records by. Let's do a quick refresher on merging, or joining.</p>
<hr>
<p><img height="1000%" title="dplyr cheat sheet joins" src="img/dplyr_joins.png"></p>
<hr>
<p>This is a section from <a href="http://www.rstudio.com/">RStudio</a>'s <a href="http://www.rstudio.com/resources/cheatsheets/">cheatsheet</a> for <a href="https://github.com/hadley/dplyr">dplyr</a>. These cheatsheets are fantastic.</p>
<p>We'll do a merge between two data sets. For simplicity say that they have one column which is an identifier for each row, and some data in other columns. There are a couple ways we can join the data.</p>
<p>Some people like to think about these in terms of Venn diagrams.</p>
<hr>
<p><img width="1000%" title="left join" src="img/left_join.png"></p>
<hr>
<p>This picture comes from <a href="https://twitter.com/codinghorror">Jeff Atwood</a>'s post called <a href="http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/">visual explanation of SQL joins</a>.</p>
<p>A former co-worker told me that seeing these pictures changed his life. I hope you like them.</p>
<p>This is a left join: you get all the keys from the left data set, regardless of whether they're in the right data set.</p>
<hr>
<p><img width="1000%" title="right join" src="img/right_join.png"></p>
<hr>
<p>Jeff Atwood doesn't have a picture of a right join on his blog, but you can make one with a little <a href="http://www.imagemagick.org/">ImageMagick</a>.</p>
<pre><code class="language-bash">$ convert -rotate 180 left_join.png right_join.png</code></pre>

<p>You're welcome.</p>
<hr>
<p><img width="1000%" title="inner join" src="img/inner_join.png"></p>
<hr>
<p>An inner join, or natural join, only gives you results for keys that appear in both the left and right data sets.</p>
<hr>
<p><img width="1000%" title="outer join" src="img/outer_join.png"></p>
<hr>
<p>And an outer join gives you everything. Great!</p>
<p>There are a few other terms we could add, but let's not.</p>
<hr>
<p><img height="1000%" title="dplyr cheat sheet joins" src="img/dplyr_joins.png"></p>
<hr>
<p>Here's the <code>dplyr</code> summary again. You can see how you can introduce missing values when doing left, right, and outer joins.</p>
<p>Ready? Here's a test.</p>
<hr>
<p>How many rows do you get when you outer join two tables?</p>
<hr>
<p>Think about this question, discuss it with somebody near you, come up with everything you can say about the number of rows you might expect when you join two tables. Introduce any quantities you think you'd like to know.</p>
<p>Take about three minutes and then come back.</p>
<p><em>three minutes pass</em></p>
<p>Say there are <em>N</em> rows in the first table and <em>M</em> rows in the second table. Then the smallest number of rows we can get from the outer join is the greater of <em>N</em> and <em>M</em>. But we might get as many as <em>N * M</em> rows, if all the keys are the same!</p>
<p>If you said the maximum was <em>N + M</em>, you were probably assuming, implicitly or explicitly, that all they keys were unique. This is a common assumption that you should really check.</p>
<hr>
<pre><code class="language-r">&gt; nrow(first)
## [1] 3
&gt; nrow(second)
## [1] 3
&gt; result &lt;- merge(first, second)
&gt; nrow(result)
## [1] 3</code></pre>

<hr>
<p><em>This code is runnable in <a href="count_trouble.R">count_trouble.R</a>.</em></p>
<p>Is it enough to check the numbers of rows when we do joins?</p>
<p>This does an inner join, which is the default for <code>merge</code> in R.</p>
<p>Think about it.</p>
<hr>
<pre><code class="language-text"> x    y1        x   y2        x    y1   y2
 1 looks        1 good        1 looks good
 2    oh        2  boy        2    oh  boy
 3  well        2   no        2    oh   no</code></pre>

<hr>
<p>There is no peace while you don't have unique IDs.</p>
<p>There are times when you don't want every ID to be unique in a table, but really really often you do. You probably want to check that uniqueness explicitly.</p>
<hr>
<p>when names are the same</p>
<hr>
<p>This has been a discussion of problems arising from names being the same.</p>
<hr>
<p>when names aren't the same</p>
<hr>
<p>Probably also want to check that the intersection you get is what you expect.</p>
<p>You don't want to silently drop a ton of rows when you merge!</p>
<p>This is the beginning of our problems with names that aren't the same.</p>
<hr>
<p>demo: let's play tennis</p>
<hr>
<p><em>Here begins material also present in <a href="https://github.com/ajschumacher/mergic/tree/master/tennis">ajschumacher/mergic:tennis</a>.</em></p>
<p>Download the <a href="https://archive.ics.uci.edu/ml/datasets/Tennis+Major+Tournament+Match+Statistics">Tennis Major Tournament Match Statistics Data Set</a> from the <a href="https://archive.ics.uci.edu/ml/">UC Irvine Machine Learning Repository</a> into an empty directory:</p>
<pre><code class="language-bash">$ wget https://archive.ics.uci.edu/ml/machine-learning-databases/00300/Tennis-Major-Tournaments-Match-Statistics.zip</code></pre>

<p>This file should be stable, but it's also included <a href="tennis/Tennis-Major-Tournaments-Match-Statistics.zip">here</a> and/or you can verify that its <code>md5</code> is <code>e9238389e4de42ecf2daf425532ce230</code>.</p>
<p>Unpack eight CSV files from the <code>Tennis-Major-Tournaments-Match-Statistics.zip</code>:</p>
<pre><code class="language-bash">$ unzip Tennis-Major-Tournaments-Match-Statistics.zip</code></pre>

<p>You should see that the first two columns of each file contain player names, though the column names are not consistent. For example:</p>
<pre><code class="language-bash">$ head -2 AusOpen-women-2013.csv | cut -c 1-40
## Player1,Player2,Round,Result,FNL1,FNL2,F
## Serena Williams,Ashleigh Barty,1,1,2,0,5

$ head -2 USOpen-women-2013.csv | cut -c 1-40
## Player 1,Player 2,ROUND,Result,FNL.1,FNL
## S Williams,V Azarenka,7,1,2,1,57,44,43,2</code></pre>

<p>Make a <code>names.txt</code> with all the names that appear:</p>
<pre><code class="language-bash">$ for filename in *2013.csv
do
    for field in 1 2
    do
        tail +2 $filename | cut -d, -f$field &gt;&gt; names.txt
    done
done</code></pre>

<p>Now you have a file with 1,886 lines, each one of 669 unique strings, as you can verify:</p>
<pre><code class="language-bash">$ wc -l names.txt
## 1886

$ sort names.txt | uniq | wc -l
## 669</code></pre>

<p>There are too many unique strings&#8212;sometimes more than one string for the same player. As a result, a count of the most common names will not accurately tell us who played the most in these 2013 tennis competitions.</p>
<pre><code class="language-bash">$ sort names.txt | uniq -c | sort -nr | head
##  21 Rafael Nadal
##  17 Stanislas Wawrinka
##  17 Novak Djokovic
##  17 David Ferrer
##  15 Roger Federer
##  14 Tommy Robredo
##  13 Richard Gasquet
##  11 Victoria Azarenka
##  11 Tomas Berdych
##  11 Serena Williams</code></pre>

<p>The list above is not the answer we&#8217;re looking for. We want to be correct.</p>
<hr>
<p>single field deduplication</p>
<hr>
<p>We're going to think about this problem, which is pretty common, of de-duplicating (or making a merge table for) a single text field.</p>
<hr>
<pre><code>Lukas Lacko             F Pennetta
Leonardo Mayer          S Williams
Marcos Baghdatis        C Wozniacki
Santiago Giraldo        E Bouchard
Juan Monaco             N.Djokovic
Dmitry Tursunov         S.Giraldo
Dudi Sela               Y-H.Lu
Fabio Fognini           T.Robredo
...                     ...</code></pre>

<hr>
<p><em>Here begins material also presented in a <a href="/20150514-mergic/">lightning talk</a> at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>.</em></p>
<p>To be clear, the problem looks like this. And the problem often looks like this: You have either two columns with slightly different versions of identifiers, or one long list of things that you need to resolve to common names. These problems are fundamentally the same.</p>
<p>Do you see the match here? (It's Santiago!)</p>
<p>So we need to find the strings that refer to the same person.</p>
<hr>
<p>demo: Open Refine</p>
<hr>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good.</p>
<p>An interesting side story is that Open Refine was formerly Google Refine, and before that <a href="http://en.wikipedia.org/wiki/Metaweb">Metaweb</a>'s &#8220;Freebase Gridworks&#8221;. Google is shutting down <a href="http://www.freebase.com/">Freebase</a>, and we have to hope that <a href="https://www.wikidata.org/">Wikidata</a> will then be the open match for Google's <a href="http://en.wikipedia.org/wiki/Knowledge_Graph">Knowledge Graph</a>.</p>
<p>Thanks to <a href="https://twitter.com/jqnatividad">Joel Natividad</a> for pointing out an <a href="https://github.com/OpenRefine/OpenRefine/issues/983">interesting algorithmic development</a> connected with ongoing work on Open Refine. He also pointed out that there is a Python module called <a href="https://github.com/PaulMakepeace/refine-client-py">refine-client</a> for using Open Refine from Python.</p>
<p>Steps of simple Open Refine demo:</p>
<ul>
<li>Start the Open Refine app</li>
<li>Browse to <a href="http://localhost:3333/">http://localhost:3333/</a></li>
<li>Click &#8220;Create Project&#8221;</li>
<li>Click &#8220;Choose Files&#8221;</li>
<li>Select <code>names.txt</code></li>
<li>Click &#8220;Next &#187;&#8220;</li>
<li>Click &#8220;Create Project &#187;&#8221;</li>
<li>Click the down arrow next to &#8220;Column 1&#8221;, then follow &#8220;Edit cells&#8221; to &#8220;Cluster and edit&#8230;&#8221;</li>
</ul>
<p><img width="1000%" title="Open Refine" src="img/open_refine.png"></p>
<p>Open Refine has <a href="https://github.com/OpenRefine/OpenRefine/wiki/Clustering">introductory</a> and <a href="https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth">in-depth</a> documentation about their &#8220;clustering&#8221; mechanisms.</p>
<p>In this interface, we can use &#8220;key collision&#8221; or &#8220;nearest neighbor&#8221; methods.</p>
<p>The &#8220;key collision&#8221; method maps every value to one &#8220;key&#8221;, and items are identical if they have the same key. This allows us to avoid calculating anything for all pairs. (This should remind you of hashing.)</p>
<p>There are four &#8220;keying functions&#8221; in Open Refine:</p>
<ul>
<li>&#8220;fingerprint&#8221; standardizes a string by case and punctuation.</li>
<li>&#8220;ngram-fingerprint&#8221; standardizes a bit further, using character ngrams.</li>
<li>&#8220;metaphone3&#8221; standardizes by the phonetic <a href="http://www.amorphics.com/">Metaphone 3</a> algorithm so that things that sound the same in English should be keyed together. (There are strange licensing issues around Metaphone 3.)</li>
<li>&#8220;cologne-phonetic&#8221; standardizes by the <a href="http://de.wikipedia.org/wiki/K%C3%B6lner_Phonetik">K&#246;lner Phonetik</a> algorithm so that things that sound the same in German should be keyed together. (There is a <a href="https://commons.apache.org/proper/commons-codec/apidocs/org/apache/commons/codec/language/ColognePhonetic.html">real open source version</a>.)</li>
</ul>
<p>The &#8220;nearest neighbor&#8221; method calculates pairwise distances, which is slow. Open Refine uses blocking to break things up into blocks that it won&#8217;t compare across, which reduces the number of comparisons to improve speed of calculation.</p>
<p>There are two &#8220;distance functions&#8221; in Open Refine:</p>
<ul>
<li>&#8220;levenshtein&#8221; is the well-known <a href="http://en.wikipedia.org/wiki/Levenshtein_distance">Levenshtein edit distance</a></li>
<li>&#8220;PPM&#8221; estimates how different strings are by how well they compress separately versus together, using <a href="http://en.wikipedia.org/wiki/Prediction_by_partial_matching">Prediction by Partial Matching</a>.</li>
</ul>
<p>The &#8220;radius&#8221; is the distance below which two items will be clustered together. With a higher value for &#8220;radius&#8221;, groups will tend to be larger.</p>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good, but there are several things I would like:</p>
<ul>
<li>See all the items rather than just the ones being grouped.</li>
<li>Customize / break up groupings that are incorrect, while preserving others.</li>
<li>Easily use a custom distance function.</li>
<li>Easily use a custom function for choosing the &#8220;New Cell Value&#8221;.</li>
<li>See what would happen with different &#8220;radii&#8221; without trying them all.</li>
<li><strong>Have a record of the whole transformation that&#8217;s easy to review, edit, and reapply.</strong></li>
</ul>
<hr>
<p><img height="1000%" title="ermahgerd mergic" src="img/ermahgerd.png"></p>
<hr>
<p>So I made <code>mergic</code>.</p>
<hr>
<ul>
<li>simple</li>
<li>customizable</li>
<li>reproducible</li>
</ul>
<hr>
<p>The goals of <code>mergic</code> are to be:</p>
<ul>
<li>simple, meaning largely text-based and obvious; the tool disappears</li>
<li>customizable, meaning you can easily use a custom distance function</li>
<li>reproducible, meaning everything you do can be done again automatically</li>
</ul>
<hr>
<p>the tool disappears</p>
<hr>
<p>Good tools disappear.</p>
<p>Whatever text editor you use, the your work product is a text file. You can use any text editor, or use different ones for different purposes, and so on.</p>
<p>Your merge process shouldn't rely on any particular piece of software for its replicability.</p>
<hr>
<p>any distance function</p>
<hr>
<p>Your distance function can make all the difference. You need to be able to plug in any distance function that works well for your data.</p>
<hr>
<p>really reproducible</p>
<hr>
<p>People can see what's happening, and computers can keep doing the process without clicking or re-running particular executables.</p>
<hr>
<p><img width="1000%" title="big data" src="img/big_data.png"></p>
<hr>
<p>A quick disclaimer!</p>
<p>This is John Langford's slide, about what big data is. He says that small data is data for which O(n<sup>2</sup>) algorithms are feasible. Currently <code>mergic</code> is strictly for this kind of "artisanal" data, where we want to ensure that our matching is correct but want to reduce the amount of human work to ensure that. And we are about to get very O(n<sup>2</sup>).</p>
<hr>
<pre><code>Santiago Giraldo,Leonardo Mayer
Santiago Giraldo,Dudi Sela
Santiago Giraldo,Juan Monaco
Santiago Giraldo,S Williams
Santiago Giraldo,C Wozniacki
Santiago Giraldo,S.Giraldo
Santiago Giraldo,Marcos Baghdatis
Santiago Giraldo,Y-H.Lu
...</code></pre>

<hr>
<p>So we make all possible pairs of identifiers!</p>
<p>One of the things that Open Refine gets right is that it doesn't show us humans all the pairs it's looking at.</p>
<p>All these pairs are annoying for a computer, and awful for humans. The computer can calculate a lot of pairwise distances, but I don't want to look at all the pairs.</p>
<p>Do you see the match here? (It's Santiago again!)</p>
<hr>
<pre><code>Karolina Pliskova,K Pliskova</code></pre>

<hr>
<p>Aside from being a drag to look at, there's a bigger problem with verifying equality on a pairwise basis.</p>
<p>Do these two records refer to the same person? (Tennis fans may see where I'm going with this.)</p>
<hr>
<pre><code>Kristyna Pliskova,K Pliskova</code></pre>

<hr>
<p>Karolina has a twin sister, and Kristyna also plays professional tennis! This may well not be obvious if you only look at pairs individually. What matters is the set of names that are transitively judged as equal.</p>
<hr>
<p>sets &gt; pairs</p>
<hr>
<p>Both perceptually and logically, it's better to think in sets than in a bunch of individual pairs.</p>
<hr>
<p>workflow support for reproducible deduplication and merging</p>
<hr>
<p>This is what <code>mergic</code> is for. <code>mergic</code> is a simple tool designed to make it less painful when you need to merge things that don't yet merge.</p>
<hr>
<p>demo: mergic tennis</p>
<hr>
<p>With all that background, let's see how <code>mergic</code> attempts to support a good workflow.</p>
<pre><code class="language-bash">$ pew new pydata</code></pre>

<p>I'll start by making a new <a href="https://virtualenv.pypa.io/">virtual environment</a> using <a href="https://github.com/berdario/pew">pew</a>.</p>
<pre><code class="language-bash">$ pip install mergic</code></pre>

<p><code>mergic</code> is very new (version 0.0.4.1) and it currently installs with no extra dependencies.</p>
<pre><code class="language-bash">$ mergic -h</code></pre>

<p><code>mergic</code> includes a command-line script based on <a href="https://docs.python.org/2/library/argparse.html">argparse</a> that uses a default string distance function.</p>
<pre><code>usage: mergic [-h] {calc,make,check,diff,apply,table} ...

positional arguments:
  {calc,make,check,diff,apply,table}
    calc                calculate all partitions of data
    make                make a JSON partition from data
    check               check validity of JSON partition
    diff                diff two JSON partitions
    apply               apply a patch to a JSON partition
    table               make merge table from JSON partition

optional arguments:
  -h, --help            show this help message and exit</code></pre>

<p>In the tennis data, names appear sometimes with full first names and sometimes with only first initials. To get good comparisons, we should:</p>
<ul>
<li>Transform all the data to the same format, as nearly as possible.</li>
<li>Use a good distance on the transformed data.</li>
</ul>
<p>We can do both of these things with a simple custom script, <a href="tennis/tennis_mergic.py">tennis_mergic.py</a>. It only <a href="requirements.txt">requires</a> the <code>mergic</code> and <code>python-Levenshtein</code> packages.</p>
<pre><code class="language-python">#!/usr/bin/env python

import re
import Levenshtein
import mergic


def first_initial_last(name):
    initial = re.match("^[A-Z]", name).group()
    last = re.search("(?&lt;=[ .])[A-Z].+$", name).group()
    return "{}. {}".format(initial, last)


def distance(x, y):
    x = first_initial_last(x)
    y = first_initial_last(y)
    return Levenshtein.distance(x, y)


mergic.Blender(distance).script()</code></pre>

<p>Note that there's a transformation step in there, normalizing the form of the names to have just a first initial and last name. This kind of normalization can be very important.</p>
<p>As a more extreme example, a friend of mine has used the following transform: Google it. Then you can use a distance on the result set to deduplicate.</p>
<p>Now <a href="tennis/tennis_mergic.py">tennis_mergic.py</a> can be used just like the standard <code>mergic</code> script.</p>
<pre><code class="language-bash">$ ./tennis_mergic.py calc names.txt
## num groups, max group, num pairs, cutoff
## ----------------------------------------
##        669,         1,         0, -1
##        358,         5,       384, 0
##        348,         6,       414, 1
##        332,         6,       470, 2
##        262,        85,      5117, 3
##        165,       324,     52611, 4
##         86,       496,    122899, 5
##         46,       584,    170287, 6
##         24,       624,    194407, 7
##         16,       641,    205138, 8
##         10,       650,    210940, 9
##          4,       663,    219459, 10
##          2,       668,    222778, 11
##          1,       669,    223446, 12</code></pre>

<p>There is a clear best cutoff here, as the size of the max group jumps from 6 items to 85 and the number of within-group comparisons jumps from 470 to 5,117. So we create a partition where the Levenshtein distance between names in our standard first initial and last name format is no more than two, and put the result in a file called <code>groups.json</code>:</p>
<pre><code class="language-bash">$ ./tennis_mergic.py make names.txt 2 &gt; groups.json</code></pre>

<p><strong>This kind of JSON grouping file could be produced and edited by anything, not just <code>mergic</code>.</strong></p>
<p>As expected, the proposed grouping has combined things over-zealously in some places:</p>
<pre><code class="language-bash">$ head -5 groups.json
## {
##     "Yen-Hsun Lu": [
##         "Di Wu",
##         "Yen-Hsun Lu",
##         "Y-H.Lu",</code></pre>

<p>Manual editing can produce a corrected version of the original grouping, which could be saved as <code>edited.json</code>:</p>
<pre><code class="language-bash">$ head -8 edited.json
## {
##     "Yen-Hsun Lu": [
##         "Yen-Hsun Lu",
##         "Y-H.Lu"
##     ],
##     "Di Wu": [
##         "Di Wu"
##     ],</code></pre>

<p>Parts of the review process would be difficult or impossible for a computer to do accurately.</p>
<ul>
<li>There are the Pl&#237;&#353;kov&#225; twins, Karol&#237;na and Krist&#253;na. When we see that <code>K Pliskova</code> appears, we have to go back and see that this occurred in the <code>USOpen-women-2013.csv</code> file, and only Karol&#237;na played in the <a href="http://en.wikipedia.org/wiki/2013_US_Open_%E2%80%93_Women%27s_Singles">2013 US Open</a>.</li>
<li>In a similar but less interesting way, <code>B.Becker</code> turns out to refer to Benjamin, not Brian.</li>
<li>An <code>A Wozniak</code> appears with <code>C Wozniack</code> and <code>C Wozniacki</code>. The first initial does turn out to differentiate the Canadian from the Dane.</li>
<li>The name <code>A.Kuznetsov</code> refers to <em>both</em> Andrey <em>and</em> Alex in <code>Wimbledon-men-2013.csv</code>. This can't be resolved by <code>mergic</code>. One way to resolve the issues is to edit <code>Wimbledon-men-2013.csv</code> so that <code>A.Kuznetsov,I.Sijsling</code> becomes <code>Alex Kuznetsov,I.Sijsling</code>, based on checking <a href="http://en.wikipedia.org/wiki/2013_Wimbledon_Championships_%E2%80%93_Men%27s_Singles">records from that competition</a>.</li>
<li><code>Juan Martin Del Potro</code> is unfortunately too different from <code>J.Del Potro</code> in the current formulation to be grouped automatically, but a human reviewer can correct this. Similarly for <code>Anna Schmiedlova</code> and <code>Anna Karolina Schmiedlova</code>.</li>
</ul>
<p>After editing, you can check that the new grouping is still valid. At this stage we aren't using anything custom any more, so the default <code>mergic</code> is fine:</p>
<pre><code class="language-bash">$ mergic check edited.json
## 669 items in 354 groups</code></pre>

<p>The <code>mergic</code> diffing tools make it easy to make comparisons that would otherwise be difficult, letting us focus on and save only changes that are human reviewers make rather than whole files.</p>
<pre><code class="language-bash">$ mergic diff groups.json edited.json &gt; diff.json</code></pre>

<p>Now <code>diff.json</code> only has the entries that represent changes from the original <code>groups.json</code>.</p>
<p>The edited version can be reconstructed from the original and the diff with <code>mergic apply</code>:</p>
<pre><code class="language-bash">$ mergic apply groups.json diff.json &gt; rebuilt.json</code></pre>

<p>The order of <code>rebuilt.json</code> may not be identical to the original <code>edited.json</code>, but the diff will be empty, meaning the file is equivalent:</p>
<pre><code class="language-bash">$ mergic diff edited.json rebuilt.json
## {}</code></pre>

<p>Finally, to generate a CSV merge table that you'll be able to use with any other tool:</p>
<pre><code class="language-bash">$ mergic table edited.json &gt; merge.csv</code></pre>

<p>Now the file <code>merge.csv</code> has two columns, <code>original</code> and <code>mergic</code>, where <code>original</code> contains all the values that appeared in the original data and <code>mergic</code> contains the deduplicated keys. You can join this on to your original data and go to town.</p>
<p>Here's how we might do that to quickly get a list of who played the most in these 2013 tennis events:</p>
<pre><code class="language-bash">$ join -t, &lt;(sort names.txt) &lt;(sort merge.csv) | cut -d, -f2 | sort | uniq -c | sort -nr | head
##  24 Novak Djokovic
##  22 Rafael Nadal
##  21 Serena Williams
##  21 David Ferrer
##  20 Na Li
##  19 Victoria Azarenka
##  19 Agnieszka Radwanska
##  18 Stanislas Wawrinka
##  17 Tommy Robredo
##  17 Sloane Stephens</code></pre>

<p>Note that this is not the same as the result we got before resolving these name issues:</p>
<pre><code class="language-bash">$ sort names.txt | uniq -c | sort -nr | head
##  21 Rafael Nadal
##  17 Stanislas Wawrinka
##  17 Novak Djokovic
##  17 David Ferrer
##  15 Roger Federer
##  14 Tommy Robredo
##  13 Richard Gasquet
##  11 Victoria Azarenka
##  11 Tomas Berdych
##  11 Serena Williams</code></pre>

<p>As it happens, using a cutoff of 0 and doing no hand editing will still give the correct top ten. In general the desired result and desired level of certainty in its correctness will inform the level of effort that is justified.</p>
<hr>
<p>distance matters</p>
<hr>
<p>Having a good distance function might be the most important thing. It's hard to imagine a machine learning as good a distance function as you could just right based on your human intelligence.</p>
<p>There is work on learnable edit distances; notably there's a current Python project to implement hidden alignment conditional random fields for classifying string pairs: <a href="https://github.com/dirko/pyhacrf">pyhacrf</a>. Python dedupe is <a href="https://github.com/datamade/dedupe/issues/14">eager</a> to incorporate this.</p>
<p>See also: <a href="http://www.cs.utexas.edu/users/ml/papers/marlin-kdd-03.pdf">Adaptive Duplicate Detection Using Learnable String Similarity Measures</a></p>
<hr>
<p>extension to multiple fields</p>
<hr>
<p>We've been talking about single field deduplication.</p>
<hr>
<pre><code class="language-text">name
----
Bob
Rob
Robert</code></pre>

<hr>
<p>This means that we have one field, say name.</p>
<hr>
<pre><code class="language-text">name, name
----------
Bob, Bobby
Bob, Robert
Bobby, Robert</code></pre>

<hr>
<p>And we look at all the possible pairs and calculate those pairwise distances.</p>
<hr>
<p><img width="1000%" title="one dimensional" src="img/one_dimensional.png"></p>
<hr>
<p>While we described it even in Open Refine as &#8220;clustering&#8221;, we've really been doing a classification task: either a pair is in the same group or they aren't. We've had one dimension, and we hope that we can just divide true connections from different items with a simple cutoff.</p>
<hr>
<pre><code class="language-text">name,    hometown
-----------------
Bob,     New York
Rob,     NYC
Robert,  "NY, NY"</code></pre>

<hr>
<p>Often, there's more than one field involved, and it might be good to treat all the fields separately.</p>
<p>So let's calculate distances between each field entry for each pair of rows, in this case.</p>
<hr>
<p><img height="1000%" title="two dimensional" src="img/two_dimensional.png"></p>
<hr>
<p>The reason it might be good to treat the fields separately is that they might be more useful together; we might be able to classify all the true duplicates using the information from both fields.</p>
<p>Maybe you can find two clusters, one for true duplicates and one for different items.</p>
<p>Or maybe you could get some training data and use whatever classification algorithm you like.</p>
<p>Let's look at a couple packages that do these things.</p>
<hr>
<p>R: RecordLinkage</p>
<hr>
<p>The R <a href="http://cran.r-project.org/web/packages/RecordLinkage/index.html">RecordLinkage</a> package, which has a good <a href="http://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf">R Journal article</a> and a number of fine vignettes, does quite a lot of interesting things along the lines of what we've been discussing.</p>
<p>You'll also notice that it imports <code>e1071</code> and <code>rpart</code> and others to plug in machine learning for determining duplicates.</p>
<hr>
<p>demo: mergic on RecordLinkage data</p>
<hr>
<p>Let's look at some of the example data that comes with <code>RecordLinkage</code>.</p>
<p>We write the data out to CSV very simply with <a href="RLdata/RLdata500.R">RLdata500.R</a>:</p>
<pre><code class="language-r"># install.packages('RecordLinkage')
library('RecordLinkage')
data(RLdata500)
write.table(RLdata500, "RLdata500.txt",
            row.names=FALSE, col.names=FALSE,
            quote=FALSE, sep=",", na="")</code></pre>

<p>Then we can take a look at the data:</p>
<pre><code class="language-bash">$ head -4 RLdata500.txt
## CARSTEN,,MEIER,,1949,7,22
## GERD,,BAUER,,1968,7,27
## ROBERT,,HARTMANN,,1930,4,30
## STEFAN,,WOLFF,,1957,9,2</code></pre>

<p>The data is fabricated name and birth date from a hypothetical German hospital. It has a number of columns, but for <code>mergic</code> we'll just treat the rows of CSV as single strings.</p>
<pre><code class="language-bash">$ mergic calc RLdata500.csv
## ...
##        451,         2,        49, 0.111111111111
##        450,         2,        50, 0.115384615385
##        449,         3,        52, 0.125
## ...</code></pre>

<p>Looking through the possible groupings, we see a cutoff of about 0.12 that will produce 50 groups of two items, which looks promising.</p>
<p>This is slightly artificial, but only slightly so; we could well be doing this for two columns to merge on, in which case so we would hope to find groups of two elements.</p>
<pre><code class="language-bash">$ mergic make RLdata500.csv 0.12
## {
##     "MATTHIAS,,HAAS,,1955,7,8": [
##         "MATTHIAS,,HAAS,,1955,7,8",
##         "MATTHIAS,,HAAS,,1955,8,8"
##     ],
##     "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
##         "HELGA,ELFRIEDE,BERGER,,1989,1,18",
##         "HELGA,ELFRIEDE,BERGER,,1989,1,28"
##     ],
## ...</code></pre>

<p>In this example, the partition at a cutoff of 0.12 happens to be exactly right and we correctly group everything. This says something about how realistic this example data set is, something about your tool of choice if it can't easily get perfect performance on this example data set, and also something about information leakage.</p>
<hr>
<p><code>dedupe</code></p>
<hr>
<p>The Python <a href="https://github.com/datamade/dedupe">dedupe</a> project from <a href="http://datamade.us/">DataMade</a> in Chicago is very cool, and I'd better not neglect it.</p>
<p>It's a Python library that implements sophisticated multi-field deduplication and has a lot of connected software. One of these is the <code>csvdedupe</code>.</p>
<hr>
<p>demo: csvdedupe</p>
<hr>
<p>We'll use the RecordLinkage data with a header.</p>
<pre><code class="language-r">write.table(RLdata500, "RLdata500.csv",
            row.names=FALSE,
            quote=FALSE, sep=",", na="")</code></pre>

<p>Then we start the process, specifying which columns of the data to consider for matching.</p>
<pre><code class="language-bash">$ csvdedupe RLdata500.csv --field_names $(head -1 RLdata500.csv | tr ',' ' ')</code></pre>

<p>We go into an interactive supervision stage in which <code>dedupe</code> asks us to clarify things. The hope is that it will learn what matters.</p>
<p>You can build this kind of behavior into your own systems; there is an <a href="http://datamade.github.io/dedupe-examples/docs/csv_example.html">example</a>.</p>
<p>At the end you get output that you can use much like the <code>table</code> output from <code>mergic</code>. It needs some transforming to be easily reviewed by humans though.</p>
<hr>
<p>real clustering?</p>
<hr>
<p>The &#8220;clustering&#8221; that we've been doing hasn't been much like usual clustering.</p>
<hr>
<p><img width="1000%" title="one dimensional" src="img/one_dimensional.png"></p>
<hr>
<p>In part, this is because we we've only had distances between strings without having a real &#8220;string space&#8221;.</p>
<p>I'm going to just sketch out this direction; I think it's interesting but I haven't seen any real results in it yet.</p>
<hr>
<p>dog, doge, kitten, kitteh</p>
<hr>
<p>Say these are the items we're working with. We can use Levenshtein edit distance to make a distance matrix.</p>
<hr>
<pre><code class="language-text">       dog doge kitten kitteh
   dog   0    1      6      6
  doge   1    0      5      5
kitten   6    5      0      1
kitteh   6    5      1      0</code></pre>

<hr>
<p>So here's a distance matrix, and it looks the way we'd expect. But we still don't have <em>coordinates</em> for our words.</p>
<p>Luckily, there is at least one technique for coming up with coordinates when you have a distance matrix. Let's use <a href="http://en.wikipedia.org/wiki/Multidimensional_scaling">multidimensional scaling</a>! There's a nice <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html">implementation in sklearn</a>.</p>
<hr>
<pre><code class="language-python">from sklearn.manifold import MDS
mds = MDS(dissimilarity='precomputed')
coords = mds.fit_transform(distances)</code></pre>

<hr>
<p>Here's all the code it takes.</p>
<hr>
<p><img width="1000%" title="MDS coordinates from distance matrix" src="img/mds_words.png"></p>
<hr>
<p>And here's the result! This is at least a fun visualization, and I wonder if doing clustering in a space like this might sometimes lead to better results.</p>
<p>The key thing here is that we're clustering on the elements themselves, rather than indirectly via the pairwise distances.</p>
<p>There are other ways of getting coordinates for words. This includes the very interesting <a href="https://code.google.com/p/word2vec/">word2vec</a> and related techniques.</p>
<p>Also, you might have items are already naturally coordinates, for example if you have medical data like a person's height or weight.</p>
<hr>
<p>deep thoughts</p>
<hr>
<p>By way of conclusion, I'd like to suggest that this problem of deduplication is no good and we should take steps to:</p>
<ul>
<li>prevent it from being necessary, by having our systems recommend or enforce standard naming</li>
<li>make it possible to do deduplication once and reintegrate the results back into the data system</li>
</ul>
<p>It should be possible to make changes and share them back to data providers. It should be possible to edit data while preserving the data's history. These kind of collaborative data editing are not super easy to implement, and I hope systems emerge that handle it better than current systems.</p>
<hr>
<p>questions for discussion</p>
<hr>
<p>I'd like to ask you to consider and discuss with your peers:</p>
<ul>
<li>What workflows and tools do you use for these kinds of tasks?</li>
<li>Does the JSON partition format used by <code>mergic</code> make sense for your use?</li>
<li>Does the merge table format used by <code>mergic</code> make sense for your use?</li>
<li>What else would make this kind of process better for you?</li>
</ul>
<hr>
<p><img width="1000%" title="Open Data Science Conference" src="img/open_data_sci_con.png"></p>
<hr>
<p>I also hope to see you at <a href="http://opendatascicon.com/">Open Data Science Con</a> in Boston!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>
<p>I'd love to hear from you!</p>
<hr>
<h3>Other interesting things:</h3>
<ul>
<li><a href="http://infolab.stanford.edu/serf/">Stanford Entity Resolution Framework</a></li>
<li><a href="http://infolab.stanford.edu/serf/swoosh_vldbj.pdf">Swoosh: a generic approach to entity resolution</a></li>
<li><a href="http://www.umiacs.umd.edu/~getoor/Tutorials/ER_VLDB2012.pdf">Entity Resolution: Tutorial</a></li>
<li><a href="http://www.datacommunitydc.org/blog/2013/08/entity-resolution-for-big-data">Entity Resolution for Big Data (summary)</a></li>
<li><a href="http://dbs.uni-leipzig.de/file/learning_based_er_with_mr.pdf">Learning-based Entity Resolution with MapReduce</a></li>
<li><a href="http://linqs.cs.umd.edu/projects/ddupe/">D-Dupe: A Novel Tool for Interactive Data Deduplication and Integration</a></li>
</ul>    
    ]]></description>
<link>http://planspace.org/20150520-practical_mergic/</link>
<guid>http://planspace.org/20150520-practical_mergic/</guid>
<pubDate>Wed, 20 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>mergic</title>
<description><![CDATA[

<p><em>A lightning talk at the <a href="http://www.meetup.com/PyDataNYC/events/222329250/">May meeting</a> (<a href="http://www.bloomberg.com/event-registration/?id=39288">registration</a>) of the <a href="http://www.meetup.com/PyDataNYC/">PyData NYC meetup group</a>, introducing <a href="https://github.com/ajschumacher/mergic">mergic</a>.</em></p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>Hi! I'm Aaron. This is my blog and <a href="https://twitter.com/planarrowspace">my twitter handle</a>. You can get from one to the other. <a href="big.html">This presentation</a> and a corresponding write-up (you're reading it) are on my blog (which you're on).</p>
<hr>
<p><img height="1000%" title="ermahgerd mergic" src="ermahgerd.png"></p>
<hr>
<p>Down to business!</p>
<hr>
<pre><code>Lukas Lacko             F Pennetta
Leonardo Mayer          S Williams
Marcos Baghdatis        C Wozniacki
Santiago Giraldo        E Bouchard
Juan Monaco             N.Djokovic
Dmitry Tursunov         S.Giraldo
Dudi Sela               Y-H.Lu
Fabio Fognini           T.Robredo
...                     ...</code></pre>

<hr>
<p>The problem often looks like this: You have either two columns with slightly different versions of identifiers, or one long list of things that you need to resolve to common names. These problems are fundamentally the same.</p>
<p>Do you see the match here? (It's Santiago!)</p>
<hr>
<p>workflow support for reproducible deduplication and merging</p>
<hr>
<p>This is what <code>mergic</code> is for. <code>mergic</code> is a simple tool designed to make it less painful when you need to merge things that don't yet merge.</p>
<hr>
<p><img width="1000%" title="big data" src="big_data.png"></p>
<hr>
<p>A quick disclaimer!</p>
<p>This is John Langford's slide, about what big data is. He says that small data is data for which O(n<sup>2</sup>) algorithms are feasible. Currently <code>mergic</code> is strictly for this kind of "artisanal" data, where we want to ensure that our matching is correct but want to reduce the amount of human work to ensure that. And we are about to get very O(n<sup>2</sup>).</p>
<hr>
<pre><code>Santiago Giraldo,Leonardo Mayer
Santiago Giraldo,Dudi Sela
Santiago Giraldo,Juan Monaco
Santiago Giraldo,S Williams
Santiago Giraldo,C Wozniacki
Santiago Giraldo,S.Giraldo
Santiago Giraldo,Marcos Baghdatis
Santiago Giraldo,Y-H.Lu
...</code></pre>

<hr>
<p>So we make all possible pairs of identifiers! This is annoying for a computer, and awful for humans. The computer can calculate a lot of pairwise distances, but I don't want to look at all the pairs.</p>
<p>Do you see the match here? (It's Santiago again!)</p>
<hr>
<pre><code>INFO:dedupe.training:1.0
name : stanislas wawrinka

name : stanislas wawrinka

Do these records refer to the same thing?
(y)es / (n)o / (u)nsure / (f)inished

O.o?</code></pre>

<hr>
<p>The is a "screen shot" of the <a href="https://github.com/datamade/csvdedupe">csvdedupe</a> interface, which is based on the Python <a href="https://github.com/datamade/dedupe">dedupe</a> project, which is very cool. It could be exactly what you want for larger amounts of more complex data. There's even work on getting learnable edit distances implemented now, which would be great to see. But for very simple data sets, <code>dedupe</code> can be overkill. Also, you don't get much sense of the big picture of your data set, and it's still very pair-oriented.</p>
<hr>
<pre><code>Karolina Pliskova,K Pliskova</code></pre>

<hr>
<p>Aside from being a drag to look at, there's a bigger problem with verifying equality on a pairwise basis.</p>
<p>Do these two records refer to the same person? (Tennis fans may see where I'm going with this.)</p>
<hr>
<pre><code>Kristyna Pliskova,K Pliskova</code></pre>

<hr>
<p>Karolina has a twin sister, and Kristyna also plays professional tennis! This may well not be obvious if you only look at pairs individually. What matters is the set of names that are transitively judged as equal.</p>
<hr>
<p>sets &gt; pairs</p>
<hr>
<p>Both perceptually and logically, it's better to think in sets than in a bunch of individual pairs.</p>
<hr>
<p><img width="1000%" title="Open Refine" src="open_refine.png"></p>
<hr>
<p><a href="http://openrefine.org/">Open Refine</a> is quite good. Their interface shows you some useful diagnostics, and you can see sets of things. There's even some idea of repeatable transformations. But there's so much functionality wrapped up in a mostly graphical interface that it's hard to make it part of an easily repeatable workflow. And while there are a bunch of built-in distance functions, I'm not sure whether it's possible to use a custom distance function in Open Refine.</p>
<hr>
<ul>
<li>simple</li>
<li>customizable</li>
<li>reproducible</li>
</ul>
<hr>
<p>So the goals of <code>mergic</code> are to be:</p>
<ul>
<li>simple, meaning largely text-based and obvious</li>
<li>customizable, meaning you can easily use a custom distance function</li>
<li>reproducible, meaning everything you do can be done again automatically</li>
</ul>
<hr>
<p>demo</p>
<hr>
<p>Here's a quick run-through of the <code>mergic</code> workflow. It's similar to the one in the <a href="https://github.com/ajschumacher/mergic">README</a>.</p>
<pre><code class="language-bash">pew new pydata</code></pre>

<p>I'll start by making a new <a href="https://virtualenv.pypa.io/">virtual environment</a> using <a href="https://github.com/berdario/pew">pew</a>.</p>
<pre><code class="language-bash">pip install mergic</code></pre>

<p><code>mergic</code> is very new (version 0.0.4) and it currently installs with no extra dependencies.</p>
<pre><code class="language-bash">mergic -h</code></pre>

<p><code>mergic</code> includes a command-line script based on <a href="https://docs.python.org/2/library/argparse.html">argparse</a> that uses a default string distance function.</p>
<pre><code>usage: mergic [-h] {calc,make,check,diff,apply,table} ...

positional arguments:
  {calc,make,check,diff,apply,table}
    calc                calculate all partitions of data
    make                make a JSON partition from data
    check               check validity of JSON partition
    diff                diff two JSON partitions
    apply               apply a patch to a JSON partition
    table               make merge table from JSON partition

optional arguments:
  -h, --help            show this help message and exit</code></pre>

<p>The command line script has a number of sub-commands that expose its functionality.</p>
<pre><code class="language-bash">head -4 RLdata500.csv</code></pre>

<p>We'll try <code>mergic</code> out with an example data set from <a href="http://www.r-project.org/">R</a>'s <a href="http://journal.r-project.org/archive/2010-2/RJournal_2010-2_Sariyar+Borg.pdf">RecordLinkage</a> package.</p>
<pre><code>CARSTEN,,MEIER,,1949,7,22
GERD,,BAUER,,1968,7,27
ROBERT,,HARTMANN,,1930,4,30
STEFAN,,WOLFF,,1957,9,2</code></pre>

<p>The data is fabricated name and birth date from a hypothetical German hospital. It has a number of columns, but for <code>mergic</code> we'll just treat the rows of CSV as single strings.</p>
<pre><code class="language-bash">mergic calc RLdata500.csv</code></pre>

<p>The <code>calc</code> subcommand calculates all the pairwise distances and provides diagnostics about possible groupings that could be produced.</p>
<pre><code>num groups, max group, num pairs, cutoff
----------------------------------------
       500,         1,         0, -0.982456140351
       497,         2,         3, 0.0175438596491</code></pre>

<p>With a cutoff lower than any actual encountered string distance, every item stays separate, the maximum group size is one, and there are no pairs within those groups to evaluate.</p>
<pre><code>         2,       499,    124251, 0.416666666667
         1,       500,    124750, 0.418181818182</code></pre>

<p>On the other extreme, we could group every item together in a giant mega-group.</p>
<pre><code>       451,         2,        49, 0.111111111111
       450,         2,        50, 0.115384615385
       449,         3,        52, 0.125</code></pre>

<p><code>mergic</code> gives you a choice about how big the groups it will produce will be. In this case, there's a cutoff of about 0.12 that will produce 50 groups of two items, which looks promising.</p>
<pre><code class="language-bash">mergic make RLdata500.csv 0.12</code></pre>

<p>We can make a grouping with that cutoff, and the result is a JSON-formatted partition.</p>
<pre><code class="language-json">{
    "MATTHIAS,,HAAS,,1955,7,8": [
        "MATTHIAS,,HAAS,,1955,7,8",
        "MATTHIAS,,HAAS,,1955,8,8"
    ],
    "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
        "HELGA,ELFRIEDE,BERGER,,1989,1,18",
        "HELGA,ELFRIEDE,BERGER,,1989,1,28"
    ],</code></pre>

<p>In this example, the partition at a cutoff of 0.12 happens to be exactly right and we correctly group everything. (This says something about how realistic this example data set is, something about your tool of choice if it can't easily get perfect performance on this example data set, and also something about information leakage.)</p>
<pre><code class="language-json">{
    "MATTHIAS,,HAAS,,1955,7,8": [
        "MATTHIAS,,HAAS,,1955,8,8"
    ],
    "HELGA,ELFRIEDE,BERGER,,1989,1,18": [
        "MATTHIAS,,HAAS,,1955,7,8",
        "HELGA,ELFRIEDE,BERGER,,1989,1,18",
        "HELGA,ELFRIEDE,BERGER,,1989,1,28"
    ],</code></pre>

<p>The above would be a strange change to make, but you could make such a change and save your changed version as a new file.</p>
<pre><code class="language-bash">mergic diff base.json edited.json &gt; diff.json
mergic apply base.json diff.json</code></pre>

<p><code>mergic</code> includes functionality for creating and applying diffs that compare two partitions. You can preserve just the changes that you make by hand, which provides a record of the changes that had a human in the loop versus the changes that were computer-generated.</p>
<pre><code class="language-bash">mergic table edited.json</code></pre>

<p>To actually accomplish the desired merge or deduplication after creating a good grouping in JSON, <code>mergic</code> will generate a two-column merge table in CSV that can be used with most any data system.</p>
<pre><code>"HANS,,SCHAEFER,,2003,6,22","HANS,,SCHAEFER,,2003,6,22"
"HARTMHUT,,HOFFMSNN,,1929,12,29","HARTMHUT,,HOFFMSNN,,1929,12,29"
"HARTMUT,,HOFFMANN,,1929,12,29","HARTMHUT,,HOFFMSNN,,1929,12,29"</code></pre>

<p>These merge tables are awful to work with by hand, which is why <code>mergic</code> leaves their generation as a final step after humans work with the more understandable JSON groupings.</p>
<hr>
<p><img width="1000%" title="custom distance function documentation on GitHub" src="custom_distance.png"></p>
<hr>
<p>It's easy to write a script with a custom distance function and immediately use it with all the workflow support of the <code>mergic</code> script.</p>
<p>Often, a custom distance function makes or breaks your effort. It's worth thinking about and experimenting with, and <code>mergic</code> makes it easy!</p>
<hr>
<p><img width="1000%" title="New York Open Statistical Programming Meetup; Practical Mergic: How to Join Anything" src="open_stats_prog_meetup.png"></p>
<hr>
<p>If you're interested in this kind of thing, I'll be doing <a href="http://www.meetup.com/nyhackr/events/222328498/">a longer talk</a> at the <a href="http://www.meetup.com/nyhackr/">New York City Open Statistical Programming Meetup</a> next week Wednesday.</p>
<hr>
<p><img width="1000%" title="Open Data Science Conference" src="open_data_sci_con.png"></p>
<hr>
<p>I also hope to see you at <a href="http://opendatascicon.com/">Open Data Science Con</a> in Boston!</p>
<hr>
<p>Thanks!</p>
<hr>
<p>Thank you!</p>
<hr>
<p></p><center>
planspace.org
<p>@planarrowspace
</p></center>
<hr>
<p>This is just me again.</p>    
    ]]></description>
<link>http://planspace.org/20150514-mergic/</link>
<guid>http://planspace.org/20150514-mergic/</guid>
<pubDate>Thu, 14 May 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Working with Captured Variables in Python Closures</title>
<description><![CDATA[

<p>You can make <a href="http://en.wikipedia.org/wiki/Closure_%28computer_programming%29">closures</a> in Python like this:</p>
<pre><code class="language-python">def add_some(x=0):
    def adder(y):
        return x+y
    return adder

add3 = add_some(3)

print(add3(7))
## 10</code></pre>

<p>The <code>adder</code> function "remembers" <code>x</code>.</p>
<p>But this will fail:</p>
<pre><code class="language-python">def a_counter(count=0):
    def counter():
        count += 1
        return count
    return counter

the_counter = a_counter()

print(the_counter())
## UnboundLocalError: local variable 'count' referenced before assignment</code></pre>

<p>You can't assign to <code>count</code> in there!</p>
<p>In Python 3, you can resolve this with <code>nonlocal</code>:</p>
<pre><code class="language-python">def a_counter(count=0):
    def counter():
        nonlocal count
        count += 1
        return count
    return counter

the_counter = a_counter()

print(the_counter())
## 1

print(the_counter())
## 2

print(the_counter())
## 3</code></pre>

<p>Great! I'd rather not have to specify <code>nonlocal</code>, but I'm happy to know of another feature unique to Python 3.</p>
<p>In Python 2, you can't get assignment of captured variables inside your closure, but you can mutate a captured variable. <code>&#175;\_(&#12484;)_/&#175;</code></p>
<pre><code class="language-python">def a_counter(count={'val': 0}):
    def counter():
        count['val'] += 1
        return count['val']
    return counter

the_counter = a_counter()

print(the_counter())
## 1

print(the_counter())
## 2

print(the_counter())
## 3</code></pre>

<p>Hooray for closures!</p>    
    ]]></description>
<link>http://planspace.org/20150425-working_with_captured_variables_in_python_closures/</link>
<guid>http://planspace.org/20150425-working_with_captured_variables_in_python_closures/</guid>
<pubDate>Sat, 25 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Two Highlights from NY R Conference 2015</title>
<description><![CDATA[

<p>Here are the two things I thought were most interesting at <a href="http://www.rstats.nyc/">NY R</a> on Saturday April 25, 2015:</p>
<hr>
<p><img alt="MRAN" src="mran.png"></p>
<p><a href="https://twitter.com/revojoe">Joe Rickert</a> showed <a href="http://www.revolutionanalytics.com/">Revolution Analytics</a>' <a href="http://mran.revolutionanalytics.com/">Managed R Archive Network</a> (MRAN). It snapshots all of <a href="http://cran.r-project.org/">CRAN</a> every day so that you can finally do a real lock-in of the versions for all your R dependencies, using the <a href="http://cran.r-project.org/web/packages/checkpoint/index.html">checkpoint</a> package. This was <a href="http://blog.revolutionanalytics.com/2014/10/introducing-rrt.html">announced</a> a while ago, but it was new to me. I love any sort of data store that gives you <code>as-of</code>, even if it's just by snapshotting.</p>
<hr>
<p><a href="https://twitter.com/wesmckinn">Wes McKinney</a> spoke about data frame design and how such tooling should develop further. Here's a transcription of a key <a href="https://twitter.com/planarrowspace/status/591990689635905536">slide</a>:</p>
<ul>
<li><strong>The Great Data Tool Decoupling&#8482;</strong><ul>
<li>Thesis: over time, user interfaces, data storage, and execution engines will decouple and specialize</li>
<li>In fact, you should really want this to happen<ul>
<li>Share systems among languages</li>
<li>Reduce fragmentation and "lock-in"</li>
<li>Shift developer focus to usability</li>
</ul>
</li>
<li>Prediction: we'll be there by 2025; sooner if we all get our act together</li>
</ul>
</li>
</ul>
<p>I've thought about this kind of decoupling for data visualization (see <a href="http://planspace.org/20150119-gog_a_separate_layer_for_visualization/">gog</a>) and I think it's a cool direction in general.</p>
<hr>
<p>There were certainly other interesting and good things as well!</p>    
    ]]></description>
<link>http://planspace.org/20150425-two_highlights_from_ny_r_conference_2015/</link>
<guid>http://planspace.org/20150425-two_highlights_from_ny_r_conference_2015/</guid>
<pubDate>Sat, 25 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Forward Selection with statsmodels</title>
<description><![CDATA[

<p>Python's <a href="http://statsmodels.sourceforge.net/stable/">statsmodels</a> doesn't have a built-in method for choosing a linear model by <a href="http://en.wikipedia.org/wiki/Stepwise_regression">forward selection</a>. Luckily, it isn't impossible to write yourself. So <a href="https://github.com/trevor-smith">Trevor</a> and I sat down and hacked out the following. It tries to optimize <a href="http://en.wikipedia.org/wiki/Coefficient_of_determination#Adjusted_R2">adjusted R-squared</a> by adding features that help the most one at a time until the score goes down or you run out of features.</p>
<pre><code class="language-python">import statsmodels.formula.api as smf

def forward_selected(data, response):
    """Linear model designed by forward selection.

    Parameters:
    -----------
    data : pandas DataFrame with all possible predictors and response

    response: string, name of response column in data

    Returns:
    --------
    model: an "optimal" fitted statsmodels linear model
           with an intercept
           selected by forward selection
           evaluated by adjusted R-squared
    """
    remaining = set(data.columns)
    remaining.remove(response)
    selected = []
    current_score, best_new_score = 0.0, 0.0
    while remaining and current_score == best_new_score:
        scores_with_candidates = []
        for candidate in remaining:
            formula = "{} ~ {} + 1".format(response,
                                           ' + '.join(selected + [candidate]))
            score = smf.ols(formula, data).fit().rsquared_adj
            scores_with_candidates.append((score, candidate))
        scores_with_candidates.sort()
        best_new_score, best_candidate = scores_with_candidates.pop()
        if current_score &lt; best_new_score:
            remaining.remove(best_candidate)
            selected.append(best_candidate)
            current_score = best_new_score
    formula = "{} ~ {} + 1".format(response,
                                   ' + '.join(selected))
    model = smf.ols(formula, data).fit()
    return model</code></pre>

<p>There isn't just one way to design this kind of thing. You could select on some other evaluation metric. You could use internal cross-validation. You might not want to do use forward selection at all. But hey!</p>
<p>Here's how ours can be applied to a classic data set on <a href="http://data.princeton.edu/wws509/datasets/#salary">Discrimination in Salaries</a>:</p>
<pre><code class="language-python">import pandas as pd

url = "http://data.princeton.edu/wws509/datasets/salary.dat"
data = pd.read_csv(url, sep='\\s+')

model = forward_selected(data, 'sl')

print model.model.formula
# sl ~ rk + yr + 1

print model.rsquared_adj
# 0.835190760538</code></pre>    
    ]]></description>
<link>http://planspace.org/20150423-forward_selection_with_statsmodels/</link>
<guid>http://planspace.org/20150423-forward_selection_with_statsmodels/</guid>
<pubDate>Thu, 23 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>R Squared Can Be Negative</title>
<description><![CDATA[

<p>Let's do a little linear regression in Python with <a href="http://scikit-learn.org/">scikit-learn</a>:</p>
<pre><code class="language-python">import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.cross_validation import train_test_split

X, y = np.random.randn(100, 20), np.random.randn(100)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)</code></pre>

<p>It is a property of <a href="http://en.wikipedia.org/wiki/Ordinary_least_squares">ordinary least squares regression</a> that for the training data we fit on, the <a href="http://en.wikipedia.org/wiki/Coefficient_of_determination">coefficient of determination R<sup>2</sup></a> and the square of the <a href="http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient">correlation coefficient</a> r<sup>2</sup> of the model's predictions with the actual data are equal.</p>
<pre><code class="language-python"># coefficient of determination R^2
print model.score(X_train, y_train)
## 0.203942898079

# squared correlation coefficient r^2
print np.corrcoef(model.predict(X_train), y_train)[0, 1]**2
## 0.203942898079</code></pre>

<p>This does not hold for new data, and if our model is sufficiently bad the coefficient of determination can be negative. The squared correlation coefficient is never negative but can be quite low.</p>
<pre><code class="language-python"># coefficient of determination R^2
print model.score(X_test,  y_test)
## -0.277742673311

# squared correlation coefficient r^2
print np.corrcoef(model.predict(X_test), y_test)[0, 1]**2
## 0.0266856746214</code></pre>

<p>These declines in performance worsen with <a href="http://en.wikipedia.org/wiki/Overfitting">overfitting</a>.</p>    
    ]]></description>
<link>http://planspace.org/20150417-negative_r_squared/</link>
<guid>http://planspace.org/20150417-negative_r_squared/</guid>
<pubDate>Fri, 17 Apr 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>RStudio in a Web Browser</title>
<description><![CDATA[

<p>The most annoying part of <code>R</code> workshops is installing software and downloading necessary files. With RStudio Server, workshop participants can skip all that entirely.</p>
<p>Here's what RStudio looks like running locally. To attain the setup shown, you need to install <a href="http://www.r-project.org/">R</a>, install <a href="http://www.rstudio.com/">RStudio</a>, install necessary R <a href="http://cran.r-project.org/">packages</a>, separately download necessary code and data, and navigate to the correct working directory.</p>
<p><img alt="RStudio running locally" src="local.png"></p>
<p>Here's what RStudio looks like in a browser. To attain the setup shown, you go to a URL and log in.</p>
<p><img alt="RStudio in a browser" src="web.png"></p>
<p>In a workshop, it's very nice to be able to start doing things with <code>R</code> without messing with setup. Somebody does have to set up the environment in advance though.</p>
<p>Setting up RStudio Server is very easy, especially if you're already familiar with Amazon Web Services (<a href="http://aws.amazon.com/">AWS</a>):</p>
<ul>
<li>Spin up an AWS Elastic Compute Cloud (<a href="http://aws.amazon.com/ec2/">EC2</a>) machine using <a href="http://www.louisaslett.com/RStudio_AMI/">Louis Aslett's RStudio Amazon Machine Image (AMI)</a>.<ul>
<li><a href="http://www.louisaslett.com/RStudio_AMI/">Louis's page</a> has good concise directions on how to do this&#8212;follow his directions!</li>
</ul>
</li>
<li>Secure Shell (SSH) into your EC2 machine and set up the environment as you want it to be.<ul>
<li>Install globally required packages while running R as root (<code>sudo R</code>).</li>
<li>Put necessary files (code, data, etc.) as desired in <code>/home/rstudio</code> (the prototypical user).</li>
<li>Run <a href="https://gist.github.com/ajschumacher/12f7484d06cacd4b4cd3">build_logins.sh</a>, a script developed by <a href="https://twitter.com/joshdata">Josh Tauberer</a>, to create the desired number of user accounts. (See documentation in the script itself.)</li>
</ul>
</li>
<li>You can optionally give your EC2 machine an AWS <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html">elastic IP address</a>, which will let you start and stop the machine (avoiding the cost of keeping it running when you aren't using it) but still know in advance what address the machine will be accessible at.<ul>
<li>Be sure your EC2 machine and elastic IP are both &#8220;EC2 classic&#8221; or both Virtual Private Cloud (VPC). (Some instance types are only available in VPC!)</li>
</ul>
</li>
</ul>
<p>For an in-person workshop, it can be effective to distribute login information on slips of paper like this:</p>
<p><img alt="workshop login information" src="paper.png"></p>
<p>I've led an <a href="/20150220-data_science_isnt_magic/">introductory workshop</a> with ten active participants on one EC2 m3.2xlarge, which runs at 56 cents per hour. For a larger group, I ran two m3.2xlarge machines and three c4.8xlarge machines ($1.856/hour). That used all my five elastic IP addresses and provided 124 vCPUs and 240 gigs of RAM. We had 94 active users and could probably have supported quite a few more without problems.</p>
<p>RStudio in the cloud is a great fit for workshops because it eliminates install and setup pain for participants. It lets you decouple the install process from other workshop activities. The experience of using RStudio in a browser is nice enough that it makes me wonder whether anyone offers cloud RStudio as a service&#8212;is that a business that should exist? Workshops certainly aren't the only place where RStudio Server could make sense.</p>
<p>There are also other options for setting up your server. You don't need to use Louis's AMIs; you don't need to use AWS at all. The <a href="http://www.bioconductor.org/">Bioconductor Project</a> maintains a <a href="http://www.bioconductor.org/help/bioconductor-cloud-ami/">Cloud AMI</a> that comes with a lot of other pre-installed R packages, for example. (When I last checked it was based on an older version of Ubuntu.) For total control, you can install <a href="http://www.rstudio.com/products/rstudio/download-server/">RStudio Server</a> from scratch on whatever type of system you like.</p>    
    ]]></description>
<link>http://planspace.org/20150221-rstudio_in_a_web_browser/</link>
<guid>http://planspace.org/20150221-rstudio_in_a_web_browser/</guid>
<pubDate>Sat, 21 Feb 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Data Science isn't Magic</title>
<description><![CDATA[

<p>This is the flow for a <a href="http://dc.opendataday.org/">DC Open Data Day</a> 2015 workshop. It may not make sense out of context. For a fun summary in tweets (with photos!) you might check out <a href="https://storify.com/planarrowspace/data-science-isn-t-magic-workshop-at-dc-open-data">a storification of it</a>.</p>
<p>View this page as <a href="big.html">slides</a> to make this site's base URL appear quite large so that people can find this easily.</p>
<hr>
<p>planspace.org</p>
<hr>
<h3>Workshop Outline</h3>
<ul>
<li>Intro / Disclaimer</li>
<li><a href="80_percent_definitions/">80% Definitions</a> (<a href="80_percent_definitions/big.html">slides</a>)</li>
<li><a href="osemn/">Data Science is OSEMN</a> (<a href="osemn/big.html">slides</a>)</li>
<li><a href="problem/">What's the Problem?</a> (<a href="problem/big.html">slides</a>)</li>
<li>Find NYC attendance data<ul>
<li><a href="http://schools.nyc.gov/">schools.nyc.gov</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/">About Us</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/">Our Schools</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/data/">Data About Schools</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/data/Attendance.htm">Daily Attendance Rates</a></li>
<li><a href="http://schools.nyc.gov/aboutus/data/attendancexml/">XML</a></li>
<li><a href="http://schools.nyc.gov/AboutUs/schools/data/attendancexml/">http://schools.nyc.gov/AboutUs/schools/data/attendancexml/</a></li>
</ul>
</li>
<li>Data Science <a href="tools/">Tools</a> (<a href="tools/big.html">slides</a>)</li>
<li>Connect to RStudio in the cloud<ul>
<li>Backup plan:<ul>
<li>Install the appropriate <code>R</code> distribution for your system from this <a href="http://watson.nci.nih.gov/cran_mirror/">mirror</a>.</li>
<li>Install the <a href="http://www.rstudio.com/ide/download/desktop">RStudio IDE</a> for <code>R</code>. The RStudio site should suggest an appropriate package for your system.</li>
<li>Download and unzip the <a href="https://github.com/ajschumacher/odddsim/archive/master.zip">files</a> we're using. (They're <a href="https://github.com/ajschumacher/odddsim">on GitHub</a>, so you can clone if you prefer.)</li>
</ul>
</li>
</ul>
</li>
<li>Working with <code>R</code><ul>
<li>Working with one day of data (<code>01-day_attendance.R</code>)</li>
<li>Selecting usable data points (<code>02-select_totals.R</code>)</li>
<li>The relationship between temperature and attendance (<code>03-merge_and_plot.R</code>)</li>
</ul>
</li>
<li>Bonus: Introducing the DC voter file</li>
</ul>
<h3>Additional Resources</h3>
<ul>
<li>For learning <code>R</code>:<ul>
<li><a href="http://tryr.codeschool.com/">Try R</a> is an interactive web site that guides you through <code>R</code> functionality in your web browser.</li>
<li>The <a href="https://raw.githubusercontent.com/ajschumacher/gadsdc/master/02-R/walking_intro.Rmd">walking introduction to R</a> is an <code>R</code> script that you can open and work through in RStudio.</li>
</ul>
</li>
<li>For more fun data:<ul>
<li><a href="http://blogs.worldbank.org/opendata/accessing-world-bank-data-apis-python-r-ruby-stata">Accessing the World Bank Data APIs in Python, R, Ruby &amp; Stata</a></li>
<li>The <a href="https://github.com/ajschumacher/dc_voter_reg">DC voter registration file</a></li>
</ul>
</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20150220-data_science_isnt_magic/</link>
<guid>http://planspace.org/20150220-data_science_isnt_magic/</guid>
<pubDate>Fri, 20 Feb 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>To Migrate from GitHub to BitBucket</title>
<description><![CDATA[

<p><a href="https://github.com/">GitHub</a> has great design and unlimited collaborators but no free private repos. <a href="https://bitbucket.org/">BitBucket</a> has unlimited free private repos but a limit of five collaborators on them. Assuming you want your repos private so that very few people have access to them, BitBucket is perfect.</p>
<p>BitBucket makes migrating from GitHub ridiculously easy. They have a <a href="https://blog.bitbucket.org/2013/04/02/finnovation-brings-you-a-heap-of-new-features/">one-click integration</a> and everything. But it's harder to migrate issues.</p>
<p>Joe Workman <a href="http://joeworkman.tumblr.com/post/40133335482/migrating-from-github-to-bitbucket">made</a> <a href="http://www.bytebucket.org/joeworkman/git2bit">git2bit</a>, which magically puts issues from a GitHub repository into a BitBucket repository. The systems aren't exactly the same, but it makes mostly intelligent choices for how to convert.</p>
<p>You already have Ruby installed, so it should be this easy to install <code>git2bit</code>:</p>
<pre><code class="language-bash">gem install git2bit</code></pre>

<p>Alas, <code>git2bit</code> depends on both <a href="https://rubygems.org/gems/bitbucket_rest_api">bitbucket_rest_api</a> and <a href="https://rubygems.org/gems/github_api">github_api</a>, taking their most recent versions by default, and these two gems now require conflicting versions of <a href="https://rubygems.org/gems/hashie">hashie</a>.</p>
<p>This is what I did:</p>
<pre><code class="language-bash">gem install hashie -v 2.0.5
gem install github_api -v 0.11.3
gem install bitbucket_rest_api -v 0.1.5
gem install git2bit -v 1.0.2</code></pre>

<p>At last! Use <code>git2bit --help</code> to see the fairly clear options.</p>
<p>It seems the default is to migrate only open issues, but if you use the <code>--closed</code> flag it will migrate both open and closed issues, which is what I wanted. Be aware that there's no checking for duplication, so if you run <code>git2bit</code> twice you will get double the issues on the BitBucket side. It's easy to make an empty repo to test that things are working as intended.</p>
<p>Other minutiae:</p>
<p>If you want to have a different identity for some repos (the ones on BitBucket, say) this is easy to set on a per-repo basis. These commands will alter a repo's <code>.git/config</code>:</p>
<pre><code class="language-bash">git config user.name "Your Name Here"
git config user.email your@email.com</code></pre>

<p>On BitBucket you'll want to <a href="https://confluence.atlassian.com/display/BITBUCKET/Add+an+SSH+key+to+an+account">Add an SSH key to an account</a> and <a href="https://confluence.atlassian.com/display/BITBUCKET/Use+the+SSH+protocol+with+Bitbucket">Use the SSH protocol with Bitbucket</a>.</p>
<p>To point a current local repo up to BitBucket:</p>
<pre><code class="language-bash">git remote remove origin
git remote add origin git@bitbucket.org:accountname/reponame.git</code></pre>

<p>And if you didn't use the one-click migration but are instead doing your initial push to BitBucket from your local repo:</p>
<pre><code class="language-bash">git push -u origin --all
git push -u origin --tags</code></pre>

<p>That's just copied from BitBucket's directions when you create a new repo, which I like for being more complete than GitHub's.</p>    
    ]]></description>
<link>http://planspace.org/20150126-to_migrate_from_github_to_bitbucket/</link>
<guid>http://planspace.org/20150126-to_migrate_from_github_to_bitbucket/</guid>
<pubDate>Mon, 26 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Monads by Diagram</title>
<description><![CDATA[

<p>Let's get some notation out of the way:</p>
<p><img alt="intput -&gt; function -&gt; output" src="function_notation.jpg"></p>
<p>A &#8220;function&#8221; is represented by a rectangle. Its arguments (&#8220;input&#8221;) appear above it, and its result (&#8220;output&#8221;) appears to its right.</p>
<p><img alt="value and monadic value" src="values.jpg"></p>
<p>We have some idea of &#8220;values&#8221; which can be worked with. We introduce the idea of a &#8220;monadic value&#8221; which can be thought of as &#8220;wrapping&#8221; our more familiar values.</p>
<p><img alt="unit" src="unit.jpg"></p>
<p>There's a function called &#8220;unit&#8221; which takes a value and returns a monadic value. Now we can make monadic values from our usual values.</p>
<p><img alt="bind" src="bind.jpg"></p>
<p>There's a function called &#8220;bind&#8221; which takes a function and a monadic value. The argument function takes a value and returns a monadic value. The argument monadic value gets unwrapped by &#8220;bind&#8221; and its value becomes the argument to the passed function. Now we can do things to our monadic values.</p>
<p>That's it. Monads are defined. They should obey some reasonable rules, but that&#8217;s it.</p>
<p><em>And yet you want more?</em></p>
<p><img alt="map" src="map.jpg"></p>
<p>Nobody wants to write functions that take a normal value and return a monadic value. The &#8220;map&#8221; function takes a normal function that knows nothing about monads and returns a function that takes and returns monadic values. Just use the resulting function on your monadic values! So convenient!</p>
<p><img alt="join" src="join.jpg"></p>
<p>The &#8220;join&#8221; function unwraps a doubly-monadic value. You want this.</p>
<p><img alt="bind via map and join" src="bind_via_map_and_join.jpg"></p>
<p>We can combine the &#8220;map&#8221; and &#8220;join&#8221; functions to get the &#8220;bind&#8221; function. Pretty neat!</p>
<p><em>And yet you want a real explanation?</em></p>
<p>For a lucid formal exposition, read <a href="http://homepages.inf.ed.ac.uk/wadler/papers/marktoberdorf/baastad.pdf">Monads for Functional Programming</a>. The above is my attempt to express the key definitions there, without the mathematical Haskell notation.</p>
<p>For a plain-talking explanation with practical examples in friendly Ruby, <a href="http://codon.com/refactoring-ruby-with-monads">read</a> or (preferably) <a href="https://www.youtube.com/watch?v=uTR__8RvgvM">watch</a> Refactoring Ruby with Monads. <a href="https://twitter.com/tomstuart">Tom Stuart</a> uses <code>from_value</code> for &#8220;unit&#8221;, <code>and_then</code> for &#8220;bind&#8221;, and <code>within</code> for calling &#8220;map&#8221; and applying the result.</p>
<p>For a great overview of even more kinds of things and much better illustration than I can manage, read <a href="http://adit.io/posts/2013-04-17-functors,_applicatives,_and_monads_in_pictures.html">Functors, Applicatives, And Monads In Pictures</a>. This follows the Haskell in using <code>fmap</code> for &#8220;map&#8221;.</p>
<p>And <a href="http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html">don't forget</a>:</p>
<blockquote>
<p>&#8220;a monad is a monoid in the category of endofunctors, what's the problem?&#8221;</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20150125-monads_by_diagram/</link>
<guid>http://planspace.org/20150125-monads_by_diagram/</guid>
<pubDate>Sun, 25 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Use pew, not virtualenvwrapper, for Python virtualenvs</title>
<description><![CDATA[

<p>Have you noticed how good <a href="http://docs.python-guide.org/">The Hitchhiker&#8217;s Guide to Python</a> is? The documentation there on <a href="http://docs.python-guide.org/en/latest/dev/virtualenvs/">virtualenv</a> is exemplary.</p>
<p>Of course nobody wants to use virtualenvs directly, which is why virtualenvwrapper exists. But as <a href="http://datagrok.org/">Lamb</a> pointed out, <a href="https://gist.github.com/datagrok/2199506">Virtualenv's <code>bin/activate</code> is Doing It Wrong</a>, and virtualenvwrapper doesn't do it right either. Also, it's annoying to run env'ed Python via <a href="http://en.wikipedia.org/wiki/Cron">cron</a>.</p>
<p>The solution is to use <a href="https://github.com/berdario/pew">pew</a> instead. The Python Environment Wrapper (pew) handles your environment elegantly. This includes not mangling your <code>PS1</code>, though you can still display <code>VIRTUAL_ENV</code> information in your prompt if you want, and however you want. Also, suddenly all your virtualenv tasks are pew sub-commands, which means all you have to remember is <code>pew</code>. Typing <code>pew</code> alone shows all the pew commands, with helpful descriptions.</p>
<p>I no longer set the <code>WORKON_HOME</code> environment variable. This means pew will use the default <code>~/.local/share/virtualenvs</code>, which is a good choice. It's consistent with <a href="https://www.python.org/dev/peps/pep-0370/">PEP 370</a>, and it means you don't rely on <code>WORKON_HOME</code> to find your virtualenvs, which is nice for cron.</p>
<p>The location of <code>pew</code> is <code>/usr/local/bin</code>. For convenience, I suggest adding a <code>PATH</code> line to your crontab. The default <code>PATH</code> is <code>/usr/bin:/bin</code>, so the following adds one entry:</p>
<pre><code>PATH=/usr/local/bin:/usr/bin:/bin</code></pre>

<p>With that done, you can easily run any Python script you want in any virtualenv you want, via cron, with concise syntax:</p>
<pre><code>* * * * * pew in my_env my_script.py</code></pre>

<p>The example assumes <code>my_script.py</code> is in your home directory, executable, and with the standard <code>#!/usr/bin/env python</code> shebang.</p>
<p>I find this to be a very nice setup: <code>pew workon my_env</code> and develop <code>my_script.py</code> however I want, and <code>pew in my_env my_script.py</code> to run in that virtualenv from cron or anywhere else.</p>
<hr>
<p>Related tools: <a href="https://virtualenv.pypa.io/">virtualenv</a>, <a href="https://virtualenvwrapper.readthedocs.org/">virtualenvwrapper</a>, <a href="https://github.com/sashahart/vex">vex</a>, <a href="https://github.com/zimbatm/direnv">direnv</a>, probably more...</p>
<p>Thanks to <a href="https://twitter.com/jessicagarson">Jessica</a>, <a href="https://twitter.com/necaris">Rami</a>, and <a href="https://twitter.com/reconbot">Francis</a> for thoughts on this.</p>    
    ]]></description>
<link>http://planspace.org/20150120-use_pew_not_virtualenvwrapper_for_python_virtualenvs/</link>
<guid>http://planspace.org/20150120-use_pew_not_virtualenvwrapper_for_python_virtualenvs/</guid>
<pubDate>Tue, 20 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>gog: a separate layer for visualization</title>
<description><![CDATA[

<p><em>A short presentation at the DC <a href="http://www.meetup.com/TrackMaven-Monthly-Challenge/">Monthly Challenge</a> on <a href="http://www.meetup.com/TrackMaven-Monthly-Challenge/events/219314544/">Monday January 19, 2015</a>.</em></p>
<hr>
<p><img alt="gog" src="gog.png"></p>
<hr>
<p><code>gog</code> is a name for a simple idea I've been exploring about making it easy to quickly see data from whatever computing environment you happen to be in.</p>
<p>First I'll talk about <em>quickly</em>, then I'll talk about <em>from whatever</em>, and then I'll show some <code>gog</code> prototype work.</p>
<p>There are two kinds of <em>quickly</em> that I care about. One is getting images produced quickly, and another is working with or exploring them quickly.</p>
<hr>
<pre><code class="language-r">plot(data)</code></pre>

<hr>
<p>This is <code>R</code>. <code>R</code> can produce plots quickly. The <code>plot</code> function is generic and will dispatch to a method that knows how to plot whatever you're plotting. This is very nice for the user.</p>
<hr>
<p><img alt="one-dimensional R plot" src="plot_r_1.png"></p>
<hr>
<p>If you plot a single vector, you get an image like this with the index in the horizontal direction.</p>
<hr>
<p><img alt="two-dimensional R plot" src="plot_r_2.png"></p>
<hr>
<p>If you plot a data frame with two fields, you get a familiar scatterplot.</p>
<hr>
<p><img alt="three-dimensional R plot" src="plot_r_3.png"></p>
<hr>
<p>If you plot a data frame with more than two fields, you get a scatterplot matrix.</p>
<hr>
<p><img alt="linear model diagnostic R plots" src="plot_r_4.png"></p>
<hr>
<p>And if you plot a linear model object, you get four diagnostic plots.</p>
<p>All of these plots were created with <code>plot(data)</code>. There are a lot of defaults chosen in order to get something on the screen. You might also want to see another view, but the pure convenience of being able to <code>plot(data)</code> so easily is really wonderful.</p>
<p>The only way to reduce the friction of graphing even further would be to have plots automatically generated in the background based on whatever data you have on your system. I <a href="https://twitter.com/planarrowspace/status/555893329460994048">think</a> that would be a fun project, in fact.</p>
<p>Once you have an image, you want to be able to work with it quickly and easily. This means interaction with the image itself, and this is a place that <code>R</code> is not particularly strong, at least with base graphics.</p>
<hr>
<p><img alt="paper plot" src="paper_plot.png"></p>
<hr>
<p>A lot of statistical graphics behave essentially like paper. I like paper a lot, but computers can do more. We should expect, at a minimum, to be able to point to a data element and find out more about it. There are tons of other direct manipulation ways to interact with graphics (like brushing) that we should be able to have easily at our disposal.</p>
<p>Most of the world agrees that the way to get interactivity is to use web frontend things, and I'm inclined to think so too. It does often seem that the time to produce a visualization has a dramatic inverse relationship with the amount of interactivity supported, however.</p>
<p>In summary:</p>
<ul>
<li>Interactivity lets us quickly work with plots.</li>
<li>A simple user API lets us quickly make plots.</li>
</ul>
<p>What about <em>from whatever</em>?</p>
<hr>
<p><img alt="diagram of R graphics ecosystem" src="graphics_r.png"></p>
<hr>
<p><code>R</code> can make a lot of different graphics. Parts of the ecosystem are organized in clever ways that make things convenient for <code>R</code> users, and there's just a ton available. It seems fine to make our graphics in <code>R</code>. But what about other languages?</p>
<hr>
<p><img alt="diagram of R, Python, and Julia graphics ecosystem" src="graphics_multi.png"></p>
<hr>
<p>The current status quo is mostly to not share graphics capabilities between languages. So if Python want to have <code>ggplot</code>, <a href="https://yhathq.com/">somebody</a> has to <a href="http://ggplot.yhathq.com/">port</a> it over, which takes a bunch of work. And newer languages can be at a disadvantage purely because they don't have as much tooling for visualization, despite other strengths.</p>
<p>What I'm suggesting is that we might be able to pull the graphics parts out of our data languages.</p>
<hr>
<p><img alt="diagram of gog graphics ecosystem" src="graphics_gog.png"></p>
<hr>
<p>So this is the idea. You make the control surface inside various languages quite small and easy, so that everybody can connect into a sort of shared library of visualization pieces.</p>
<hr>
<p><img alt="cover of The Grammar of Graphics" src="gg_cover.jpg"></p>
<hr>
<p>As an aside:</p>
<p>The grammar of graphics seems to be a good idea, and the <a href="http://www.amazon.com/The-Grammar-Graphics-Statistics-Computing/dp/0387245448">book</a> is also quite good. A lot of what's good about Tableau is good because of the grammar of graphics.</p>
<p>However, the design that should be influenced by the grammar of graphics is not the part that <code>gog</code> is directly concerned with.</p>
<hr>
<pre><code class="language-r">ggplot(data) +
  aes(x=thing) +
  geom_histogram() +
  # etc.</code></pre>

<hr>
<p>To make a comparison to R's <code>ggplot2</code>, notice that you always start by passing data in. Then <code>ggplot2</code> let's you specify everything about your plot&#8212;and you <em>have</em> to specify a good deal before you get any plot at all.</p>
<hr>
<pre><code class="language-r">gog(data)
# etc. happens elsewhere</code></pre>

<hr>
<p>What <code>gog</code> suggests is to standardize the data passing, but then the way that the plot gets made is none of <code>gog</code>'s business. It's probably a good idea to start with some default plot and then let users work with it further.</p>
<p>So what is all this nonsense, and how could it work?</p>
<hr>
<p><img alt="gogd console" src="gogd.png"></p>
<hr>
<p>The connecting component is a <code>gog</code> HTTP server which runs on port 4808 and just accepts POST requests at <code>/data</code> and rebroadcasts it to anything listening to a websocket also at <code>/data</code>. The assumption is that we're passing a JSON array of objects.</p>
<p>I have <a href="https://github.com/ajschumacher/gogd">a gog server</a> written in Clojure, but a server can be implemented and hosted however you want, as long as everything connects.</p>
<hr>
<p><img alt="charted.co welcome screen" src="charted.png"></p>
<hr>
<p>People have already made some visualizations that are suitable for adapting to use with <code>gog</code>. As an example, I took the code from <a href="http://www.charted.co/">charted.co</a>, which was designed to load data from CSV files, and adapted it to also allow input from <code>gog</code>.</p>
<hr>
<pre><code class="language-python"># pip install gogpy
from gogpy import gog
import numpy as np
import pandas as pd
gog(pd.DataFrame(np.random.sample(10)))</code></pre>

<hr>
<p>The <code>gogpy</code> package is ridiculously small, but can still be <code>pip</code> installed from <a href="https://pypi.python.org/pypi">PyPI</a> for your convenience. Currently the <code>gog</code> function will only take a <code>pandas</code> <code>DataFrame</code>.</p>
<hr>
<p><img alt="charted.co graph" src="charted_demo.png"></p>
<hr>
<p>Thanks to the work that went into making <code>charted.co</code>, we have a graph already.</p>
<hr>
<pre><code class="language-python">import time
data = pd.DataFrame(np.random.sample(100))
for i in range(91):
    gog(data[i:i+10])
    time.sleep(2)</code></pre>

<hr>
<p>Live updates are the only thing happening&#8212;it just depends on when you send the data.</p>
<hr>
<pre><code class="language-r"># install_github("ajschumacher/gogr")
library("gogr")
data &lt;- data.frame(0-runif(100))
for (i in 1:91) {
    gog(data[i:(i+9), ,drop=F])
    Sys.sleep(2)
}</code></pre>

<hr>
<p>The <code>R</code> package <a href="https://github.com/ajschumacher/gogr">gogr</a> isn't on <a href="http://cran.r-project.org/">CRAN</a> but can be installed with <a href="https://github.com/hadley/devtools">devtools</a>. It's trivially easy to use any visualization that supports <code>gog</code> from any environment that supports <code>gog</code>.</p>
<hr>
<pre><code class="language-r">rstudio::viewer("http://localhost:8000")
library("gogr")
data &lt;- iris
names(data)[1:2] &lt;- c("x", "y")
gog(data)</code></pre>

<hr>
<p>It turns out that the <a href="http://www.rstudio.com/">RStudio</a> viewer pane is really just <a href="https://www.webkit.org/">WebKit</a> and it's easy to plug in <code>gog</code> visualizations.</p>
<hr>
<p><img alt="visualizing iris sepal length vs. sepal width" src="gogi_iris.png"></p>
<hr>
<p>I made this little visualization, a simple scatterplot with <a href="http://d3js.org/">D3</a>. It's very crude at the moment, but it's enough to show sepal length versus sepal width, and to show information about data points as we explore around. This could obviously be much improved with such luxuries as axes, and replace the renaming of columns to &#8217;x&#8217; and &#8217;y&#8217; with defaulting and interaction. And using &#8220;title&#8221; elements for tooltips has only the advantage of expediency.</p>
<p>But even in its current state this visualization allows for fun data manipulation experiments with animated updates which would be much more work to achieve with other techniques. (See <a href="demo.R">demo.R</a> for the complete <code>R</code> demo source.)</p>
<hr>
<pre><code class="language-python"># access meetup API and extract data, then:
data = pd.DataFrame({'x': rsvped, 'y': ids, 'name': names})
gog(data)</code></pre>

<hr>
<p>Of course nobody cares about irises - let's pull some data about humans we know, using the Meetup API. (See <a href="demo.py">demo.py</a> for complete Python demo source.)</p>
<hr>
<p><img alt="visualizing RSVP time vs. member ID" src="gogi_meetup.png"></p>
<hr>
<p>Even though we collected data with Python, we can visualize it wherever we want, including back inside the RStudio interface.</p>
<p>I haven't tried it, but it strikes me that it would be pretty straightforward to cobble together an RStudio-style Python IDE, at least with a REPL and <code>gog</code> visualization container, using existing components.</p>
<hr>
<p>Thank you!</p>
<hr>
<p>This is all very preliminary and there are no doubt a lot of complications and other things to consider. The question of how to manage possibly many <code>gog</code> visualization frontends or have any sort of dispatch system is not at all addressed, for instance. And there are a number of more or less related projects, which I've started to collect at <a href="https://github.com/ajschumacher/gog">ajschumacher/gog</a>. Any and all feedback, suggestions, or anything else appreciated!</p>    
    ]]></description>
<link>http://planspace.org/20150119-gog_a_separate_layer_for_visualization/</link>
<guid>http://planspace.org/20150119-gog_a_separate_layer_for_visualization/</guid>
<pubDate>Mon, 19 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>A Great Python Book explains Hash Tables</title>
<description><![CDATA[

<p><a href="http://mitpress.mit.edu/books/introduction-computation-and-programming-using-python-0">Introduction to Computation and Programming Using Python</a> is the book you should get if you think you want to learn Python. It packs in tons of sophisticated content and makes it accessible, not dumbed down.</p>
<p><a href="http://mitpress.mit.edu/books/introduction-computation-and-programming-using-python-0"><img alt="Introduction to Computation and Programming Using Python" src="cover.jpg"></a></p>
<p>This book seems to be something of a hidden gem. It isn't what comes up first when you search for Python resources. It doesn't have a lot of flash or promotion behind it. Of course it does have the weight of its publisher, The MIT Press. And there's the blurb on the back cover from Hal Abelson, coauthor of <a href="https://mitpress.mit.edu/sicp/">Structure and Interpretation of Computer Programs</a>. So there are clear signs of the book's quality.</p>
<p>One example of what's good about this book is the explanation of <em>hashing</em>. This is a very important topic, but it has a little bit of complexity to it and so it's often almost entirely omitted when teaching people to code. This is the depth sometimes given in introductory Python courses:</p>
<pre><code class="language-python"># A Python dictionary is a set of key-value pairs
my_dict = {'a_key': 'a_value', 'another_key': 'another_value'}
# which allows access to the values via the keys (lookup)
my_dict['another_key']  # 'another_value'
# This data structure is also known as a `map` or `hash-map`</code></pre>

<p>That's true, and you can start to use the language functionality with that, but it doesn't help you understand the underlying key idea. Below is part of the treatment from <em>Introduction to Computation and Programming Using Python</em>, in section 10.3 <em>Hash Tables</em>:</p>
<section style="font-family: sans-serif">

<p>... [Python] dictionaries use a technique called hashing to do the lookup in time that is nearly independent of the size of the dictionary. The basic idea behind a <strong>hash table</strong> is simple. We convert the key to an integer, and then use that integer to index into a list, which can be done in constant time. In principle, values of any immutable type can be easily converted to an integer. After all, we know that the internal representation of each object is a sequence of bits, and any sequence of bits can be viewed as representing an integer.  For example, the internal representation of <code>'abc'</code> is the string of bits 011000010110001001100011, which can be viewed as a representation of the decimal integer 6,382,179. Of course, if we want to use the internal represntation of strings as indices into a list, the list is going to have to be pretty darn long.</p>

<p>What about situation swhere the keys are already integers? Imagine, for the moment, that we are implementing a dictionary all of whose keys are U.S. Social Security numbers. (A United States Social Security number is a nine-digit integer.) If we represented the dictionary by a list with 10<sup>9</sup> elements and used Social Security numbers to index into the list, we could do lookups in constant time. Of course, if the dictionary contained entires for only ten thousand (10<sup>4</sup>) people, this would waste quite a lot of space.</p>

<p>Which gets us to the subject of hash functions. A <strong>hash function</strong> maps a large space of inputs (e.g., all natural numbers) to a smaller space of outputs (e.g., the natural numbers between 0 and 5000). Hash functions can be used to convert a large space of keys to a smaller space of integer indices.</p>

<p>Since the space of possible outputs is smaller than she space of possible inputs, a shash function is a <strong>many-to-one mapping</strong>, i.e., multiple different inputs may be mapped to the same output. When two inputs are mapped to the same output, it is called a <strong>collision</strong>&#8212;a topic which we will return to shortly. A good hash function produces a <strong>uniform distribution</strong>, i.e., every output in the range is equally probable, which minimizes the probability of collisions.</p>

<p>Designing good hash functions is surprisingly challenging. The problem is that one wants the outputs to be uniformly distributed given the expected distribution of inputs. Suppose, for example, that one hashed surnames by performing some calculation on the first three letters. In the Netherlands, where roughly 5% of surnames begin with &#8220;van&#8221; and another 5% with &#8220;de,&#8221; the distribution would be far from uniform.</p>

<p>Figure 10.6 uses a simple hash function (recall that <code>i%j</code> returns the remainder when the integer <code>i</code> is divided by the integer <code>j</code>) to implement a dictionary with integers as keys.</p>

<p>The basic idea is to represent an instance of class <code>intDict</code> by a list of <strong>hash buckets</strong>, where each bucket is a list of key/value pairs. By making each bucket a list, we handle collisions by storing all of the values that hash to the same bucket in the list.</p>

<p>The hash table works as follows: The instance variable <code>buckets</code> is initialized to a list of <code>numBuckets</code> empty lists. To store or look up an entry with key <code>dictKey</code>, we use the hash function <code>%</code> to convert <code>dictKey</code> into an integer, and use that integer to index into <code>buckets</code> to find the hash bucket associated with <code>dictKey</code>. We then search that bucket (which is a list) linearly to see if there is an entry with the key <code>dictKey</code>. If we are doing a lookup and there is an entry with the key, we simply return the value stored with that key. If there is no entry with that key, we return <code>None</code>. If a value is to be stored, then we either replace the value in the existing entry, if one was found, or append a new entry to the bucket if none was found.</p>

<p>There are many other ways to handle collisions, some considerably more efficient than using lists. But this is probably the simplest mechanism, and it works fine if the hash table is big enough and the hash function provides a good enough approximation to a uniform distribution.</p>

<p>Notice that the <code>__str__</code> method produces a representation of a dictionary that is unrelated to the order in which elements were added to it, but is instead ordered by the values to which the keys happen to hash. This explains why we can't predict the order of the keys in an object of type <code>dict</code>.</p>

<p><strong>Figure 10.6 Implementing dictionaries using hashing</strong></p>

<pre><code class="language-python">class intDict(object):
    """A dictionary with integer keys"""

    def __init__(self, numBuckets):
        """Create an empty dictionary"""
        self.buckets = []
        self.numBuckets = numBuckets
        for i in range(numBuckets):
            self.buckets.append([])

    def addEntry(self, dictKey, dictVal):
        """Assumes dictKey an int. Adds an entry."""
        hashBucket = self.buckets[dictKey%self.numBuckets]
        for i in range(len(hashBucket)):
            if hashBucket[i][0] == dictKey:
                hashBucket[i] = (dictKey, dictVal)
                return
        hashBucket.append((dictKey, dictVal))

    def getValue(self, dictKey):
        """Assumes dictKey an int. Returns entry associated
           with the key dictKey"""
        hashBucket = self.buckets[dictKey%self.numBuckets]
        for e in hashBucket:
            if e[0] == dictKey:
                return e[1]
        return None

    def __str__(self):
        result = '{'
        for b in self.buckets:
            for e in b:
                result = result + str(e[0]) + ':' + str(e[1]) + ','
        return result[:-1] + '}' #result[:-1] omits the last comma</code></pre>

</section>

<p>The text goes on to show a further example and explanation of how the <code>intDict</code> works.</p>
<p>The selection above does depend on knowing something about what &#8220;constant time&#8221; means, and possibly &#8220;immutable.&#8221; But if you were reading the book, you would have learned these already.</p>
<p>The organization and exposition is really extraordinary. Even in just the small hashing section above, there&#8217;s a sequence of three or four concrete examples to motivate the thinking, and then it leads into an understandable implementation. This is phenomenal educational writing.</p>
<p>This is <a href="http://mitpress.mit.edu/books/introduction-computation-and-programming-using-python-0">the book</a> for people who want to learn.</p>    
    ]]></description>
<link>http://planspace.org/20150111-a_great_python_book_explains_hash_tables/</link>
<guid>http://planspace.org/20150111-a_great_python_book_explains_hash_tables/</guid>
<pubDate>Sun, 11 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Learn to Code with Emacs</title>
<description><![CDATA[

<p>To <a href="/20141231-two_problems_with_learn-to-code_sites/">learn to code</a> effectively, use:</p>
<ul>
<li>tools that are actually used for coding</li>
<li>materials that are reusable as references</li>
</ul>
<p>Here's one way to do it:</p>
<ol>
<li><a href="http://emacs.link/">Install Emacs</a>.</li>
<li>Do the built-in Emacs Tutorial, accessible from the start screen.</li>
<li>Learn about the Info system (C-h i, select Info).</li>
<li>Work through the Introduction to Programming Emacs Lisp.</li>
</ol>
<p>These steps will all make sense if you do them in order.</p>
<p>Here's what you get, if you follow the steps above:</p>
<ul>
<li>a powerful editor that you can use with any programming language</li>
<li>programming skills together with computer science concepts</li>
<li>the ability to use that programming language to enhance that editor</li>
</ul>
<p>This is a powerful combination, and one that blends seamlessly into further learning and productive work.</p>    
    ]]></description>
<link>http://planspace.org/20150101-learn_to_code_with_emacs/</link>
<guid>http://planspace.org/20150101-learn_to_code_with_emacs/</guid>
<pubDate>Thu, 01 Jan 2015 12:00:00 -0500</pubDate>
</item>
<item>
<title>Two Problems with Learn-to-Code Sites</title>
<description><![CDATA[

<p>Lots of people want to learn to code, and this is good. But lots of the web sites that want to teach people to code are bad.</p>
<p>The popular learn-to-code sites generally teach a popular language, which is not necessarily a bad thing.</p>
<p>But there are two fundamental problems with the approach of many popular sites:</p>
<ol>
<li>They teach all inside a browser. This removes setup difficulty by providing a custom toy environment but leaves the student with no independent working setup of their own.</li>
<li>They take a one pass adventure game guided tour approach. This enforces a progression and generally makes it difficult or impossible to scan through and refer back to the material.</li>
</ol>
<p>There is definitely value in making it easy for people to try something they otherwise wouldn't try. After trying, a different approach may be better for facilitating learning.</p>
<p>To learn to code effectively, use:</p>
<ol>
<li>tools that are actually used for coding</li>
<li>materials that are reusable as references</li>
</ol>
<p>This means that you should:</p>
<ol>
<li>use an editor and language implementation installed on your own machine</li>
<li>have, read, and work from one or more good documents, guides, or books</li>
</ol>
<p>Books are a really good idea. This should be obvious, but it seems, especially when it comes to technology, that people want to imagine you can "learn by doing" everything, starting from nothing and then just checking Stack Overflow occasionally. No. Read a book. It's a mistake in an in-person class to think that you can get by without reading a book. You should read a book. It's also a mistake to think you can get by with just online activities. Get a book.</p>
<p>The "toy environment" and "guided tour" design problems of most online learn-to-code systems are fundamental problems. It additionally seems to be, too frequently, that the materials presented in these systems are less well organized and produced than one would expect from a book. Further, the interactive exercises that are ostensibly a major advantage of online systems can totally break the experience if they have the tiniest blocking error in design or implementation. And finally projects, which are an excellent way to learn, too often feel inane and constricting rather than interesting and eye-opening when forced into the frameworks of automated systems.</p>    
    ]]></description>
<link>http://planspace.org/20141231-two_problems_with_learn-to-code_sites/</link>
<guid>http://planspace.org/20141231-two_problems_with_learn-to-code_sites/</guid>
<pubDate>Wed, 31 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>Use Info in Emacs</title>
<description><![CDATA[

<p>There is a documentation system accessible from <a href="http://www.gnu.org/software/emacs/">Emacs</a> called <a href="https://www.gnu.org/software/texinfo/">Info</a>. Here's advice from a parenthetical in the <a href="http://www.gnu.org/software/emacs/manual/html_mono/eintr.html#Note-for-Novices">Note for Novices</a> of <a href="www.gnu.org/software/emacs/manual/html_mono/eintr.html">An Introduction to Programming in Emacs Lisp</a>:</p>
<blockquote>
<p>"To learn about Info, type C-h i and then select Info."</p>
</blockquote>
<p>Despite using Emacs, off and on, for many years, and also more recently hearing <a href="http://sachachua.com/">Sacha Chua</a> recommend Info in her <a href="http://sachachua.com/blog/category/podcast/emacs-chat-podcast/">Emacs chats</a>, I had never really used the Info system. But something on <a href="http://www.johndcook.com/blog/">John Cook</a>'s blog pointed me to the <a href="www.gnu.org/software/emacs/manual/html_mono/eintr.html">intro to Emacs Lisp</a> and I started reading it in a web browser until I got to that part:</p>
<blockquote>
<p>"To learn about Info, type C-h i and then select Info."</p>
</blockquote>
<p>At that point I did. I went into Emacs and learned about Info, and then I continued working through the <a href="www.gnu.org/software/emacs/manual/html_mono/eintr.html">intro to Emacs Lisp</a> inside Emacs, which was a much better way to read it than inside a browser.</p>
<p>I wish I'd come to all this sooner.</p>
<p>If you'd like to try Emacs, you should find it <a href="http://planspace.org/20141207-make_it_easy_to_install_emacs/">easy to install</a> using <a href="http://emacs.link/">emacs.link</a>. Once you have Emacs open on your computer, do the built-in tutorial. Then:</p>
<blockquote>
<p>"To learn about Info, type C-h i and then select Info."</p>
</blockquote>
<p>And then check out the Introduction to Emacs Lisp, also in the Info system.</p>
<p>Have fun!</p>    
    ]]></description>
<link>http://planspace.org/20141230-use_info_in_emacs/</link>
<guid>http://planspace.org/20141230-use_info_in_emacs/</guid>
<pubDate>Tue, 30 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>Calendar Plots should be Easy</title>
<description><![CDATA[

<p>Popularized by Mike Bostock's <a href="http://bl.ocks.org/mbostock/4063318">example</a> and the GitHub <a href="https://github.com/blog/1360-introducing-contributions">contribution calendar</a>, calendar plots have some recognition now.</p>
<p><a href="http://bit.ly/NYCsubway"><img alt="" src="calendar.png"></a></p>
<p>Calendar plots can be very good for showing daily data over time when weekly, monthly, and yearly structure are relevant. While color is not ideal for conveying numeric data, patterns can be made quickly discernible and interaction can add table lookup functionality.</p>
<p>I've made plots like <a href="http://bit.ly/NYCsubway">this</a> myself, but it has required fiddling with HTML, CSS, and JavaScript in typical <a href="http://d3js.org/">D3</a> fashion. There are some tools for making it easier to build these plots, like the <a href="http://kamisama.github.io/cal-heatmap/v2/">Cal-Heatmap</a> JavaScript plugin and the Google Charts <a href="https://developers.google.com/chart/interactive/docs/gallery/calendar">Calendar Chart</a>, but even with these it's still an HTML/JavaScript affair.</p>
<p>I want to be able to do something like this in <a href="http://www.r-project.org/">R</a> and get an interactive calendar plot:</p>
<pre><code class="language-r"># this package does not (yet) exist
library(plotcal)
plot.cal(values, dates)</code></pre>

<p>I want to be able to do something like this in <a href="https://www.python.org/">Python</a> and get an interactive calendar plot:</p>
<pre><code class="language-python"># this package does not (yet) exist
from plot_cal import plot_cal
plot_cal(values, dates)</code></pre>

<p>Such functionality could also interact more closely with <code>data.frame</code>s/<code>DataFrame</code>s, tuples, maps/dicts, etc. That would be fine.</p>
<p>Somebody should make such functions, or extend some existing packages to include them. Or if such things already exist, somebody should just tell me about it. Either way, I will say thank you.</p>    
    ]]></description>
<link>http://planspace.org/20141229-calendar_plots_should_be_easy/</link>
<guid>http://planspace.org/20141229-calendar_plots_should_be_easy/</guid>
<pubDate>Mon, 29 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>How does R calculate histogram break points?</title>
<description><![CDATA[

<p>Break points make (or break) your histogram. <a href="http://www.r-project.org/">R</a>'s default algorithm for calculating histogram break points is a little interesting. Tracing it includes an unexpected dip into R's <a href="http://en.wikipedia.org/wiki/C_%28programming_language%29">C</a> implementation.</p>
<pre><code class="language-r"># set seed so "random" numbers are reproducible
set.seed(1)
# generate 100 random normal (mean 0, variance 1) numbers
x &lt;- rnorm(100)
# calculate histogram data and plot it as a side effect
h &lt;- hist(x, col="cornflowerblue")</code></pre>

<p><img alt="" src="histogram.png"></p>
<p>The <code>hist</code> function calculates and returns a histogram representation from data. That calculation includes, by default, choosing the break points for the histogram. In the example shown, there are ten bars (or bins, or cells) with eleven break points (every 0.5 from -2.5 to 2.5). With break points in hand, <code>hist</code> counts the values in each bin. The histogram representation is then shown on screen by <code>plot.histogram</code>.</p>
<p>(By default, bin counts include values less than or equal to the bin's right break point and strictly greater than the bin's left break point, except for the leftmost bin, which includes its left break point.)</p>
<p>The choice of break points can make a big difference in how the histogram looks. Badly chosen break points can obscure or misrepresent the character of the data. R's default behavior is not particularly good with the simple data set of the integers 1 to 5 (as pointed out by <a href="https://twitter.com/hadleywickham">Wickham</a>).</p>
<pre><code class="language-r">hist(1:5, col="cornflowerblue")</code></pre>

<p><img alt="" src="bad5.png"></p>
<p>A manual choice like the following would better show the evenly distributed numbers.</p>
<pre><code class="language-r">hist(1:5, breaks=0.5:5.5, col="cornflowerblue")</code></pre>

<p><img alt="" src="good5.png"></p>
<p>It might be even better, arguably, to use more bins to show that not all values are covered.</p>
<pre><code class="language-r">hist(1:5, breaks=seq(0.55, 5.55, 0.1), col="cornflowerblue")</code></pre>

<p><img alt="" src="better5.png"></p>
<p>In any event, break points matter. When exploring data it's probably best to experiment with multiple choices of break points. But in practice, the defaults provided by R get seen a lot.</p>
<p>So how does R choose break points?</p>
<p>By default, inside of <code>hist</code> a two-stage process will decide the break points used to calculate a histogram:</p>
<ol>
<li>
<p>The function <code>nclass.Sturges</code> receives the data and returns a recommended number of bars for the histogram. The documentation says that <a href="http://en.wikipedia.org/wiki/Histogram#Number_of_bins_and_width">Sturges' formula</a> is "implicitly basing bin sizes on the range of the data" but it's just based on the number of values, as <code>ceiling(log2(length(x)) + 1)</code>. This is really fairly dull.</p>
</li>
<li>
<p>Then the data and the recommended number of bars gets passed to <code>pretty</code> (usually <code>pretty.default</code>), which tries to "Compute a sequence of about n+1 equally spaced &#8216;round&#8217; values which cover the range of the values in x. The values are chosen so that they are 1, 2 or 5 times a power of 10." This ends up calling into some parts of R implemented in C, which I'll describe a little below.</p>
</li>
</ol>
<p>Note: In what follows I'll link to a <a href="https://github.com/wch/r-source/tree/34c4d5dd3493863f6665f907dbad9bf1d800c0d4">mirror</a> of the R <a href="https://svn.r-project.org/R/">sources</a> because <a href="https://github.com/">GitHub</a> has a nice, familiar interface. I'll point to the most recent version of files without specifying line numbers. You'll want to search within the files to what I'm talking about. To see exactly what I saw go to commit <a href="https://github.com/wch/r-source/tree/34c4d5dd3493863f6665f907dbad9bf1d800c0d4">34c4d5dd</a>.</p>
<p>The <a href="https://github.com/wch/r-source/blob/trunk/src/library/grDevices/R/calc.R">source</a> for <code>nclass.Sturges</code> is trivial R, but the <code>pretty</code> <a href="https://github.com/wch/r-source/blob/trunk/src/library/base/R/pretty.R">source</a> turns out to get into C. I hadn't looked into any of R's C implementation before; here's how it seems to fit together:</p>
<p>The source for <code>pretty.default</code> is straight R until:</p>
<pre><code class="language-r">z &lt;- .Internal(pretty( # ... cut</code></pre>

<p>This <code>.Internal</code> thing is a call to something written in C. The file <a href="https://github.com/wch/r-source/blob/trunk/src/main/names.c">names.c</a> can be useful for figuring out where things go next. We find this line:</p>
<pre><code class="language-c">{"pretty",    do_pretty,  0,  11, 7,  {PP_FUNCALL, PREC_FN,   0}},</code></pre>

<p>So it goes to a C function called <code>do_pretty</code>. That can be found in <a href="https://github.com/wch/r-source/blob/trunk/src/main/util.c">util.c</a>. This is a lot of very Lisp-looking C, and mostly for handling the arguments that get passed in. For example:</p>
<pre><code class="language-c">    int n = asInteger(CAR(args)); args = CDR(args);</code></pre>

<p>That's kind of neat, but the actual work is done somewhere else again. The body of <code>do_pretty</code> calls a function <code>R_pretty</code> like this:</p>
<pre><code class="language-c">    R_pretty(&amp;l, &amp;u, &amp;n, min_n, shrink, REAL(hi), eps, 1);</code></pre>

<p>The call is interesting because it doesn't even use a return value; <code>R_pretty</code> modifies its first three arguments in place. Gross.</p>
<p>The function <code>R_pretty</code> is in its own file, <a href="https://github.com/wch/r-source/blob/trunk/src/appl/pretty.c">pretty.c</a>, and finally the break points are made to be "nice even numbers" and there's a result.</p>
<p>I was surprised by where the code complexity of this process is.</p>    
    ]]></description>
<link>http://planspace.org/20141225-how_does_r_calculate_histogram_break_points/</link>
<guid>http://planspace.org/20141225-how_does_r_calculate_histogram_break_points/</guid>
<pubDate>Thu, 25 Dec 2014 12:00:00 -0500</pubDate>
</item>
<item>
<title>DC Voter Registration Data</title>
<description><![CDATA[

<p><em>This is an expanded account; see <a href="https://github.com/ajschumacher/dc_voter_reg">dc_voter_reg</a> on GitHub for the data and concise documentation.</em></p>
<p><img alt="" src="cd.jpg"></p>
<p>DC voter registration data is public, but not very easy to get.</p>
<p>On Thursday, December 18, 2014, I took a printed copy of a <a href="http://www.dcboee.org/pdf_files/Data_Request_Form.pdf">PDF form</a> that I found on the <a href="http://www.dcboee.org/">DC Board of Elections web site</a> to 441 4th Street NW, suite 250 north, Washington, DC, 20001. I had to show ID, have my bag x-rayed, and go through a metal detector to get into the building.</p>
<p>I brought my checkbook so I could pay $2 for a CD-ROM of voter registration data. The clerk informed me that the data is updated daily. She burned my CD while I waited. She told me there are no rules on how the data can be used. I take it to be public domain.</p>
<p>On the CD were two files:</p>
<ul>
<li>The data was in a Microsoft Access database file called <code>DC VH EXPORT.MDB</code> (154 MB). I bought a PC laptop running Windows 8, purchased and installed Microsoft Access, opened the file and exported the data as text. It comes out as 130 megabytes of uncompressed text. I was able to get the column headers out and stick them on top of the file so that it can be read as nice CSV. Zipped (using the Windows built-in) the result is the 20 MB <a href="https://github.com/ajschumacher/dc_voter_reg/blob/master/20141218-dc_voters.csv.zip">20141218-dc_voters.csv.zip</a>.</li>
<li>The provided documentation file was a plain text <code>Read Me.txt</code>. I renamed this to <a href="https://github.com/ajschumacher/dc_voter_reg/blob/master/20141218-dc_voters.txt">20141218-dc_voters.txt</a>. It seems slightly out of date with respect to the data in that the later columns corresponding to elections are not the same in the documentation and in the data. These column names are interpretable as MMYYYY-T, I believe, where MM is numeric month, YYYY is four-digit year, and T is a letter corresponding to election type. I think G is General and P is primary.</li>
</ul>
<p>You can also get voter registration data on a per-ward basis in Microsoft Access or Excel formats. They will email it to you, but you have to fax in the form to make this request. I wonder if they would accommodate daily requests for every ward's data. It seems like it would be much less annoying to just put the data online automatically; it isn't even very heavy. If we dream big, maybe the data could live in <a href="http://dat-data.com/">dat</a>? Any option that doesn't require making a physical visit to their office during their fairly narrow business hours would be an improvement. Any format that doesn't require purchasing proprietary software would be an improvement.</p>
<p>Another thing you can get is images (PDFs?) of signatures for nominating petitions and ballot measures. You pay just $2 per CD-ROM, but these are only available during ten-day "challenge periods." So keep your eyes peeled! I haven't seen what these things look like.</p>
<p>At least you can get nice maps like this (or for individual wards) for just $10 each:</p>
<p><img alt="" src="map.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20141220-dc_voter_registration_data/</link>
<guid>http://planspace.org/20141220-dc_voter_registration_data/</guid>
<pubDate>Sat, 20 Dec 2014 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
