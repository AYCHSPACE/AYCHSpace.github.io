<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan ➔ space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>SQL Performance Explained, by Winand</title>
<description><![CDATA[

<p>This is the printed version of "<a href="https://use-the-index-luke.com/">Use the index, Luke</a>." It avoids the Jedi goofiness&#8212;and the author's <a href="https://winand.at/static/markus_winand.Y-sfxgk9.jpg">unsettling gaze</a>. It's pretty good, especially for a self-published book, and it delivers on the promise of explaining, very accessibly, how SQL engines run.</p>
<p>The mental model offered goes beyond just "index means \(O(log(n))\)," though there are the obligatory graphs showing linear vs. log growth. Winand describes tree traversal of indexes, but then also (depending on data and query) linked-list traversal, and how this relates to accessing data that isn't in the index, and connected trade-offs.</p>
<p>The material starts rudimentary, but reaches details such as this, on page 140:</p>
<blockquote>
<p>"The Oracle database cannot read an index backwards in order to execute a pipelined <code>group by</code> that is followed by an <code>order by</code>."</p>
</blockquote>
<p>One of the most interesting things for me turned out to be the frequent comparisons and notes specific to SQL implementations. Very often the notes were explaining that MySQL didn't offer a particular feature, or that a feature was only available in Postgres. MS SQL Server and Oracle seem to be somewhere in the middle.</p>
<p>The level of detail was also sufficient to make clear that declarative languages like SQL are not the panacea that they are sometimes made out to be. I hadn't realized, for example, that SQL engines make execution choices based on statistics they keep about the data they store, and that those statistics can be incorrect, leading to bad choices.</p>
<p>Winand is also critical of <a href="https://en.wikipedia.org/wiki/Object-relational_mapping">ORM</a>s that can lead to bad SQL. Though I didn't find these examples particularly compelling, I do think it's a related case of trading control for convenience.</p>
<p>Also, the book comes with a bookmark, which is a laminated miniature of the cover.</p>
<p><img alt="SQL Performance Explained cover" src="sql_cover.jpg"></p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20190727-sql_performance_explained_by_winand/</link>
<guid>http://planspace.org/20190727-sql_performance_explained_by_winand/</guid>
<pubDate>Sat, 27 Jul 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Expert Python Programming by Jaworski and Ziadé</title>
<description><![CDATA[

<p>The authors use "half a loaf is better than none" on page 545 in reference to <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor"><code>run_in_executor</code></a>, and that saying is also applicable to their book. <a href="https://www.packtpub.com/application-development/expert-python-programming-third-edition">Expert Python Programming</a> is not a good book, but it's also not entirely without value.</p>
<p>The editing could be better. Either no native English speakers were involved, or they didn't have enough time to work with the Polish and French authors' prose. A next section promised on page 391 never appears. On page 546 there are two different next chapters. There's even a code block on page 576 that is printed with all the newlines stripped out.</p>
<p>Beyond simple issues, explanation is sometimes poor or missing. The <a href="https://en.wikipedia.org/wiki/Token_bucket">token bucket</a> explanation is particularly unclear. They say that Python string concatenation is quadratic, but they don't explain <a href="https://stackoverflow.com/questions/44487537/why-does-naive-string-concatenation-become-quadratic-above-a-certain-length/44487738#44487738">why</a>. On page 496 they suggest that non-deterministic caching has been solved numerous times, so the reader can just go look on PyPI.</p>
<p>The publication date of this third edition is April 2019, and yet there's no mention of <a href="/20190629-using_pipenv/">Pipenv</a>. They <a href="https://github.com/pytest-dev/pytest/issues/1629#issue-161422224">still</a> say py.test rather than pytest. They mention nose but not nose2. Chunks of the book refer to Python 3.5, while the cover promises 3.7.</p>
<p>The perspective of the authors is unfamiliar to me, as when they say on page 187 that "Using binary bit-wise operations to combine options is common in Python."</p>
<p>I'd like to be able to trust the completeness of a book like this. If it doesn't know about Pipenv, what else is it missing that should really be included?</p>
<p>But it isn't all bad. I don't recall seeing the Python 3.7 <code>asyncio.run()</code>, but the larger-scale explanations of <a href="https://docs.python.org/3/library/asyncio.html"><code>asyncio</code></a> weren't awful. If I'm understanding properly, the interesting thing about <code>await</code> is that it <em>doesn't</em> wait, in the sense of blocking.</p>
<p>So I'm not upset that I read the book. I suspect the authors are technically competent people who did the best they could with limited resources from <a href="https://www.packtpub.com/">Packt</a>. The book contains their take on things, and it broadened what I was aware of.</p>
<p><img alt="cover" src="cover.png"></p>    
    ]]></description>
<link>http://planspace.org/20190707-expert_python_programming_by_jaworski_and_ziade/</link>
<guid>http://planspace.org/20190707-expert_python_programming_by_jaworski_and_ziade/</guid>
<pubDate>Sun, 07 Jul 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Using Pipenv</title>
<description><![CDATA[

<p>If you use Python, you should use <a href="https://docs.pipenv.org/en/latest/">Pipenv</a> for
environment/requirement management.</p>
<hr>
<h3>Installing</h3>
<p>Do you have Pipenv installed? (Try <code>which pipenv</code>.) If you don't have
it, install it:</p>
<pre><code class="language-bash">$ pip install --user pipenv</code></pre>

<p>Now you can read <code>pipenv --help</code>, but I'll continue to highlight
common tasks.</p>
<hr>
<h3>Existing project</h3>
<p>Is there an existing <code>Pipfile</code>? To get things installed, run this in
the project directory:</p>
<pre><code class="language-bash">$ pipenv sync</code></pre>

<hr>
<h3>New project</h3>
<p>Starting a new project? There's no explicit "new" command, but you can
do this:</p>
<pre><code class="language-bash">$ pipenv --python 3.7</code></pre>

<p>You can also skip that and just start installing things.</p>
<p>Keep <code>Pipfile</code> and <code>Pipfile.lock</code> in version control.</p>
<hr>
<h3>Installing your requirements</h3>
<p>Just use <code>pipenv</code> instead of <code>pip</code> (or other alternatives).</p>
<pre><code class="language-bash">$ pipenv install some_cool_python_package</code></pre>

<p><code>Pipfile</code> and <code>Pipfile.lock</code> will be kept up to date automatically.</p>
<hr>
<h3>Development requirements</h3>
<p>Pipenv keeps dev packages (like <a href="https://www.pylint.org/">Pylint</a>, for
example) separate. Just use the <code>--dev</code> flag:</p>
<pre><code class="language-bash">$ pipenv install --dev some_cool_python_package
$ pipenv sync --dev  # install base AND dev packages</code></pre>

<hr>
<h3>Running things</h3>
<p>Just use <code>pipenv run</code> a lot.</p>
<pre><code class="language-bash">pipenv run python whatever.py --etc</code></pre>

<hr>
<h3>That's it!</h3>
<p>Pipenv does a lot for you, while requiring very little from you.</p>
<p>I do have more thoughts though...</p>
<hr>
<h3>Why is it good?</h3>
<p>Without Pipenv, maybe you track big-picture requirements in
<code>setup.py</code>'s <code>install_requires</code>, and/or elsewhere. Maybe you have a
<code>requirements.txt</code> so you have known-good exact versions of
everything. Maybe another such file for dev requirements. And you
probably have to remember to take separate actions to keep these all
up to date and in sync. The pain! The mistakes!</p>
<hr>
<h3>Working directory</h3>
<p>I don't generally love magic based on my current working directory,
but Pipenv has won me over. I appreciate that it's explicit, at least.
No environment variables get automatically changed when you change
directories, for example. You have to run some <code>pipenv</code> command to do
anything with the environment corresponding to where you are on disk.</p>
<hr>
<h3>History</h3>
<p>I used to <a href="/20150120-use_pew_not_virtualenvwrapper_for_python_virtualenvs/">advocate for Pew</a>, so I was interested to see that Pipenv
is a kind of descendant of Pew. Pipenv's implementation used to rely
on Pew, even.</p>
<p>You can get a shell in your project environment with <code>pipenv shell</code>
and it seems to work more like <code>pew workon</code> than <code>source activate</code>.</p>
<p>Perhaps the major workflow difference is that now <code>pipenv run</code> (like
<code>pew in</code>) is commonly used, to the exclusion of changing your shell. I
think this is a good thing in that it's more explicit and removes the
risk of forgetting what the current state of your shell is.</p>
<hr>
<h3>Where's my venv, though?</h3>
<p>Unless you mess with defaults, virtual environments are stored,
consistent with <a href="https://www.python.org/dev/peps/pep-0370/">PEP 370</a>,
in <code>~/.local/share/virtualenvs/</code>. They're associated with a Pipfile
via a hash of its path.</p>
<p>Here's a simplified version of <a href="https://github.com/pypa/pipenv/blob/08b35715c0b34461df184d907654250f60b5a0d7/pipenv/project.py#L376">the hashing</a>:</p>
<pre><code class="language-python">import hashlib, base64

def short_hash(path_to_pipfile):
    hash = hashlib.sha256(path_to_pipfile.encode()).digest()[:6]
    return base64.urlsafe_b64encode(hash)</code></pre>

<p>So the venv for <code>/home/me/project/Pipfile</code> will be called
<code>project-fPIxLfK7</code>. You can always check your project with <code>pipenv
--venv</code>.</p>
<p>This does mean that if you move or otherwise rename your project
directory, Pipenv can lose track of your virtualenv &#8212; but it's super
easy to just run <code>pipenv sync</code> and you're ready to go again.</p>    
    ]]></description>
<link>http://planspace.org/20190629-using_pipenv/</link>
<guid>http://planspace.org/20190629-using_pipenv/</guid>
<pubDate>Sat, 29 Jun 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Circe by Madeline Miller</title>
<description><![CDATA[

<p>I didn't know anything about
<a href="https://en.wikipedia.org/wiki/Circe">the Circe of mythology</a>, so to
me Miller's book was mostly fan fiction for classics majors, sort of
like Twilight but backward. Then I learned there's a whole history of
misogynist vs. feminist versions of Circe, which gives the book a bit
more gravitas. I think it's a fine book regardless of how deeply into
the history you go.</p>
<p>I frequently liked the writing, which was obviously wrought but rarely
overwrought. Here are a couple lines I pulled out, in retrospect more
for their sentiment than their language per se:</p>
<blockquote>
<p>"Beneath the smooth, familiar face of things is another that waits
to tear the world in two." (p. 16)</p>
<p>"<em>Bold action and bold manner are not the same.</em>" (p. 21, italics in
original)</p>
</blockquote>
<p>Miller's Circe is on the feminist side. Here's one small example,
drawing attention to a false dichotomy that probably has a name:</p>
<blockquote>
<p>"Would I be skimmed milk for crying, or a harpy with a heart of
stone? There was nothing in between." (p. 98)</p>
</blockquote>
<p>This book loves foreshadowing, from the very first line: "When I was
born, the name for what I was did not exist." (She's a witch.)
Foreshadowing is everywhere. In particular, I thought I noticed a
couple instances before the reveal that Circe is pregnant.</p>
<p>This first selection is arguably just sexual, but "belly" made me
start to wonder whether Circe was starting to think about a child:</p>
<blockquote>
<p>"My smooth belly glowed beneath my hand, the color of honey shining
in the sun. I drew him down to me." (p. 215)</p>
</blockquote>
<p>I think this one is clearer. It follows a discussion of another mother
(with "a thousand wiles") and her child:</p>
<blockquote>
<p>"I cupped my own hands in the dark. I did not have a thousand wiles,
and I was no fixed star, yet for the first time I felt something in
that space. A hope, a living breath, that might yet grow between."
(p. 226)</p>
</blockquote>
<p>I'm not sure if this one counts as foreshadowing because it's so
explicit:</p>
<blockquote>
<p>"And last of all, still in its cedar box: silphium ground with
wormwood, the draught I had taken each moon since the first time I
lay with Hermes. Each moon except the last." (p. 234)</p>
</blockquote>
<p><a href="https://en.wikipedia.org/wiki/Silphium">Silphium</a> is an extinct
natural contraceptive.</p>
<p>In addition to all the foreshadowing, there's explicit prophecy from
the gods.</p>
<blockquote>
<p>"It was [the Fates'] favorite bitter joke: those who fight against
prophecy only draw it more tightly around their throats." (p. 291)</p>
</blockquote>
<p>As with <a href="https://en.wikipedia.org/wiki/Oedipus">Oedipus</a>, the
realization of prophecy is independent of whether you know it in
advance. I hadn't thought about this before, but it's not so different
from <a href="/20190613-exhalation_by_ted_chiang/">Ted Chiang</a>'s
single-timeline time travel. Maybe the ancient Greek's didn't like
paradoxes?</p>
<p>As you read, there are also references to earlier parts of the book,
which ask you to remember it pretty well. On page 358, it says "<em>Then,
child, make another,</em>" and you have to remember that the giant fish
god said that back on page 282, and that in context it meant "make
another world" (as in, take bold steps to change your situation).</p>
<p>Madeline Miller has made another world in Circe: a world humane to
both the goddess and the reader.</p>
<p><img alt="Circe cover" src="circe.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190626-circe_by_madeline_miller/</link>
<guid>http://planspace.org/20190626-circe_by_madeline_miller/</guid>
<pubDate>Wed, 26 Jun 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Aladdin translated by Yasmine Seale</title>
<description><![CDATA[

<p>The history of the story of Aladdin, given in some detail in the
introduction, is interesting. In the story itself, I was hoping for
something really different from Disney's Aladdin, and I was somewhat
disappointed there. There are differences, but they're not so huge.</p>
<p>The front flap of the cover promises "a newfound appreciation for the
ingenuity of the heroine, Princess Badr al-Budur, who outwits an evil
magician to ultimately save Aladdin's life." I was looking forward to
that, but Aladdin makes the plan to have the princess poison the
magician. The princess does successfully poison the magician's drink
(exactly as Aladdin directed her) but I was hoping for a little more
agency for the princess.</p>
<p>The translator and editor point out that the story has a feel
consistent with being told by someone telling stories for their life,
and I think that's true: pushing the action foward, and-then'ing the
next chapter and the next, with no time to go back and edit. It's
probably a good thing that most stories aren't produced that way.</p>
<p><img alt="Aladdin cover" src="aladdin.jpg"></p>
<hr>
<p>Thanks <a href="https://twitter.com/rememberlenny">Lenny</a> for the
recommendation!</p>    
    ]]></description>
<link>http://planspace.org/20190617-aladdin_translated_by_yasmine_seale/</link>
<guid>http://planspace.org/20190617-aladdin_translated_by_yasmine_seale/</guid>
<pubDate>Mon, 17 Jun 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Exhalation by Ted Chiang</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/Ted_Chiang">Ted Chiang</a> wrote <a href="https://en.wikipedia.org/wiki/Story_of_Your_Life">Story of Your Life</a>, which I liked a lot and became the movie <a href="https://en.wikipedia.org/wiki/Arrival_(film)">Arrival</a>. <a href="https://www.penguinrandomhouse.com/books/538034/exhalation-by-ted-chiang/9781101947883/">Exhalation</a> has two more single-timeline <a href="https://samharris.org/books/free-will/">deterministic</a> time-travel stories at the beginning, but then it opens up a bit. Chiang's <a href="https://en.wikipedia.org/wiki/Stories_of_Your_Life_and_Others">first collection</a> might be better, but read this one too.</p>
<p>Spoilers as I briefly describe Exhalation's stories:</p>
<ul>
<li>The Merchant and the Alchemist's Gate<ul>
<li>Single-timeline time-travel with portals that can take you
   whenever they existed.</li>
</ul>
</li>
<li>Exhalation<ul>
<li>Entropy kills (but also some neat stuff on trying to understand
   brains).</li>
</ul>
</li>
<li>What's Expected of Us<ul>
<li>Oh man there's no free will.</li>
</ul>
</li>
<li>The Lifecycle of Software Objects<ul>
<li>Humanlike AI requires humanlike entities. I tend to agree with
   his take here.</li>
</ul>
</li>
<li>Dacey's Patent Automatic Nanny<ul>
<li>Some kid develops an attachment to machines rather than people.</li>
</ul>
</li>
<li>The Truth of Fact, the Truth of Feeling<ul>
<li>Let's understand Trump as pre-literate / maybe perfect (video)
   memory is cool?</li>
</ul>
</li>
<li>The Great Silence<ul>
<li>Maybe parrots are really smart?</li>
</ul>
</li>
<li>Omphalos<ul>
<li>A world with physical proof of creationism finds out it was
   God's practice planet.</li>
</ul>
</li>
<li>Anxiety Is the Dizziness of Freedom<ul>
<li>Devices for communicating between two parallel universes (one
   pair per device).</li>
</ul>
</li>
</ul>
<p>I never wrote about <a href="https://en.wikipedia.org/wiki/Stories_of_Your_Life_and_Others">Stories of Your Life and Others</a>, so I'll describe those too:</p>
<ul>
<li>Tower of Babylon<ul>
<li>It gets so high that it breaks through the firmament.</li>
</ul>
</li>
<li>Understand<ul>
<li>A guy gets crazy smart and fights with another smart guy.</li>
</ul>
</li>
<li>Division by Zero<ul>
<li>Whoa what if arithmetic is
   <a href="https://en.wikipedia.org/wiki/Consistency">inconsistent</a>.</li>
</ul>
</li>
<li>Story of Your Life<ul>
<li>Cross-time awareness (single timeline) with aliens and writing.</li>
</ul>
</li>
<li>Seventy-Two Letters<ul>
<li>What if people really did grow from
   <a href="https://en.wikipedia.org/wiki/Homunculus">homunculi</a> (also
   other weird stuff).</li>
</ul>
</li>
<li>The Evolution of Human Science<ul>
<li>People make more advanced humans and we can't understand them
   any more. (Possible connection to AI explainability?)</li>
</ul>
</li>
<li>Hell is the Absence of God<ul>
<li>A world where angels are real and messy, heaven and hell are
   real, etc.</li>
</ul>
</li>
<li>Liking What You See: A Documentary<ul>
<li>Turning off visual perception of human beauty.</li>
</ul>
</li>
</ul>
<p><img alt="Exhalation" src="exhalation.jpg"></p>
<hr>
<p>Thanks <a href="https://twitter.com/rememberlenny">Lenny</a> for letting me know
that Exhalation was available!</p>    
    ]]></description>
<link>http://planspace.org/20190613-exhalation_by_ted_chiang/</link>
<guid>http://planspace.org/20190613-exhalation_by_ted_chiang/</guid>
<pubDate>Thu, 13 Jun 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Designing Data-Intensive Applications</title>
<description><![CDATA[

<p>This book was <a href="https://fullstackdeeplearning.com/march2019">recommended</a> by <a href="https://sergeykarayev.com/">Sergey Karayev</a> and I agree: this is a very good book.</p>
<p>The <a href="https://martin.kleppmann.com/">author</a> is expert in both content and communication, erudite and humane. Everyone who works with data would do well to read this book. And it has <a href="https://martin.kleppmann.com/2017/03/15/map-distributed-data-systems.html">maps</a>!</p>
<p>What is it about? Lots of things... Check out the <a href="https://www.oreilly.com/library/view/designing-data-intensive-applications/9781491903063/#toc-start">table of contents</a>. It's so good.</p>
<p><img alt="cover" src="boar_cover.png"></p>    
    ]]></description>
<link>http://planspace.org/20190606-designing_data-intensive_applications/</link>
<guid>http://planspace.org/20190606-designing_data-intensive_applications/</guid>
<pubDate>Thu, 06 Jun 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Book Thing of Baltimore</title>
<description><![CDATA[

<p>I really like <a href="https://bookthing.org/">The Book Thing of Baltimore</a>. It's like a <a href="https://littlefreelibrary.org/">Little Free Library</a> but <a href="https://en.wikipedia.org/wiki/The_Book_Thing">with 200,000 books</a>. It's as great <a href="https://www.atlasobscura.com/places/the-book-thing-baltimore-maryland">as everyone says</a> and I think there should be more of them.</p>
<p><img alt="inside the book thing" src="book_thing.jpg"></p>
<p>Thoughts at the thing:</p>
<ul>
<li>Books can seem so major, so heavy, so important... but most of them
   are ephemeral.</li>
<li>Nearly all book-reading is for entertainment.</li>
<li>Many things that we own, we don't really need to own.</li>
<li>Operations don't need to be complicated: The book thing doesn't
   track each book, doesn't cycle them or worry about staleness; you
   don't have to sign up to volunteer, you just show up. Their
   "processes" are dead simple, and it works.</li>
</ul>    
    ]]></description>
<link>http://planspace.org/20190602-book_thing_of_baltimore/</link>
<guid>http://planspace.org/20190602-book_thing_of_baltimore/</guid>
<pubDate>Sun, 02 Jun 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Null impact of letters to very unlikely voters</title>
<description><![CDATA[

<p>Before the November 2018 election, I sent 2,181 personalized letters to very unlikely voters registered in New York's state senate district 5, encouraging them to vote. I sent 1,180 "birthday letters" in the months before the election, and 1,001 more conventional letters shortly before the election. Unfortunately, I didn't meaningfully increase voting rates in either group compared to control. This null result supports the conventional wisdom that extremely low-propensity voters are not optimal targets for get-out-the-vote efforts.</p>
<pre><code>| Group           | Total | Voted | % Voted |
|-----------------|-------|-------|---------|
| Control         | 3,135 | 349   | 11.13%  |
| Birthday letter | 1,180 | 140   | 11.86%  |
| Timely letter   | 1,001 | 111   | 11.09%  |</code></pre>

<p><strong>Statistical analysis</strong>: These results are not distinguishable. The best p-value I get is 0.25. The treatments are very similar, but one "effect" is positive and the other negative.</p>
<p><strong>Group selection</strong>: Using a 2018-07-20 voter file for NY SD5, I selected active registered Democrats who had voted at least once but not voted in the last five years. Back-testing showed equivalent groups had historically voted at low rates in midterm elections: 9%, 8%, and 4% in 2006, 2010, and 2014, respectively.</p>
<p><strong>Rationale</strong>: I hoped that by communicating with more people who wouldn't otherwise vote, more could be swayed. I was aware of the common practice of targeting people judged to be around 50% likely to vote, but I hoped to find a novel result, and thought that especially in the particularly high-saliency 2018 midterm, my approach could work.</p>
<p><strong>Birthday letter</strong>: To encourage engagement, I thought it would be fun to send birthday letters. This experiment group was the subset of the larger group with birthdays from August 19 to November 6. The letters had brief birthday greetings before the pro-voting message, and a "happy birthday" sticker on the outside of the hand-addressed envelopes. Letters were mailed a week before each birthday.</p>
<p><strong>Timely letter</strong>: These letters were nearly identical to birthday letters, but with generic greetings and no stickers. They were mailed a week before the election. This group was a random selection from the larger group not selected for birthday letters. (The remainder became the control group, not receiving any letter.) Research suggests timely messaging should be <em>more</em> effective, making the outcome of this treatment even more discouraging.</p>
<p><strong>Deliverability</strong>: I got 8% of letters bounced back to my address, which I hope is high for this kind of campaign. Of bounces, 6% voted. (I wouldn't read much into this.)</p>
<p><strong>Direct response</strong>: Letters included my personal mailing and email addresses. I had hoped to make a person-to-person connection with voters. I heard back from three people:</p>
<ul>
<li>One person was an active voter who had moved, but family at his
   previous address got the letter to him. He emailed and was very
   supportive.</li>
<li>One person found me on Twitter and was briefly curious about the
   project.</li>
<li>The mother of a letter's intended recipient emailed to say she was
   pro-voting but upset that I had the address and birthday of her
   daughter.</li>
</ul>
<p><strong>Results data</strong>: I used a 2019-02-28 voter file for NY SD5. Voters were re-identified by New York State Board of Elections voter ID.</p>
<p><strong>Conclusions</strong>: I wouldn't do this again. It was a learning experience, and it made me feel like I was doing something at the time. The overall change in turnout from 2014 to 2018 was dramatically greater than any effect ever achieved by a postal get-out-the-vote effort, which makes me think other influences are important.</p>    
    ]]></description>
<link>http://planspace.org/20190531-null_impact_of_letters_to_very_unlikely_voters/</link>
<guid>http://planspace.org/20190531-null_impact_of_letters_to_very_unlikely_voters/</guid>
<pubDate>Fri, 31 May 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Understanding MyinTuition</title>
<description><![CDATA[

<p><a href="https://myintuition.org/">MyinTuition</a> is a simple online college
cost estimator for about 50 schools. I was curious about how it
worked, so I figured it out. I suspect they don't want me to say, and
I like them, so I won't. But here are some of my opinions and
experiences.</p>
<p>The rhetoric around MyinTuition is all about transparency, as in the
creator's 2014 paper about it, <a href="https://www.brookings.edu/wp-content/uploads/2016/06/12_transparency_in_college_costs_levine.pdf)">Transparency in College Costs</a>.
They want to be transparent about this message:
<a href="https://www.nytimes.com/interactive/2018/06/05/opinion/columnists/what-college-really-costs.html">Top Colleges Are Cheaper Than You Think (Unless You&#8217;re Rich)</a>.</p>
<p>College affordability is important. An expanded idea of transparency
might further include how the MyinTuition calculations are done, and
how the different schools vary. That's what I was curious about.</p>
<p>I originally thought I'd try to use machine learning to understand how
MyinTuition works, which would mean first collecting training data.
I'm not sure this was ever a good idea. For one thing, it doesn't seem
practical if data collection is severely rate-limited.</p>
<p>I explored on and off for a couple months. It was fun! It wouldn't be
as much fun to do now, what with the <a href="https://www.google.com/recaptcha/">CAPTCHA</a>s they've put
in. And unfortunately it may no longer be possible to access
MyinTuition at all using the IP addresses I was on at the time.</p>
<p>Figuring out the MyinTuition calculations and parameters for different
schools was interesting. I'll just recommend that you try a bunch of
schools and see what you get. You can find the calculators for all
participating schools <a href="https://myintuition.org/quick-college-cost-estimator/">on their site</a>.</p>
<p>Never stop learning, kids!</p>
<!-- https://bitbucket.org/ajschumacher/college_cost/ -->    
    ]]></description>
<link>http://planspace.org/20190529-understanding_myintuition/</link>
<guid>http://planspace.org/20190529-understanding_myintuition/</guid>
<pubDate>Wed, 29 May 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>The One Thing</title>
<description><![CDATA[

<p>This book is not all bad. A copy found its way from an airport
bookstore to a <a href="https://littlefreelibrary.org/">little free library</a>
to me, and I read it. Key points:</p>
<ul>
<li>Do the most important thing(s).</li>
<li>Make time to do things, blocking four-hour AM calendar "meetings"
   if need be.</li>
</ul>
<p>There's some other stuff in there about thinking big, identifying the proximal task, staying healthy, trying to not be a workaholic, etc. The book repeats pop science hits like <a href="https://en.wikipedia.org/wiki/Stanford_marshmallow_experiment">the marshmallow experiment</a> unquestioningly and without more recent <a href="https://www.theatlantic.com/family/archive/2018/06/marshmallow-test/561779/">updates</a>. There are ample cute turns of phrase that seem designed to play well in motivational speaking. It's not a great book, but it's quick to read, and it does sort of pump you up.</p>
<p>I think the core idea is not so bad. Two important aspects:</p>
<ul>
<li><strong>Just <em>completing</em> anything.</strong> There are so many distractions and
   ways to waste time, it's possible to "work" a lot and not actually
   get things done. It's helpful to identify a thing to do and then
   focus on completing it. The book mentions the inefficiency of
   multitasking, which rings true to me.</li>
<li><strong>Doing the <em>right</em> thing.</strong> There are many paths to not
   accomplishing anything. You can work on a problem for a long time,
   even get a lot of things done, without actually making real
   progress. You might end up with everything to show for your work
   except what you actually needed.</li>
</ul>
<p><img alt="The One Thing (cover)" src="the_one_thing_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190403-the_one_thing/</link>
<guid>http://planspace.org/20190403-the_one_thing/</guid>
<pubDate>Wed, 03 Apr 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Swimming with Sharks, by Joris Luyendijk</title>
<description><![CDATA[

<p>"Inside the world of the bankers,"</p>
<blockquote>
<p>"You see a cluster of islands in the fog, staffed by mercenaries."
(page 145)</p>
</blockquote>
<p><a href="https://twitter.com/JORISLUIJENDIJK">Luyendijk</a> wrote
<a href="https://www.theguardian.com/commentisfree/joris-luyendijk-banking-blog">a 2011-2013 banking blog</a>
for the Guardian based on interviews with bankers in London. It became
this 2015 book. It's quite late to the "What happened in 2008?"
party, and even makes the observation that</p>
<blockquote>
<p>"the sector has become immune to exposure." (page 252)</p>
</blockquote>
<p>It does seem like some things these days are so bad they can hardly be
made to look worse, and yet far from being stopped, some people really
like them.</p>
<p>So Luyendijk writes about how things in finance are complicated, and
bad, but it's hard to single out any bad guys by name. His Dutch
perspective is interesting: he identifies lack of job security as a
major cause of problems in finance, for example.</p>
<p>On page 254 he has one paragraph of recommendations for law to change
the system:</p>
<ol>
<li>Break up banks so nothing is too big or too complex to fail</li>
<li>Don't let units of the same company have conflicts of interest
    with one another</li>
<li>Don't allow the building/selling/owning of overly complex
    financial products</li>
<li>Only allow bonuses that have symmetric risk</li>
</ol>
<p>No problem, right?</p>
<p>The observation and recommendation that resonated the most for me was
reconnecting reward to risk. With big public companies, a trader has
essentially only upside: good performance means a big bonus, but bad
performance means at worst finding another high-paying job at a
different company.</p>
<blockquote>
<p>"nobody should have more reason to lie awake at night worrying over
the risks to the bank's capital or reputation than the bankers
taking those risks." (page 254)</p>
</blockquote>
<p>I can get behind the sentiment, at least.</p>
<p><img alt="Swimming with Sharks cover" src="sharks_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190204-swimming_with_sharks/</link>
<guid>http://planspace.org/20190204-swimming_with_sharks/</guid>
<pubDate>Mon, 04 Feb 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Unreal and The Real, Volume Two</title>
<description><![CDATA[

<p>I bought
<a href="https://smile.amazon.com/Unreal-Real-Selected-Stories-Outer/dp/1618730355/">this</a>
collection of
<a href="https://en.wikipedia.org/wiki/Ursula_K._Le_Guin">Le Guin</a>'s short
stories and then she died before I read it. Before that, I once bought
a Kurt Vonnegut print and he died before it was delivered. I'm cursed!</p>
<p>I got the book because it has The Ones Who Walk Away From Omelas,
which I heard about in
<a href="https://www.amazon.com/How-Think-Survival-Guide-World/dp/0451499603">How to Think</a>
(which by the way is better for reacting to than learning from, if
there's a difference). Le Guin notes that Omelas "has had a long and
happy career of being used by teachers to upset students and make them
argue fiercely about morality." I missed that class, so it was nice to
catch up. File with:
<a href="https://en.wikipedia.org/wiki/Trolley_problem">trolley problems</a>?</p>
<p>To guess how Le Guin chose the stories for this volume, I'd say she
was trying to span the largest possible space of worlds. There are
lots of different universes. For example:</p>
<p>What if:</p>
<ul>
<li>the ratio of boy babies to girl babies was 1:16 instead of close to
   1:1?</li>
<li>one in a thousand people grew working, flying wings around age 20?</li>
<li>wolves were occasionally werepeople?</li>
<li>a peasant broke into Sleeping Beauty's castle and lived there while
   she slept?</li>
<li>an all-woman party of explorers reached the South Pole a couple
   years before
   <a href="https://en.wikipedia.org/wiki/Amundsen%27s_South_Pole_expedition">Amundsen</a>,
   but left no trace?</li>
</ul>
<p>Spoiler alert! A lot of the fun of many of these stories is figuring
out the premise, because the perspective doesn't always make it
obvious from the beginning.</p>
<p>In The Author of the Acacia Seeds, either people actually figure out
how to study the writing of ants and penguins and so on, or it's sort
of a joke about the modern academy inventing the study of everything,
whether there's really anything there to study or not.</p>
<p>My favorite story is probably Solitude.</p>
<p>Recommended reading order: As printed, but then re-read the
introduction.</p>
<p><img alt="cover of The Unreal and The Real, Volume Two" src="cover_volume_two.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190129-unreal_and_the_real_volume_two/</link>
<guid>http://planspace.org/20190129-unreal_and_the_real_volume_two/</guid>
<pubDate>Tue, 29 Jan 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>Bird by Bird, by Anne Lamott</title>
<description><![CDATA[

<p><a href="https://smile.amazon.com/Bird-Some-Instructions-Writing-Life/dp/0385480016/">This</a>
is a charming short book containing "Some instructions on writing and
life."</p>
<p>It's very personal. Anne Lamott is cartoonishly neurotic and suggests
everyone else is too, which I've decided is nice.</p>
<p>She recommends index cards. I concur.</p>
<p>She mentions somehow that the population of the world is four billion.
I had to check the copyright date. It was 1994. Now in 2019 there are
7.7 billion people. Time flies!</p>
<p>Her writing glistens especially over short time scales. She is a
master of evocative two-item lists.</p>
<p>She has something to say (by proxy?) about not holding back:</p>
<blockquote>
<p>"Annie Dillard has said that day by day you have to give the work
before you all the best stuff you have, not saving up for later
projects. If you give freely, there will always be more."</p>
</blockquote>
<p>This is a phenomenon I've thought about in the context of breakdancing
competitions, and it is interesting to think about it also in writing,
and elsewhere.</p>
<p>Also she is very character-first. Plot comes from character, she says.</p>
<p><img alt="Bird by Bird (cover)" src="bird_by_bird_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20190128-bird_by_bird_by_anne_lamott/</link>
<guid>http://planspace.org/20190128-bird_by_bird_by_anne_lamott/</guid>
<pubDate>Mon, 28 Jan 2019 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Expectant Father and so on</title>
<description><![CDATA[

<p>I read some baby books in 2018. I got a copy of <a href="https://smile.amazon.com/Expectant-Father-Ultimate-Dads-Be/dp/0789212137/">The Expectant Father</a>, and then I got <a href="https://smile.amazon.com/New-Father-Dads-Guide-First/dp/0789211777/">The New Father</a>, by the same author. I also got the <a href="https://smile.amazon.com/FAQ-Expectant-Fathers-Armin-Brott/dp/0789212692/">FAQ</a> <a href="https://smile.amazon.com/FAQ-New-Fathers-Armin-Brott/dp/0789212706/">versions</a>, thinking (mostly wrongly) that it would be fun to see the material in multiple-choice-question form. And Erica read <a href="https://smile.amazon.com/What-Expect-When-Youre-Expecting/dp/0761187480/">What to Expect When You're Expecting</a>, so I saw bits of that as well.</p>
<p>I have mixed feelings on these baby books. Some things you can't prepare for. I think a lot of baby books are sold just to make people feel better. It's like reading about dancing: you might learn something, but you won't become a great dancer with books alone.</p>
<p>As far as facts, there are lots that you'll care about with some lowish probability. Baby books can be like encyclopedias that way. And for nearly everything, if you don't know in advance, you'll find out when you need to know. You can learn in advance about <a href="https://en.wikipedia.org/wiki/Meconium">meconium</a>, or later as needed, or not at all. Regardless, the baby poops.</p>
<p>For example, information about induction turned out to be relevant for our family. I don't recall reading much about induction, maybe because I didn't think it would be so relevant. As it happened, the best preparation I had came not from a book but from an in-person session we did at our hospital. Regardless, we relied on the medical professionals at the hospital, and I think it would have been a big mistake to favor an opinion from elsewhere over our nurses and doctors, who did a great job.</p>
<p>There is good general advice, but the parts most people agree on are pretty well known: don't smoke, eat healthy, etc. I think I prefer <a href="/20181209-brain_rules_for_baby/">Brain Rules for Baby</a> over more conventional baby books for summarizing such advice.</p>
<p>There are two things in particular that I would have liked to be more prepared for:</p>
<ul>
<li>During delivery, pushing changes at the last moment, when it's most painful, just as the baby's head is about to pop out. All of a sudden you're supposed to basically stop pushing so they can try to ease the head out without tearing anything. I don't recall hearing about this in advance. Maybe not everybody does it this way. We got these surprise directions in the middle of what was already a very intense process, and everything worked out reasonably well, but I feel like we wouldn't have minded learning about this in advance.</li>
<li>Babies cry sometimes for no apparent reason. Sometimes for considerable time (even if less than the pseudoscientific definitions associated with "colic"). I knew that babies cry, of course, but nothing prepared me for the feeling of my own daughter crying inconsolably in my arms. I don't know how much it would have helped, but I think I would have liked to hear more advice like, "Sometimes your baby will cry no matter what you do. It will be painful, but here are some things you can try. It will be okay."</li>
</ul>
<p>I'm new to all this. I'm trying to learn as much as I can. You can always learn some things from books, and there are always some things you can't. For having a child, I am strongly impressed with how valuable social learning and direct experience are when stacked against the information for sale in books.</p>
<p><img alt="cover of The Expectant Father" src="expectant_father_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181231-expectant_father/</link>
<guid>http://planspace.org/20181231-expectant_father/</guid>
<pubDate>Mon, 31 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Equation of a Plane</title>
<description><![CDATA[

<p>I wasn't a great undergraduate student, but I had at least one great
professor. She connected me with my post-graduate career path, and she
taught some of my calculus classes. One day she turned briefly from
the chalkboard and said with a smile:</p>
<blockquote>
<p>"If you don't know equation of a plane, I kill you!"</p>
</blockquote>
<p>I thought it was funny, and it wasn't what we were really working on
in class, so I didn't think much more about it. But because I never
thought it through then, doubt remained for some fifteen years: Was my
life in danger?</p>
<p>That teacher wasn't even an associate professor back then, by official
title. Recently, I learned that she's become an associate dean of the
college. I was inspired to finally think about the equation of a
plane.</p>
<p>Maybe she just meant that the equation of a plane is linear in
Cartesian coordinates. That could have been it. But I think she was
also getting at something about using the axis intercepts.</p>
<p>I learned \( y=mx+b \) like a kind of incantation, and I don't think
it was only because I went to Catholic schools. The "standard form" is
<a href="http://jwilson.coe.uga.edu/emt668/emat6680.2002/jackson/chapter%205%20lesson%20plan/day6.html">treated</a>
like a purposeless afterthought, more about following rules than
understanding anything.</p>
<p>But if the \( x \)-intercept is \( a \) and the \( y
\)-intercept is \( b \), then \( bx + ay = ab \) is a very nice
equation of a line. Maybe call it both-intercepts form.</p>
<p>Adding a \( z \)-intercept of \( c \), a plane then has a similar
equation: \( bcx + acy + abz = abc \).</p>
<p>I don't know for sure whether that's the equation that should have
spared my life back then, but it's a charming one. It's fun to think
about how it's related to other forms, and other dimensions&#8212;and how
silly a rule like "\( A \), \( B \), and \( C \) must be
integers" is.</p>
<p>I wish I had been a better student back then. I had no idea how useful
multivariate calculus and linear algebra would turn out to be, in
particular. But I've been very lucky, and I'm honored to have had
teachers who give me the pleasure of thinking things through, even
little things like this, and even many years later than I should have.
And I am deeply pleased to see that people who were so helpful to me
have themselves found greater success, and positions from which to
help many more people.</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20181228-the_equation_of_a_plane/</link>
<guid>http://planspace.org/20181228-the_equation_of_a_plane/</guid>
<pubDate>Fri, 28 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>The Whole-Brain Child</title>
<description><![CDATA[

<p>The advice of this book overlaps substantially with that in
<a href="/20181209-brain_rules_for_baby/">Brain Rules for Baby</a>. In
comparison, the science here feels less rigorous, but the advice still
seems reasonable. There's more focus on memory, which I mostly like.</p>
<blockquote>
<p>"... children whose parents talk with them about their experiences
tend to have better access to the memories of those experiences."
(page 8)</p>
</blockquote>
<p>If there's an overall theme, it is a kind of Western presentation of a
<a href="https://en.wikipedia.org/wiki/Middle_Way">middle way</a> between logic
and emotion, then between reactive thought and higher executive
function (shades of
<a href="/2011/12/17/selections-from-and-thoughts-on/">Thinking, Fast and Slow</a>).
There's a kind of meditative concept they call
<a href="https://www.drdansiegel.com/about/mindsight/">mindsight</a>.</p>
<p>The way they think about human development reminds me a little bit of
Montessori's "spontaneous discipline" - people become better.</p>
<blockquote>
<p>"As you create a whole-brain family, you also join a broader vision
of creating an entire society full of rich, relational communities
where emotional well-being is nurtured for this and future
generations." (page 148)</p>
</blockquote>
<p>Here are their "12 revolutionary strategies to nurture your child's
developing mind":</p>
<ol>
<li>Connect and redirect: Surfing emotional waves</li>
<li>Name it to tame it: Telling stories to calm big emotions</li>
<li>Engage, don't enrage: Appealing to the upstairs brain</li>
<li>Use it or lose it: Exercising the upstairs brain</li>
<li>Move it or lose it: Moving the body to avoid losing the mind</li>
<li>Use the remote of the mind: Replaying memories</li>
<li>Remember to remember: Making recollection a part of your family's daily life</li>
<li>Let the clouds of emotion roll by: Teaching that feelings come and go</li>
<li>SIFT (Sensations, Images, Feelings, Thoughts): Paying attention to what's going on inside</li>
<li>Exercise mindsight: Getting back to the hub</li>
<li>Increase the family fun factor: Making a point to enjoy each other</li>
<li>Connect through conflict: Teach kids to argue with a "we" in mind</li>
</ol>
<p>There's also a one-page (front and back) "refrigerator sheet" at the
back of the book, which is cute.</p>
<p><img alt="cover of The Whole-Brain Child" src="whole_brain_child_cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181227-the_whole_brain_child/</link>
<guid>http://planspace.org/20181227-the_whole_brain_child/</guid>
<pubDate>Thu, 27 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Gaussian Processes are Not So Fancy</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian Processes</a>
have a mystique related to the dense probabilistic terminology that's
already evident in their name. But Gaussian Processes are just models,
and they're much more like k-nearest neighbors and linear regression
than may at first be apparent.</p>
<p><img alt="Predictive mean and range" src="img/predictive_mean_and_range.png"></p>
<p>Gaussian Processes have
<a href="https://en.wikipedia.org/wiki/Kriging#Applications">applications</a>
ranging from finding gold to
<a href="https://cloud.google.com/blog/products/gcp/hyperparameter-tuning-cloud-machine-learning-engine-using-bayesian-optimization">optimizing hyperparameters</a>
of other models. The focus here is on how Gaussian Processes work,
using an example that's simple enough to show completely from
beginning to end.</p>
<hr>
<h3>A simple training data set</h3>
<p>A model is trained with predictors \( X \) and known labels \( y
\). Here's some data in Python:</p>
<pre><code class="language-python">train_X = [[0.8], [1.2], [3.8], [4.2]]
train_y = [   3,     4,    -2,    -2 ]</code></pre>

<p>Since elements of \( X \) are one-dimensional, all the data can be
shown in a simple figure:</p>
<p><img alt="Training data" src="img/training_data.png"></p>
<p>The task of a model is to predict \( y \) values for test points \(
x \).</p>
<hr>
<h3>Applying a kernel function</h3>
<p>Like <a href="https://en.wikipedia.org/wiki/Support_vector_machine">SVMs</a>,
Gaussian Processes use kernel functions. A kernel gives a closeness,
or similarity, between two points. This is related to distances, and a
kernel may involve distance. Here's the matrix of Euclidean distances
between points in our training data \( X \):</p>
<pre><code class="language-python">dist_XX = sklearn.metrics.pairwise_distances(train_X)
## array([[0. , 0.4, 3. , 3.4],
##        [0.4, 0. , 2.6, 3. ],
##        [3. , 2.6, 0. , 0.4],
##        [3.4, 3. , 0.4, 0. ]])
# Recall:
# train_X = [[0.8], [1.2], [3.8], [4.2]]</code></pre>

<p>The Gaussian radial basis function (RBF) kernel is commonly used. In
<a href="http://gaussianprocess.org/gpml/chapters/">Gaussian Processes for Machine Learning</a>,
Rasmussen and Williams call it the <em>squared exponential</em> kernel,
probably to avoid confusion with other things that are Gaussian. For
distance \( d \), it's \( e^{-\frac{1}{2}d^2}\):</p>
<pre><code class="language-python">def squared_exponential(distance):
    return np.exp(distance**2 / -2)</code></pre>

<p>It's one when distance is zero, and it goes to zero when distance is
big. This is evident for our data when we make the matrix of kernel
values for the training data \( X \):</p>
<pre><code class="language-python">kern_XX = squared_exponential(dist_XX)
## array([[1.  , 0.92, 0.01, 0.  ],
##        [0.92, 1.  , 0.03, 0.01],
##        [0.01, 0.03, 1.  , 0.92],
##        [0.  , 0.01, 0.92, 1.  ]])
# Recall:
# train_X = [[0.8], [1.2], [3.8], [4.2]]</code></pre>

<p>This matrix \( K(X, X) \) is a core component of Gaussian Processes,
and as the example shows, it reflects a core concern with nearness as
represented via a kernel function.</p>
<hr>
<h3>Using the Gaussian Process prediction equation</h3>
<p>This is Rasmussen and Williams' Equation 2.19 for the predictive
posterior distribution, which I promise isn't as bad as it looks:</p>
<p>\[ \mathbf{f_{\ast}} | X_{\ast}, X, \mathbf{f} \sim \mathcal{N}( K(X_{\ast}, X) K(X,X)^{-1} \mathbf{f}, \\ K(X_{\ast}, X_{\ast}) - K(X_{\ast}, X) K(X,X)^{-1} K(X, X_{\ast}) ) \]</p>
<p>Here \( \mathbf{f_{\ast}} \) is the predicted \( y \) for a test
point in \( X_{\ast} \) based on the training data \( X \) and \(
y \) labels \( \mathbf{f} \). It's predicted to be normally
distributed with mean \( K(X_{\ast}, X) K(X,X)^{-1} \mathbf{f} \)
and the given covariance. The mean can be considered "the" prediction,
though you can also sample.</p>
<hr>
<h3>Behaves like nearest neighbors</h3>
<p>Let's use Equation 2.19 to make a prediction for this \( X_{\ast} \):</p>
<pre><code class="language-python">test_X = [[1]]</code></pre>

<p>We find the kernel similarities between the test point and each point
of training data:</p>
<pre><code class="language-python">kern_xX = squared_exponential(
              sklearn.metrics.pairwise_distances(test_X, train_X))
## array([[0.98019867, 0.98019867, 0.01984109, 0.00597602]])</code></pre>

<p>The test point is close to the first two training points, and far from
the second two.</p>
<p>Let's evaluate just the first two terms of the predictive mean given
in Equation 2.19:</p>
<pre><code class="language-python">kern_xX.dot(np.linalg.inv(kern_XX))
## array([[ 0.50835358,  0.51127124, -0.01378216,  0.01144869]])</code></pre>

<p>It isn't exactly, but this looks a lot like a weighting for a weighted
average. And the prediction roughly fits that interpretation:</p>
<pre><code class="language-python">test_y = kern_xX.dot(np.linalg.inv(kern_XX)).dot(train_y)[0]
## 3.57</code></pre>

<p><img alt="Predictive mean at one point" src="img/predictive_mean_at_one_point.png"></p>
<p>This behavior is similar to k-nearest neighbors. Nearby points matter;
points that are far away don't. Nearest neighbors can even include a
weighting based on a kernel function.</p>
<p>Also like k-nearest neighbors, you have to use the whole training set
for every Gaussian Process prediction, comparing the test point to
every training point.</p>
<p>With Gaussian Processes, however, you don't have to specify a number
of neighbors \( k \). Every point that is near enough contributes to
the prediction.</p>
<p>The comparison here to a kind of weighted average nearest neighbors is
much more directly valid for
<a href="https://en.wikipedia.org/wiki/Kernel_regression">kernel regression</a>,
which is another technique that also uses kernels. With Gaussian
Processes, there's really a further step of curve-fitting going on.</p>
<hr>
<h3>It's linear regression</h3>
<p>The mean prediction of a Gaussian Process is the same as a linear
regression with a particular choice of coordinates. Let's talk about
how.</p>
<p>Why is \( K(X,X)^{-1} \) involved in the predictive mean?</p>
<p>Consider the kernel matrix as a transformation of the original
training data \( X \) into new variables, where each of the new
variables is kernel-nearness to one of the points of training data.
This is very much like transforming raw data to include interactions
or polynomial terms, as is common in regression. Say the transformed
data is \( Z \).</p>
<p>Linear regression coefficients
<a href="https://en.wikipedia.org/wiki/Ordinary_least_squares#Matrix/vector_formulation">can be solved for</a>
with \( (Z^T Z)^{-1} Z^T \). But with a nice symmetric square matrix
like \( K(X,X) \), this is the same as just taking \( Z^{-1} \)
directly.</p>
<p>So the mean prediction of a Gaussian Process is the same as linear
regression in the coordinates defined by kernel similarities with each
training point. It <em>is</em> linear regression.</p>
<hr>
<h3>With covariance</h3>
<p>With Gaussian Processes, kernel functions are also called covariance
functions. Things that are kernel-near vary together&#8212;they have similar
values&#8212;and this enforces smoothness.</p>
<p>Here's the covariance part of Equation 2.19 again. It has two terms:</p>
<p>\[ K(X_{\ast}, X_{\ast}) - K(X_{\ast}, X) K(X,X)^{-1} K(X, X_{\ast}) \]</p>
<p>The positive first term shows that test point(s) vary together when
they're close to one another. The negative second term (which is also
a linear regression solution, like the mean) captures how much
variance is eliminated due to being close to observed points.</p>
<!-- How to read the negative term as linear regression: The data set
X is, for each training point, kernel nearness to each other training
point. The training labels y are kernel nearness from test data to
training data. Then the test point has its nearness to the training
data, and we get out a consolidated estimate of nearness of this test
point to the training data. It can be thought of like a local weighted
average, just like for the mean. -->

<hr>
<h3>Predicting for multiple points</h3>
<p>If you get the predictive mean for many points, you can draw out a
curve:</p>
<p><img alt="Predictive mean at many points" src="img/predictive_mean_at_many_points.png"></p>
<p>You may instead want to use the predictive mean and variance at some
test point to <em>sample</em> an outcome \( y \). If you then take that
sampled value as a new point of training data (adding it to your
training set) future predictions will be consistent with that first
one, and so on.</p>
<p>You can equivalently make multiple consistent predictions
simultaneously by putting multiple test points in \( X_{\ast} \) and
sampling from a multivariate Gaussian.</p>
<p>Just like the mean alone, values sampled this way will draw out a
smooth curve. But unlike the mean alone, each random draw will be a
different curve: Gaussian Processes are random over a space of
functions. An approachable
<a href="http://katbailey.github.io/post/gaussian-processes-for-dummies/">post by Bailey</a>
focuses on this.</p>
<p>It can be fun to sample curves, but often the mean and variance alone
are useful.</p>
<p><img alt="Predictive mean and range" src="img/predictive_mean_and_range.png"></p>
<hr>
<h3>The Bayesian prior away from data</h3>
<p>Gaussian Processes have Bayesian priors. For the example here, the
assumption is that the function is always zero with covariance one,
until we see training data showing otherwise.</p>
<p>In Equation 2.19, you can think of the mean as zero plus contributions
from the training data; that's where the zero comes from. Covariance
comes from the kernel function, which gets as big as one. To change
the prior on the covariance, change the kernel function.</p>
<p><img alt="Predictive mean and range (wider view)" src="img/predictive_mean_and_range_wider_view.png"></p>
<p>The prior is visible when the bounds of the plot are expanded, which
illustrates that Gaussian Processes often focus on local interpolation
more than extrapolation.</p>
<hr>
<h3>Length scale matters</h3>
<p>The kernel specifies the scale of the variance, and in the case of the
squared exponential kernel, there's also a length scale parameter that
has significant effects.</p>
<pre><code class="language-python">def squared_exponential(distance, length_scale=1):
    return np.exp((distance / length_scale)**2 / -2)</code></pre>

<p>For our example, a length scale of one works reasonably well. For a
smaller length scale, the function is allowed to change faster, and
the prior asserts itself more quickly:</p>
<p><img alt="Predictive mean with length scale = 0.05" src="img/predictive_mean_with_small_length_scale.png"></p>
<p>For a Gaussian Process to capture a trend, its kernel needs to support
it. In the case of the squared exponential kernel, this means a long
enough length scale.</p>
<hr>
<h3>When hyperparameters are your parameters</h3>
<p>As presented here, a Gaussian Process will always exactly fit its
training data. This is often considered a sign of overfitting, and
regardless it's clearly unsustainable if training data ever has two
different \( y \) values for identical \( x \) values.</p>
<p>Noise can be added to address these issues, and the scale of the noise
is another hyperparameter, joining the overall scale and length scale
of the covariance function.</p>
<p>To get really interesting behavior may require composing kernel
functions, altering what "near" means for the model and adding even
more hyperparameters, as in
<a href="https://scikit-learn.org/stable/modules/gaussian_process.html#gpr-on-mauna-loa-co2-data">this example</a>.</p>
<p>Gaussian Process implementations like
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html">sci-kit's</a>
try to automatically fit these hyperparameters, which may remove some
of the need to know what they should be in advance. But optimization
can't do all the work of designing an appropriate kernel for a
problem, or eliminate the difficulty of distances in high-dimensional
spaces.</p>
<p>There are also some approaches for improving computational efficiency.</p>
<hr>
<h3>Not useless, but not magical</h3>
<p>Even with enhancements, the fundamental nature of Gaussian Processes
is as presented here: local smooth curve fitting built on linear
regression.</p>
<p>A Gaussian Process might be useful for you. But please don't assume
that it is sophisticated just because the language around it often is,
or that its results are automatically true just because they have
error bars.</p>
<hr>
<p>The code and plots from this post are all in
<a href="https://github.com/ajschumacher/ajschumacher.github.io/blob/master/20181226-gaussian_processes_are_not_so_fancy/gaussian_processes.ipynb">a Jupyter notebook</a>.</p>
<p>Thanks to Erica Blom, Marco Pariguana, Sylvia Blom, Travis Hoppe, and Ajay Deonarine for reading drafts of this post and providing feedback. Thanks also to everybody <a href="https://www.reddit.com/r/MachineLearning/comments/ac6er3/d_gaussian_processes_are_not_so_fancy/">on Reddit</a>, <a href="https://news.ycombinator.com/item?id=18814776">on HN</a>, <a href="https://www.datatau.com/item?id=28391">on DataTau</a>, <a href="https://twitter.com/search?f=tweets&amp;vertical=default&amp;q=https%3A%2F%2Fplanspace.org%2F20181226-gaussian_processes_are_not_so_fancy%2F&amp;src=typd">on Twitter</a>, <a href="https://pinboard.in/url:077f95c750f2b064e882bafb0d7899b7a3eaf48a/">on Pinboard</a>, and elsewhere!</p>
<!-- mathjax for formulas -->

    ]]></description>
<link>http://planspace.org/20181226-gaussian_processes_are_not_so_fancy/</link>
<guid>http://planspace.org/20181226-gaussian_processes_are_not_so_fancy/</guid>
<pubDate>Wed, 26 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Brain Rules for Baby</title>
<description><![CDATA[

<p>To be a good parent, be a good person. The core recommendations on "How to raise a smart and happy child from zero to five" are empathy and understanding emotion.</p>
<blockquote>
<p>"Human learning ... is primarily a relational exercise." (page 111)</p>
</blockquote>
<p>Medina aims for objective reasonableness, and it's usually easy for me to agree.</p>
<p>For the abbreviated book (and a few extra nuggets) check out the 15 pages of "practical tips" starting on page 287. Here's my summary of recommendations:</p>
<p>When someone is in an emotional state, use "the empathy reflex" (page 83):</p>
<ol>
<li>Describe the emotional changes you think you see.</li>
<li>Make a guess as to where those emotional changes come from.</li>
</ol>
<p>This is like a flip of "<a href="https://en.wikipedia.org/wiki/I-message">I statements</a>."</p>
<p>After empathy, the second major focus is on understanding emotion: talking about emotions, accepting them as natural, and (eventually) not being ruled by them.</p>
<p>There are lots of concrete recommendations:</p>
<ul>
<li>(During pregnancy) Gain appropriate weight, eat a balanced diet, exercise moderately, reduce stress.</li>
<li>Breast-feed for a year.</li>
<li>Talk to your baby a lot.</li>
<li>Praise effort, not IQ (encourage a growth mindset).</li>
<li>Use "authoritative parenting" which is "demanding and warm" and involves:<ul>
<li>Clear, consistent rules and rewards</li>
<li>Swift punishment</li>
<li>Rules that are explained</li>
</ul>
</li>
<li>(Your child should) Watch no TV before age two.</li>
<li>(Your child should) Exercise frequently.</li>
<li>(Your child should) Play a lot in open-ended ways, such as the "mature dramatic play" of <a href="https://toolsofthemind.org/">Tools of the Mind</a>, which may involve making a "play plan."</li>
<li>(Your child should) Learn sign language.</li>
<li>(Your child should) Study a musical instrument for ten years.</li>
</ul>
<p>Medina gives a very reasonable presentation of intelligence and IQ, including this historical tidbit on how we got the term "Intelligence Quotient" in the first place:</p>
<blockquote>
<p>"The score was the ratio of a child's mental age to his or her chronological age, multiplied by 100. So a 10-year-old who could solve problems normally solved only by 15-year-olds had an IQ of 150: (15/10) X 100." (page 95)</p>
</blockquote>
<p>On page 100 we get five ingredients of human intelligence stew, which I like:</p>
<ul>
<li>The desire to explore</li>
<li>Self-control</li>
<li>Creativity</li>
<li>Verbal communication</li>
<li>Interpreting nonverbal communication</li>
</ul>
<p>The associating, questioning, observing, experimenting, and networking from an old Harvard Business Review article on "<a href="https://hbr.org/2009/12/the-innovators-dna">Innovators DNA</a>" get woven in, and it's not awful.</p>
<p>Three last details I enjoyed:</p>
<ul>
<li>Episodic memory is a distinct kind of memory, and may be particularly useful for creative thinking. (page 106) Maybe consciously draw on it when brainstorming?</li>
<li>There's a test of creativity called the <a href="https://en.wikipedia.org/wiki/Torrance_Tests_of_Creative_Thinking">Torrance Tests of Creative Thinking</a>. (page 107) It's mostly things like "how many uses can you come up with for X?"</li>
<li>There's a Harvard-developed test of morality, now at <a href="http://www.moralsensetest.com/">moralsensetest.com</a>. (page 222) It's mostly variations on <a href="https://en.wikipedia.org/wiki/Trolley_problem">the trolley problem</a>.</li>
</ul>
<p><img alt="Brain Rules for Baby cover" src="cover.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20181209-brain_rules_for_baby/</link>
<guid>http://planspace.org/20181209-brain_rules_for_baby/</guid>
<pubDate>Sun, 09 Dec 2018 12:00:00 -0500</pubDate>
</item>
<item>
<title>Is it worth doing even if it fails?</title>
<description><![CDATA[

<p>Many things can "succeed" or "fail." Avoid anything that isn't worth
doing even if it fails.</p>
<p>If two tasks each have a probability of success and payout, but one is
worth doing regardless of success and one is only about payout, it
almost doesn't matter what the probabilities and payouts are. One
guarantees a good use of time.</p>
<p>This filter also averts unhealthy fixation on "winning" and excessive
disappointment with "losing." It lets you focus on the task itself,
which is the right focus.</p>
<p>You can't always choose what you have to do. But you can try to find
or create value even in endeavors you wouldn't otherwise choose.</p>
<p>Asking "Is it worth doing even if it fails?" encourages healthy things
like sports, discourages antisocial things like fraud, and provides a
positive direction to move everything in between. I think it's a
useful question.</p>    
    ]]></description>
<link>http://planspace.org/20181204-worth_doing_even_if_it_fails/</link>
<guid>http://planspace.org/20181204-worth_doing_even_if_it_fails/</guid>
<pubDate>Tue, 04 Dec 2018 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
