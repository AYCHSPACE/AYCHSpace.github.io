<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>plan âž” space</title>
    <link>http://planspace.org/</link>
    <description>plan space from outer nine</description>
    <language>en-us</language>
    <atom:link href="http://planspace.org/rss.xml" rel="self" type="application/rss+xml" />
<item>
<title>Simple Regression with a TensorFlow Estimator</title>
<description><![CDATA[

<p>With TensorFlow 1.1, the <a href="https://www.tensorflow.org/api_guides/python/contrib.learn#estimators">Estimator</a> API is now at <code>tf.estimator</code>. A number of "canned estimators" are <a href="https://www.tensorflow.org/extend/estimators">at</a> <code>tf.contrib.learn</code>. This higher-level API bakes in some best practices and makes it much easier to do a lot quickly with TensorFlow, similar to using APIs available in other languages.</p>
<hr>
<h2>Data</h2>
<p>This example will use the very simple <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/">US Presidential Party and GDP Growth dataset</a>: <a href="/20170505-simple_dataset_us_presidential_party_and_gdp_growth/president_gdp.csv">president_gdp.csv</a>.</p>
<p>The regression problem will be to predict annualized percentage GDP growth from presidential party.</p>
<hr>
<h2>R</h2>
<p><a href="https://www.r-project.org/">R</a> is made for problems such as this, with an API that makes it quite easy:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; model = lm(growth ~ party, data)
&gt; predict(model, data.frame(party=c('R', 'D')))
##        1        2
## 2.544444 4.332857</code></pre>

<p>The dataset is very small, and we won't introduce a train/test split. Linear regression is just a way of calculating means: we expect our model to predict the mean GDP growth conditional on party. Annual GDP growth during Republican presidents has been about 2.5%, and during Democratic presidents about 4.3%.</p>
<hr>
<h2>sklearn</h2>
<p>Moving into Python, let's first read in the data and get it ready, using <a href="http://www.numpy.org/">NumPy</a> and <a href="http://pandas.pydata.org/">Pandas</a>.</p>
<pre><code class="language-python">import numpy as np
import pandas as pd

data = pd.read_csv('president_gdp.csv')
party = data.party == 'D'
party = np.expand_dims(party, axis=1)
growth = data.growth</code></pre>

<p>With R, we relied on automatic handling of categorical variables. Here we explicitly change the strings 'R' and 'D' to be usable in a model: Boolean values will become zeros and ones. We also adjust the <code>party</code> data shape to be one row per observation.</p>
<p>Tracking <a href="/20170321-various_tensorflow_apis_for_python/">TensorFlow Python APIs</a>, the Estimator API comes from TF Learn, which is inspired by <a href="http://scikit-learn.org/">scikit-learn</a>. Here's the regression with scikit:</p>
<pre><code class="language-python">import sklearn.linear_model

model = sklearn.linear_model.LinearRegression()
model.fit(X=party, y=growth)
model.predict([[0], [1]])
## array([ 2.54444444,  4.33285714])</code></pre>

<hr>
<h2>TensorFlow</h2>
<p>This will abuse the API a little to maximize comparability to the examples above; you'll see warnings when you run the code, which will be addressed in the next section.</p>
<pre><code class="language-python">import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col])</code></pre>

<p>Unlike with scikit, we need to specify the structure of our regressors when we instantiate the model object. This is done with FeatureColumns. There are several <a href="https://www.tensorflow.org/tutorials/wide#selecting_and_engineering_features_for_the_model">options</a>; <code>real_valued_column</code> is probably the simplest but others are useful for general categorical data, etc.</p>
<p>We're providing that data as a simple matrix, so it's important that we use the empty string <code>''</code> for <code>column_name</code>. If there is a substantial <code>column_name</code>, we'll have to provide data in dictionaries with column names as keys.</p>
<pre><code class="language-python">model.fit(x=party, y=growth, steps=1000)
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>TensorFlow needs to be told how many steps of gradient descent to run, or it will keep going indefinitely, without additional configuration. A thousand iterations gets very close to the results achieved with R and with scikit.</p>
<p>There are a lot of things that <code>LinearRegressor</code> takes care of. In this code, we did not have to explicitly:</p>
<ul>
<li>Create any TensorFlow variables.</li>
<li>Create any Tensorflow ops.</li>
<li>Choose an optimizer or learning rate.</li>
<li>Create a TensorFlow session.</li>
<li>Run ops in a session.</li>
</ul>
<p>This API also does a lot more than the R or scikit examples above, and allows for even more extensions.</p>
<hr>
<h2>TensorFlow Extensions</h2>
<p>The Estimator API does a lot by default, and allows for a lot more optionally.</p>
<p>First, there is a <code>model_dir</code>. Above, TensorFlow automatically used a temporary directory. It's nicer to explicitly choose a <code>model_dir</code>.</p>
<pre><code class="language-python">model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')</code></pre>

<p>The <code>model_dir</code> is used for two main purposes:</p>
<ul>
<li>Saving TensorBoard summaries (log info)</li>
<li>Saving model checkpoints</li>
</ul>
<h3>Automatic TensorBoard</h3>
<p><a href="/20170430-tensorflows_queuerunner/">Like</a> an <code>input_producer</code>, an Estimator automatically writes information for TensorBoard. To check them out, point TensorBoard at the <code>model_dir</code> and browse to <code>localhost:6006</code>.</p>
<pre><code class="language-bash">$ tensorboard --logdir tflinreg</code></pre>

<p>For the example above, we get the model graph and two scalar summaries.</p>
<p>Here's what was was constructed in the TensorFlow graph for our <code>LinearRegressor</code>:</p>
<p><img alt="graph" src="img/graph.png"></p>
<p>In the scalar summaries, we get a measure of how fast the training process was running, in global steps per second:</p>
<p><img alt="steps per second" src="img/steps_per_sec.png"></p>
<p>The variation in speed shown here is not particularly meaningful.</p>
<p>And we get the training loss:</p>
<p><img alt="loss" src="img/loss.png"></p>
<p>We didn't really need to train for a full thousand steps.</p>
<p>By default, summaries are generated every 100 steps, but this can be set via <code>save_summary_steps</code> in a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>, along with several other settings.</p>
<p>Further customization, with support for additional metrics, validation on separate data, and even automatic early stopping, is available with <a href="https://www.tensorflow.org/get_started/monitors">ValidationMonitor</a>.</p>
<h3>Automatic Model Save/Restore</h3>
<p>After training for 1,000 steps above, TensorFlow saved the model to the <code>model_dir</code>. If we point to the same <code>model_dir</code> again in a new Python session, the model will be automatically restored from that checkpoint.</p>
<pre><code class="language-python">import numpy as np
import tensorflow as tf

party_col = tf.contrib.layers.real_valued_column(column_name='')
model = tf.contrib.learn.LinearRegressor(feature_columns=[party_col],
                                         model_dir='tflinreg')
list(model.predict(np.array([[0], [1]])))
## [2.5422058, 4.3341689]</code></pre>

<p>For more control over how often and when checkpoints are saved, see <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig"><code>RunConfig</code></a>.</p>
<h3>Using input functions</h3>
<p>Above, training data was provided via <code>x</code> and <code>y</code> arguments, which is like how scikit works, but not really what TensorFlow Estimators should use.</p>
<p>The appropriate mechanism is to make an <a href="https://www.tensorflow.org/get_started/input_fn">input function</a> that returns the equivalents to <code>x</code> and <code>y</code> when called. The function is passed as the <code>input_fn</code> argument to <code>model.fit()</code>, for example.</p>
<p>This approach is flexible and makes it easy to avoid, for example, keeping track of separate data structures for data and labels.</p>
<h3>Distributed Training</h3>
<p>Among the <code>tf.contrib.learn</code> <a href="https://www.tensorflow.org/api_guides/python/contrib.learn">goodies</a> is <a href="https://www.tensorflow.org/api_docs/python/tf/contrib/learn/Experiment"><code>tf.contrib.learn.Experiment</code></a>, which works with an Estimator to help do distributed training. It looks like this one is still settling down, with a lot of deprecated bits at the moment. I'm interested to see more about this. For now, you could check out a Google Cloud ML <a href="https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/iris/trainer/task.py">example</a> that works with <code>learn_runner</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</link>
<guid>http://planspace.org/20170506-simple_regression_with_a_tensorflow_estimator/</guid>
<pubDate>Sat, 06 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Simple Dataset: US Presidential Party and GDP Growth</title>
<description><![CDATA[

<p>Here's a simple <a href="president_gdp.csv">dataset</a> for use in examples. It's taken from the <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">online appendix</a> to <a href="https://www.aeaweb.org/articles?id=10.1257/aer.20140913">Presidents and the US Economy: An Econometric Exploration</a> by Blinder and Watson.</p>
<p><a href="president_gdp.csv"><code>president_gdp.csv</code></a></p>
<p>The fields are:</p>
<ul>
<li><code>term</code>: A short text description of the presidential term, like "Reagan 2".</li>
<li><code>party</code>: The political party of the presidency, either "D" for the <a href="https://en.wikipedia.org/wiki/Democratic_Party_(United_States)">Democratic Party</a> or "R" for the <a href="https://en.wikipedia.org/wiki/Republican_Party_(United_States)">Republican Party</a>.</li>
<li><code>growth</code>: The average annualized growth in US Gross Domestic Product (<a href="https://en.wikipedia.org/wiki/Gross_domestic_product">GDP</a>) for that presidential term, expressed as percentage points.</li>
</ul>
<p>For more details, please see the <a href="http://pubs.aeaweb.org/doi/pdfplus/10.1257/aer.20140913">paper</a> and <a href="https://assets.aeaweb.org/assets/production/articles-attachments/aer/app/10604/20140913_app.pdf">appendix</a>.</p>
<p>The only changes I've made are to order the data chronologically and put it in the convenient CSV format.</p>
<p>For example, in <a href="https://www.r-project.org/">R</a>, you can do the following:</p>
<pre><code class="language-r">&gt; data = read.csv('president_gdp.csv')
&gt; lm(growth ~ party, data)
&gt; t.test(growth ~ party, data)</code></pre>

<p>Because the <a href="president_gdp.csv">dataset</a> is so small, I'll also display it as spaced-out text here:</p>
<pre><code>term,             party,  growth
Truman,               D,    6.57
Eisenhower 1,         R,    2.72
Eisenhower 2,         R,    2.26
Kennedy-Johnson,      D,    5.74
Johnson 2,            D,    4.95
Nixon 1,              R,    3.57
Nixon-Ford,           R,    1.97
Carter,               D,    3.56
Reagan 1,             R,    3.12
Reagan 2,             R,    3.89
G.H.W. Bush,          R,    2.05
Clinton 1,            D,    3.53
Clinton 2,            D,    4.00
G.W. Bush 1,          R,    2.78
G.W. Bush 2,          R,    0.54
Obama 1,              D,    1.98</code></pre>    
    ]]></description>
<link>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</link>
<guid>http://planspace.org/20170505-simple_dataset_us_presidential_party_and_gdp_growth/</guid>
<pubDate>Fri, 05 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Canned Models with Keras in TensorFlow</title>
<description><![CDATA[

<p>With TensorFlow 1.1, <a href="https://github.com/fchollet/keras">Keras</a> is now at <code>tf.contrib.keras</code>. With TensorFlow 1.2, it'll be at <code>tf.keras</code>. This is great for making new models, but we also get canned models previously found <a href="https://github.com/fchollet/deep-learning-models">outside core Keras</a>. It's so easy to classify images!</p>
<pre><code class="language-python">import tensorflow as tf

model = tf.contrib.keras.applications.ResNet50()</code></pre>

<p>This will automatically download trained weights for a model based on <a href="https://arxiv.org/abs/1512.03385">Deep Residual Learning for Image Recognition</a>. The weights are cached below your home directory, in <code>~/.keras/models/</code>.</p>
<p>Convenient image tools are also included. Let's use an <a href="https://github.com/ajschumacher/imagen/blob/master/imagen/n01882714_4157_koala_bear.jpg">image</a> of a koala from the <a href="https://github.com/ajschumacher/imagen">imagen</a> ImageNet subset.</p>
<p><img alt="original koala" src="n01882714_4157_koala_bear.jpg"></p>
<pre><code class="language-python">filename = 'n01882714_4157_koala_bear.jpg'
image = tf.contrib.keras.preprocessing.image.load_img(
    filename, target_size=(224, 224))</code></pre>

<p>This model can take input images that are 224 pixels on a side, so we have to make our image that size. We're just doing it by squishing, in this case.</p>
<p><img alt="smaller koala" src="smaller_koala.jpg"></p>
<p>We'll make that into an array that the model can take as input.</p>
<pre><code class="language-python">import numpy as np

array = tf.contrib.keras.preprocessing.image.img_to_array(image)
array = np.expand_dims(array, axis=0)</code></pre>

<p>Now we can classify the image!</p>
<pre><code class="language-python">probabilities = model.predict(array)</code></pre>

<p>We have one thousand probabilities, one for each class the model knows about. To interpret the result, we can use another helpful function.</p>
<pre><code class="language-python">tf.contrib.keras.applications.resnet50.decode_predictions(probabilities)
## [[(u'n01882714', u'koala', 0.99466419),
##   (u'n02497673', u'Madagascar_cat', 0.0013330306),
##   (u'n01877812', u'wallaby', 0.00085774728),
##   (u'n02137549', u'mongoose', 0.00063530984),
##   (u'n02123045', u'tabby', 0.00056512095)]]</code></pre>

<p>Great success! The model is highly confident that it's looking at a koala. Not bad.</p>
<p>It's pretty fun that this kind of super-easy access to quite good pre-trained models is now available all within the TensorFlow package. Just <code>pip install</code> and go!</p>
<hr>
<p>The thousand ImageNet categories this model knows about include some things that are commonly associated with people, but not a "person" class. Still, just for fun, what will <code>ResNet50</code> say about me?</p>
<pre><code class="language-python">## [[(u'n02883205', u'bow_tie', 0.3144455),
##   (u'n03787032', u'mortarboard', 0.059674311),
##   (u'n02992529', u'cellular_telephone', 0.049916871),
##   (u'n04357314', u'sunscreen', 0.048197504),
##   (u'n04350905', u'suit', 0.03481029)]]</code></pre>

<p>I guess I'll take it?</p>
<p><img alt="Aaron" src="aaron.jpg"></p>
<hr>
<p><strong>Notes:</strong></p>
<p>The model may have been trained on the very koala picture we're testing it with. I'm okay with that. Feel free to test your own koala pictures!</p>
<p>There's also another function, <code>resnet50.preprocess_input</code>, which in theory should help the model work better, but my tests gave seemingly worse results when using that pre-processing. It would be used like this:</p>
<pre><code class="language-python">array = tf.contrib.keras.applications.resnet50.preprocess_input(array)</code></pre>

<p>Keras in TensorFlow also contains <code>vgg16</code>, <code>vgg19</code>, <code>inception_v3</code>, and <code>xception</code> models as well, along the same lines as <code>resnet50</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</link>
<guid>http://planspace.org/20170502-canned_models_with_keras_in_tensorflow/</guid>
<pubDate>Tue, 02 May 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Sampling ImageNet</title>
<description><![CDATA[

<p><a href="http://image-net.org/">ImageNet</a> is a standard image dataset. It's pretty big; just the IDs and URLs of the images take over a gigabyte of text. I collected a fun <a href="https://github.com/ajschumacher/imagen">sampling</a> for small-scale purposes.</p>
<hr>
<p>ImageNet is distributed primarily as a text file of <a href="http://image-net.org/download-imageurls">image URLs</a>. The compressed file is 334 megabytes. The unpacked file is 1.1 gigabytes.</p>
<pre><code class="language-bash">$ wget http://image-net.org/imagenet_data/urls/imagenet_fall11_urls.tgz
$ tar zxvf imagenet_fall11_urls.tgz
$ wc fall11_urls.txt
##  14197122 28414665 1134662781 fall11_urls.txt
$ head -3 fall11_urls.txt
## n00004475_6590   http://farm4.static.flickr.com/3175/2737866473_7958dc8760.jpg
## n00004475_15899  http://farm4.static.flickr.com/3276/2875184020_9944005d0d.jpg
## n00004475_32312  http://farm3.static.flickr.com/2531/4094333885_e8462a8338.jpg</code></pre>

<p>The first field is an image ID. The part before the underscore is a WordNet ID, so the first image is of <code>n00004475</code>. What's that?</p>
<p>The mapping from WordNet ID to a brief text label can be downloaded from a link on the ImageNet <a href="http://image-net.org/download-API">API page</a>.</p>
<pre><code class="language-bash">$ wget http://image-net.org/archive/words.txt
$ wc words.txt
##   82114  302059 2655750 words.txt
$ head -3 words.txt
## n00001740   entity
## n00001930   physical entity
## n00002137   abstraction, abstract entity</code></pre>

<p>There are 82,114 WordNet IDs. Now we can decode the one we're interested in.</p>
<pre><code class="language-bash">$ grep n00004475 words.txt
## n00004475    organism, being</code></pre>

<p>So the first picture in ImageNet is of an "organism, being". What does such a thing look like?</p>
<p><img alt="organism, being" src="img/n00004475_6590.jpg"></p>
<p>There are eight examples of "organism, being" and two of the others are cats.</p>
<p>I think 82,114 categories is too many to try to sample randomly from, for my purposes. I'll use the <a href="http://image-net.org/challenges/LSVRC/2017/browse-det-synsets">200 categories</a> specified for the <a href="http://image-net.org/challenges/LSVRC/2017/">ILSVRC2017</a> object detection <a href="http://image-net.org/challenges/LSVRC/2017/#det">challenge</a>.</p>
<pre><code class="language-bash">wget -O 200words.html http://image-net.org/challenges/LSVRC/2017/browse-det-synsets</code></pre>

<p>I used Emacs to pull out the 200 WordNet IDs and convenient extra-short descriptions, saved in <a href="200words.csv">200words.csv</a>. The script <a href="make_urls_subset.py">make_urls_subset.py</a> produces <a href="200words100urls.csv">200words100urls.csv</a> with 100 random URLs for each of the categories. Finally, <a href="get_fives.py">get_fives.py</a> downloads five working JPGs for each category. A couple came back with "missing" images, so I manually replaced those with others from the list.</p>
<p>The results are packaged up on <a href="https://github.com/">GitHub</a> at <a href="https://github.com/ajschumacher/imagen">ajschumacher/imagen</a> and feature such beauties as <a href="img/n02118333_27_fox.jpg">n02118333_27_fox.jpg</a>.</p>
<p><img alt="n02118333_27_fox.jpg" src="img/n02118333_27_fox.jpg"></p>    
    ]]></description>
<link>http://planspace.org/20170430-sampling_imagenet/</link>
<guid>http://planspace.org/20170430-sampling_imagenet/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow's QueueRunner</title>
<description><![CDATA[

<p>A TensorFlow <a href="https://www.tensorflow.org/programmers_guide/threading_and_queues#queuerunner">QueueRunner</a> helps to feed a TensorFlow <a href="http://planspace.org/20170327-tensorflow_and_queues/">queue</a> using threads which are optionally managed with a TensorFlow <a href="http://planspace.org/20170324-the_tensorflow_coordinator_for_python_threading/">Coordinator</a>. QueueRunner objects can be used directly, or via higher-level APIs, which also offer automatic TensorBoard summaries.</p>
<pre><code class="language-python">import tensorflow as tf
session = tf.Session()</code></pre>

<hr>
<h2>QueueRunner Directly</h2>
<p>To use a QueueRunner you need a TensorFlow queue and an op that puts a new thing in the queue every time that op is evaluated. Typically, such an op will itself involve a queue, which is a bit of a tease. To avoid that circularity, this example will use random numbers.</p>
<pre><code class="language-python">queue = tf.FIFOQueue(capacity=10, dtypes=[tf.float32])
random_value_to_enqueue = tf.random_normal([])  # shape=[] means a single value
enqueue_op = queue.enqueue(random_value_to_enqueue)
random_value_from_queue = queue.dequeue()</code></pre>

<p>At this point if you evaluate <code>random_value_from_queue</code> in the session it will block, because nothing has been put in the queue yet.</p>
<pre><code class="language-python">queue_runner = tf.train.QueueRunner(queue, [enqueue_op])</code></pre>

<p>Still nothing has been enqueued, but <code>queue_runner</code> stands ready to make threads that will do the enqueuing.</p>
<p>If you put more enqueue ops in the list, or the same one multiple times, more threads will be started when things get going.</p>
<pre><code class="language-python">coordinator = tf.train.Coordinator()
threads = queue_runner.create_threads(session, coord=coordinator, start=True)</code></pre>

<p>Using <code>start=True</code> means we won't have to call <code>.start()</code> for each thread ourselves.</p>
<pre><code class="language-python">&gt;&gt;&gt; len(threads)
## 2</code></pre>

<p>There are two threads running: one for handling coordinated shutdown, and one for the enqueue op.</p>
<p>Now at last we can get at random values from the queue!</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(random_value_from_queue)
## 0.69283932
&gt;&gt;&gt; session.run(random_value_from_queue)
## -0.53802371</code></pre>

<p>The feeding thread will try to keep the queue at capacity, which was set to 10, so there should always be more items available to dequeue.</p>
<p>Since we used a coordinator, we can shut the threads down nicely.</p>
<pre><code class="language-python">coordinator.request_stop()
coordinator.join(threads)</code></pre>

<hr>
<h2>QueueRunner with Higher-Level APIs</h2>
<p>It's possible to work with QueueRunner directly, as shown above, but it's easier to use higher-level TensorFlow APIs that themselves use QueueRunner.</p>
<p>It's common for TensorFlow queue-chains to start with a list of filenames (sometimes a list of just one filename) to read data from. The <code>string_input_producer</code> function makes a queue using provided strings.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'])
letter_from_queue = queue.dequeue()</code></pre>

<p>This is a <code>FIFOQueue</code>, just as before, but notice we don't have an enqueue op for it. Like many things in <code>tf.train</code>, here TensorFlow has already done some work for us. A QueueRunner has already been made, and it was added to the <code>queue_runners</code> collection.</p>
<pre><code class="language-python">&gt;&gt;&gt; tf.get_collection('queue_runners')
## [&lt;tensorflow.python.training.queue_runner_impl.QueueRunner object at 0x121ee2c10&gt;]</code></pre>

<p>You could access and run that QueueRunner directly, but <code>tf.train</code> makes things easier.</p>
<!--

Finally, here's a place where it would be useful to be
doing a session context manager!

-->

<pre><code class="language-python">coordinator.clear_stop()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p><code>tf.train.start_queue_runners</code> automatically starts the threads.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'</code></pre>

<p>By default, the queue will go through the original items multiple times, or multiple epochs, and shuffle the order of strings within an epoch.</p>
<p>Limiting the number of epochs uses a local variable, which must be initialized.</p>
<pre><code class="language-python">queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, num_epochs=1)
letter_from_queue = queue.dequeue()
initializer = tf.local_variables_initializer()
session.run(initializer)
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)</code></pre>

<p>Now the QueueRunner will close the queue when there isn't anything more to put in it, so the dequeue op will eventually give an <code>OutOfRangeError</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(letter_from_queue)
## 'a'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'b'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'c'
&gt;&gt;&gt; session.run(letter_from_queue)
## 'd'
&gt;&gt;&gt; session.run(letter_from_queue)
## OutOfRangeError</code></pre>

<hr>
<h2>Automatic TensorBoard Summaries</h2>
<p>A single-epoch queue will be helpful for illustrating an interesting thing about <code>tf.train.string_input_producer</code>: it automatically adds a TensorBoard summary to the graph.</p>
<p><img alt="shock" src="img/shock_or_something.jpg"></p>
<p>It's nice to have direct control over every detail of your program, but the conveniences of higher-level APIs are also pretty nice. The summary that gets added is a scalar summary representing the percent full that the queue is.</p>
<p><img alt="Doctor Strangelog" src="img/strangelog.jpg"></p>
<pre><code class="language-python">tf.reset_default_graph()  # Starting queue runners will fail if a queue is
session = tf.Session()    # closed, so we need to clear things out.
queue = tf.train.string_input_producer(['a', 'b', 'c', 'd'],
                                       shuffle=False, capacity=4, num_epochs=1)
letter_from_queue = queue.dequeue()
summaries = tf.summary.merge_all()
summary_writer = tf.summary.FileWriter('logs')
initializer = tf.local_variables_initializer()
session.run(initializer)
coordinator = tf.train.Coordinator()
threads = tf.train.start_queue_runners(sess=session, coord=coordinator)
for i in range(4):
    summary_writer.add_summary(session.run(summaries), i)
    session.run(letter_from_queue)
summary_writer.add_summary(session.run(summaries), 4)
summary_writer.flush()  # Ensure everything is written out.</code></pre>

<p>After opening up TensorBoard with <code>tensorboard --logdir=logs</code> and going to <code>http://localhost:6006/</code> and and turning off plot smoothing, you'll see this:</p>
<p><img alt="log" src="img/fraction_full.png"></p>
<p>This shows that the queue, with capacity four, started 100% full and then every time something was dequeued from it it became 25 percentage points less full until it was empty.</p>
<p>For this example, the queue wasn't being refilled, so we knew it would become less and less full. But if you're reading data into an input queue that you expect to keep full, it's a great diagnostic to be able to check how full it actually is while things are running, to find find out if loading data is a bottleneck.</p>
<p>The automatic TensorBoard logging here is also a nice first taste of all the things that happen with even higher-level TensorFlow APIs.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170430-tensorflows_queuerunner/</link>
<guid>http://planspace.org/20170430-tensorflows_queuerunner/</guid>
<pubDate>Sun, 30 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>From Behaviorist to Constructivist AI</title>
<description><![CDATA[

<p><a href="https://en.wikipedia.org/wiki/B._F._Skinner">B. F. Skinner</a> might be satisfied that neural networks achieve intelligence when they perform tasks well. This behaviorist perspective leads to misunderstandings of current technology and limits development toward systems that think. Pervasive epistemological confusion about categories is one example. In general, a constructivist approach will become necessary for advanced machine learning and artificial intelligence.</p>
<p><img alt='bird in operant conditioning chamber or "Skinner box"' src="img/skinner_box.jpg"></p>
<p>Training a neural network by <a href="https://en.wikipedia.org/wiki/Backpropagation">backpropagating</a> from a loss function is a lot like <a href="https://en.wikipedia.org/wiki/Operant_conditioning">operant conditioning</a>. Error becomes punishment. The objective and result of eliciting particular behavior is the same whether you're doing object detection <a href="https://motherboard.vice.com/en_us/article/america-secretly-tried-to-destroy-totalitarianism-with-pigeons">with a pigeon</a> in a <a href="https://en.wikipedia.org/wiki/Operant_conditioning_chamber">Skinner box</a> or with a convolutional neural network.</p>
<p>Little could be less like real intelligence than blurting out a name for every object you see. Little could better epitomize behaviorist stimulus-response. This is the intelligence of the <a href="https://en.wikipedia.org/wiki/ImageNet">ImageNet</a> classifiers that popularized deep learning.</p>
<p>These classifiers really shouldn't be anthropomorphized. A cat/dog classifier is not thinking about cats and dogs. An engineer designed the network with a cat neuron and a dog neuron, and got them to light them up as desired.</p>
<p>An image classifier has continuous input and categorical output. The categories are specified by design. This is clearly a limitation on the output side, and a very different limitation from any constraint on the input side in image resolution or color space. A cat/dog classifier cannot say anything other than cat and dog.</p>
<p>One could argue that this output is not strictly categorical because it might be read, for example, as 95% cat and 5% dog, but this does not undo the designed categories, and this sort of non-<a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac</a> enhancement is not generally provided when systems take categorical input.</p>
<p>Language models are frequently categorically constrained in both input and output. At the word level, this means a model can't deal with words it's never seen before. This leads to approaches like <a href="https://research.googleblog.com/2016/09/a-neural-network-for-machine.html">Google's neural machine translation</a> falling back to <a href="https://arxiv.org/abs/1508.07909">subword units</a> for rare words. But even if a language model goes to the character level, this is still a categorical constraint, and a system trained on "e" could be perfectly blind to "&#233;."</p>
<p>Whether categories are imposed on the input or output side, they make it obvious that the system is limited. The system cannot handle categories not specified in the design.</p>
<p>Categorical input is also foreign to humans. It would be like having a separate sense for every category. Instead of feeling "warm" or "not-warm", for instance, you could feel "cat" or "not-cat" and "dog" or "not-dog," and however many more. But if you didn't have a sense for "aardvark," you would be congenitally blind to "aardvark."</p>
<p>Continuous sensor data, like images and audio, is much more interesting and analogous to the human experience. There are still limitations - for example, you don't really see the edges of your field of view, and you can't really imagine what sensing magnetism would be like - but unstructured input provides a starting point for forming gestalt perceptions.</p>
<p><img height="240" alt="ouija board" src="img/ouija_board.jpeg"></p>
<p><a href="https://en.wikipedia.org/wiki/Word_embedding">Word embeddings</a>, or word vectors, might seem to handle the categorical problem, but this is largely misdirection. Word embeddings are representations of words that have fixed dimensionality, so that a language system no longer needs a separate input for every possible word, but only a separate input for every dimension of the word embeddings. There might be 50,000 words, but only 200 or 300 dimensions for an embedding.</p>
<p>Using word embeddings doesn't solve the categorical problem; it just pushes the problem to the embeddings. A system can still only handle words that embeddings have been generated for.</p>
<p>Word embeddings are useful in that they are representations of words that tend to give good results when used as input to various algorithms. In the same way, convolutional classifiers learn representations of images that tend to give good results when used as input to other algorithms. In both cases, having a good representation is useful. In both cases, people may or may not find the representations interpretable.</p>
<p>There tends to be <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/3/yoshua-bengio-machines-that-dream">excitement</a> about the meaningfulness of word embeddings when people amuse themselves with arithmetic like <em>king - man + woman = queen</em>, or make visualizations with reasonable-seeming clusters. But the understanding that's happening here is happening in the people; having nice representations does not mean that a system has achieved understanding.</p>
<p>It seems important that an intelligent system should be able to develop internal concepts without those concepts being built into the system's design, so there was interest when Google <a href="https://googleblog.blogspot.com/2012/06/using-large-scale-brain-simulations-for.html">found</a> a cat neuron in a 2011 system.</p>
<p><img height="240" src="img/google_cat.jpg" alt="Google cat neuron"></p>
<p>The system was trained to take an image as input and generate the same image as output. This is easy for a computer to do just by copying, so to make it interesting you have to put restrictions on the flow of information from input to output. The restricted system learns good representations for the images. Then, by testing lots of images with and without cats, Google found one point in the system that tended to respond positively to cats and negatively to other things.</p>
<p>The temptation is to declare that the system formed an idea of cats. You could just as well say that a mold formed an idea of its cast.</p>
<p>Visual systems learn many useful internal representations. For example, they learn oriented edge detectors. Humans have these too. It should be clear that in neither case is there an idea of an oriented edge. The Google researchers were correct in their <a href="https://arxiv.org/abs/1112.6209">paper</a>'s title when they said that their system learned high-level features rather than concepts.</p>
<p>More recently, <a href="https://openai.com/">OpenAI</a>'s <a href="https://blog.openai.com/unsupervised-sentiment-neuron/">unsupervised sentiment neuron</a> is another case of humans interpreting neurons. The model is categorical at input and output on the character level, learning to predict the next character in text. OpenAI used all 4,096 dimensions of their learned representation, but their title comes from noticing that just one of those dimensions captured a lot of sentiment-related information.</p>
<p>Word embeddings are patterns of activation over perhaps 300 neurons. They are <a href="http://www.bcp.psych.ualberta.ca/~mike/Pearl_Street/Dictionary/contents/D/distributed.html">distributed representations</a>. A cat neuron or sentiment neuron, on the other hand, is in line with the implausible "<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">unit/value principle</a>" that a single neuron represents a single concept.</p>
<p>It is easy to think that words <em>are</em> categories, and so a distributed representation of a word is a distributed representation of a category. With images, it's clear that one picture of a cat is different from another picture of a cat. But words are not categories. The difference between words and images is principally one of cardinality (there are many more images than words) and composibility (images more easily contain many things). But just as a word vector for <em>cat</em> is close to a word vector for <em>kitty</em>, distributed representations for pictures of cats should also be close to one another, and the Google system could just as well have been mined for distributed representations. Perhaps it is the human desire to categorize that makes us comfortable with multi-dimensional representations when we've provided categories in advance, but look for single-dimensional representations when we haven't. (Or maybe it's just easier.)</p>
<p>Regardless of whether distributed or unit representations are better, having a representation does not imply thought. These representations flash through their networks, coming before the result like Pavlovian slobber. This is not to say these representations couldn't be used in a system that thinks, but that current usage is too limited. One thing that's missing is state that develops over time.</p>
<p>Sequence models (such as the sentiment neuron example) introduce a limited kind of time-awareness, and a kind of memory. There could be something here (<a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/9/ilya-sutskever-unsupervised-learning-attention-and-other-mysteries">attention</a> in particular is interesting) but most <a href="https://www.oreilly.com/ideas/the-future-of-machine-intelligence/page/10/oriol-vinyals-sequence-to-sequence-machine-learning">usage</a> still seems to be learning representations and doing encoding to and decoding from these representations.</p>
<p>To be clear: Good representations are useful, whether or not they are utilized for anything like higher-level thought. But it seems unlikely that conventional models used in supervised or unsupervised learning will spontaneously invent higher-level thought, no matter how good their representations are.</p>
<p>By its name, <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> seems purely behavioristic, but it also recognizes the idea of internal models, as illustrated in <a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Home.html">David Silver</a>'s taxonomy of reinforcement learning agents. These models are generally something like an agent's internal conception of the world around it.</p>
<p><img alt="taxonomy of reinforcement learning agents" src="img/rl_taxonomy.png"></p>
<p>Lots of reinforcement learning is model-free, and where there are internal models they are often heavily specified in advance or quite distant from what we think of as mental models. The idea, if not current implementations, is key. The behaviorist perspective is one of only inputs and outputs. Model-based reinforcement learning suggests, at least in spirit, a missing piece. Humans certainly aren't model-free, for example.</p>
<p>What the constructivist perspective adds is that it isn't enough to simply have an internal model. Intelligence entails building and working with new models as a part of problem-solving.</p>
<p>One view of building different models for different situations might be selectively enlisting parts of a large system for a given task. <a href="https://arxiv.org/abs/1701.08734">PathNet</a> tries to "discover which parts of the network to re-use for new tasks." This is interesting, but the focus on selecting a subset of wiring seems distant from the imaginative process of building a mental model, likely drawing on representations which may be distributed.</p>
<p>It becomes important to understand how a representation behaves. Say a system does have a cat neuron; can it reason about cats? The cat neuron can be excited, but this seems more like experiencing the <a href="https://en.wikipedia.org/wiki/Qualia">quale</a> of cat-ness than like imagining a cat. Experiencing an emotion is even more clearly not an abstraction, so the case of a sentiment neuron makes this distinction even clearer.</p>
<p>Imagining a cat might be an algebraic operation, in the sense that it posits an entity, a variable, which has cat properties or is a cat. In <a href="https://mitpress.mit.edu/books/algebraic-mind">The Algebraic Mind</a>, Gary Marcus argues that connectionist (neural network) models lack this kind of kind of ability.</p>
<p><img alt="The Algebraic Mind" src="img/algebraic_mind_cover.jpg"></p>
<p>An intelligent system should be able not only to represent things but to build and manipulate models composed from these representations.</p>
<p>For example, whether you like Chomsky or not, understanding a sentence seems like an algebraic procedure in the sense of apprehending values for variables like subject, verb, and object. The plug and play composability of noun phrases and the like also suggests a constructive mental process.</p>
<p>Or take the example of number: can a system perceive that a picture has three cats, as opposed to two cats? There's some depth here, as a system could represent two or three entities all of which are cats individually, or it could represent number concepts explicitly. It's hard to find current models that do either in a meaningful way.</p>
<p>The Algebraic Mind was published in 2001. There have been many advances in machine learning since 2001, but they largely haven't been advances toward reasoning. In this sense, Marcus thinks artificial intelligence is <a href="https://www.technologyreview.com/s/603945/is-artificial-intelligence-stuck-in-a-rut/">in a rut</a>.</p>
<p>If you take the view that machine learning is a type of statistics, then you may not care about any of this. But for machine learning as artificial intelligence, it may be that algebraic (or symbolic) considerations will be necessary.</p>
<p>A fair criticism is that if it isn't clear how to make direct progress in the constructivist direction, time is better spent advancing what have become traditional techniques. There is certainly value in this kind of advancement.</p>
<p>There may also be some reason for hope in <a href="https://deepmind.com/blog/differentiable-neural-computers/">differentiable neural computers</a> (DNCs) and related work. In some ways, the linking of DNC memory locations is like Marcus's proposal for representing structured data, and the DNC memory itself is something like Marcus's idea of registers. But it looks like DNCs work with categorical input and output, relying on it for seemingly algebraic task performance.</p>
<p>It seems likely that artificial general intelligence will use a composite approach. It may process visual input with convolutions. It may use distributed representations for internal concepts. It may access memory along the lines of a differentiable neural computer. It may have an attention mechanism that allows it to focus on the state of the external world one moment and its internal world the next. Combining all these ideas into a system that can come up with its own ideas is an intriguing project.</p>
<blockquote>
<p>&#8220;There is no reason, as yet, to be confident that an intermediate symbolic representation will not be required for modeling higher cognitive processes.&#8221; (<a href="http://onlinelibrary.wiley.com/doi/10.1207/s15516709cog0603_1/epdf">Feldman and Ballard, 1982</a>)</p>
</blockquote>    
    ]]></description>
<link>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</link>
<guid>http://planspace.org/20170429-from_behaviorist_to_constructivist_ai/</guid>
<pubDate>Sat, 29 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Everything in the Graph? Even Glob?</title>
<description><![CDATA[

<p>Programming with TensorFlow is largely building TensorFlow graphs. It can be like using Python to write a program for another computer: the TensorFlow runtime. If you want all your computation to be in the graph, you need to be able to express everything with TensorFlow ops.</p>
<p>It's not necessarily bad to do work outside the TensorFlow graph. Maybe there's a package that does exactly what you need for one part of your program. It could make sense to use TensorFlow for some things, and then pull off the graph and use the other package.</p>
<p>Advantages of staying inside the graph might include better performance, as you aren't moving data into and out of the TensorFlow runtime's internal formats. And if you're developing in Python but want to take your graph and ship it using TensorFlow serving, or perhaps use another language, anything outside the graph will need special attention.</p>
<p>The desire to stay in-graph leads to lots of ops existing that you might not think are essential for a machine learning library. This includes functionality for file formats like PNG and JPEG, queues and queue management, and even the lowly <code>glob</code>.</p>
<p><a href="https://en.wikipedia.org/wiki/Glob_(programming)"><code>glob</code></a> refers to patterns used for matching filenames. For example, in a shell, you might use a <code>glob</code> to list all the text files in a directory.</p>
<pre><code class="language-bash">$ ls *.txt
## one.txt        two.txt</code></pre>

<p>In Python, the standard library includes <a href="https://docs.python.org/3/library/glob.html"><code>glob</code></a> functionality.</p>
<pre><code class="language-python">&gt;&gt;&gt; import glob
&gt;&gt;&gt; glob.glob('*.txt')
## ['one.txt', 'two.txt']</code></pre>

<p>TensorFlow has a similar operation: <a href="https://www.tensorflow.org/api_docs/python/tf/matching_files"><code>tf.matching_files</code></a>.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; glob = tf.matching_files('*.txt')
&gt;&gt;&gt; session.run(glob)
## array(['./one.txt', './two.txt'], dtype=object)</code></pre>

<p>TensorFlow's globbing is limited in that it doesn't support wildcards in directory names, and it doesn't support the recursive globbing found in the <code>glob</code> of Python 3.5+.</p>
<p>Every time a <code>tf.matching_files</code> op is evaluated, it goes and reads filenames from disk. What's often seen instead is <a href="https://www.tensorflow.org/api_docs/python/tf/train/match_filenames_once"><code>tf.train.match_filenames_once</code></a>, which introduces an extra level of lazy evaluation in that it will execute <code>tf.matching_files</code> the first time it's evaluated, and then it caches the results, returning the same list of files when evaluated again even if the files on disk become different. Because this relies on a local variable for the cache (<a href="https://github.com/tensorflow/tensorflow/blob/a5b1fb8e56ceda0ee2794ee05f5a7642157875c5/tensorflow/python/training/input.py#L55">source</a>) you'll have to initialize.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; glob = tf.train.match_filenames_once('*.txt')
&gt;&gt;&gt; initializer = tf.local_variables_initializer()
&gt;&gt;&gt; session.run(initializer)
&gt;&gt;&gt; session.run(glob)
## array(['./one.txt', './two.txt'], dtype=object)</code></pre>

<p>With functionality like this, TensorFlow really subsumes a lot of things that could at least in theory be handled separately. The choice of whether to put any particular piece of computation into the TensorFlow graph or not remains up to developer.</p>
<p><img alt="glob" src="glob.png"></p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170428-everything_in_the_graph_even_glob/</link>
<guid>http://planspace.org/20170428-everything_in_the_graph_even_glob/</guid>
<pubDate>Fri, 28 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Sparse Tensors and TFRecords</title>
<description><![CDATA[

<p>When a matrix, array, or tensor has lots of values that are zero, it can be called <em>sparse</em>. You might want to represent the zeros implicitly with a <em>sparse representation</em>. TensorFlow has support for this, and the support extends to its TFRecords <code>Example</code> format.</p>
<p>Here is a sparse one-dimensional tensor:</p>
<pre><code class="language-python">[0, 7, 0, 0, 8, 0, 0, 0, 0]</code></pre>

<p>The tensor is sparse, in that it has a lot of zeros, but the representation is dense, in that all those zeros are represented explicitly.</p>
<p>A sparse representation of the same tensor will focus only on the non-zero values.</p>
<pre><code class="language-python">values = [7, 8]</code></pre>

<p>We have to also remember where those values occur, by their indices:</p>
<pre><code class="language-python">indices = [1, 5]</code></pre>

<p>The one-dimensional <code>indices</code> form will work with some methods, for this one-dimensional example, but in general indices have multiple dimensions, so it will be more consistent (and work everywhere) to represent <code>indices</code> like this:</p>
<pre><code class="language-python">indices = [[1], [5]]</code></pre>

<p>With <code>values</code> and <code>indices</code>, we don't have quite enough information yet. How many zeros are there to the right of the last value? We have to represent the dense shape of the tensor.</p>
<pre><code class="language-python">dense_shape = [9]</code></pre>

<p>These three things together, <code>values</code>, <code>indices</code>, and <code>dense_shape</code>, are a sparse representation of the tensor.</p>
<p>TensorFlow accepts lists of values and NumPy arrays to define dense tensors, and it returns NumPy arrays when dense tensors are evaluated. But what to do with sparse tensors? SciPy has <a href="https://docs.scipy.org/doc/scipy/reference/sparse.html">several</a> sparse matrix representations, but not a good match for TensorFlow's general sparse tensor form. So for sparse tensors, instead of reusing an existing Python class, TensorFlow provides <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensorValue"><code>tf.SparseTensorValue</code></a>. These are values that exist outside the TensorFlow graph, so they can be made without a <code>tf.Session</code>, for example.</p>
<pre><code class="language-python">tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[1], [5]], values=[7, 8], dense_shape=[9])</code></pre>

<p>Using <a href="https://www.tensorflow.org/api_docs/python/tf/SparseTensor"><code>tf.SparseTensor</code></a> puts that in the TensorFlow graph.</p>
<pre><code class="language-python">tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
## &lt;tensorflow.python.framework.sparse_tensor.SparseTensor at 0x11a4e0c10&gt;</code></pre>

<p>That <code>tf.SparseTensor</code> will be constant, since we specified all the pieces of it, and if you run it in a session, you'll get back the equivalent <code>tf.SparseTensorValue</code>.</p>
<p>TensorFlow has <a href="https://www.tensorflow.org/api_guides/python/sparse_ops">operations</a> specifically for working with sparse tensors, such as <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_matmul"><code>tf.sparse_matmul</code></a>. And you can change a sparse matrix to a dense one with <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_tensor_to_dense"><code>tf.sparse_tensor_to_dense</code></a>. These operations live in the graph, so they have to be run to see a result.</p>
<pre><code class="language-python">sparse = tf.SparseTensor(values=values, indices=indices, dense_shape=dense_shape)
dense = tf.sparse_tensor_to_dense(sparse)
session.run(dense)
## array([0, 7, 0, 0, 0, 8, 0, 0, 0], dtype=int32)</code></pre>

<p>Going from dense to sparse seems a little less <a href="http://stackoverflow.com/questions/39838234/sparse-matrix-from-a-dense-one-tensorflow">straightforward</a> at the moment, so let's continue assuming we already have the components of our sparse representation.</p>
<p>Going to more dimensions is quite natural. Here's a two-dimensional tensor with three non-zero values:</p>
<pre><code class="language-python">[[0, 0, 0, 0, 0, 7],
 [0, 5, 0, 0, 0, 0],
 [0, 0, 0, 0, 9, 0],
 [0, 0, 0, 0, 0, 0]]</code></pre>

<p>This can be represented in sparse form as:</p>
<pre><code class="language-python">indices = [[0, 5],
           [1, 1],
           [2, 4]]

values = [7, 5, 9]

dense_shape = [4, 6]

tf.SparseTensorValue(values=values, indices=indices, dense_shape=dense_shape)
## SparseTensorValue(indices=[[0, 5], [1, 1], [2, 4]], values=[7, 5, 9], dense_shape=[4, 6])</code></pre>

<p>Now, to represent this in a TFRecords <code>Example</code> requires a little bit of transformation. <a href="/20170323-tfrecords_for_humans/">TFRecords</a> only support lists of integers, floats, and bytestrings. The values are easily represented in one <code>Feature</code>, but to represent the <code>indices</code>, each dimension will need its own <code>Feature</code> in the <code>Example</code>. The <code>dense_shape</code> isn't represented at all; that's left to be specified at parsing.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'index_0': tf.train.Feature(int64_list=tf.train.Int64List(value=[0, 1, 2])),
    'index_1': tf.train.Feature(int64_list=tf.train.Int64List(value=[5, 1, 4])),
    'values': tf.train.Feature(int64_list=tf.train.Int64List(value=[7, 5, 9]))
}))
my_example_str = my_example.SerializeToString()</code></pre>

<p>This TFRecord sparse representation can then be <a href="/20170426-parsing_tfrecords_inside_the_tensorflow_graph/">parsed inside the graph</a> as a <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a>.</p>
<pre><code class="language-python">my_example_features = {'sparse': tf.SparseFeature(index_key=['index_0', 'index_1'],
                                                  value_key='values',
                                                  dtype=tf.int64,
                                                  size=[4, 6])}
serialized = tf.placeholder(tf.string)
parsed = tf.parse_single_example(serialized, features=my_example_features)
session.run(parsed, feed_dict={serialized: my_example_str})
## {'sparse': SparseTensorValue(indices=array([[0, 5], [1, 1], [2, 4]]),
##                              values=array([7, 5, 9]),
##                              dense_shape=array([4, 6]))}</code></pre>

<p>Support for multi-dimensional sparse features seems to be new in TensorFlow 1.1, and TensorFlow gives this warning when you use <code>SparseFeature</code>:</p>
<pre><code>WARNING:tensorflow:SparseFeature is a complicated feature config
                   and should only be used after careful consideration
                   of VarLenFeature.</code></pre>

<p><code>VarLenFeature</code> doesn't support real sparsity or multi-dimensionality though; it only supports "ragged edges" as in the case when one example has three elements and the next has seven, for example.</p>
<p>It is a little awkward to put together a sparse representation for TFRecords, but it does give you a lot of flexibility. To put a point on it, I don't know what you can do with a <code>SequenceExample</code> that you can't do with a regular <code>Example</code> using all of <code>FixedLenFeature</code>, <code>VarLenFeature</code>, and <code>SparseFeature</code>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170427-sparse_tensors_and_tfrecords/</link>
<guid>http://planspace.org/20170427-sparse_tensors_and_tfrecords/</guid>
<pubDate>Thu, 27 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Parsing TFRecords inside the TensorFlow Graph</title>
<description><![CDATA[

<p>You can parse TFRecords using the standard protocol buffer <code>.FromString</code> method, but you can also parse them inside the TensorFlow graph.</p>
<p>The examples here assume you have in memory the serialized Example <code>my_example_str</code> and SequenceExample <code>my_seq_ex_str</code> from <a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a>. You could create them, or read them from <a href="/20170323-tfrecords_for_humans/my_example.tfrecords">my_example.tfrecords</a> and <a href="/20170323-tfrecords_for_humans/my_seq_ex.tfrecords">my_seq_ex.tfrecords</a>. That loading could be via <code>tf.python_io.tf_record_iterator</code> or via <code>tf.TFRecordReader</code> following the pattern shown in <a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a>.</p>
<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/parse_single_example"><code>tf.parse_single_example</code></a> decoder works like <code>tf.decode_csv</code>: it takes a string of raw data and turns it into structured data, based on the options it's created with. The structured data it turns it into is <em>not</em> a protocol buffer message object, but a dictionary that is hopefully easier to work with.</p>
<pre><code class="language-python">import tensorflow as tf

serialized = tf.placeholder(tf.string)

my_example_features = {'my_ints': tf.FixedLenFeature(shape=[2], dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session = tf.Session()

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': array([5, 6]),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>The <code>shape</code> parameter is part of the schema we're defining. A <code>shape</code> of <code>[]</code> means a single element, so the result returned won't be in an array, as for <code>my_float</code>. The <code>shape</code> of <code>[1]</code> means an array containing one element, like for <code>my_bytes</code>. Within a Feature, things are always listed, so the choice of how to get a single element back out is decided by the choice of <code>shape</code> argument. A <code>shape</code> of <code>[2]</code> means a list of two elements, naturally enough, and there's no alternative.</p>
<p>The <code>dtype=object</code> is how NumPy works with strings.</p>
<p>When some feature might have differing numbers of values across records, they can all be read with <code>tf.VarLenFeature</code>. This distinction is made only when parsing. Records are made with however many values you put in; you don't specify <code>FixedLen</code> or <code>VarLen</code> when you're making an <code>Example</code>. So the <code>my_ints</code> feature just parsed as <code>FixedLen</code> can also be parsed as <code>VarLen</code>.</p>
<pre><code class="language-python">my_example_features = {'my_ints': tf.VarLenFeature(dtype=tf.int64),
                       'my_float': tf.FixedLenFeature(shape=[], dtype=tf.float32),
                       'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_example = tf.parse_single_example(serialized, features=my_example_features)

session.run(my_example, feed_dict={serialized: my_example_str})
## {'my_ints': SparseTensorValue(indices=array([[0], [1]]),
##                               values=array([5, 6]),
##                               dense_shape=array([2])),
##  'my_float': 2.7,
##  'my_bytes': array(['data'], dtype=object)}</code></pre>

<p>When parsing as a <code>VarLenFeature</code>, the result is a sparse representation. This can seem a little silly, because features here will always be dense from left to right. Early versions of TensorFlow <a href="https://github.com/tensorflow/tensorflow/issues/976">didn't</a> have the current behavior. But this sparseness is a mechanism by which TensorFlow can support non-rectangular data, for example when forming batches from multiple variable length features, or as seen next with a <code>SequenceExample</code>:</p>
<pre><code class="language-python">my_context_features = {'my_bytes': tf.FixedLenFeature(shape=[1], dtype=tf.string)}
my_sequence_features = {'my_ints': tf.VarLenFeature(shape=[2], dtype=tf.int64)}
my_seq_ex = tf.parse_single_sequence_example(
                serialized,
                context_features=my_context_features,
                sequence_features=my_sequence_features)

result = session.run(my_seq_ex, feed_dict={serialized: my_seq_ex_str})
result
## ({'my_bytes': array(['data'], dtype=object)},
##  {'my_ints': SparseTensorValue(
##                  indices=array([[0, 0], [0, 1], [1, 0], [1, 1], [1, 2]]),
##                  values=array([5, 6, 7, 8, 9]),
##                  dense_shape=array([2, 3]))})</code></pre>

<p>The result is a tuple of two dicts: the context data and the sequence data.</p>
<p>Since the <code>my_ints</code> sequence feature is parsed as a <code>VarLenFeature</code>, it's returned as a sparse tensor. This example has to be parsed as a <code>VarLenFeature</code>, because the two entries in <code>my_ints</code> are of different lengths (<code>[5, 6]</code> and <code>[7, 8, 9]</code>).</p>
<p>The way the <code>my_ints</code> values get combined into one sparse tensor is the same as the way it would be done when making a batch from multiple records each containing a <code>VarLenFeature</code>.</p>
<p>To make it clearer what's going on, we can look at the sparse tensor in dense form:</p>
<pre><code class="language-python">session.run(tf.sparse_tensor_to_dense(result[1]['my_ints']))
## array([[5, 6, 0],
##        [7, 8, 9]])</code></pre>

<p>The other option for parsing sequence features is <code>tf.FixedLenSequenceFeature</code>, which will work if each entry of the sequence feature is the same length. The result then is a dense tensor.</p>
<p>To parse multiple <code>Example</code> records in one op, there's <a href="https://www.tensorflow.org/versions/master/api_docs/python/tf/parse_example"><code>tf.parse_example</code></a>. This returns a dict with the same keys you'd get from parsing a single <code>Example</code>, with the values combining the values from all the parsed examples, in a batch-like fashion. There isn't a corresponding op for <code>SequenceExample</code> records.</p>
<p>More could be said about sparse tensors and TFRecords. The <a href="https://www.tensorflow.org/api_docs/python/tf/sparse_merge">tf.sparse_merge</a> op is one way to combine sparse tensors, similar to the combination that happened for <code>my_ints</code> in the <code>SequenceExample</code> above. And there's <a href="https://www.tensorflow.org/api_docs/python/tf/SparseFeature"><code>tf.SparseFeature</code></a> for parsing out general sparse features directly from TFRecords (better documentation in <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/parsing_ops.py">source</a>).</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170426-parsing_tfrecords_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 26 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>mystery.tfrecords</title>
<description><![CDATA[

<p>Here's a practical puzzle: what's in the file <a href="mystery.tfrecords"><code>mystery.tfrecords</code></a>?</p>
<hr>
<p>You don't need any more extra information specific to that file. Here are a few posts about the format and related issues, some of which might be helpful:</p>
<ul>
<li><a href="/20170323-tfrecords_for_humans/">TFRecords for Humans</a></li>
<li><a href="/20170330-tfrecords_via_proto/">TFRecords via Protocol Buffer Definitions</a></li>
<li><a href="/20170403-images_and_tfrecords/">Images and TFRecords</a></li>
<li><a href="/20170412-reading_from_disk_inside_the_tensorflow_graph/">Reading from Disk inside the TensorFlow Graph</a></li>
</ul>
<!--

Welcome to hidden additional notes! Finding this is a way of solving
the puzzle, I suppose...

Here's how `mystery.tfrecords` was made:

<pre><code class="language-python">import tensorflow as tf

with open('success.jpg') as f:
    success = f.read()

example = tf.train.Example(features=tf.train.Features(feature={
    'jpg': tf.train.Feature(bytes_list=tf.train.BytesList(value=[success]))
}))

example_str = example.SerializeToString()

with tf.python_io.TFRecordWriter('mystery.tfrecords') as writer:
    writer.write(example_str)</code></pre>

Here's one way to get the contents back out:

<pre><code class="language-python">reader = tf.python_io.tf_record_iterator('mystery.tfrecords')

examples = [tf.train.Example().FromString(example_str)
            for example_str in reader]
# Using `SequenceExample` rather than `Example` also works.

len(examples)  # 1
# So we know there's just one example in there.

example = examples[0]

example.features.feature.keys()  # 'jpg'
# If parsed as a SequenceExample, this would instead be:
# `example.context.feature.keys()`

# It should be clear from the 'jpg' key, but you can also check:
len(example.features.feature['jpg'].int64_list.value)  # 0
len(example.features.feature['jpg'].float_list.value)  # 0

len(example.features.feature['jpg'].bytes_list.value)  # 1

jpg = example.features.feature['jpg'].bytes_list.value[0]

# If you don't trust the key, you can check the magic number:
jpg[:2]  # '\xff\xd8'
# That's the JPG magic number, FFD8.

with open('success.jpg', 'wb') as f:
    f.write(jpg)</code></pre>

Success!

-->

<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170425-mystery.tfrecords/</link>
<guid>http://planspace.org/20170425-mystery.tfrecords/</guid>
<pubDate>Tue, 25 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow as Automatic MPI</title>
<description><![CDATA[

<p>TensorFlow raises the level of abstraction in distributed programs from message-passing to data structures and operations directly on those data structures. The difference is analogous to the difference between programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a> and programming in a high-level language. TensorFlow may not have every possible high-performance feature for cluster computing, but what it offers is compelling.</p>
<hr>
<h2>Message-Passing</h2>
<p>Leaving aside shared filesystems or directly accessed shared memory, systems that run across multiple computers have to communicate by sending messages to one another. A low-level approach might use <a href="https://en.wikipedia.org/wiki/Transmission_Control_Protocol">TCP</a> sockets directly. Some convenience could be obtained by using Remote Procedure Calls (<a href="https://en.wikipedia.org/wiki/Remote_procedure_call">RPC</a>).</p>
<p>The Message-Passing Interface (<a href="https://en.wikipedia.org/wiki/Message_Passing_Interface">MPI</a>) was created in the early 1990s to make distributed programming easier within the High Performance Computing (HPC) community. It standardized interfaces like <code>MPI_Send</code> and <code>MPI_Recv</code> to facilitate exchanging data between processes of a distributed system.</p>
<p>The <a href="https://en.wikipedia.org/wiki/Actor_model">actor model</a> also focuses on message-passing, as for example in <a href="https://en.wikipedia.org/wiki/Erlang_(programming_language)">Erlang</a> or with <a href="http://akka.io/">Akka</a> in Java or Scala.</p>
<p>Message-passing gives you fine-grained control over how nodes communicate. You can use message-passing to build up a variety of algorithms and new systems. For example, <a href="http://spark.apache.org/">Spark</a> was built in part with Akka agents for some time, before switching to an RPC-based implementation. TensorFlow also builds its own abstractions using message-passing.</p>
<hr>
<h2>Automatic Message-Passing with TensorFlow</h2>
<p>TensorFlow uses message-passing, but it's largely invisible to TensorFlow users. The TensorFlow API lets you say where data and operations should live, and TensorFlow automatically handles any necessary messaging.</p>
<p>Programming with explicit messages is like the low-level memory shifting necessary when programming in <a href="https://en.wikipedia.org/wiki/Assembly_language">assembly</a>. For example, here is some assembly pseudo-code:</p>
<pre><code>copy value from position `a` to register 1
copy value from position `b` to register 2
add register 1 and 2
copy result to position `c`</code></pre>

<p>In a high-level language, this could be:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>Similarly, with TensorFlow, you focus on the computation you want performed, and don't worry about how values may need to be moved around to make it happen. A distributed analog to the assembly above might look like this:</p>
<pre><code>send value of `a` from machine A to machine D
send value of `b` from machine B to machine D
add values on machine D
send result to machine C</code></pre>

<p>And with TensorFlow:</p>
<pre><code class="language-python">c = a + b</code></pre>

<p>At least, that's the ideal. TensorFlow may still need explicit direction on where things should be placed (<a href="https://www.tensorflow.org/api_docs/python/tf/device">with tf.device</a>) especially when multiple machines are involved. But that direction is more succinct and declarative than the full imperative detail of message-passing.</p>
<hr>
<h2>Faster Messages and Collective Communications</h2>
<p>TensorFlow's user API is essentially message-less, but TensorFlow still uses messages under the hood. The contents of the messages are serialized protocol buffers sent via <a href="http://www.grpc.io/">gRPC</a>, which uses <a href="https://en.wikipedia.org/wiki/HTTP/2">HTTP/2</a>.</p>
<p>A message-based approach can be made faster by using a faster transport, and by using faster algorithms. TensorFlow has some early support for both, but other frameworks may offer more. You may or may not care, as the simplicity of TensorFlow's approach may or may not outweigh possible performance boosts.</p>
<p>One easy transport speedup could come in hardware with NVIDIA's <a href="http://www.nvidia.com/object/nvlink.html">NVLink</a> interconnect, which is faster than <a href="https://en.wikipedia.org/wiki/PCI_Express">PCIe</a>. As far as I can tell NVLink won't require code changes, but it looks like it will be pretty rare: it may be that only some supercomputers (<a href="https://www.olcf.ornl.gov/summit/">Summit</a>, <a href="https://asc.llnl.gov/coral-info">Sierra</a>) will use NVLink, with IBM's <a href="https://en.wikipedia.org/wiki/IBM_POWER_microprocessors">POWER</a> processors.</p>
<p>Some clusters connect machines with <a href="https://en.wikipedia.org/wiki/InfiniBand">InfiniBand</a> rather than <a href="https://en.wikipedia.org/wiki/Ethernet">ethernet</a>. InfiniBand Remote Direct Memory Access (<a href="https://en.wikipedia.org/wiki/Remote_direct_memory_access">RDMA</a>) is supposed to be pretty fast.</p>
<p>Jun Shi at Yahoo contributed an <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/verbs/README.md">implementation</a>, <a href="https://github.com/tensorflow/tensorflow/pull/8943">merged</a> four days ago, that begins to let TensorFlow use RDMA for transport via IB verbs, in addition to gRPC.</p>
<p>MPI implementations often support InfiniBand, and an advantage of using MPI is that you can let that implementation worry about InfiniBand support without writing your own RDMA code. There's an <a href="https://github.com/tensorflow/tensorflow/pull/7710">open pull request</a> to give TensorFlow this kind of MPI integration.</p>
<p>On the algorithm side, there are <a href="https://computing.llnl.gov/tutorials/mpi/#Collective_Communication_Routines">collective communication</a> operations that can be optimized, and MPI implementations tend to be good at these. For example, <a href="http://research.baidu.com/bringing-hpc-techniques-deep-learning/">ring allreduce</a> is more efficient for syncing up model parameters than sending them all to a central parameter server and then sending them all back out. Baidu has a <a href="https://github.com/baidu-research/tensorflow-allreduce">fork</a> of TensorFlow that adds their <a href="https://github.com/baidu-research/baidu-allreduce">implementation</a> of ring allreduce using MPI at <a href="https://github.com/baidu-research/tensorflow-allreduce/tree/master/tensorflow/contrib/mpi"><code>tf.contrib.mpi</code></a>.</p>
<p>Outside the MPI world, the NVIDIA Collective Communications Library (<a href="https://devblogs.nvidia.com/parallelforall/fast-multi-gpu-collectives-nccl/">NCCL</a>) works with multiple GPUs on the same machine. TensorFlow has some support for this via <a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/nccl"><code>tf.contrib.nccl</code></a>.</p>
<p>Quite distinct from TensorFlow, the Facebook Incubator project <a href="https://github.com/facebookincubator/gloo">Gloo</a> focuses on collective communication algorithms and supports transport via TCP/IP and InfiniBand without MPI. Gloo is used by <a href="https://caffe2.ai/">Caffe2</a>; the Caffe2 example <a href="https://github.com/caffe2/caffe2/blob/master/caffe2/python/examples/resnet50_trainer.py">resnet50 trainer</a> uses Gloo's ring allreduce.</p>
<!--

Just for fun, here's an Akka implementation of allreduce that I found:
https://github.com/brianmartin/akka-allreduce
I believe that's more of a tree allreduce than a ring allreduce.

-->

<hr>
<h2>Single-Machine Parallelism</h2>
<p>The HPC community has an approach called "MPI everywhere" which means running one single-threaded MPI processes per CPU core. The simplicity of writing single-threaded programs with all parallelism via MPI is attractive, but it's not necessarily the best way to use a machine with a lot of cores and possibly GPUs.</p>
<p>The hybrid approach is to have parallelism both across multiple machines and within each individual machine, usually via threads. HPC folks seem to like to use <a href="https://en.wikipedia.org/wiki/OpenMP">OpenMP</a> for multithreading.</p>
<p>TensorFlow supports this kind of within-process parallelism. TensorFlow uses threads pretty freely internally, and you can add your own additional parallelism with Python's <a href="https://docs.python.org/3/library/threading.html">threading</a> or <a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a> libraries, among many possible options. Just as with MPI though, when programs are run by a cluster manager, resources may be restricted.</p>
<hr>
<h2>Code organization</h2>
<p>It isn't strictly required for either, but it seems most common to write MPI programs and distributed TensorFlow programs with one entry point that quickly specializes based on the particular role in the cluster for that node. As this implies, every invocation of the program has to know something about the cluster and its place in it.</p>
<hr>
<h2>Cluster Topology</h2>
<p>MPI implementations provide a mechanism in starting a group of processes by which every process knows how many there are in its group and its position or <em>rank</em> in the group. The rank 0 process, called the <em>root</em>, is generally special.</p>
<p>TensorFlow leaves the starting of distributed processes to a cluster manager like Kubernetes. It's common to have several task groups like <code>ps</code> and <code>worker</code>, with a given process having a task type and a number within that group.</p>
<p>Gloo is interestingly different in that individual processes may not start with knowledge of the cluster topology, but only with a reference to a central broker which could be a file on a shared filesystem or a Redis server. Each process checks in with and gets information about other members of the cluster via the central broker. Gloo calls this process <em>rendezvous</em>. As with MPI, processes find out about just one group of processes, and in fact Gloo can use MPI for its initial rendezvous setup.</p>
<hr>
<h2>See also</h2>
<ul>
<li><a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">TensorFlow as a Distributed Virtual Machine</a></li>
<li><a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a></li>
<li><a href="/20170411-distributed_mapreduce_with_tensorflow/">Distributed MapReduce with TensorFlow</a></li>
</ul>
<hr>
<p>Thanks to <a href="https://twitter.com/danfujita123">Dan Fujita</a> for suggesting more connections between TensorFlow and MPI, and sharing some MPI code that was helpful. Thanks also to <a href="https://twitter.com/ljdursi">Jonathan Dursi</a>, whose <a href="https://www.dursi.ca/">blog</a> I read with interest. And thanks to <a href="https://github.com/cliffwoolley">Cliff Woolley</a> of NVIDIA for <a href="https://github.com/NVIDIA/nccl/issues/86">clarifying</a> the meaning of NCCL.</p>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</link>
<guid>http://planspace.org/20170423-tensorflow_as_automatic_mpi/</guid>
<pubDate>Sun, 23 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Reading from Disk inside the TensorFlow Graph</title>
<description><![CDATA[

<p>As I've <a href="/20170312-use_only_what_you_need_from_tensorflow/">noted</a>, the TensorFlow input pipeline misleadingly <a href="https://www.tensorflow.org/programmers_guide/reading_data">described</a> as "reading from files" is far more complicated than many people need or want to deal with. Indeed, TensorFlow developers realize this and are <a href="https://github.com/tensorflow/tensorflow/issues/7951">thinking about</a> adding alternative interfaces for getting data in to TensorFlow programs.</p>
<p>You can still use any method you like for loading data in Python and then put it in your TensorFlow graph via the <code>feed_dict</code> method. But say you do want to do your file reading with ops that live inside the TensorFlow graph. How does it work?</p>
<p>TensorFlow does have <a href="https://www.tensorflow.org/api_docs/python/tf/read_file"><code>tf.read_file</code></a> and <a href="https://www.tensorflow.org/api_docs/python/tf/write_file"><code>tf.write_file</code></a>, which let you read and write whole files at once, something like regular Python <code>file.read()</code> and <code>file.write()</code>. But you likely want something a little more helpful.</p>
<p>In Python, you can read lines from a file like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; reader = open('filename.txt'):
&gt;&gt;&gt; for line in reader:
&gt;&gt;&gt;    print(line)</code></pre>

<p>TensorFlow has readers that generalize this idea of getting multiple things, like lines, from files. The immediate example is the <a href="https://www.tensorflow.org/api_docs/python/tf/TextLineReader"><code>tf.TextLineReader</code></a>. There are also <a href="https://www.tensorflow.org/api_guides/python/io_ops#Readers">readers</a> that read records from TFRecords files, or just fixed number of bytes at a time from arbitrary files. Reading a whole file at a time is sort of degenerate case.</p>
<p>Instead of taking a filename, however, TensorFlow readers take a queue of filenames. This can be useful:</p>
<ul>
<li>You may have lots of files that you want to read from, possibly using multiple machines.</li>
<li>You may want to read through one or more files multiple times, for multiple epochs of training.</li>
</ul>
<p>It's a little awkward to use a queue when you just want to read from one file, but that's how TensorFlow works.</p>
<p>TensorFlow's file reading gets tangled up with its threading <code>QueueRunner</code> because they're often shown together, but this is not necessary. We <a href="/20170327-tensorflow_and_queues/">can</a> set up a quick and dirty one-item queue like this:</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; filename_queue = tf.FIFOQueue(capacity=1, dtypes=[tf.string])
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; session.run(filename_queue.enqueue('limerick.txt'))
&gt;&gt;&gt; session.run(filename_queue.close())</code></pre>

<p>We make a reader and tell it to read from files in the queue. There are two outputs that update every time you evaluate either of them:</p>
<ul>
<li>A <code>key</code>, based on the current filename, which you may not need and don't need to explicitly evaluate.</li>
<li>A <code>value</code>, which is the bit of data we're probably interested in.</li>
</ul>
<pre><code class="language-python">&gt;&gt;&gt; reader = tf.TextLineReader()
&gt;&gt;&gt; key, value = reader.read(filename_queue)
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:1', 'This limerick goes in reverse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:2', "Unless I'm remiss"]
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:3', 'The neat thing is this:']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:4', 'If you start from the bottom-most verse']
&gt;&gt;&gt; session.run([key, value])
['limerick.txt:5', "This limerick's not any worse"]
&gt;&gt;&gt; session.run([key, value])
# raises `OutOfRangeError`</code></pre>

<p>The <a href="limerick.txt">limerick</a> is <a href="http://www.smbc-comics.com/?id=3201">due to</a> Zach Weinersmith.</p>
<p>If we hadn't closed the filename queue, that last <code>session.run</code> would block, waiting for somebody to add another filename to the filename queue.</p>
<p>If we had added more filenames to the filename queue, the reader would continue happily reading from the next, and the next.</p>
<p>The records here are lines of simple ASCII text, so we can immediately see them clearly in the REPL. But for many types of data you'll need a decoder.</p>
<p>To read CSV files, you would read text lines just as above, and then use the <a href="https://www.tensorflow.org/api_docs/python/tf/decode_csv"><code>tf.decode_csv</code></a> decoder on  <code>value</code>. The <a href="https://www.tensorflow.org/api_docs/python/tf/decode_raw"><code>tf.decode_raw</code></a> decoder turns raw bytes into standard TensorFlow datatypes. And you can parse TFRecords examples.</p>
<p>Boom! Reading files inside the graph!</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170412-reading_from_disk_inside_the_tensorflow_graph/</guid>
<pubDate>Wed, 12 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Distributed MapReduce with TensorFlow</title>
<description><![CDATA[

<p>Using many computers to count words is a tired Hadoop example, but might be unexpected with TensorFlow. In 50 lines, a TensorFlow program can implement not only map and reduce steps, but a whole MapReduce system.</p>
<hr>
<h2>Set up the cluster</h2>
<p>The design will have three roles, or jobs. There will be one task in the <code>files</code> job, distributing units of work. There will be one task for the <code>reduce</code> role, keeping track of results. There could be arbitrarily many tasks doing the <code>map</code> job, but for this example there will be two.</p>
<p>TensorFlow programs often use <code>ps</code> (parameter server) and <code>worker</code> tasks, but this is largely a convention. The program here won't follow the convention.</p>
<p>The four tasks can run on four computers, or on fewer. To stay local, here's a cluster definition that runs them all on your local machine.</p>
<pre><code class="language-python">cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}</code></pre>

<p>I have a <a href="make_configs.py"><code>make_configs.py</code></a> script that produces four shell scripts (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>). Each should be sourced (like <code>source config_map_0.sh</code>) in a separate shell. This setting of environment variables is work that could be handled by a cluster manager like <a href="https://kubernetes.io/">Kubernetes</a>, but these scripts will get it done.</p>
<p>Every task will run <a href="count.py"><code>count.py</code></a>. The first few lines establish the cluster.</p>
<pre><code class="language-python">import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)</code></pre>

<p>For more on TensorFlow clusters, see <a href="/20170410-tensorflow_clusters_questions_and_code/">TensorFlow Clusters: Questions and Code</a>.</p>
<p>To avoid lots of indentation, I'm not using <code>tf.Session</code> as a context manager, which is fine for this example. I'll similarly avoid <code>with</code> blocks when reading files later.</p>
<hr>
<h2>Set up the data</h2>
<p>For distributed data processing to make sense, you likely want a distributed file system like HDFS providing a way for workers to grab chunks of data to work on. You might have hundred-megabyte <a href="/20170323-tfrecords_for_humans/">TFRecords files</a> prepared, for example.</p>
<p>For this example, we'll use <a href="/20170331-on_tyranny/">a dataset of 22 small text files</a>. We'll generate a <a href="filenames.txt">list of filenames</a>. Then, assuming every member of the cluster can access the file system, we can give a worker a filename and have it read the file.</p>
<pre><code class="language-bash">$ wget http://planspace.org/20170331-on_tyranny/on_tyranny.tar.gz
$ tar zxvf on_tyranny.tar.gz
$ find on_tyranny -type f &gt; filenames.txt</code></pre>

<hr>
<h2>Make a filename distributor</h2>
<p>The <code>files</code> task will host a <a href="/20170327-tensorflow_and_queues/">queue</a> of filenames.</p>
<pre><code class="language-python">with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')</code></pre>

<p>This code will be executed by every member of the cluster, but it won't make multiple queues. The <code>tf.device</code> context specifies that the queue lives on the <code>files</code> task machine, and the <code>shared_name</code> uniquely identifies this queue. So just one queue gets made, but every member of the cluster can refer to it with the Python variable name <code>filename_queue</code>.</p>
<p>The next part of the code only runs on the <code>files</code> task:</p>
<pre><code class="language-python">if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()</code></pre>

<p>Device assignment here is left up to TensorFlow, which is fine.</p>
<p>The <code>file</code> task uses normal Python file reading to get filenames from <a href="filenames.txt"><code>filenames.txt</code></a> and put them in the queue.</p>
<p>I'm loading the queue explicitly to avoid getting into <code>Coordinator</code> and <code>QueueRunner</code>; you could also use a <code>string_input_producer</code>, for example.</p>
<p>Then the <code>files</code> task runs <code>server.join()</code>, which keeps it running so that the queue doesn't disappear. We'll have to kill the process eventually because it won't know when to stop. This is another thing a cluster manager could be responsible for.</p>
<hr>
<h2>Make a reduce node</h2>
<p>There's just going to be one variable storing the total word count.</p>
<pre><code class="language-python">with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')</code></pre>

<p>Like the code defining the queue, every task in the cluster will run this code, so every task in the cluster can refer to this variable. Variables in specific places are "de-duplicated" using <code>name</code> instead of <code>shared_name</code>.</p>
<p>There's very little code that only the <code>reduce</code> task runs.</p>
<pre><code class="language-python">if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)</code></pre>

<p>The <code>reduce</code> task initializes and then displays the current value of <code>total_word_count</code> every two seconds.</p>
<p>It would be a bit more like Hadoop MapReduce to have the reducer explicitly receive data emitted from mappers, perhaps via another queue. Then the reducer would have to run some code to reduce down data from that queue.</p>
<p>The absence of any reducing code in the <code>reduce</code> task demonstrates the way distribution works in TensorFlow. The <code>reduce</code> task owns a variable, but we can add to that variable from another machine.</p>
<p>Like the <code>files</code> task, the <code>reduce</code> task doesn't have any way of knowing when the counting process is done and then shutting down, which I think is okay for this example.</p>
<hr>
<h2>Make map nodes</h2>
<p>The <code>map</code> task has already run code establishing the filename queue and the total word count variable. Here's what each <code>map</code> task does:</p>
<pre><code class="language-python">if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign_add(total_word_count,
                                 word_count_to_add,
                                 use_locking=True)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>Each <code>map</code> task pulls a filename from the queue, reads the file, counts its words, and then adds its count to the total.</p>
<p>A <code>map</code> task will run until the queue is empty, and then it will die with an <code>OutOfRangeError</code>. This would be a little more careful:</p>
<pre><code class="language-python">        try:
            filename = session.run(filename_from_queue)
        except tf.errors.OutOfRangeError:
            break</code></pre>

<p>Inside the <code>map</code> task, the actual work that's done is not part of the TensorFlow graph. We can execute arbitrary Python here.</p>
<p>There's a five second pause in the loop so that things don't happen too fast to watch.</p>
<p>The total word count is stored off in the <code>reduce</code> task, possibly on a different computer, but that doesn't matter. This is part of what's cool about TensorFlow.</p>
<hr>
<h2>Run</h2>
<p>To execute, we run <a href="count.py"><code>count.py</code></a> in four places with the appropriate environment variables set. That's it. We've counted words with many computers.</p>
<hr>
<h2>So what?</h2>
<p>Programming a cluster with TensorFlow is just like programming a single computer with TensorFlow. This is pretty neat, and it makes a lot of things possible beyond just distributed stochastic gradient descent.</p>
<p>The example above demonstrates a distributed queue. If you can do that, do you need a separate queueing system like <a href="https://www.rabbitmq.com/">RabbitMQ</a>? Maybe not in every situation.</p>
<p>The example above sends filenames to workers, which is a pretty general model. What if you feel comfortable sending executable filenames, or some representation of code? You might implement something like <a href="http://www.celeryproject.org/">Celery</a> pretty quickly.</p>
<p>The example I've shown can probably fail in more ways than I even realize. It would be more work to make it robust. It would be again more work to make it more general. But it's pretty exciting to be able to write something like this at all.</p>
<p>And while even the typical TensorFlow distributed training is itself a kind of MapReduce process, TensorFlow is general enough that it could be used for wildly different architectures. TensorFlow is an amazing tool.</p>
<hr>
<h2>Code</h2>
<p>Here's <a href="make_configs.py"><code>make_configs.py</code></a>:</p>
<pre><code class="language-python">import json

cluster = {'files': ['localhost:2222'],
           'reduce': ['localhost:2223'],
           'map': ['localhost:2224', 'localhost:2225']}
for task_type, addresses in cluster.items():
    for index in range(len(addresses)):
        tf_config = {'cluster': cluster,
                     'task': {'type': task_type,
                              'index': index}}
        tf_config_string = json.dumps(tf_config, indent=2)
        with open('config_{}_{}.sh'.format(task_type, index), 'w') as f:
            f.write("export TF_CONFIG='{}'\n".format(tf_config_string))
            # GPUs won't be needed, so prevent accessing GPU memory.
            f.write('export CUDA_VISIBLE_DEVICES=-1\n')</code></pre>

<p>The files produced by <a href="make_configs.py"><code>make_configs.py</code></a> (<a href="config_files_0.sh">1</a>, <a href="config_reduce_0.sh">2</a>, <a href="config_map_0.sh">3</a>, <a href="config_map_1.sh">4</a>) look like this:</p>
<pre><code class="language-json">export TF_CONFIG='{
  "cluster": {
    "files": [
      "localhost:2222"
    ], 
    "map": [
      "localhost:2224", 
      "localhost:2225"
    ], 
    "reduce": [
      "localhost:2223"
    ]
  }, 
  "task": {
    "index": 0, 
    "type": "map"
  }
}'
export CUDA_VISIBLE_DEVICES=-1</code></pre>

<p>And here's <a href="count.py"><code>count.py</code></a> all together:</p>
<pre><code class="language-python">from __future__ import print_function

import time
import tensorflow as tf

config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)
session = tf.Session(server.target)

with tf.device('/job:files/task:0'):
    filename_queue = tf.FIFOQueue(capacity=10, dtypes=[tf.string],
                                  shared_name='filename_queue')

if config.task_type == 'files':
    filename_to_enqueue = tf.placeholder(tf.string)
    enqueue_filename = filename_queue.enqueue(filename_to_enqueue)
    close_filename_queue = filename_queue.close()
    for line in open('filenames.txt'):
        filename = line.strip()
        session.run(enqueue_filename,
                    feed_dict={filename_to_enqueue: filename})
    session.run(close_filename_queue)
    server.join()

with tf.device('/job:reduce/task:0'):
    total_word_count = tf.Variable(0, name='total')

if config.task_type == 'reduce':
    initializer = tf.global_variables_initializer()
    session.run(initializer)
    while True:
        total_word_count_now = session.run(total_word_count)
        print('{} words so far'.format(total_word_count_now))
        time.sleep(2)

if config.task_type == 'map':
    filename_from_queue = filename_queue.dequeue()
    word_count_to_add = tf.placeholder(tf.int32)
    add_to_total = tf.assign(total_word_count,
                             total_word_count + word_count_to_add)
    while True:
        filename = session.run(filename_from_queue)
        text = open(filename).read()
        this_word_count = len(text.split())
        time.sleep(5)
        print('{} words in {}'.format(this_word_count, filename))
        session.run(add_to_total,
                    feed_dict={word_count_to_add: this_word_count})</code></pre>

<p>All the files needed to demo this are together in a <a href="https://github.com/ajschumacher/mapreduce_with_tensorflow">repo on GitHub</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</link>
<guid>http://planspace.org/20170411-distributed_mapreduce_with_tensorflow/</guid>
<pubDate>Tue, 11 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow Clusters: Questions and Code</title>
<description><![CDATA[

<p>One way to think about TensorFlow is as a framework for <a href="https://en.wikipedia.org/wiki/Distributed_computing">distributed computing</a>. I've suggested that TensorFlow is a <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">distributed virtual machine</a>. As such, it offers a lot of flexibility. TensorFlow also suggests some conventions that make writing programs for distributed computation tractable.</p>
<h2>When is there a cluster?</h2>
<p>A <a href="http://hadoop.apache.org/">Hadoop</a> or <a href="http://spark.apache.org/">Spark</a> cluster is generally long-lived. The cluster runs, processing jobs as they come to it. A company might have a thousand-node Spark cluster, for example, used by everyone in a division.</p>
<p>In contrast, TensorFlow clusters generally spring into being for the purpose of running a particular TensorFlow program. There are computers on the network, and they become members of TensorFlow clusters based on what they're running.</p>
<p>To make this process manageable, you might use a system like <a href="https://kubernetes.io/">Kubernetes</a> or <a href="https://cloud.google.com/ml-engine/">Google Cloud ML</a> to intelligently run TensorFlow programs on specific machines in a larger pool.</p>
<h2>What does the cluster do for me?</h2>
<p>In systems like Hadoop MapReduce and Spark, there's usually considerable distance between the programming interface and how the computation actually gets distributed. If you're writing your own map and reduce steps for Hadoop, you're close to that mechanism, but you're still plugging in to the pre-built larger architecture. Most users prefer higher-level APIs like <a href="https://pig.apache.org/">Pig</a> on Hadoop or the standard Spark APIs. And both Hadoop and Spark support at least one SQL-style interface, even farther from the underlying implementations. As a user of Hadoop or Spark, you don't think about putting computation on particular machines, or worry about the different roles that different machines might play.</p>
<p>With TensorFlow, the abstraction for distributed computing is the same as the abstraction for local computing: the computation graph. Distributing TensorFlow programs means having graphs that span multiple computers. You're responsible for what parts of the graph go where, and what every computer in the cluster does.</p>
<h2>How many programs do I write?</h2>
<p>One familiar model of distributed computing is client-server, like the web. Web servers and browsers are quite different programs.</p>
<p>Closer to TensorFlow applications, a central server could do some computation on request, or it could offer data to be processed on the client side, like <a href="https://setiathome.berkeley.edu/">SETI@home</a> or <a href="https://folding.stanford.edu/">Folding@home</a>. Again, server and client code are distinct.</p>
<p>Nothing prevents you from writing separate "server" and "client" TensorFlow programs, but it isn't necessary, natural, or recommended. One of TensorFlow's design goals was to get away from the client-server divide in DistBelief.</p>
<p>The TensorFlow distributed runtime is peer-to-peer: every machine can accept graph nodes from any other machine, and every machine can put graph nodes on any other machine. Generally, every machine will run the same program. Whether the system behaves like a client-server system or something else entirely is up to you.</p>
<h2>Which computer does what?</h2>
<p>Distributed TensorFlow works by running the same program on multiple machines, but that doesn't mean that every machine does exactly the same thing.</p>
<p>If a system with separate client and server programs is like a system of two symbiotic species, your TensorFlow program is like the DNA of an organism whose cells specialize based on their environment.</p>
<p>The dominant convention is to have two main roles: <em>parameter servers</em> (<code>ps</code>) and <em>workers</em>. You might also have a <em>master</em> role, and one of your workers can be the <em>chief</em> worker, but <code>ps</code> and <code>worker</code> is usually the main division.</p>
<p>Usually a machine running your TensorFlow program will learn what its role should be based on the <code>TF_CONFIG</code> environment variable, which should be set by your cluster manager.</p>
<h2>Who knows what about the cluster topology?</h2>
<p>In Hadoop and Spark, the system is keeping track of all the machines in the cluster. On the web, servers generally only know about client addresses long enough to provide a response.</p>
<p>Usually every machine in a TensorFlow cluster will be given a complete listing of machines in the cluster.</p>
<p>If some machines really don't need to know about others in the same cluster, for example if workers never communicate with one another, it's also possible to provide less complete information.</p>
<p>The cluster topology is also most often provided via the <code>TF_CONFIG</code> environment variable.</p>
<h2>Code?</h2>
<p>Let's say you have a network which includes the following machines:</p>
<ul>
<li><code>192.168.0.10</code></li>
<li><code>192.168.0.11</code></li>
<li><code>192.168.0.12</code></li>
</ul>
<p>The machines are all running the same version of TensorFlow. Let's see how we could get them set up to run a distributed TensorFlow program.</p>
<p>We'll have one parameter server (<code>ps</code>) and two workers. Each machine will know about the cluster's topology.</p>
<pre><code class="language-python">import tensorflow as tf

cluster = {'ps': ['192.168.0.10:2222'],
           'worker': ['192.168.0.11:2222', '192.168.0.12:2222']}
cluster_spec = tf.train.ClusterSpec(cluster)
server = tf.train.Server(cluster_spec)</code></pre>

<p>In the strings like <code>'192.168.0.10:2222'</code>, the protocol (gRPC) is omitted, and the port (<code>2222</code>) is the default for TensorFlow communication.</p>
<p>The <code>server</code> that every machine starts is what enables the communication of the TensorFlow distributed runtime; its behavior is largely at a lower level than the code we'll write.</p>
<p>This code gets the network topology going, but it doesn't tell an individual machine which role it should have. Working out which IP address or hostname refers to the current machine is not necessarily straightforward, but hard-coding different values into different copies of the code would be an even worse idea than hard-coding the topology once. This is where the environment variable <code>TF_CONFIG</code> becomes very useful.</p>
<p>We'll put cluster and task information into the <code>TF_CONFIG</code> environment variable as a JSON serialization. On the machine that will be our parameter server, the data will look like this as a Python dictionary:</p>
<pre><code class="language-python">tf_config = {'cluster': {'ps': ['192.168.0.10:2222'],
                         'worker': ['192.168.0.11:2222', '192.168.0.12:2222']},
             'task': {'type': 'ps',
                      'index': 0}}</code></pre>

<p>You can set the environment variable in the usual way in a shell, making small changes to achieve JSON syntax.</p>
<pre><code class="language-bash">$ export TF_CONFIG='{"cluster": {"ps": ["192.168.0.10:2222"], "worker": ["192.168.0.11:2222", "192.168.0.12:2222"]}, "task": {"type": "ps", "index": 0}}'</code></pre>

<p>To avoid fussing with that directly, you could do it in Python.</p>
<pre><code class="language-python">import os
import json

os.environ['TF_CONFIG'] = json.dumps(tf_config)</code></pre>

<p>Really, it'll be best if your cluster manager sets <code>TF_CONFIG</code> for you.</p>
<p>Once <code>TF_CONFIG</code> is set, you can read it and get to work.</p>
<pre><code class="language-python">tf_config = json.loads(os.environ['TF_CONFIG'])
cluster_spec = tf.train.ClusterSpec(tf_config['cluster'])
task_type = tf_config['task']['type']
task_id = tf_config['task']['index']
server = tf.train.Server(cluster_spec,
                         job_name=task_type,
                         task_index=task_id)</code></pre>

<p>Another way to do that is with <code>tf.contrib.learn.RunConfig</code>, which automatically checks the <code>TF_CONFIG</code> environment variable.</p>
<pre><code class="language-python">config = tf.contrib.learn.RunConfig()
server = tf.train.Server(config.cluster_spec,
                         job_name=config.task_type,
                         task_index=config.task_id)</code></pre>

<p>At this point, you are ready to begin writing a distributed TensorFlow program.</p>
<!-- helpful references:

https://github.com/tensorflow/tensorflow/blob/17c47804b86e340203d451125a721310033710f1/tensorflow/contrib/learn/python/learn/estimators/run_config.py

https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/census/tensorflowcore/trainer/task.py

-->

<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</link>
<guid>http://planspace.org/20170410-tensorflow_clusters_questions_and_code/</guid>
<pubDate>Mon, 10 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>How Not To Program the TensorFlow Graph</title>
<description><![CDATA[

<p>Using TensorFlow from Python is like using Python to program <a href="/20170328-tensorflow_as_a_distributed_virtual_machine/">another computer</a>. Some Python statements build your TensorFlow program, some Python statements execute that program, and of course some Python statements aren't involved with TensorFlow at all.</p>
<p>Being thoughtful about the graphs you construct can help you avoid confusion and performance pitfalls. Here are a few considerations.</p>
<h2>Avoid having many identical ops</h2>
<!-- Better re-write; maybe separate out a section about Python names for TF ops, losing references to your ops, etc. -->

<p>Lots of methods in TensorFlow create ops in the computation graph, but do not execute them. You may want to execute multiple times, but that doesn't mean you should create lots of copies of the same ops.</p>
<p>A clear example is <code>tf.global_variables_initializer()</code>.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; session = tf.Session()
# Create some variables...
&gt;&gt;&gt; initializer = tf.global_variables_initializer()
# Variables are not yet initialized.
&gt;&gt;&gt; session.run(initializer)
# Now variables are initialized.
# Do some more work...
&gt;&gt;&gt; session.run(initializer)
# Now variables are re-initialized.</code></pre>

<p>If the call to <code>tf.global_variables_initializer()</code> is repeated, for example directly as the argument to <code>session.run()</code>, there are downsides.</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.global_variables_initializer())
&gt;&gt;&gt; session.run(tf.global_variables_initializer())</code></pre>

<p>A new initializer op is created every time the argument to <code>session.run()</code> here is evaluated. This creates multiple initializer ops in the graph. Having multiple copies isn't a big deal for small ops in an interactive session, and you might even want to do it in the case of the initializer if you've created more variables that need to be included in initialization. But think about whether you need lots of duplicate ops.</p>
<p>Creating ops inside <code>session.run()</code>, you also don't have a Python variable referring to those ops, so you can't easily reuse them.</p>
<p>In Python, if you create an object that nothing refers to, it can be garbage collected. The abandoned object will be deleted and and memory it used will be freed. That doesn't happen to things in the TensorFlow graph; everything you put in the graph stays there.</p>
<p>It's pretty clear that <code>tf.global_variables_initializer()</code> returns an op. But ops are also created in less obvious ways.</p>
<p>Let's compare to how NumPy works.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>At this point there are two arrays in memory, <code>x</code> and <code>y</code>. The <code>y</code> has the value 2.0, but there's no record of <em>how</em> it came to have that value. The addition has left no record of itself.</p>
<p>TensorFlow is different.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Now only <code>x</code> is a TensorFlow variable; <code>y</code> is an <code>add</code> op, which can return the result of that addition if we ever run it.</p>
<p>One more comparison.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = np.array(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>Here <code>y</code> is assigned to refer to one result array <code>x + 1.0</code>, and then reassigned to point to a different one. The first one will be garbage collected and disappear.</p>
<pre><code class="language-python">&gt;&gt;&gt; x = tf.Variable(1.0)
&gt;&gt;&gt; y = x + 1.0
&gt;&gt;&gt; y = x + 1.0</code></pre>

<p>In this case, <code>y</code> refers to one <code>add</code> op in the TensorFlow graph, and then <code>y</code> is reassigned to point to a different <code>add</code> op in the graph. Since <code>y</code> only points to the second <code>add</code> now, we don't have a convenient way to work with the first one. But both the <code>add</code> ops are still around, in the graph, and will stay there.</p>
<p>(As an aside, Python's mechanism for defining class-specific addition <a href="http://www.python-course.eu/python3_magic_methods.php">and so on</a>, which is how <code>+</code> is made to create TensorFlow ops, is pretty neat.)</p>
<p>Especially if you're just working with the default graph and running interactively in a regular REPL or a notebook, you can end up with a lot of abandoned ops in your graph. Every time you re-run a notebook cell that defines any graph ops, you aren't just redefining ops&#8212;you're creating new ones.</p>
<p>Often it's okay to have a few extra ops floating around when you're experimenting. But things can get out of hand.</p>
<pre><code class="language-python">for _ in range(1e6):
    x = x + 1</code></pre>

<p>If <code>x</code> is a NumPy array, or just a regular Python number, this will run in constant memory and finish with one value for x.</p>
<p>But if <code>x</code> is a TensorFlow variable, there will be over a million ops in your TensorFlow graph, just defining a computation and not even <em>doing</em> it.</p>
<p>One immediate fix for TensorFlow is to use a <code>tf.assign</code> op, which gives behavior more like what you might expect.</p>
<pre><code class="language-python">increment_x = tf.assign(x, x + 1)
for _ in range(1e6):
    session.run(increment_x)</code></pre>

<p>This revised version does not create any ops inside the loop, which is generally good advice. TensorFlow does have <a href="https://www.tensorflow.org/api_guides/python/control_flow_ops">control flow constructs</a> including <a href="https://www.tensorflow.org/api_docs/python/tf/while_loop">while loops</a>. But only use these when really needed.</p>
<p>Be conscious of when you're creating ops, and only create the ones you need. Try to keep op creation distinct from op execution. And after interactive experimentation, eventually get to a state, probably in a script, where you're only creating the ops that you need.</p>
<h2>Avoid constants in the graph</h2>
<p>A particularly unfortunate op to needlessly add to a graph is accidental constant ops, especially large ones.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones = np.ones((1000, 1000))</code></pre>

<p>There are a million ones in the NumPy array <code>many_ones</code>. We can add them up.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_ones.sum()
## 1000000.0</code></pre>

<p>What if we add them up with TensorFlow?</p>
<pre><code class="language-python">&gt;&gt;&gt; session.run(tf.reduce_sum(many_ones))
## 1000000.0</code></pre>

<p>The result is the same, but the mechanism is quite different. This not only added some ops to the graph&#8212;it put a copy of the entire million-element array into the graph as a constant.</p>
<p>Variations on this pattern can result in accidentally loading an entire data set into the graph as constants. A program might still run, for small data sets. Or your system might fail.</p>
<p>One simple way to avoid storing data in the graph is to use the <code>feed_dict</code> mechanism.</p>
<pre><code class="language-python">&gt;&gt;&gt; many_things = tf.placeholder(tf.float64)
&gt;&gt;&gt; adder = tf.reduce_sum(many_things)
&gt;&gt;&gt; session.run(adder, feed_dict={many_things: many_ones})
## 1000000.0</code></pre>

<p>As before, be clear about what you're adding to the graph and when. Concrete data usually only enters the graph at moments of evaluation.</p>
<h2>TensorFlow as Functional Programming</h2>
<p>TensorFlow ops are like functions. This is especially clear when an op has one or more placeholder inputs; evaluating the op in a session is like calling a function with those arguments. So Python functions that return TensorFlow ops are like <a href="https://en.wikipedia.org/wiki/Higher-order_function">higher-order functions</a>.</p>
<p>You might decide that it's worth putting a constant into the graph. For example, if you have to reshape a lot of tensors to 28x28, you might make an op that does that.</p>
<pre><code class="language-python">&gt;&gt;&gt; input_tensor = tf.placeholder(tf.float32)
&gt;&gt;&gt; reshape_to_28 = tf.reshape(input_tensor, shape=[28, 28])</code></pre>

<p>This is like <a href="https://en.wikipedia.org/wiki/Currying">currying</a> in that the <code>shape</code> argument has now been set. The <code>[28, 28]</code> has become a constant in the graph and specified that argument. Now to evaluate <code>reshape_to_28</code> we only have to provide <code>input_tensor</code>.</p>
<p>Broader connections have been suggested between <a href="http://colah.github.io/posts/2015-09-NN-Types-FP/">neural networks, types, and functional programming</a>. It's interesting to think of TensorFlow as a system that supports this kind of construction.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</link>
<guid>http://planspace.org/20170404-how_not_to_program_the_tensorflow_graph/</guid>
<pubDate>Tue, 04 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Images and TFRecords</title>
<description><![CDATA[

<p>There are a number of ways to work with images in TensorFlow and, if you wish, with TFRecords. These methods aren't so mysterious if you <a href="/20170323-tfrecords_for_humans/">understand TFRecords</a> and a little bit about how digital images work.</p>
<hr>
<h2>Representations for Images</h2>
<!--

(I kind of like this paragraph, at least in that it gives credit for the image source, but it's not really key to the development here.)

Wikipedia [tells me](https://en.wikipedia.org/wiki/Keep_Calm_and_Carry_On) that this [poster](https://commons.wikimedia.org/wiki/File:Freedom_Is_In_Peril_Defend_It_With_All_Your_Might.svg), and the more widely known "Keep Calm and Carry On," were not very well received [in 1939](img/freedom_in_tube.jpg). Perhaps its time has come.

-->

<p>For example, this image is 600 pixels tall and 400 pixels wide. Every pixel has some intensity in red, green, and blue: three values, or channels, for every pixel. This image has shape [600, 400, 3]. (The order of the dimensions doesn't matter as long as everybody agrees on the convention.) The display on your screen is like a dense matrix; it is a <a href="https://en.wikipedia.org/wiki/Raster_graphics">raster graphic</a>.</p>
<p><img alt="FREEDOM IS IN PERIL / DEFEND IT WITH ALL YOUR MIGHT" src="img/freedom.png"></p>
<p>Neural networks that work with images typically work with this kind of dense matrix representation. For this image, the matrix will have 600 * 400 * 3 = 720,000 values. If each value is an unsigned 8-bit integer, that's 720,000 bytes, or about three quarters of a megabyte.</p>
<p>Images sometimes also have an alpha transparency channel, which is a fourth channel in a color image, but not necessary if there's nothing else "underneath" the image. And not all images are color; greyscale (black and white) images can have just one channel.</p>
<p>Using unsigned 8-bit integers (256 possible values) for each value in the image array is enough for displaying images to humans. But when working with image data, it isn't uncommon to switch to 32-bit floats, for example. This quadruples the size of the data in memory. As always, remain aware of your data types.</p>
<h3>Dense Array</h3>
<p>One way to store complete raster image data is by serializing a NumPy array to disk with <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.save.html"><code>numpy.save</code></a>.</p>
<pre>720,080 bytes  <a href="img/freedom.npy">freedom.npy</a></pre>

<p>The file <code>freedom.npy</code> has 80 more bytes than the ones required to store the array values. Those extra bytes specify things like the dimensions of the array. If we save raw array values in TFRecords, we'll also have to keep track of this additional information.</p>
<p>Because storing one or more value for every pixel takes so many bytes, file formats for images typically do something cleverer.</p>
<h3>JPEG</h3>
<p><a href="https://en.wikipedia.org/wiki/JPEG">JPEG</a> is lossy. When you save an array as JPG and then read from the JPG, it will generally look about the same, but you won't necessarily get back exactly the same values for your array. JPEG is like <a href="https://en.wikipedia.org/wiki/MP3">MP3</a> for images. JPEG is good for photographs.</p>
<pre> 25,136 bytes  <a href="img/freedom.jpg">freedom.jpg</a></pre>

<p><code>freedom.jpg</code> is less than 4% the size of the NumPy array, and it still looks pretty good. It does have some ringing artifacts around the letter edges. JPEG file size depends on compression parameters when saving, and on the encoder used. For example, Google <a href="https://research.googleblog.com/2017/03/announcing-guetzli-new-open-source-jpeg.html">released</a> their <a href="https://github.com/google/guetzli/">Guetzli</a> JPEG encoder, which can produce smaller files but takes more computation to do so.</p>
<pre> 46,567 bytes  <a href="img/freedom2.jpg">freedom2.jpg</a></pre>

<p><code>freedom2.jpg</code> is another JPEG file, saved with higher quality. Ringing artifacts are pretty much gone. The file is still under 7% the size of the NumPy arrray.</p>
<h3>PNG</h3>
<p><a href="https://en.wikipedia.org/wiki/Portable_Network_Graphics">PNG</a> is lossless. If you save as PNG and then read from the PNG, you can get back exactly what you had originally. PNG is like <a href="https://en.wikipedia.org/wiki/Zip_(file_format)">ZIP</a> for images, but image viewers do the "un-zipping" automatically so they can put a raster image on the screen. PNG is like <a href="https://en.wikipedia.org/wiki/GIF">GIF</a> without animation.</p>
<pre> 33,192 bytes  <a href="img/freedom.png">freedom.png</a></pre>

<p><code>freedom.png</code> is under 5% the size of the NumPy array, and it preserves the image perfectly. It's less often used, but compression parameters can also affect PNG size, and the encoder can make a difference too. PNG uses deflate (zlip) compression, so Google's <a href="https://github.com/google/zopfli">Zopfli</a> algorithm can be used, for example, while <a href="https://github.com/google/brotli">Brotli</a> cannot.</p>
<h3>SVG</h3>
<p><a href="https://en.wikipedia.org/wiki/Scalable_Vector_Graphics">SVG</a> doesn't represent pixels directly at all, but represents in a text format the geometry of shapes in the image. SVG images can look good at any zoom level; they don't suffer from pixelization. They can also be edited with different tools than raster images. And for simple images, an SVG can be quite small. SVG is like <a href="https://en.wikipedia.org/wiki/HTML">HTML</a>; you can look at it as text, but to see it as intended requires a program like a web browser.</p>
<pre> 42,721 bytes  <a href="img/freedom.svg">freedom.svg</a></pre>

<p><code>freedom.svg</code> turns out not be a super efficient encoding of the image; it represents all the letter outlines instead of using plain text in some font, for example. But it represents the image fundamentally differently from just recreating pixels, and it's still under 6% the size of the NumPy array file.</p>
<h3>TFRecords?</h3>
<p>TFRecords don't know anything about image formats. You just put bytes in them. So you have your choice of whether that means you store dense arrays of values or a well-known image format. TensorFlow provides <a href="https://www.tensorflow.org/api_guides/python/image">image format support</a> for JPEG, PNG, and GIF in the computation graph.</p>
<hr>
<h2>Images without TensorFlow</h2>
<p>As always, <a href="/20170312-use_only_what_you_need_from_tensorflow/">you have a choice about whether you need to do everything with TensorFlow</a>. If you're loading data batches with a <code>feed_dict</code>, in particular, feel free to use whatever Python functionality you're comfortable with. Even if you're not, you'll probably want to do some work with your data outside of TensorFlow before you move all your computation into the graph.</p>
<p>The Matplotlib <a href="http://matplotlib.org/users/image_tutorial.html">image tutorial</a> recommends using <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imread"><code>matplotlib.image.imread</code></a> to read image formats from disk. This function will automatically change image array values to floats between zero and one, and it doesn't give you options about how to read the image.</p>
<p>The <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imread.html">scipy.misc.imread</a> function uses the Python Imaging Library (PIL) to support many image formats, including PNG and JPEG. This function also has some useful options. The original <code>freedom.png</code> has an alpha channel which we don't need. Passing <code>mode='RGB'</code> tells the reader to give us just three color channels.</p>
<pre><code class="language-python">&gt;&gt;&gt; import scipy.misc
&gt;&gt;&gt; image = scipy.misc.imread('freedom.png', mode='RGB')
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>Matplotlib's <a href="http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.imshow">imshow</a> is good for checking out what image arrays look like. (Also specify <code>%matplotlib inline</code> if you're in a notebook.)</p>
<pre><code class="language-python">&gt;&gt;&gt; import matplotlib.pyplot as plt
&gt;&gt;&gt; plt.imshow(image)</code></pre>

<p>NumPy gives us a way to save and load its arrays.</p>
<pre><code class="language-python">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; np.save('freedom.npy', image)
&gt;&gt;&gt; same_image = np.load('freedom.npy')</code></pre>

<p>As long as we have <code>scipy.misc</code> imported, we can use <a href="https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.imsave.html">scipy.misc.imsave</a> to write various image formats as well. For saving, <a href="http://matplotlib.org/api/image_api.html#matplotlib.image.imsave">matplotlib.image.imsave</a> actually has more options. Neither of these offer control over compression level, for example.</p>
<pre><code class="language-python">&gt;&gt;&gt; scipy.misc.imsave('freedom.jpg', image)</code></pre>

<hr>
<h2>PNG and JPEG with TensorFlow</h2>
<p>TensorFlow has ops for decoding and encoding PNGs and JPEGs, so we can put these operations into the computation graph.</p>
<p>Above, <code>imread</code> both read a file from disk and decoded it, and <code>imsave</code> both encoded an image and wrote it to disk. The TensorFlow functionality decouples the decoding and encoding from file reading and writing. This example will avoid using TensorFlow's file reading and writing, to focus just on the encoding and decoding.</p>
<p>The <code>channels=3</code> passed to <code>tf.image.decode_image()</code> is the equivalent of <code>mode='RGB'</code> above. We don't want to get the alpha channel from the PNG file.</p>
<pre><code class="language-python">&gt;&gt;&gt; import tensorflow as tf
&gt;&gt;&gt; with open('freedom.png', 'rb') as f:
...     png_bytes = f.read()
&gt;&gt;&gt; bytes = tf.placeholder(tf.string)
&gt;&gt;&gt; decode_png = tf.image.decode_image(bytes, channels=3)
&gt;&gt;&gt; session = tf.Session()
&gt;&gt;&gt; image = session.run(decode_png, feed_dict={bytes: png_bytes})
&gt;&gt;&gt; type(image)
## numpy.ndarray
&gt;&gt;&gt; image.shape
## (600, 400, 3)
&gt;&gt;&gt; image.dtype
## dtype('uint8')</code></pre>

<p>The <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_image"><code>tf.image.decode_image</code></a> here intelligently uses <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_png"><code>tf.image.decode_png</code></a>, <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_jpeg"><code>tf.image.decode_jpeg</code></a>, or <a href="https://www.tensorflow.org/api_docs/python/tf/image/decode_gif"><code>tf.image.decode_gif</code></a>, similar to how above <code>imread</code> can handle a variety of formats. You can also use the appropriate function directly.</p>
<p>Above, <code>imsave</code> supported writing various formats, choosing the one appropriate for the filename specified. In TensorFlow, you have to use <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_jpeg"><code>tf.image.encode_jpeg</code></a> or <a href="https://www.tensorflow.org/api_docs/python/tf/image/encode_png"><code>tf.image.encode_png</code></a> directly, and both provide extra arguments controlling compression and more.</p>
<pre><code class="language-python">&gt;&gt;&gt; tensor = tf.placeholder(tf.uint8)
&gt;&gt;&gt; encode_jpeg = tf.image.encode_jpeg(tensor)
&gt;&gt;&gt; jpeg_bytes = session.run(encode_jpeg, feed_dict={tensor: image})
&gt;&gt;&gt; with open('freedom.jpg', 'wb') as f:
...     f.write(jpeg_bytes)</code></pre>

<p>With the default settings, TensorFlow encodes a larger JPEG than <code>imsave</code>, coming in at 46,567 bytes. It looks pretty good.</p>
<hr>
<h2>PNG and JPEG in TFRecords</h2>
<p>We can put plain bytes into <code>Example</code> TFRecords <a href="/20170323-tfrecords_for_humans/">without too much trouble</a>. So PNG or JPEG images are easily handled.</p>
<pre><code class="language-python">my_example = tf.train.Example(features=tf.train.Features(feature={
    'png_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=[png_bytes]))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_png_bytes = same_example.features.feature['png_bytes'].bytes_list.value[0]</code></pre>

<p>When the <code>same_png_bytes</code> is decoded by <code>tf.image.decode_image</code>, as above, or <code>tf.image.decode_png</code> directly, you'll get back a tensor with the correct dimensions, because PNG (and JPEG) include that information in their encodings.</p>
<hr>
<h2>Image Arrays in TFRecords</h2>
<p>If you want to save dense matrix representations in TFRecords, there's a little bit of bookkeeping to do, but it isn't too bad.</p>
<pre><code>image_bytes = image.tostring()
image_shape = image.shape

my_example = tf.train.Example(features=tf.train.Features(feature={
    'image_bytes': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image_bytes])),
    'image_shape': tf.train.Feature(int64_list=tf.train.Int64List(value=image_shape))
}))

my_example_str = my_example.SerializeToString()
with tf.python_io.TFRecordWriter('my_example.tfrecords') as writer:
    writer.write(my_example_str)

reader = tf.python_io.tf_record_iterator('my_example.tfrecords')
those_examples = [tf.train.Example().FromString(example_str)
                  for example_str in reader]
same_example = those_examples[0]

same_image_bytes = same_example.features.feature['image_bytes'].bytes_list.value[0]
same_image_shape = list(
    same_example.features.feature['image_shape'].int64_list.value)</code></pre>

<p>With the information recovered from TFRecord form, it's easy to use NumPy to put the image back together.</p>
<pre><code class="language-python">same_image = np.fromstring(same_image_bytes, dtype=np.uint8)
same_image.shape = same_image_shape</code></pre>

<p>You can do the same using TensorFlow.</p>
<pre><code class="language-python">shape = tf.placeholder(tf.int32)
new_image = tf.reshape(tf.decode_raw(bytes, tf.uint8), shape)
same_image = session.run(encode_jpeg, feed_dict={bytes: same_image_bytes,
                                                 shape: same_image_shape})</code></pre>

<p>In this example, however, the parsing of the <code>Example</code> was already done outside the TensorFlow graph, so there isn't a strong reason to stay inside the graph here.</p>
<hr>
<h3>Comparison to Caffe and LMDB</h3>
<p><a href="http://caffe.berkeleyvision.org/">Caffe</a> is an older deep learning framework that can work with data stored in <a href="https://symas.com/offerings/lightning-memory-mapped-database/">LMDB</a> on-disk databases, similar to how TensorFlow can work with data stored in TFRecords files.</p>
<p>Like TensorFlow, Caffe defines a protocol buffer message for training examples. In Caffe, it's called <code>Datum</code>. These are saved just like in TensorFlow, by serializing them and putting them in a file on disk, but instead of a TFRecords file, which just puts records in a row and reads them out in that order, here Caffe will work with LMDB, which has the semantics of a key-value store.</p>
<p>TensorFlow's <code>Example</code> format is super flexible, but the trade-off is that it doesn't automatically do things for you. Caffe's <code>Datum</code>, on the other hand, expects you to put in a dense image array, and an integer class label. So here there isn't any fiddling with array size, for example, but the trade-off is that it won't work easily for arbitrary data structures that we might eventually want to store.</p>
<p>In the TFRecords examples above, we stored only image data, and said nothing about a class label or anything else. This is because TFRecords lets you decide what you want to save, rather than defining a format in advance. You could save an integer label, or a float regression label, or a string of text, or an image mask, and on and on. Here, we're just using the built-in Caffe integer label.</p>
<pre><code class="language-python">import caffe
import lmdb

# `image` was read above
label = 9  # for a classification problem

datum = caffe.io.array_to_datum(arr=image, label=label)
datum_str = datum.SerializeToString()

env = lmdb.open('lmdb_data')
txn = env.begin(write=True)
txn.put(key='my datum', value=datum_str)

cur = txn.cursor()
same_datum_str = cur.get('my datum')

same_datum = caffe.proto.caffe_pb2.Datum().FromString(same_datum_str)
same_image = caffe.io.datum_to_array(same_datum)
same_label = datum.label</code></pre>

<p>To actually work with the Caffe training process, there are some other conditions for the data to satisfy. Caffe expects [channels, height, width] instead of [height, width, channels], for example.</p>
<hr>
<h2>What should you use?</h2>
<p>Prefer doing fewer separate manipulation steps to whatever your original data is, if you can help it: try not to have lots of different versions of your data in different places on disk. This will make your life easier.</p>
<p>If you are choosing a format, JPEG is good for photos.</p>
<p>Think about whether you need to put everything into the TensorFlow computation graph. Think about whether you need to use TFRecords. Try to spend your time on things that help solve your problems.</p>
<p>For two more complete <em>in situ</em> examples of converting images to TFRecords, check out <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/how_tos/reading_data/convert_to_records.py">code for MNIST images</a> and <a href="https://github.com/tensorflow/models/blob/master/inception/inception/data/build_imagenet_data.py">code for ImageNet images</a>. The ImageNet code can be run on the command-line.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170403-images_and_tfrecords/</link>
<guid>http://planspace.org/20170403-images_and_tfrecords/</guid>
<pubDate>Mon, 03 Apr 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>On Tyranny: Twenty Lessons from the Twentieth Century</title>
<description><![CDATA[

<p>Shortly after the U.S. presidential election of 2016, <a href="http://history.yale.edu/people/timothy-snyder">Timothy Snyder</a> wrote a popular post that was widely <a href="https://qz.com/846940/a-yale-history-professors-20-point-guide-to-defending-democracy-under-a-trump-presidency/">re-published</a> and then expanded to become the book <a href="https://www.amazon.com/Tyranny-Twenty-Lessons-Twentieth-Century/dp/0804190119">On Tyranny: Twenty Lessons from the Twentieth Century</a>.</p>
<p>I've put together text versions of the post, in one file and in 22 files, because I think the ideas are important and so that I can use these versions in text data processing examples.</p>
<ul>
<li>Whole post in a single file:<ul>
<li><a href="on_tyranny.txt"><code>on_tyranny.txt</code></a></li>
</ul>
</li>
<li>Compressed directory of 22 files:<ul>
<li><a href="on_tyranny.tar.gz"><code>on_tyranny.tar.gz</code></a></li>
<li><a href="on_tyranny.zip"><code>on_tyranny.zip</code></a></li>
</ul>
</li>
<li>Each section individually:<ul>
<li><a href="on_tyranny/00_introduction.txt"><code>00_introduction.txt</code></a></li>
<li><a href="on_tyranny/01_do_not_obey_in_advance.txt"><code>01_do_not_obey_in_advance.txt</code></a></li>
<li><a href="on_tyranny/02_defend_an_institution.txt"><code>02_defend_an_institution.txt</code></a></li>
<li><a href="on_tyranny/03_recall_professional_ethics.txt"><code>03_recall_professional_ethics.txt</code></a></li>
<li><a href="on_tyranny/04_distinguish_certain_words.txt"><code>04_distinguish_certain_words.txt</code></a></li>
<li><a href="on_tyranny/05_be_calm_when_the_unthinkable_arrives.txt"><code>05_be_calm_when_the_unthinkable_arrives.txt</code></a></li>
<li><a href="on_tyranny/06_be_kind_to_our_language.txt"><code>06_be_kind_to_our_language.txt</code></a></li>
<li><a href="on_tyranny/07_stand_out.txt"><code>07_stand_out.txt</code></a></li>
<li><a href="on_tyranny/08_believe_in_truth.txt"><code>08_believe_in_truth.txt</code></a></li>
<li><a href="on_tyranny/09_investigate.txt"><code>09_investigate.txt</code></a></li>
<li><a href="on_tyranny/10_practice_corporeal_politics.txt"><code>10_practice_corporeal_politics.txt</code></a></li>
<li><a href="on_tyranny/11_make_eye_contact_and_small_talk.txt"><code>11_make_eye_contact_and_small_talk.txt</code></a></li>
<li><a href="on_tyranny/12_take_responsibility_for_the_face_of_the_world.txt"><code>12_take_responsibility_for_the_face_of_the_world.txt</code></a></li>
<li><a href="on_tyranny/13_hinder_the_one-party_state.txt"><code>13_hinder_the_one-party_state.txt</code></a></li>
<li><a href="on_tyranny/14_give_regularly_to_good_causes.txt"><code>14_give_regularly_to_good_causes.txt</code></a></li>
<li><a href="on_tyranny/15_establish_a_private_life.txt"><code>15_establish_a_private_life.txt</code></a></li>
<li><a href="on_tyranny/16_learn_from_others_in_other_countries.txt"><code>16_learn_from_others_in_other_countries.txt</code></a></li>
<li><a href="on_tyranny/17_watch_out_for_the_paramilitaries.txt"><code>17_watch_out_for_the_paramilitaries.txt</code></a></li>
<li><a href="on_tyranny/18_be_reflective_if_you_must_be_armed.txt"><code>18_be_reflective_if_you_must_be_armed.txt</code></a></li>
<li><a href="on_tyranny/19_be_as_courageous_as_you_can.txt"><code>19_be_as_courageous_as_you_can.txt</code></a></li>
<li><a href="on_tyranny/20_be_a_patriot.txt"><code>20_be_a_patriot.txt</code></a></li>
<li><a href="on_tyranny/21_credits.txt"><code>21_credits.txt</code></a></li>
</ul>
</li>
</ul>
<p>The contents of <a href="on_tyranny.txt"><code>on_tyranny.txt</code></a> inline:</p>
<pre>
Americans are no wiser than the Europeans who saw democracy yield to
fascism, Nazism, or communism. Our one advantage is that we might
learn from their experience. Now is a good time to do so. Here are
twenty lessons from the twentieth century, adapted to the
circumstances of today.

Lesson 1. Do not obey in advance.

Much of the power of authoritarianism is freely given. In times like
these, individuals think ahead about what a more repressive government
will want, and then start to do it without being asked. You&#8217;ve already
done this, haven&#8217;t you? Stop. Anticipatory obedience teaches
authorities what is possible and accelerates unfreedom.


Lesson 2. Defend an institution.

Defend an institution. Follow the courts or the media, or a court or a
newspaper. Do not speak of &#8220;our institutions&#8221; unless you are making
them yours by acting on their behalf. Institutions don&#8217;t protect
themselves. They go down like dominoes unless each is defended from
the beginning.


Lesson 3. Recall professional ethics.

When the leaders of state set a negative example, professional
commitments to just practice become much more important. It is hard to
break a rule-of-law state without lawyers, and it is hard to have show
trials without judges.


Lesson 4. When listening to politicians, distinguish certain words.

Look out for the expansive use of &#8220;terrorism&#8221; and &#8220;extremism.&#8221; Be
alive to the fatal notions of &#8220;exception&#8221; and &#8220;emergency.&#8221; Be angry
about the treacherous use of patriotic vocabulary.


Lesson 5: Be calm when the unthinkable arrives.

When the terrorist attack comes, remember that all authoritarians at
all times either await or plan such events in order to consolidate
power. Think of the Reichstag fire. The sudden disaster that requires
the end of the balance of power, the end of opposition parties, and so
on, is the oldest trick in the Hitlerian book. Don&#8217;t fall for it.


Lesson 6: Be kind to our language.

Avoid pronouncing the phrases everyone else does. Think up your own
way of speaking, even if only to convey that thing you think everyone
is saying. (Don&#8217;t use the internet before bed. Charge your gadgets
away from your bedroom, and read.) What to read? Perhaps The Power of
the Powerless by V&#225;clav Havel, 1984 by George Orwell, The Captive Mind
by Czes&#322;aw Milosz, The Rebel by Albert Camus, The Origins of
Totalitarianism by Hannah Arendt, or Nothing is True and Everything is
Possible by Peter Pomerantsev.


Lesson 7: Stand out.

Someone has to. It is easy, in words and deeds, to follow along. It
can feel strange to do or say something different. But without that
unease, there is no freedom. And the moment you set an example, the
spell of the status quo is broken, and others will follow.


Lesson 8: Believe in truth.

To abandon facts is to abandon freedom. If nothing is true, then no
one can criticize power, because there is no basis upon which to do
so. If nothing is true, then all is spectacle. The biggest wallet pays
for the most blinding lights.


Lesson 9: Investigate.

Figure things out for yourself. Spend more time with long articles.
Subsidize investigative journalism by subscribing to print media.
Realize that some of what is on your screen is there to harm you.
Learn about sites that investigate foreign propaganda pushes.


Lesson 10: Practice corporeal politics.

Power wants your body softening in your chair and your emotions
dissipating on the screen. Get outside. Put your body in unfamiliar
places with unfamiliar people. Make new friends and march with them.


Lesson 11: Make eye contact and small talk.

This is not just polite. It is a way to stay in touch with your
surroundings, break down unnecessary social barriers, and come to
understand whom you should and should not trust. If we enter a culture
of denunciation, you will want to know the psychological landscape of
your daily life.


Lesson 12: Take responsibility for the face of the world.

Notice the swastikas and the other signs of hate. Do not look away and
do not get used to them. Remove them yourself and set an example for
others to do so.


Lesson 13: Hinder the one-party state.

The parties that took over states were once something else. They
exploited a historical moment to make political life impossible for
their rivals. Vote in local and state elections while you can.


Lesson 14: Give regularly to good causes, if you can.

Pick a charity and set up autopay. Then you will know that you have
made a free choice that is supporting civil society helping others
doing something good.


Lesson 15: Establish a private life.

Nastier rulers will use what they know about you to push you around.
Scrub your computer of malware. Remember that email is skywriting.
Consider using alternative forms of the internet, or simply using it
less. Have personal exchanges in person. For the same reason, resolve
any legal trouble. Authoritarianism works as a blackmail state,
looking for the hook on which to hang you. Try not to have too many
hooks.


Lesson 16: Learn from others in other countries.

Keep up your friendships abroad, or make new friends abroad. The
present difficulties here are an element of a general trend. And no
country is going to find a solution by itself. Make sure you and your
family have passports.


Lesson 17: Watch out for the paramilitaries.

When the men with guns who have always claimed to be against the
system start wearing uniforms and marching around with torches and
pictures of a Leader, the end is nigh. When the pro-Leader
paramilitary and the official police and military intermingle, the
game is over.


Lesson 18: Be reflective if you must be armed.

If you carry a weapon in public service, God bless you and keep you.
But know that evils of the past involved policemen and soldiers
finding themselves, one day, doing irregular things. Be ready to say
no. (If you do not know what this means, contact the United States
Holocaust Memorial Museum and ask about training in professional
ethics.)


Lesson 19: Be as courageous as you can.

If none of us is prepared to die for freedom, then all of us will die
in unfreedom.


Lesson 20: Be a patriot.

The incoming president is not. Set a good example of what America
means for the generations to come. They will need it.


This was written by Timothy Snyder:
http://history.yale.edu/people/timothy-snyder

This text is from the version published here:
https://qz.com/846940/a-yale-history-professors-20-point-guide-to-defending-democracy-under-a-trump-presidency/

The short post version was expanded to a book:
https://www.amazon.com/Tyranny-Twenty-Lessons-Twentieth-Century/dp/0804190119
</pre>    
    ]]></description>
<link>http://planspace.org/20170331-on_tyranny/</link>
<guid>http://planspace.org/20170331-on_tyranny/</guid>
<pubDate>Fri, 31 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TFRecords via Protocol Buffer Definitions</title>
<description><![CDATA[

<p>I've written about <a href="/20170323-tfrecords_for_humans/">how to use TFRecords</a>, and I've written about <a href="/20170329-protocol_buffers_in_python/">how protocol buffer messages are defined</a>. Since the data structures in TFRecords are defined as protocol buffer messages, we can use their definitions to understand them.</p>
<p>Protocol buffer definitions remind me a little bit of <a href="https://en.wikipedia.org/wiki/Syntax_diagram">railroad diagrams</a> like the ones in Douglas Crockford's <a href="http://shop.oreilly.com/product/9780596517748.do">JavaScript: The Good Parts</a>. Here's how Crockford builds up number literals in JavaScript, starting from digits:</p>
<p><img alt="integer railroad diagram" src="img/railroad_integer.png"></p>
<p><img alt="fraction railroad diagram" src="img/railroad_fraction.png"></p>
<p><img alt="exponent railroad diagram" src="img/railroad_exponent.png"></p>
<p><img alt="number railroad diagram" src="img/railroad_number.png"></p>
<p>Small components are fully defined, which can then be built into more complex constructs. (<a href="http://archive.oreilly.com/pub/a/javascript/excerpts/javascript-good-parts/syntax-diagrams.html">More examples</a>.) Protocol buffer definitions are built in much the same way.</p>
<p>With comments, the TensorFlow source files <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/feature.proto">feature.proto</a> and <a href="https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/example/example.proto">example.proto</a> total 400 lines. There are only about 30 lines are not comments. These lines (rearranged slightly) do all the defining of the <code>Example</code> and <code>SequenceExample</code> formats that are used for TFRecords.</p>
<pre><code>message BytesList {
  repeated bytes value = 1;
}
message FloatList {
  repeated float value = 1 [packed = true];
}
message Int64List {
  repeated int64 value = 1 [packed = true];
}
message Feature {
  oneof kind {
    BytesList bytes_list = 1;
    FloatList float_list = 2;
    Int64List int64_list = 3;
  }
}
message Features {
  map&lt;string, Feature&gt; feature = 1;
}
message Example {
  Features features = 1;
}
message FeatureList {
  repeated Feature feature = 1;
}
message FeatureLists {
  map&lt;string, FeatureList&gt; feature_list = 1;
}
message SequenceExample {
  Features context = 1;
  FeatureLists feature_lists = 2;
}</code></pre>

<p>These brief definitions give rise to most of the functionality <a href="/20170329-protocol_buffers_in_python/">described</a> for creating, writing, and reading the TFRecords formats.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170330-tfrecords_via_proto/</link>
<guid>http://planspace.org/20170330-tfrecords_via_proto/</guid>
<pubDate>Thu, 30 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>Protocol Buffers in Python</title>
<description><![CDATA[

<p>Google's data interchange format, <a href="https://en.wikipedia.org/wiki/Protocol_Buffers">Protocol Buffers</a>, is pretty straightforward.</p>
<hr>
<h3>Installing Protobuf</h3>
<p>Python support for protocol buffers can be installed with <code>pip</code>:</p>
<pre><code class="language-bash">pip install protobuf</code></pre>

<p>The package is imported as <code>google.protobuf</code>, but you likely won't need to import it.</p>
<p>To define new protocol buffer formats, you'll also want the <code>protoc</code> tool. One way to get it is to find and install a system-specific package, but you can also get it by installing another Google Python package, <code>grpcio-tools</code>:</p>
<pre><code class="language-bash">pip install grpcio-tools</code></pre>

<p>This won't put a <code>protoc</code> executable in your <code>PATH</code>, but it will let you run <code>protoc</code> via Python, as <code>python -m grpc_tools.protoc</code>. For convenience, you can add an alias in your shell:</p>
<pre><code class="language-bash">alias protoc='python -m grpc_tools.protoc'</code></pre>

<hr>
<h3>Defining Protobuf Messages</h3>
<p>The details of protocol buffer messages types are defined in <code>.proto</code> files like <a href="my_example.proto"><code>my_example.proto</code></a>.</p>
<pre><code>syntax = "proto3";

message Bottle {
  string note = 1;
}</code></pre>

<p>Syntax version 3 has to be specified, as the default is still version 2.</p>
<p>We're defining a fairly dull message. A <code>Bottle</code> can contain one <code>note</code>, which is a string.</p>
<p>The number one there is not setting a default value, but specifying a numbering that's used internally when reading and writing binary representations of our messages.</p>
<hr>
<h3>Generating Code for our Protobuf</h3>
<p>Assuming we're in the same directory as <code>my_example.proto</code>, we can use <code>protoc</code> to generate some Python code corresponding to the message type we defined:</p>
<pre><code class="language-bash">protoc --proto_path=./ --python_out=./ my_example.proto</code></pre>

<p>This will produce a new file <code>my_example_pb2.py</code>. (Syntax version 3 does not affect the filename here.)</p>
<hr>
<h3>Using Protobuf Messages in Python</h3>
<p>With the generated <code>my_example_pb2.py</code> code, we can use our message type in Python, and take advantage of features like serialization and deserialization.</p>
<pre><code class="language-python">import my_example_pb2

my_bottle = my_example_pb2.Bottle(note='Ahoy!')

with open('my_bottle.pb', 'wb') as f:
    f.write(my_bottle.SerializeToString())

with open('my_bottle.pb', 'rb') as f:
    new_bottle = my_example_pb2.Bottle().FromString(f.read())</code></pre>

<p>This looks a lot like <a href="/20170323-tfrecords_for_humans/">examples with TFRecords</a>, because they use the same mechanism.</p>
<hr>
<h3>More Information</h3>
<p>The example here is deliberately minimal. For more detail, the <a href="https://developers.google.com/protocol-buffers/">Google protocol buffers site</a> is quite good, with a very nice <a href="https://developers.google.com/protocol-buffers/docs/pythontutorial">Python tutorial</a>.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170329-protocol_buffers_in_python/</link>
<guid>http://planspace.org/20170329-protocol_buffers_in_python/</guid>
<pubDate>Wed, 29 Mar 2017 12:00:00 -0500</pubDate>
</item>
<item>
<title>TensorFlow as a Distributed Virtual Machine</title>
<description><![CDATA[

<p>TensorFlow has a flexible API, and it has automatic differentiation, and it can run on GPUs. But the thing that's really neat about TensorFlow is that it gives you a fairly general way to easily program across multiple computers.</p>
<p>TensorFlow's distributed runtime, the big bottom box in this figure from the <a href="https://research.google.com/pubs/pub45381.html">2016 paper</a> "TensorFlow: A system for large-scale machine learning", is the part of TensorFlow that runs the computation graph.</p>
<p><img alt="layered TensorFlow architecture" src="img/tensorflow_layers.png"></p>
<p>The computation graph, specified with <a href="https://en.wikipedia.org/wiki/Protocol_Buffers">protocol buffers</a>, is much higher level than <a href="https://en.wikipedia.org/wiki/Java_virtual_machine">Java virtual machine</a> (JVM) bytecode. But I think it's interesting to think of the TensorFlow distributed runtime as a sort of <a href="https://en.wikipedia.org/wiki/Virtual_machine">virtual machine</a>. This is not a whole system virtual machine, but a process virtual machine, like the JVM, to "execute computer programs in a platform-independent environment." The Python API can be like a <a href="https://en.wikipedia.org/wiki/Domain-specific_language">domain-specific language</a> for programming the TensorFlow graph.</p>
<p>Unlike the JVM, and unlike any other system I know, TensorFlow lets you directly put computation on multiple machines, pretty much however you want, and then it quietly handles all the details for you. Wherever it needs to, TensorFlow adds send and receive nodes to allow the graph to be executed as specified. This is shown in this figure from the <a href="https://research.google.com/pubs/pub45166.html">2015 paper</a> "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems" (also <a href="https://arxiv.org/abs/1603.04467">available on arXiv</a> and commonly linked as the <a href="http://download.tensorflow.org/paper/whitepaper2015.pdf">PDF on tensorflow.org</a>).</p>
<p><img alt="distributing a TensorFlow graph" src="img/distributed_graph.png"></p>
<p>The box labels in the figure are for devices, but with TensorFlow they could be on the same machine or on different machines, and it hardly matters.</p>
<p>The flexibility and ease that result are incredible. It isn't terribly hard to think of implementing your own map-reduce system using TensorFlow. It might even be reasonably performant, if you have a distributed file system! The common TensorFlow distributed model training techniques are a kind of map-reduce, after all. But you could do pretty near anything! This is pretty cool.</p>
<hr>
<p>I'm working on <a href="http://conferences.oreilly.com/oscon/oscon-tx/public/schedule/detail/57823">Building TensorFlow systems from components</a>, a workshop at <a href="https://conferences.oreilly.com/oscon/oscon-tx">OSCON 2017</a>.</p>    
    ]]></description>
<link>http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/</link>
<guid>http://planspace.org/20170328-tensorflow_as_a_distributed_virtual_machine/</guid>
<pubDate>Tue, 28 Mar 2017 12:00:00 -0500</pubDate>
</item>
  </channel>
</rss>
